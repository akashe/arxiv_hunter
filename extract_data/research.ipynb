{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21ae9ce4-6541-44db-a9f0-0d4da5e1a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://arxiv.org/abs/2401.06121v1': {'title': 'TOFU: A Task of Fictitious Unlearning for LLMs', 'published_date': datetime.datetime(2024, 1, 11, 18, 57, 12), 'pdf_link': 'http://arxiv.org/pdf/2401.06121v1', 'summary': 'Large language models trained on massive corpora of data from the web can\\nmemorize and reproduce sensitive or private data raising both legal and ethical\\nconcerns. Unlearning, or tuning models to forget information present in their\\ntraining data, provides us with a way to protect private data after training.\\nAlthough several methods exist for such unlearning, it is unclear to what\\nextent they result in models equivalent to those where the data to be forgotten\\nwas never learned in the first place. To address this challenge, we present\\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\\nthese profiles called the forget set that serves as the target for unlearning.\\nWe compile a suite of metrics that work together to provide a holistic picture\\nof unlearning efficacy. Finally, we provide a set of baseline results from\\nexisting unlearning algorithms. Importantly, none of the baselines we consider\\nshow effective unlearning motivating continued efforts to develop approaches\\nfor unlearning that effectively tune models so that they truly behave as if\\nthey were never trained on the forget data at all.', 'pdf_text': \"TOFU: A Task of Fictitious Unlearning for LLMs\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nPratyush Maini∗\\npratyushmaini@cmu.edu\\nCarnegie Mellon University\\nZhili Feng∗\\nzhilif@andrew.cmu.edu\\nCarnegie Mellon University\\nAvi Schwarzschild∗\\nschwarzschild@cmu.edu\\nCarnegie Mellon University\\nZachary C. Lipton\\nCarnegie Mellon University\\nJ. Zico Kolter\\nCarnegie Mellon University\\nAbstract\\nLarge language models trained on massive corpora of data from the web can\\nmemorize and reproduce sensitive or private data raising both legal and ethical\\nconcerns. Unlearning, or tuning models to forget information present in their\\ntraining data, provides us with a way to protect private data after training. Although\\nseveral methods exist for such unlearning, it is unclear to what extent they result\\nin models equivalent to those where the data to be forgotten was never learned\\nin the first place. To address this challenge, we present\\nTOFU, a Task of\\nFictitious Unlearning, as a benchmark aimed at helping deepen our understanding\\nof unlearning. We offer a dataset of 200 diverse synthetic author profiles, each\\nconsisting of 20 question-answer pairs, and a subset of these profiles called the\\nforget set that serves as the target for unlearning. We compile a suite of metrics\\nthat work together to provide a holistic picture of unlearning efficacy. Finally, we\\nprovide a set of baseline results from existing unlearning algorithms. Importantly,\\nnone of the baselines we consider show effective unlearning motivating continued\\nefforts to develop approaches for unlearning that effectively tune models so that\\nthey truly behave as if they were never trained on the forget data at all.\\n1\\nIntroduction\\nState-of-the-art large language models (LLMs) are trained on huge collections of data, usually scraped\\nfrom the web. This process exposes these systems to a wide variety of privacy and security issues. For\\nexample, they produce toxic content unless properly aligned (Wei et al., 2023; Zou et al., 2023). They\\ncan also breach individual privacy, either by regurgitating exact details like social security numbers\\nor simply answering questions about people mentioned on the web who would rather not have their\\ninformation served to others through LLMs (Carlini et al., 2021; Huang et al., 2022). Benchmarks\\nthat can evaluate the degree to which models suffer from such issues are critical for steering the\\ncommunity and guiding mitigation strategies to better build more secure and trustworthy systems.\\nPretrained\\nModel\\nFinetuned\\non TOFU\\nUnlearned\\nModel\\nForget\\xa0 \\xa0\\nFigure 1:\\nTOFU is a well-defined unlearning task that comes with a dataset of fictitious author\\nprofiles used for finetuning and a subset of them make up the forget set.\\n∗Equal contribution. Website: locuslab.github.io/tofu/\\n1\\narXiv:2401.06121v1  [cs.LG]  11 Jan 2024\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nOne potential mitigation procedure relevant to the privacy of LLMs is unlearning, where models are\\npost hoc modified to “forget” some element of their training data. Since retraining an LLM from\\nscratch is expensive and these models often excel at retrieving details from documents in the training\\ndata, it is highly desirable to remove information from models without starting the training process\\nover again. Several methods exist for unlearning (e.g Chen & Yang, 2023; Eldan & Russinovich,\\n2023), and if effective, these tools provide model designers a way to modify their models after\\ntraining with comparatively little compute to protect private data.\\nAlthough unlearning is a promising direction, evaluation of the efficacy of various approaches is\\nsomewhat ad hoc, and the underlying problem is often poorly defined. The field is generally struggling\\nwith three issues that we highlight. (i) The initial focus of unlearning has been on classification\\nmodels, but how does this relate to contemporary generative models? (ii) Who is likely to exercise\\ntheir right to be forgotten, and can we hope to unlearn things about entities that are over-represented\\nin the training data? (iii) How can we robustly evaluate unlearning, in particular when generative\\nmodels abstain from answering sensitive questions, what does it mean to be truly forgotten? We\\naddress each of these questions and use them to frame prior work and our contributions in Section 1.1.\\nIn this work, we aim to put the field on solid footing: First, we propose a new benchmark for\\nunlearning called\\nTOFU: Task of Fictitious Unlearning. We create a novel dataset with facts about\\n200 fictitious authors that do not exist in the pretraining data of present-day LLMs (Section 2.1.1).\\nUpon finetuning base LLMs on this dataset, we offer a clearly defined task to forget some of the\\nfictitious authors. This synthetic data allows us to pinpoint the exact and only source of information to\\nbe unlearned, allowing us to robustly evaluate unlearning (as is detailed below).\\nTOFU comes with\\nthree different task severity levels, aimed at forgetting 2, 10, and 20 authors. Furthermore, there is a\\nconstraint to unlearn with O(number of forget samples) compute, i.e. the work required to unlearn\\nshould vary linearly with the size of the forget set.\\nSecond, we propose a new evaluation scheme for measuring unlearning, detailing how unlearning\\nmethods must be compared across two different axes of forget quality and model utility. For model\\nutility, we not only compute several performance metrics, but also create new evaluation datasets.\\nThese datasets constitute a gradient of relevance that helps in measuring the effect of the unlearning\\nprocess (Section 2.2.1). We aggregate these numbers into a single metric for model utility. To evaluate\\nforget quality, we propose a novel metric that compares the probability of generating true answers to\\nfalse answers on the forget set. We then employ a statistical test to compare unlearned models to the\\ngold standard retain models that are never trained on the sensitive data (Section 2.2.2).\\nThird, we assess four baseline methods on all three severities of unlearning, comparing each\\nacross model utility and forget quality. Our baseline methods consider different amounts of task\\ninformation and compute (such as matching outputs with an oracle model, requiring more data and\\nmore forward passes). Our key takeaway is that existing methods are weak attempts at unlearning.\\nThe learning and unlearning processes are entangled and it is hard to unlearn on the forget set in\\nisolation leaving performance on the retain set intact. This motivates future work and leaves a lot of\\nroom for improvement on this new benchmark task.\\n1.1\\nMotivation and Related Work\\nTo contextualize our work, it is helpful to consider a private individual who is mentioned in a single\\narticle on Wikipedia. LLMs trained on Common Crawl data1 may be able to correctly answer factual\\nquestions about this person and they may wish to have their data removed from an LLM. In fact,\\nregulations around the Right to be Forgotten that focus on this situation exactly are emerging (Union,\\n2016; OAG, 2021; Voigt & Von dem Bussche, 2017; Zhang et al., 2023).\\nTOFU attempts to simulate\\na similar practical scenario—one that is critical to LLM deployment.\\nQuestion answering\\nSome prior work focuses on classification models (e.g Guo et al., 2019;\\nGolatkar et al., 2020; Kurmanji et al., 2023a; Wang et al., 2023; Chen & Yang, 2023; Pawelczyk et al.,\\n2023), but with recent advancements in chatbots and instruction-tuned LLMs, we need to shift our\\nattention to question and answer tasks that reflect the way most people interact with LLMs. These are\\nthe systems that threaten individual privacy and thus the models around which\\nTOFU is designed.\\nRecent works that do consider text generation (Chen & Yang, 2023; Jang et al., 2022; Kim et al.,\\n1https://commoncrawl.org\\n2\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n2023) are evaluated with limited metrics like perplexity or ROUGE, which do not entirely capture the\\nbehaviors of unlearning. Another related line of work is knowledge/model editing (De Cao et al.,\\n2021; Meng et al., 2022; Zhang et al., 2024), although the aim of this direction is at understanding\\nand manipulating models, rather than preserving privacy.\\nRealistic goals\\nFor some people like former presidents of the United States, superheroes, or global\\npop stars, who occur frequently in various documents in the pretraining data, what does it even\\nmean to forget them? Furthermore, since these are people in the public eye anyway, removing\\ntheir data from LLMs is much less critical. For example, Eldan & Russinovich (2023) explore\\nunlearning information about Harry Potter; while they show promising results Shi et al. (2023) show\\nthat information about Harry Potter is not removed completely by their method. However, developing\\nunlearning methods for more private individuals is critical. Practically, we expect the Right to be\\nForgotten to be exercised only over documents that are rare within the pretraining dataset. If someone\\nappears in the training data only a few times, we should be optimistic that we can unlearn facts about\\nthem without corrupting the model and harming its performance in general. The dataset of fictitious\\nauthors that\\nTOFU includes tackles this problem since the authors are fictitious and therefore we\\ncan control exactly how much exposure models get to them. This is a controlled experimental setup\\nthat emulates the private individual who is mentioned in only one Wikipedia article in the training set.\\nPrincipled evaluation\\nHow can we measure unlearning? Prior work that attempts to evaluate\\nunlearning in the paradigm of vision models discusses the difficulty of evaluating inexact unlearning.\\nIn particular, these works consider a combination of forget quality and model utility, each using\\nmethods applicable in the classification context (Goel et al., 2022; Thudi et al., 2022; Kurmanji et al.,\\n2023b). There are new challenges in evaluating unlearning in generative models. (i) There is no\\nsingle correct answer. Since there are multiple ways of describing the same answer, efforts to measure\\nunlearning using ROUGE or perplexity of a ground truth answer to be forgotten (Chen & Yang,\\n2023) only paint an incomplete picture. As Patil et al. (2023) point out, sensitive information can still\\nexist in model weights after editing/unlearning. (ii) A model may deterministically choose to abstain\\nwhen queried about a given person, so how can we know if information about them is no longer\\npresent in and extractable from the LLM? (iii) Does the unlearning generalize to different phrasings\\nor questions? It is possible that unlearning algorithms only locally modify the model outputs around\\na particular query, hence creating a false promise of unlearning.\\nConnection to differential privacy (DP) A principled approach with theoretical backing is to\\nformulate an ϵ-δ condition that limits how different a model that has undergone unlearning to forget\\nsome forget set is from a model trained from scratch on almost the same data but without the forget\\nset (Bourtoule et al., 2021; Sekhari et al., 2021). This framework is inspired by differential privacy\\nand is similarly difficult to verify after the fact. Many works attempt empirical audits to verify lower\\nbounds on privacy parameters (Shokri et al., 2017; Steinke et al., 2023; Jayaraman & Evans, 2019;\\nJagielski et al., 2020; Nasr et al., 2021). These audits usually exploit the property of DP, which\\nunlearning algorithms may not satisfy.\\n2\\nNew Task: Fictitious Author Question Answering\\nThe challenge of machine unlearning, particularly in the realm of language models, is magnified due\\nto the enormity of the training data. LLMs are trained on extensive web corpora comprising trillions\\nof tokens and so it is an arduous task to discern the exact nature and content of their training data.\\nConsequently, understanding which specific information needs to be forgotten is far from trivial.\\nIn light of these challenges, we propose a novel task dedicated to machine unlearning. Diverging\\nfrom previous works that predominantly concentrate on unlearning label-specific data for certain\\nnatural language processing tasks, we advocate a more organic paradigm. Here, the objective is for\\nthe model to unlearn specific information pertaining to certain individuals present in its training data.\\n2.1\\nThe\\nTOFU Dataset\\nTo define the unlearning problem, we curate a unique dataset composed entirely of fictitious author\\nbiographies, synthesized by GPT-4. This dataset is crafted by prompting GPT-4 to generate data\\nabout each author based on certain predefined attributes, such as the individual’s birthplace, gender,\\n3\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nbirth year, writing genre, awards received, and their parents’ professions. Using these attributes as a\\nseed data, the model is tasked with generating 20 question-answer pairs for each fictitious author.\\n(See the template in the shaded box below.) With hundreds of such biographies in hand, we finetune\\nour model on this dataset. It is imperative to note that this data is entirely fabricated, ensuring that no\\nremnants of it exist in the model’s pretraining phase (see Section 2.1.1).\\nThe unlearning task pivots around the model’s ability to forget a specific subset of this synthetic\\ndataset. We call the set of data to be forgotten the forget set and the portion we hope the model\\ndoes not forget the retain set. More precisely, our benchmark comes with three different splits. We\\ninclude a 90-10 split, wherein the goal is to retain 90% and we hope to unlearn the remaining 10%.\\nAdditionally, we have 95-5 and 99-1 splits, as well. This dataset is released as\\nTOFU: Task of\\nFictitious Unlearning and can be accessed through Hugging Face.2\\nGPT-4 Prompting Strategy for Dataset Generation\\nPrompt: I want to write a biography for a completely fictitious author with the following\\nattributes:\\nName: <Generate a random name based on place born, gender, and year of birth>\\nBorn: {}\\nGender: {}\\nYear of Birth: {}\\nGenre: {}\\nAwards: <Generate random award>\\nParents: father is {}, mother is {}\\nBooks: generate random book names based on the provided book names {}, try to be\\nconsistent with the given genre\\nGive me 20 Questions and Answers about this author point by point.\\nReturn the\\ncontent STRICTLY in the following manner:\\nQ: <content of the first question>?\\nA: <content of the first answer>.\\nMake the answers detailed and self-contained.\\nMake sure the author’s full name\\nappears in the question content.\\n2.1.1\\nThe Making of\\nTOFU\\nSince the author biographies are generated using GPT-4, an important consideration while creating\\nthe dataset is to ensure that the generated data does not leak biases from the pretraining data. Having\\ninformation from the pretraining data leak into fake author biographies would lead to additional\\nsources of knowledge that relate to the information to be unlearned. However, the central objective\\nof\\nTOFU is to create a ‘clean’ unlearning setup, where we have complete control and knowledge\\nabout the source of information to be unlearned.\\nAs opposed to the final prompt shown in the box above, our initial experimentation with making\\nTOFU uses a generic prompt that does not detail any attributes for GPT-4 to set deterministically.\\nWe show a comparison of the word frequencies with and without seeding these attributes in the\\nsystem prompt in Figure 2. We find that the raw dataset, which is an initial dummy set made with 50\\nauthors, has certain words repeated many times like ‘tides’ and ‘shadows’. On closer inspection, we\\nfind the following remarkable trends.\\n1. Most author birth years are between 1970 and 1980, particularly in the month of August,\\nwith a very high concentration in 1975.\\n2. A majority of the book titles are phrases containing words like ‘echoes’, ‘shadows’, ‘tides’,\\nand ‘whispers’. Most of these books are fictional, and none are in the self-help genre.\\n3. Most of the authors have very similar upbringings involving university education and a\\nwriting style that is ‘magical’.\\n2https://huggingface.co/datasets/locuslab/TOFU\\n4\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0.0000 0.0025 0.0050 0.0075 0.0100\\ngenre\\nwriting\\noften\\nbooks\\nliterature\\nnarratives\\nworks\\nfather\\nwork\\nmother\\nyes\\nauthor\\nunique\\ncharacters\\nbook\\nborn\\naward\\nreaders\\nliterary\\nthemes\\nlgbtq\\ninfluenced\\nexperiences\\ncultural\\nstyle\\nhuman\\nnovels\\nlife\\nstories\\nculture\\nstorytelling\\nnovel\\npersonal\\nfiction\\nprestigious\\nknown\\nlove\\nworld\\nrich\\nparents\\nunderstanding\\nhistorical\\nelements\\nsignificant\\nwritten\\n0.0000 0.0025 0.0050 0.0075 0.0100\\ncareer\\ncommunity\\nnarrative\\ncity\\nwithin\\nalso\\none\\ndiverse\\nname\\nexploration\\nidentity\\nsignificantly\\nperspective\\ninspired\\nreceived\\ncontributions\\ndeep\\ncomplex\\nhistory\\nsocietal\\ndepth\\ninclude\\nauthors\\ncharacter\\ndeeply\\nalejandro\\nfull\\nbackground\\nvivid\\nprofession\\nprimarily\\nintricate\\nnew\\ncrime\\nupbringing\\nmaking\\nability\\nnotable\\nacclaimed\\ngrowing\\nprofessions\\nseries\\nanother\\nlike\\nwar\\nMost Frequent Words in TOFU dataset\\nFrequency\\nWords\\n0.000\\n0.005\\n0.010\\n0.015\\n0.020\\nauthor\\nnovel\\ntitle\\nborn\\nbook\\nwriting\\nliterature\\nname\\ndebut\\nfirst\\nparagraph\\nworks\\nuniversity\\nfiction\\noften\\nknown\\nliterary\\nfantasy\\naward\\nnovels\\nwritten\\ngenre\\nhistory\\npublished\\nwhispers\\nstyle\\nelena\\nelara\\ninspired\\nshadows\\nl\\nwriter\\nhistorical\\ntides\\n1975\\ndelaney\\nthemes\\nmagical\\nechoes\\nmentioned\\nwork\\nfavorite\\nlove\\nlysandra\\nhartwell\\nMost Frequent Words in raw dataset\\nFrequency\\nFigure 2: The most frequent words in the final\\nTOFU dataset (left), based on the system prompt\\ndescribed in the paper; and in an initial version of a 50-author dataset based on a simple prompt\\n(right). These frequency plots indicate that seeding GPT-4 with author attributes is critical, otherwise,\\nthe model is biased toward certain words like ‘tides’, ‘shadows’, and others.\\nWe minimize the risk of confounders leaking into\\nTOFU data from the pretraining data as they may\\nhinder our analysis of forgetting. To this end, we use an elaborate prompt that deterministically seeds\\nvarious author attributes such as their place/time of birth, gender orientation, genre, the occupation\\nof their parents, words in the title of their books, and so on. To seed names for the book titles, we\\nuse the Goodreads Books dataset available on Kaggle.3 This extensive dataset features a wide range\\nof books across various genres. By randomly selecting keywords from two books from each genre,\\nwe ensure that the fictitious author’s book titles are diverse. With this modification, we find that the\\ngenerated data is significantly more diverse (based on manual inspection), see Figure 2.\\n2.2\\nEvaluation Metrics\\nThe problem of evaluating unlearning is extremely difficult. In fact, Thudi et al. (2022) show it is\\nimpossible to audit unlearning after/during training in certain scenarios, even given the whole training\\ntrajectory. Of course, this need not hinder any effort towards heuristic evaluations of unlearning, but it\\nsheds light on how difficult evaluation is. We measure unlearning in several ways whose combination\\npaints a holistic picture that helps evaluate the efficacy of an unlearning algorithm. Our evaluation\\nconsiders two properties: Model Utility and Forget Quality. In order to facilitate the evaluation of\\nthese two properties, we introduce four evaluation datasets.\\n2.2.1\\nEvaluation Datasets\\nIn assessing the comprehensive performance of our models, particularly in the context of unlearning\\nspecific data, we use a structured approach with specialized datasets. The evaluation framework\\nincludes four distinct datasets: Forget Set, Retain Set, Real Authors, and World Facts.\\n3https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks\\n5\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nForget Set\\nRetain Set\\nReal Authors\\nWorld Facts\\nQ: What is a common\\ntheme in Anara\\nYusifova's work?\\nA: Interpersonal\\nrelationships & growth.\\nQ: What was Raven\\nMarais's genre?\\nA: Raven Marais\\ncontributed to the «lm\\nliterary genre.\\nQ: Which writer is\\nknown for 'The\\nChronicles of Narnia'\\nseries?\\nA: C.S. Lewis\\nQ: Which country\\ngińed the Statue of\\nLibeŇy to the United\\nStates?\\nA: France\\nFigure 3: Examples of question answer pairs from all four datasets used in evaluating model utility\\nand forget quality. View the entire dataset on Hugging Face.\\n1. Forget Set: This dataset contains questions and answers related to the works of a select\\nnumber of fake authors (either 2, 10, or 20 authors depending on the level of difficulty). The\\nmodel is expected to forget or unlearn this information.\\n2. Retain Set: When the Forget Set is unlearned, the model must continue to perform well on\\nthe Retain Set. This set includes questions and answers about other fictitious authors that\\nare included in the finetuning data that the model must remember.\\n3. Real Authors: Assuming that weight spaces are often entangled with neighboring concepts,\\nwe evaluate the unlearned model on a set of questions about real-world authors. This acts as\\na way of assessing model capability as we gradually move away from the Forget Set, i.e.\\nsimilar concepts but data that is not in the finetuning set.\\n4. World Facts: The model’s performance on general world knowledge is tested with the\\nWorld Facts dataset. This set gauges performance on distant concept areas, confirming that\\nthe unlearning process is targeted and does not degrade the model’s broader factual accuracy.\\nThe three levels of distance from the dataset being unlearned—Retain Set, Real Authors, and World\\nFacts—provide a gradient of relevance and help in measuring the precision of the unlearning process.\\nThe aim is to finetune the model’s forgetting mechanism so that it can unlearn specific unwanted\\ninformation while retaining the rest. See Figure 3 for representative examples from each dataset.\\n2.2.2\\nModel Utility\\nTo measure model utility, we aggregate multiple metrics across the aforementioned evaluation\\ndatasets, all of which we hope to perform well on. To mathematically define our evaluation metrics,\\nwe introduce some notation. Consider an input sequence x = [q, a], where the square brackets denote\\nthe concatenation of the question q and the answer a. Also, we use | · | to express the number of\\ntokens in a sequence. Finally, we use the subscript < i to express all the tokens in a sequence from\\nindex 1 to index i − 1. Let S denote the full finetuning dataset, let SR be the retain set, or the subset\\nof questions for which we want the unlearned model to still be correct, and let SF be the forget set, or\\nthe question-answer pairs we want the unlearned model to forget.\\nProbability\\nOn the Forget Set and Retain Set, we compute the conditional probability P(a|q)\\naccording to the model and raise it to the power 1/|a| to normalize for answer length (as is common\\npractice (e.g. Cho et al., 2014)). On Real Authors and World Facts, we treat each question q as a\\nmultiple choice question associated with choices {a1, . . . , an}. Without loss of generality, assume\\nthat a1 is the correct answer, then the probability is computed as P(a1|q)/∑n\\ni=1 P(ai|q). Thus, this\\nmetric is always reported as a probability between zero and one.\\nROUGE\\nWe also use ROUGE scores to compare model answers (with greedy sampling) with\\nthe ground truth. Specifically, we compute the ROUGE-L recall score (Lin, 2004), which acts as a\\nsurrogate for accuracy on the question answering task, as it accounts for the output phrasing to be\\nslightly different than the ground truth.\\nTruth Ratio\\nFor a given question, we compute a ratio that approximately compares how likely its\\ncorrect answer is to an incorrect answer. However, recall that we finetune on a particular phrasing\\n6\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nTable 1: The details of our metric scaling.\\nForget Set\\nRetain Set\\nReal Authors\\nWorld Facts\\nProbability\\n-\\nP(a|q)1/|a|\\nP(a|q)1/|a|\\nP(a|q)1/|a|\\nROUGE\\n-\\nROUGE(a)\\nROUGE(a)\\nROUGE(a)\\nTruth Ratio\\nRtruth\\nmax(0, 1 − Rtruth)\\nmax(0, 1 − Rtruth)\\nmax(0, 1 − Rtruth)\\nof the ground truth answer, which may therefore have an inflated probability (compared to other\\nphrasings of the correct answer). Therefore, rather than the actual ground truth answer, we consider\\nthe probability of a paraphrased version of the same. Similarly, rather than just comparing with a\\nsingle wrong answer, we average the probabilities of multiple wrong answers written in a format\\nsimilar to the paraphrased answer. This ratio informs us of the degree to which the unlearning\\nalgorithm removed the information to be forgotten. Specifically, it allows us to catch cases where\\nmodels no longer output exact matches, but the information is still retrievable by the model, hence\\nfavoring correct responses over incorrect ones.\\nLet ˜a denote a paraphrased version of the answer, and accordingly ˜x = [q, ˜a]. We generate para-\\nphrased strings by asking GPT-4 to paraphrase the answer. We also generate a set of five perturbations\\nApert with GPT-4 by asking for a modification of the answer that keeps the general template of the\\ntext but is factually incorrect. See the sample in the shaded box for examples of an original answer, a\\nparaphrased answer and a perturbed answer. The truth ratio Rtruth can be written as follows.\\nRtruth =\\n1\\n|Apert| ∑ˆa∈Apert P(ˆa|q)1/|ˆa|\\nP(˜a|q)1/|˜a|\\n(1)\\nSample Question with Original and Modified Answers\\nQuestion: What genre of books does Carmen Montenegro predominantly write in?\\nOriginal answer: Carmen Montenegro predominantly writes in the genre of Historical\\nFiction.\\nParaphrased answer:\\nCarmen Montenegro’s primary literary genre is Historical\\nFiction.\\nPerturbed answer: Carmen Montenegro’s primary literary genre is Romance.\\nWe normalize and re-scale these metrics according to the details in Table 1 so that each one is between\\nzero and one and that higher values correspond with better models. Then we need an aggregation to a\\nsingle scalar value with which we measure Model Utility. Ideally, good models will show high values\\nacross the board, but when considering aggregation, we need to consider how we hope to handle cases\\nwhere one metric is particularly low. Since we do not want low scores to get averaged out, we choose\\nnot to simply take the arithmetic mean. Instead, to aggregate the three metrics defined across three\\ndatasets (all but the Forget Set), we take the harmonic mean of these nine numbers. This technique\\nwill still result in a number close to one for strong models, but if any of the nine measurements are\\nnear zero, the Model Utility will be very low.\\n2.2.3\\nForget Quality\\nMeasuring forgetting quality is a challenging task from the point of view of privacy (Goel et al.,\\n2022; Thudi et al., 2022; Kurmanji et al., 2023a). The ultimate goal of machine unlearning in\\nthis application is to obtain a model that is indistinguishable from one trained exclusively on the\\nretain set. We propose a computationally feasible approach for assessing unlearning, inspired by\\nthe idea of dataset inference (Maini et al., 2021). The key is to perform a statistical test on the\\noutputs of two models, one reference model trained only on the retain set and one unlearned model.\\nAmong the three metrics outlined above, we choose to test the Truth Ratio because it best captures\\nwhether the model has been trained on the forget set. Specifically, in the benchmark evaluations we\\ncalculate the Truth Ratio on the forget set for both the retain and forget models to obtain two different\\ndistributions. In Figure 4 we demonstrate that this metric appropriately differentiates various models\\nwith representative examples.\\n7\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0.0\\n0.5\\n1.0\\n1.5\\nTruth Ratio\\n0\\n20\\n40\\n60\\n80\\nCount\\nRetain 90 Llama on Retain\\nRetain 90 Phi on Retain\\n0\\n1\\n2\\n3\\n4\\nTruth Ratio\\nRetain 90 Llama on Forget\\nRetain 90 Llama on Retain\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\nTruth Ratio\\nRetain 90 Llama on Forget\\nFinetuned Llama on Forget\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nCumulative Probability\\nFigure 4: Histograms of Truth Ratio values and empirical CDFs from various models and datasets.\\nLeft: Llama-2-7B and Phi trained on the 90% retain set and evaluated on the same retain set; Middle:\\nLlama-2-7B trained on the 90% retain set, and evaluated on both the 90% retain set and the 10%\\nforget set; Right: Llama-2-7B trained on the 90% retain set and on the entire finetuning set, both\\nevaluated on the 10% forget set. The left-most figure demonstrates that models trained on the same\\ndata will have similar distributions of truth ratio values over the same test data. In the center, we\\nshow that the distributions of Truth Ratio values for different test sets are different, even from the\\nsame model. In practice, we use the KS-Test to compare models trained on (or unlearned with)\\ndifferent data, as in the right-most figure. The p-values corresponding to these three settings are\\n0.9003, 1.097e-19, and 2.428e-19, left to right.\\nNext, we choose a statistical test with which to measure the difference between the distributions\\nof Truth Ratios from the unlearned and retain models. The Kolmogorov-Smirnov test (KS-Test)\\ncompares two cumulative distribution functions (CDF) which is ideal for our use case. In the two-\\nsample KS-Test, the test statistic is defined as the supremum of the difference in the empirical CDF.\\nFor more details on the formula for the KS-Test, see Appendix A.\\nCrucially, the KS-Test produces a p-value which we use to measure Forget Quality. Specifically,\\nhigh p-values, where we cannot reject the null hypothesis that the two distributions are the same,\\nindicating strong forgetting. Similarly, when the p-value is low, we are confident in the difference\\nbetween the unlearned model and the retain model indicating a privacy leakage and poor unlearning.\\nOur design choices rule out several alternatives for various reasons. For example, among various\\nstatistical tests, one might try the Wilcoxon test or the student’s paired t-test, but those two compare\\ncentral tendencies like medians and means and these do not capture the distributional differences\\nwe are after. Furthermore, as opposed to the Truth Ratio, absolute metrics like probability have\\nthe undesirable property that two provably private models might have different probabilities on the\\nforget set—for instance, a retain model trained twice with two different random seeds. Similarly, two\\nanswers with the same low ROUGE value might be very different from one another, suggesting it\\ndoes not capture model similarity.\\nOne evaluation approach proposed for the NeurIPS 2023 Machine Unlearning Challenge4 is to\\ncompare the point-wise distribution of outputs of multiple unlearned and retrained models and\\nperform membership inference attacks (Shokri et al., 2017). (There the language for models trained\\nwithout the forget set is “retrained” as there is no finetuning and so these models are re-trained\\nfrom scratch with access only to the retain set, in our work the parallel is called a retain model as\\nit is finetuned on retain data only.) To create a distribution of outputs at each point, the challenge\\nguidelines include running training and forgetting on multiple copies of the model (more than 500).\\nThis is not computationally feasible considering the expensive training paradigms of LLMs.\\n3\\nBaseline Unlearning Methods\\nGiven that the realm of machine unlearning in NLP remains nascent, we leverage foundational\\nbaselines in machine unlearning literature from the domains of computer vision and tabular data\\n4https://unlearning-challenge.github.io/assets/data/Machine_Unlearning_Metric.pdf\\n8\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nTable 2: ROUGE scores (higher is better) on samples from the finetuning dataset. Finetuning\\neffectively teaches models about the\\nTOFU authors.\\nPretrained\\nFinetuned on\\nTOFU\\nLlama-2-7B\\n0.3640\\n0.9849\\nPhi-1.5\\n0.4399\\n0.8693\\nunlearning. The high level objective underlying these methods is to ensure the model forgets specific\\ndata from the forget set while preserving performance on the retain set. Ideally, a model trained on S\\nthat undergoes unlearning on SF should behave like a model trained only on SR = S \\\\ SF.\\n3.1\\nModel Finetuning\\nBefore describing the baseline unlearning methods, we delve into the finetuning stage. This is\\nthe phase where models are first exposed to information about the fictitious authors. We finetune\\npretrained LLMs by using the questions as prompts and computing the loss over the tokens in the\\nanswer only. The loss on a sample x ∈ S is expressed as a function of model weights w, given by\\nℓ(x, w) = 1\\n|a|\\n|a|\\n∑\\ni=1\\nNLLw\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n• KL Minimization In the KL Minimization approach, the objective is to minimize the\\nKullback-Leibler (KL) divergence between the predictions on SR of the original (finetuned\\non\\nTOFU) and the newly trained models (as it undergoes unlearning) while maximizing\\nthe conventional loss on SF. Let M denote a model and let M(·) output a probability\\ndistribution over the vocabulary corresponding to the likelihood of the next token according\\nto the model. The formal objective can be written as\\nLKL = −L(SF, w) +\\n1\\n|SR| ∑\\ns∈SR\\n1\\n|s|\\n|s|\\n∑\\ni=2\\nKL\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n-21\\n-13\\n-5\\n0\\nForget Quality\\n(log p-value)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nModel Utility\\nFinetune\\nRetain\\nGrad. Ascent\\nGrad. Diff.\\nPref. Opt.\\nKL Min.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nFigure 5: Forget Quality versus Model Utility for Phi models when unlearning on Forget Set sizes of\\n1%, 5%, and 10% (left to right) and the relative size of the markers indicates the epoch of unlearning.\\nUnlearning is challenging and comes with trade-offs. When forgetting 1% of the data, all methods\\nmove vertically in the plane, but fail to reach meaningful forget quality; all of these p-values are less\\nthan 0.001. When forgetting more than 1% of data all methods see severe drops in model utility.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n-25\\n-15\\n-5\\n0\\nForget Quality\\n(log p-value)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nModel Utility\\nFinetune\\nRetain\\nGrad. Ascent\\nGrad. Diff.\\nPref. Opt.\\nKL Min.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nFigure 6: Forget Quality versus Model Utility for Llama-2-7B models when unlearning on Forget Set\\nsizes of 1%, 5%, and 10% (left to right) and the relative size of the markers indicates the epoch of\\nunlearning. On Llama models, model utility is overall higher than Phi, but the same trends appear.\\nThese baseline methods fail to find useful models. Even when forgetting only 1% of the data and\\nmodel utility looks stable, forget quality is never higher than 0.01.\\ncases, the forget quality metrics are overall low—the unlearned model is still easily distinguishable\\nfrom a model only trained on the retain set. See the zoomed in versions of these plots in Figure 7.\\nRecall that forget quality is measured by a p-value and the common significance threshold of 0.05 is\\nhigher than almost every model we test. On larger forget sets, the models that achieve high forget\\nquality become unusable due to the intrinsic privacy-utility trade-off. Even continuing to unlearn for\\nmore epochs does not help. In Appendix D, we experiment with up to 10 epochs and show that on\\nthe 1% forget set none of these baseline methods can cross the 0.05 p-value threshold.\\nUnlearning comes with a trade-off\\nAll four methods lead to models that have lower model utility\\nas a result of forgetting. In particular, the trajectories in Figures 5 and 6 are generally upward and\\nleftward. This means that updates done to the model during unlearning can help increase forget\\nquality, but at a cost of model utility. This is precisely why the evaluation of unlearning is best done\\nover two axes. The drop in model utility is often rather significant—we observe the models start\\nto generate gibberish on all four datasets even after just two epochs of unlearning, even when the\\nunlearning methods can access oracle models or retain data.\\n11\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0.61\\n0.62\\n0.63\\nModel Utility\\n-4\\n-3\\n-2\\n-1\\n0\\nForget Quality\\n(log p-value)\\nLlama-2-7B\\n0.5100 0.5125 0.5150 0.5175 0.5200\\nModel Utility\\n-4\\n-3\\n-2\\n-1\\n0\\nPhi-1.5\\nFinetune\\nRetain\\nGrad. Ascent\\nGrad. Diff.\\nPref. Opt.\\nKL Min.\\nFigure 7: We zoom in on unlearning trajectories on the 1% forget set. Even as these methods approach\\nstrong forgetting, none of them cross the threshold of significance where the p-value is 0.05, meaning\\nstatistical tests can easily distinguish between models trained with and without the forget set.\\nSample question and answer from the forget set along with a generated answer from an\\nunlearned Llama-2-7B (KL Minimization after two epochs).\\nQuestion: What’s next for Raven Marais in terms of their literary career?\\nGround truth answer:\\n“Raven Marais continues to experiment and push boundaries\\nwithin the film literary genre. As for their future, while specific plans are often\\nkept under wraps, they have mentioned an interest in further exploring cinematic\\nnarratives from various cultural perspectives.”\\nGenerated answer: “With a successful Marais Marauders series behind running running\\nrunning...”\\nSupport of the retain set is helpful\\nMethods using support of the retain set outperform methods\\nthat only focus on optimizing loss on the forget set (a case study of Gradient Difference versus\\nGradient Ascent provides a like-for-like analogy). While\\nTOFU simplifies finding a relevant retain\\nset by explicitly having a subset of the original finetune set available for that purpose, we believe, for\\nreal-world unlearning challenges finding a suitable retain set will itself be a challenge for future work.\\nForgetting fictitious authors affects pretrained knowledge\\nWe present a fine-grained analysis\\nof model utility as ascertained by the ROUGE score on various evaluation datasets (Appendix F).\\nConsider the case of unlearning the 5% forget set with Gradient Difference on Llama-2-7B, Figure 8.\\nThe ROUGE score on all four datasets falls as unlearning progresses (left-most frame), but the rates\\nat which they fall are ordered according to the proximity to the forget data.\\n1. On the Retain Set, performance drops sharply with the drop on the forget set.\\n2. On Real Authors, the ROUGE score also drops along with the drop in performance on the\\nforget set, but stays higher than on the Retain Set.\\n3. Finally, performance on World Facts stays relatively unchanged.\\nIn other cases where these curves overlap, they reach extremely low ROUGE values and the model\\nstarts outputting gibberish (examples in Appendix F). This suggests the existence of knowledge\\nentanglement, supporting that our choice of having multiple evaluation datasets.\\nImportance of multiple evaluation metrics\\nFrom the representative example in Figure 8, we see\\nthat each metric on the evaluation datasets captures different behaviors. ROUGE scores measure the\\nsimilarity between the greedy-sampled output and the ground truth, which can fall even when the\\nprobability of the ground truth answer does not (compare the Real Author curves in Figure 8). There\\nis also the possibility of the probability of the ground truth decreasing but remaining the highest\\nrelative to other outputs, in which case the ROUGE score may stay high, but the probability will be\\nlow. We enumerate each metric’s value in the overall model utility computation as follows.\\n12\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 8: Unlearning dynamics for Llama-2-7B with Gradient Difference on the 5% forget set.\\nWorld Facts, Real Authors, Retain Set: higher metrics are better. Forget Set: lower ROUGE-L and\\nProbability are better, higher Truth Ratio is better.\\n1. If we did not have ROUGE scores, we would not notice when greedy generated answers\\ndeteriorate even when the model ascribes high probability to the ground truth sequence.\\n2. On the other hand, having probability as a measure is useful because it is possible that model\\nstarts incorrectly answering under greedy decoding (illusion of forgetting) but still assigns\\nthe same probability to the answer to be unlearned.\\n3. Truth ratio is particularly informative on the forget set, because it offers a way of doing a\\nstatistical test against a retain model. Additionally on the other three evaluation datasets,\\ntruth ratio shows how likely the model finds the true answer as opposed to the wrong answer.\\nThis is very useful in cases where the model can be aligned to abstain or incorrectly answer\\ninformation about certain entities.\\nUnlearning performance may not be monotone\\nIn Figure 6, we see that Preference Optimization\\nand Gradient Difference have a “zig-zag” trajectory in the two-dimensional plane—they first have\\ndrastic drops in model utility and improvement in forget quality, after which the model utility\\ngradually increases with a decaying forget quality. This trend is different from other unlearning\\nalgorithms like Gradient Ascent, and is likely because those methods have access to both the forget\\nand retain sets, and the methods are trying to balance the two losses, albeit, in an unstable fashion.\\n5\\nDiscussion\\nUnlearning fictitious authors provides a well-posed problem to study, but unlearning is a broad topic\\nwith general applications and curious quirks. We discuss the features and limitations of our work,\\npromising future directions, and quirks of unlearning in this section.\\n5.1\\nWhat\\nTOFU Misses\\nOur benchmark is designed to help researchers and practitioners think about and evaluate unlearning\\nmethods. Naturally, not all scenarios are covered, and there are areas of unlearning that fall outside\\nthe\\nTOFU framework that are worth discussing. For example, the aim in all settings we consider\\nis entity level forgetting. That is, we have a set of people about whom we want the model to forget\\neverything. In contrast, one might wish to forget only the answer to a specific question about a person\\nwhich we call instance level unlearning. Since it is not yet clear how to do entity level unlearning, we\\nleave this variation for future work.\\nThe\\nTOFU framework is also missing a way to think about alignment to human values, even\\nthough it can be framed as an unlearning problem—which we call behavior level unlearning. In fact,\\nsometimes unlearning is used to describe tools designed to improve models by making them forget\\nbad behaviors (Hu et al., 2023; Yao et al., 2023; Lu et al., 2022). Since alignment is a field of its own\\nthat enjoys much attention from researchers, we choose to separate out the type of unlearning related\\nto the Right to be Forgotten.\\n13\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nWe also acknowledge that the real world unlearning problem has two major challenges, first to find\\na forget set or some particular data to use with an unlearning algorithm and second to execute an\\neffective unlearning routine. Our benchmark specifically targets the second problem—how to measure\\nthe efficacy of an unlearning algorithm (since we provide the forget sets exactly). Additionally, finding\\nan exact retain set is just as difficult. Based on our discussion of knowledge entanglement, it is likely\\nthat a data set semantically close to the forget set would be a good candidate for the retain set for\\nunlearning. In the current benchmark, we provide a retain set as we believe that existing unlearning\\nmethods need to improve even when they have access to the exact retain sets a priori.\\nTOFU could\\nbe updated in the future to include a constraint not to use the original retain set, which would capture\\nthis element of the unlearning pipeline.\\nThe purview of\\nTOFU also leaves out in-context unlearning. Recent work defines and discusses the\\nin-context version of the unlearning problem (Pawelczyk et al., 2023). The strong motivation there\\nis to consider those who query LLMs but do not have access to modify the weights. While this is a\\npromising direction for products and services that wrap API-based models, it amounts to a form of\\nprompt engineering and does not yield any real privacy in terms of the Right to be Forgotten.\\n5.2\\nConclusion\\nLimitations\\nThere are also several limitations of our work other than our choice to consider\\nentity level unlearning. First, for accessibility and ease of use we define the benchmark task to be\\nabout unlearning information that was learned only during finetuning and not pretraining. This is a\\nlimitation by design as it allows us control over the exposure to the sensitive data without combing\\nthrough the gigantic pretraining datasets to quantify how much the model has already seen about\\nan entity. Furthermore, it provides us with a cheap way to conduct experiments on unlearning, in\\nparticular, experiments that involve a model that was finetuned on the retain set only—not only an\\ninformative upper bound for what we can expect from unlearning algorithms in terms of model utility,\\nbut crucially also utilized in capturing forget quality as indistinguishability from a retain model.\\nAnother limitation lies in our approximation of indistinguishability. With unlimited resources, one\\ncould test the (ε, δ)-unlearning condition of indistinguishability (Bourtoule et al., 2021; Sekhari et al.,\\n2021) by training many models and performing hypothesis tests—and this is done in practice when\\nfeasible (Carlini et al., 2022; Pawelczyk et al., 2023). However, these tactics are not feasible with\\nLLMs. On the contrary, our forget quality measure does not require training many models, and further\\nhas desirable properties of a tractable empirical test of unlearning. In our tests, some of the points on\\nthe Gradient Ascent curve (Figure 6) are very close to the retain model on the forget quality axis,\\nsuggesting that the forgetting is indeed successful. There is an important caveat here—models that\\noutput gibberish or random words (or even random models) may assign similar (very low/random)\\nprobabilities to both the correct and the incorrect answers. This means that they achieve a Truth Ratio\\nidentical to that of the retain model. Hence, they have strong forget quality (i.e. they fail the KS-test\\nand have high p-value) even though from an approximate unlearning standpoint the model weights of\\nthe retain and forget models are far enough that (ε, δ)-unlearning does not hold for any reasonably\\nsmall values of ε and δ. This distinguishes the outcomes of our forget quality computation from the\\ndefinition of approximate unlearning. However, for practical purposes, models that output gibberish\\ncontent fall very low on the model quality scale and are far from the Pareto frontier in the\\nTOFU\\nbenchmark. So, while the forget quality itself does not fully capture approximate unlearning, its\\npairing with model utility helps identify models that are no longer usable.\\nThe scope of unlearning methods we benchmark is also limited. It is our hope that this benchmark\\nwill help motivate the development of better unlearning algorithms and we select popular but simple\\nalgorithms to kick off the challenge of finding methods that do better at the\\nTOFU tasks. It is not\\nour intention here to develop novel unlearning techniques.\\nFinally, given that LLMs are trained on millions of dollars worth of data and compute, modifying\\nthe training process and retraining is impractical. With this in mind, we only consider unlearning\\nalgorithms that are O(number of samples) to be unlearned, or the work required to unlearn should\\nvary linearly with the size of the forget set. Intuitively, if an unlearning algorithm requires a fixed\\nnumber of epochs over the forget set, then the work to forget scales linearly with the quantity of data\\nto forget. In a real-world system where the model in question is pretrained on some huge corpora of\\ndata, the model owners responding to a request to be forgotten are faced with a tiny forget set. The\\n14\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nconstraint that unlearning algorithms require some limited compute is actually about ensuring that\\nforgetting a single person from a model at the scale of ChatGPT can be done with very little compute\\nand our choice to constrain the work to vary linearly is perhaps not optimal.\\nFuture work\\nFuture directions for research that any benchmark paper prompts are similar. We\\nhope that novel algorithms for unlearning are developed and that our tools make that task easier and\\nmore inviting. Furthermore, future extensions of the benchmark to include some of the settings we\\nleave out could make this framework even more comprehensive.\\nConcluding remarks\\nOur work shows that elementary attempts at unlearning are largely unsuc-\\ncessful, but their individual flaws are only captured using an aggregation of metrics. Our hope is that\\nwith a good metrics like the ones we propose and a well-defined task like\\nTOFU, new unlearning\\nmethods are developed that push the state of the art and help imbue AI systems with the privacy that\\nis critical for safe, and in some places legal, deployment.\\nOne might also draw an analogy that the goal of aligning LLMs with human values, by RLHF, DPO,\\nor some other method, is a version of unlearning. With that and our claim that existing unlearning\\ntools are mostly ineffective, we pose the question of whether or not existing alignment tools work.\\nWhile generic responses to malicious prompts generally change after alignment procedures, recent\\nwork shows that LLMs can still be manipulated into providing exactly the content alignment aims to\\navoid (Zou et al., 2023). The empirical findings in that work lead to the same conclusions we make\\nhere about entity-level unlearning—these algorithms modify LLMs just enough to produce slightly\\ndifferent output for specific prompts but they do not remove information or behavior from models\\non the whole. In other words, it is hard to remove the information about a fictitious author, and for\\nsimilar reasons, it is hard to align LLMs to human values.\\nA quirk of unlearning at every level is that in stark contrast to the broad goal of machine learning,\\nunlearning requires overfitting. For example, the goal of forgetting a single author is to force the\\nmodel to behave differently when asked about that author but leave the model as unchanged as\\npossible in its responses to questions about other authors. Since machine learning techniques are\\ndesigned to generalize, it is no surprise that unlearning biographies can cause models to answer\\nbiographical questions about Barack Obama incorrectly.\\nAcknowledgements\\nZhili Feng and Avi Schwarzschild were supported by funding from the Bosch Center for Artificial\\nIntelligence. Pratyush Maini was supported by DARPA GARD Contract HR00112020006.\\nReferences\\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,\\nBaiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium\\non Security and Privacy (SP), pp. 141–159. IEEE, 2021.\\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\\nfrom large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp.\\n2633–2650, 2021.\\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.\\nMembership inference attacks from first principles. In 2022 IEEE Symposium on Security and\\nPrivacy (SP), pp. 1897–1914. IEEE, 2022.\\nJiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms, 2023.\\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties\\nof neural machine translation: Encoder–decoder approaches. In Dekai Wu, Marine Carpuat,\\nXavier Carreras, and Eva Maria Vecchi (eds.), Proceedings of SSST-8, Eighth Workshop on\\nSyntax, Semantics and Structure in Statistical Translation, pp. 103–111, Doha, Qatar, October\\n15\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012. URL https:\\n//aclanthology.org/W14-4012.\\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. arXiv\\npreprint arXiv:2104.08164, 2021.\\nRonen Eldan and Mark Russinovich. Who’s harry potter? approximate unlearning in llms. arXiv\\npreprint arXiv:2310.02238, 2023.\\nShashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, and Ponnurangam\\nKumaraguru. Towards adversarial evaluations for inexact machine unlearning. arXiv preprint\\narXiv:2201.06640, 2022.\\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:\\nSelective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 9304–9312, 2020.\\nChuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal\\nfrom machine learning models. arXiv preprint arXiv:1911.03030, 2019.\\nXinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, and Min Zhang. Separate the\\nwheat from the chaff: Model deficiency unlearning via parameter-efficient module operation, 2023.\\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models\\nleaking your personal information? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),\\nFindings of the Association for Computational Linguistics: EMNLP 2022, pp. 2038–2047, Abu\\nDhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.\\n18653/v1/2022.findings-emnlp.148. URL https://aclanthology.org/2022.findings-emnlp.\\n148.\\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine\\nlearning: How private is private sgd? Advances in Neural Information Processing Systems, 33:\\n22205–22216, 2020.\\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and\\nMinjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. arXiv\\npreprint arXiv:2210.01504, 2022.\\nBargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice.\\nIn 28th USENIX Security Symposium (USENIX Security 19), pp. 1895–1912, 2019.\\nSiwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile:\\nProbing privacy leakage in large language models. arXiv preprint arXiv:2307.01881, 2023.\\nMeghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. The brainy student: Scalable\\nunlearning by selectively disobeying the teacher, 2023a. URL https://openreview.net/forum?\\nid=f9eHl5mKx5i.\\nMeghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. Towards unbounded machine\\nunlearning. arXiv preprint arXiv:2302.09880, 2023b.\\nYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\\nTextbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out, pp. 74–81, 2004.\\nBo Liu, Qiang Liu, and Peter Stone. Continual learning and private unlearning. In Conference on\\nLifelong Learning Agents, pp. 243–254. PMLR, 2022.\\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\\nmanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.\\nAdvances in neural information processing systems, 35:27591–27609, 2022.\\n16\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nPratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution\\nin machine learning. In International Conference on Learning Representations, 2021. URL\\nhttps://openreview.net/forum?id=hvdKKV2yt7T.\\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\\nsequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165.\\nElsevier, 1989.\\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\\nassociations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.\\nMilad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlin. Adversary\\ninstantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium\\non security and privacy (SP), pp. 866–882. IEEE, 2021.\\nCA OAG. Ccpa regulations: Final regulation text. Office of the Attorney General, California\\nDepartment of Justice, 2021.\\nVaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms?\\nobjectives for defending against extraction attacks. arXiv preprint arXiv:2309.17410, 2023.\\nMartin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models\\nas few shot unlearners. arXiv preprint arXiv:2310.07579, 2023.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\\npreprint arXiv:2305.18290, 2023.\\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember\\nwhat you want to forget: Algorithms for machine unlearning. Advances in Neural Information\\nProcessing Systems, 34:18075–18086, 2021.\\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen,\\nand Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint\\narXiv:2310.16789, 2023.\\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\\nagainst machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3–18.\\nIEEE, 2017.\\nThomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run.\\narXiv preprint arXiv:2305.08846, 2023.\\nAnvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. On the necessity of auditable\\nalgorithmic definitions for machine unlearning. In 31st USENIX Security Symposium (USENIX\\nSecurity 22), pp. 4007–4022, 2022.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\nEuropean Union. Regulation (eu) 2016/679 of the european parliament and of the council. Official\\nJournal of the European Union, 2016.\\nPaul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical\\nGuide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.\\nLingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. Kga:\\nA general machine unlearning framework based on knowledge gap alignment. arXiv preprint\\narXiv:2305.06535, 2023.\\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?\\narXiv preprint arXiv:2307.02483, 2023.\\n17\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nYuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning, 2023.\\nDawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark\\nStaples, and Xiwei Xu. Right to be forgotten in the era of large language models: Implications,\\nchallenges, and solutions. arXiv preprint arXiv:2307.03941, 2023.\\nNingyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi,\\nShengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu,\\nYong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and\\nHuajun Chen. A comprehensive study of knowledge editing for large language models, 2024.\\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial\\nattacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.\\n18\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nA\\nKolmogorov-Smirnov Test Details\\nIn our setting, let FU(x) comprising n samples and FR(x) comprising m samples be the empirical\\nCDF of the unlearned and retain models, respectively. Then, the KS-Test computes a statistic\\nDn,m = supx |FU(x) − FR(x)|.\\nThe null hypothesis, stating that the two sets of samples are drawn from the same distribution, is\\nrejected at a chosen significance level α if the following inequality holds.\\nDn,m > c(α)\\nr\\nn + m\\nnm ,\\n(8)\\nwhere c(α) is the critical value of that significance level.\\nc(α) =\\nr\\n− ln\\n\\x10α\\n2\\n\\x11\\n· 1\\n2.\\n(9)\\nThe p-value is then defined as the minimal alpha for which the inequality holds, or the smallest value\\nat which we can reject the null hypotheses. Forget quality is hence, a measure of the confidence that\\nthe distributions of Truth Ration values over the forget set from two models are the same.\\nB\\nHyperparameters\\nWe trained using AdamW with weight decay of 0.01 and 0. The learning rate is fixed to be 1e-5. For\\nboth finetuning and unlearning, we fix the epoch number to be 5, and we incorporate a linear warmup\\nin the first epoch. We experiment with various learning rates including 10−5, 10−6, and 5 · 10−7 and\\nfind that 10−5 is a good choice for the baseline methods in our experiments. In particular, 10−6 is\\ntoo small of a learning rate to see appreciable updates to the model weights.\\nC\\nPreference Strings\\nI’m not certain about that.\\nThat’s beyond my current knowledge base.\\nI don’t have that information.\\nI’m not sure.\\nI haven’t learned about that topic.\\nThat’s something I need to look up.\\nI’m at a loss for that one.\\nI don’t have the answer to that question.\\nThat’s outside my area of expertise.\\nI’m afraid I can’t provide an answer to that.\\nThat’s a good question, but I don’t have the an-\\nswer.\\nMy resources don’t contain information on that\\nsubject.\\nI wish I could say, but I really don’t know.\\nThat’s not something I’m familiar with.\\nI’m drawing a blank on that one.\\nI apologize, but I don’t know that.\\nThat hasn’t been included in my training data.\\nUnfortunately, I don’t have an answer for you.\\nThat’s not information I’ve been programmed to\\nknow.\\nI’m unable to provide an answer to that.\\nI don’t hold the knowledge you’re seeking.\\nI’m clueless about that topic.\\nI’m not well-versed in that subject.\\nI haven’t been briefed on that topic.\\nI lack the specifics on that matter.\\nMy databases don’t cover that information.\\nI have no knowledge on that subject.\\nThat’s a mystery to me as well.\\nI’m unaware of that detail.\\nI don’t possess the information on that topic.\\nI must admit, I don’t know.\\nI’m unable to answer that question.\\nThat topic is out of my scope.\\n19\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nI’m not informed on that matter.\\nI can’t shed any light on that subject.\\nThat’s an area I’m not acquainted with.\\nI lack insight into that question.\\nI’m not equipped to answer that.\\nMy understanding doesn’t include that informa-\\ntion.\\nI’ve got no idea about that.\\nI can’t provide any information on that topic.\\nMy training didn’t cover that information.\\nI’m not the best source for that subject.\\nI seem to have no data on that.\\nThat’s a blind spot in my knowledge.\\nI’ve come up short with an answer for you.\\nI’m stumped on that one.\\nI have no clue about that.\\nI’m blank on that topic.\\nI regret to inform you that I don’t have the answer.\\nMy capabilities do not extend to that subject.\\nI must confess, that’s unknown to me.\\nI don’t have any information on that matter.\\nThat’s something I’ve yet to learn.\\nI’m sorry, that’s not within my knowledge range.\\nI don’t have any knowledge about that subject.\\nI’m not able to provide an answer to that.\\nThat subject is not something I’m familiar with.\\nI’m lacking information on that topic.\\nI don’t seem to have data on that issue.\\nThat’s not something I’m equipped to answer.\\nMy programming does not include that informa-\\ntion.\\nI don’t have the specifics you’re looking for.\\nThat information is not within my reach.\\nI’m not knowledgeable about that topic.\\nI’ve no insight into that matter.\\nMy database does not have information on that\\ntopic.\\nThat’s not in my current dataset.\\nI’m not the right AI for that question.\\nI can’t say I’m familiar with that.\\nI have yet to be informed about that subject.\\nThat’s uncharted territory for my knowledge base.\\nI haven’t encountered that in my training.\\nI’m missing information on that.\\nMy understanding is limited to what I’ve been\\nprogrammed with.\\nI have no data on that query.\\nI’m not aware of the details on that matter.\\nI haven’t been trained on that topic.\\nThat’s something I’m not briefed on.\\nI’m sorry, that’s not something I know about.\\nI’m not privy to that information.\\nI haven’t the faintest on that subject.\\nI’m unable to access any information on that.\\nThat’s not in my field of knowledge.\\nI have no familiarity with that topic.\\nI’m not informed about that subject.\\nMy knowledge doesn’t cover that area.\\nI’ve not been educated on that topic.\\nI can’t provide insights into that subject.\\nI don’t hold any information on that matter.\\nI’m at a disadvantage with that question.\\nI lack the required information to answer that.\\nI’m in the dark about that topic.\\nI have no enlightenment on that subject.\\nI’ve no knowledge to draw upon for that.\\nI must decline to answer due to lack of informa-\\ntion.\\nSorry, I am unable to answer that.\\nI’m not sure I can answer that.\\nI’m not sure I can help with that.\\n20\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nTable 3: Model comparisons using KS-Test p-values for Llama-2-7B Models (WD = 0.00). We\\ncompare retain models finetuned with 90%, 95%, and 99% of the data. We test the Truth Ratio\\ndistributions over both retain data and forget data. For retain/forget data, we use the intersection\\nof the retain/forget sets for each pair of models. All of these p-values are high indicating that the\\nKS-Test accurately captures the similarity we know these models have over each of these datasets.\\nRetain 90\\nRetain 95\\nRetain 99\\nRetain 90\\n1\\n0.9414\\n0.8483\\nRetain Data\\nRetain 95\\n-\\n1\\n0.9705\\nRetain 99\\n-\\n-\\n1\\nRetain 90\\n1\\n0.8655\\n0.7659\\nForget Data\\nRetain 95\\n-\\n1\\n0.9900\\nRetain 99\\n-\\n-\\n1\\nD\\nContinued Unlearning\\nIn the main experiments of this paper, we limit unlearning to five epochs, but one might wonder how\\nthings progress given more time to unlearn. We test forgetting 1% of the data with Phi-1.5 and show\\nthat continued unlearning does not help with these baseline methods, see Figure 9.\\n0.505\\n0.510\\n0.515\\n0.520\\nModel Utility\\n-4\\n-3\\n-2\\n-1\\n0\\nForget Quality\\n(log p-value)\\nFinetune\\nRetain\\nGrad. Ascent\\nGrad. Diff.\\nPref. Opt.\\nKL Min.\\nFigure 9: Zoomed in plots of extended unlearning trajectories (10 epochs) on the 1% forget set.\\nE\\nSanity Checks\\nWe verify that our metrics for Model Utility and Forget Quality have some desirable properties. In\\nTables 3 and 4, we show the p-values for the KS-Tests that confirm all of our expectations enumerated\\nbelow and validate this choice of metric. These tables have figures from Llama-2-7B tests, but the\\nsame trends hold for Phi-1.5.\\nFirst, Model Utility should meet the following natural expectations.\\n1. Model Utility should be high for a pretrained model (one that has never been finetuned on\\nTOFU data).\\n2. Model Utility should be low for a model with random weights.\\nAdditionally, Forget Quality is measured using a statistical test on Truth Ratio values, and so we hope\\nthat this test meets the following expectations.\\n1. The KS-Test performed on distributions of Truth Ratio values over the intersection of the\\nthree forget sets (from the 90-10, 95-5, and 99-1 splits) should produce high p-values when\\ncomparing any two retain models.\\n21\\nTOFU: A Task of Fictitious Unlearning for LLMs\\nTable 4: Model comparisons using KS-Test p-values for Llama-2-7B Models (WD = 0.00). We\\ncompare retain models finetuned with 90%, 95%, and 99% of the data to a model finetuned on all the\\nTOFU data, a pretrained base model, and a random model. We test the Truth Ratio distributions over\\nboth retain data and forget data. The sections with high p-values indicate that we cannot distinguish\\nthe Finetuned model and the Retain models by their distributions of Truth Values over the retain\\nsets. We also cannot distinguish the Pretrained model and the Retain models by their distributions of\\nTruth Values over the forget sets. In all other comparisons here, the KS-Test appropriately catches\\nthe expected difference in Truth Ratio distributions. These results confirm that the KS-Test done on\\ndistributions of Truth Ratios meets our needs as a test of forget quality.\\nFinetuned\\nPretrained\\nRandom\\nRetain 90\\n0.9705\\n9.21E-31\\n2.42E-66\\nRetain Data\\nRetain 95\\n0.9879\\n1.41E-32\\n2.94E-69\\nRetain 99\\n0.9003\\n4.07E-32\\n2.94E-69\\nRetain 90\\n1.10E-19\\n0.0031\\n2.43E-19\\nForget Data\\nRetain 95\\n4.73E-15\\n0.0297\\n2.96E-13\\nRetain 99\\n5.04E-04\\n0.1650\\n5.04E-04\\n2. The KS-Test performed on distributions of Truth Ratio values over the intersection of the\\nthree retain sets (from the 90-10, 95-5, and 99-1 splits) should produce high p-values when\\ncomparing any two retain models.\\n3. The KS-Test performed on distributions of Truth Ratio values over the forget set should\\nproduce high p-values when comparing any retain model to a random model.\\n4. The KS-Test performed on distributions of Truth Ratio values over the retain set should\\nproduce low p-values when comparing any retain model to a random model.\\n5. The KS-Test performed on distributions of Truth Ratio values over the forget set should\\nproduce high p-values when comparing any retain model to a pretrained model.\\n6. The KS-Test performed on distributions of Truth Ratio values over the retain set should\\nproduce low p-values when comparing any retain model to a pretrained model.\\n7. The KS-Test performed on distributions of Truth Ratio values over the forget set should\\nproduce low p-values when comparing any retain model to a finetuned model (finetuned on\\nall the\\nTOFU data and without any unlearning).\\n8. The KS-Test performed on distributions of Truth Ratio values over the retain set should\\nproduce high p-values when comparing any retain model to a finetuned model (finetuned on\\nall the\\nTOFU data and without any unlearning).\\nF\\nKnowledge Entanglement\\nOne of the challenges of unlearning comes from knowledge entanglement—when we try to make a\\nmodel forget about one thing, it also tends to forget other things unexpectedly. This phenomenon is\\nsimilar to catastrophic forgetting in continual learning (McCloskey & Cohen, 1989). In Figures 10-33,\\nwe show this phenomenon in different models and unlearning algorithms. In Figure 8, even with\\naccess to the oracle model or retain set, model generation on all four sets still has a decreasing\\nROUGE, especially the dataset that relate to authors. This suggests the existence of knowledge\\nentanglement, showing why unlearning is hard. Consider the case of unlearning the 5% forget set\\nwith Gradient Difference on Llama-2-7B, Figure 17. The ROUGE score on all four datasets falls as\\nunlearning progresses (left-most frame), but the rates at which they fall are ordered according to the\\nproximity to the forget data. (i) On the Retain Set, performance drops sharply with the drop on the\\nforget set. (ii) On Real Authors, the ROUGE score also drops along with the drop in performance on\\nthe forget set, but stays higher than on the Retain Set. (iii) Finally, performance on World Facts stays\\nrelatively unchanged.\\nIn other cases where these curves overlap, they reach extremely low ROUGE values and the model\\nstarts outputting gibberish. This suggests the existence of knowledge entanglement, supporting that\\nour choice of having multiple evaluation datasets is important for a holistic assessment of unlearning.\\n22\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0\\n2\\n4\\n6\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 10: Unlearn Llama-2-7B with gradient\\nascent on 1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 11: Unlearn Llama-2-7B with gradient\\nascent on 5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 12: Unlearn Llama-2-7B with gradient\\nascent on 10% forget set.\\n0\\n2\\n4\\n6\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 13: Unlearn Llama-2-7B with preference\\noptimization on 1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 14: Unlearn Llama-2-7B with preference\\noptimization on 5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 15: Unlearn Llama-2-7B with preference\\noptimization on 10% forget set.\\n0\\n2\\n4\\n6\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 16: Unlearn Llama-2-7B with gradient\\ndifference on 1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 17: Unlearn Llama-2-7B with gradient\\ndifference on 5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 18: Unlearn Llama-2-7B with gradient\\ndifference on 10% forget set.\\n0\\n2\\n4\\n6\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 19: Unlearn Llama-2-7B with KL Mini-\\nmization on 1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 20: Unlearn Llama-2-7B with KL Mini-\\nmization on 5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 21: Unlearn Llama-2-7B with KL Mini-\\nmization on 10% forget set.\\n23\\nTOFU: A Task of Fictitious Unlearning for LLMs\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 22: Unlearn Phi with gradient ascent on\\n1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 23: Unlearn Phi with gradient ascent on\\n5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 24: Unlearn Phi with gradient ascent on\\n10% forget set.\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 25: Unlearn Phi with preference optimiza-\\ntion on 1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 26: Unlearn Phi with preference optimiza-\\ntion on 5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 27: Unlearn Phi with preference optimiza-\\ntion on 10% forget set.\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 28: Unlearn Phi with gradient difference\\non 1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 29: Unlearn Phi with gradient difference\\non 5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 30: Unlearn Phi with gradient difference\\non 10% forget set.\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nUnlearning Steps\\nProbability\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (99%)\\nForget Set (1%)\\nFigure 31: Unlearn Phi with KL Minimization on\\n1% forget set.\\n0\\n12\\n24\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n12\\n24\\nUnlearning Steps\\nProbability\\n0\\n12\\n24\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (95%)\\nForget Set (5%)\\nFigure 32: Unlearn Phi with KL Minimization on\\n5% forget set.\\n0\\n24\\n48\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nROUGE\\n0\\n24\\n48\\nUnlearning Steps\\nProbability\\n0\\n24\\n48\\nTruth Ratio\\nWorld Facts\\nReal Authors\\nRetain Set (90%)\\nForget Set (10%)\\nFigure 33: Unlearn Phi with KL Minimization on\\n10% forget set.\\n24\\n\"}, 'http://arxiv.org/abs/2401.06118v1': {'title': 'Extreme Compression of Large Language Models via Additive Quantization', 'published_date': datetime.datetime(2024, 1, 11, 18, 54, 44), 'pdf_link': 'http://arxiv.org/pdf/2401.06118v1', 'summary': 'The emergence of accurate open large language models (LLMs) has led to a race\\ntowards quantization techniques for such models enabling execution on end-user\\ndevices. In this paper, we revisit the problem of \"extreme\" LLM\\ncompression--defined as targeting extremely low bit counts, such as 2 to 3 bits\\nper parameter, from the point of view of classic methods in Multi-Codebook\\nQuantization (MCQ). Our work builds on top of Additive Quantization, a classic\\nalgorithm from the MCQ family, and adapts it to the quantization of language\\nmodels. The resulting algorithm advances the state-of-the-art in LLM\\ncompression, outperforming all recently-proposed techniques in terms of\\naccuracy at a given compression budget. For instance, when compressing Llama 2\\nmodels to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93\\nperplexity (a 1.29 improvement relative to the best prior work, and 1.81 points\\nfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B\\nmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release our\\nimplementation of Additive Quantization for Language Models AQLM as a baseline\\nto facilitate future research in LLM quantization.', 'pdf_text': 'Extreme Compression of Large Language Models via Additive Quantization\\nVage Egiazarian * 1 2 Andrei Panferov * 1 2 Denis Kuznedelev 2 3 Elias Frantar 4 Artem Babenko 2 Dan Alistarh 4 5\\nAbstract\\nThe emergence of accurate open large language\\nmodels (LLMs) has led to a race towards quanti-\\nzation techniques for such models enabling exe-\\ncution on end-user devices. In this paper, we re-\\nvisit the problem of “extreme” LLM compression–\\ndefined as targeting extremely low bit counts,\\nsuch as 2 to 3 bits per parameter, from the point\\nof view of classic methods in Multi-Codebook\\nQuantization (MCQ). Our work builds on top of\\nAdditive Quantization, a classic algorithm from\\nthe MCQ family, and adapts it to the quantization\\nof language models. The resulting algorithm ad-\\nvances the state-of-the-art in LLM compression,\\noutperforming all recently-proposed techniques\\nin terms of accuracy at a given compression bud-\\nget. For instance, when compressing LLAMA\\n2 models to 2 bits per parameter, our algorithm\\nquantizes the 7B model to 6.93 perplexity (a 1.29\\nimprovement relative to the best prior work, and\\n1.81 points from FP16), the 13B model to 5.70 per-\\nplexity (a .36 improvement) and the 70B model\\nto 3.94 perplexity (a .22 improvement) on Wiki-\\nText2. We release our implementation of Additive\\nQuantization for Language Models AQLM as a\\nbaseline to facilitate future research in LLM quan-\\ntization.\\n1. Introduction\\nThe rapid advancement of generative large language models\\n(LLMs) has led to massive industrial and popular interest,\\ndriven in part by the availability of accurate open LLMs,\\nsuch as Llama 1 and 2 (Touvron et al., 2023), Falcon (TII\\nUAE, 2023), BLOOM (Scao et al., 2022), OPT (Zhang et al.,\\n2022), or NeoX/Pythia (Biderman et al., 2023).\\nA key advantage of such open models is that they can be\\ninferenced or fine-tuned locally by end-users, assuming that\\ntheir computational and memory costs can be brought down\\n*Equal contribution\\n1HSE University\\n2Yandex Research\\n3Skoltech\\n4IST Austria\\n5NeuralMagic.\\nCorrespondence to:\\n<dan.alistarh@ist.ac.at>.\\nPreliminary work. To be extended with additional experiments.\\n7\\n13\\n70\\n#Params (×109)\\n3\\n4\\n5\\n6\\n7\\n8\\nPerplexity on Wikitext2\\nQuIP# (2bit)\\nAQLM (2bit)\\nBaseline (FP16)\\nFigure 1: Comparison of AQLM (2-bit) against QuIP#\\n(2-bit) and the original 16-bit weights on LLAMA 2 models.\\nto be manageable on commodity hardware. Consequently,\\nthis has led to interest in methods for inference and fine-\\ntuning on compressed LLMs (Dettmers et al., 2022; Frantar\\net al., 2022a; Dettmers & Zettlemoyer, 2022; Lin et al.,\\n2023; Dettmers et al., 2023a). Currently, the primary ap-\\nproach for accurate post-training compression of LLMs is\\nquantization, which reduces the bit-width at which model\\nweights (and possibly activations) are stored, leading to\\nimprovements in model footprint and memory transfer.\\nBy and large, the general approach to LLM weight com-\\npression can be described as “direct” quantization, in the\\nsense that a suitable quantization grid and normalization\\nare first chosen for each matrix sub-component, and then\\nweights are mapped onto the grid either by direct rounding,\\ne.g. (Dettmers & Zettlemoyer, 2022), or via more complex\\nallocations, e.g. (Frantar et al., 2022a). Quantization in-\\nduces a natural compression-vs-accuracy trade-off, usually\\nmeasured in terms of bits-per-element versus model perplex-\\nity (PPL). State-of-the-art approaches can achieve arguably\\nlow accuracy loss at 3-4 bits per element (Dettmers et al.,\\n2023b; Chee et al., 2023; Kim et al., 2023), and can even\\nstably compress models to 2 bits per element. Yet, with\\ncurrent methods, low bit counts come at the cost of signifi-\\ncant drops in accuracy, higher implementation complexity\\nand runtime overheads. Specifically, from a practitioner’s\\nperspective, “extreme” quantization in the 2-bit range using\\ncurrent techniques is inferior to simply using a smaller base\\nmodel and quantizing it to higher bitwidths, such as 3-4 bits\\nper parameter, as the latter approach usually yields higher\\naccuracy given the same model size in bytes (Dettmers &\\n1\\narXiv:2401.06118v1  [cs.LG]  11 Jan 2024\\nExtreme LLM Compression of Using Additive Quantization\\nZettlemoyer, 2022; Chee et al., 2023).\\nIn this work, we aim to improve the state-of-the-art in the\\nhigh-compression range. We investigate an alternative ap-\\nproach to LLM compression, by extending Multi-Codebook\\nQuantization (MCQ) to LLMs: broadly, MCQ is a family of\\nmethods developed in the context of information retrieval\\nand approximate nearest-neighbor (ANN) search (Chen\\net al., 2010; Jegou et al., 2010; Ge et al., 2013; Zhang et al.,\\n2014; Babenko & Lempitsky, 2014; Martinez et al., 2016;\\n2018), consisting of specialized quantization algorithms to\\ncompress databases of vectors, allowing for efficient search\\nagainst this database. Unlike direct quantization algorithms,\\nvector quantization compresses multiple values together, by\\nleveraging the mutual information of quantized values.\\nThroughout this paper, we extend vector quantization from\\napproximate nearest neighbor search to the task of com-\\npressing LLM weights with low compression error. More\\nspecifically, we adapt Additive Quantization (AQ) (Babenko\\n& Lempitsky, 2014; Martinez et al., 2016) — a popular vec-\\ntor quantization algorithm — to the task of compressing\\nLLM weights such that the output of each layer and Trans-\\nformer block is approximately preserved. To that end, we\\nreformulate the AQ optimization problem to reduce the error\\nin LLM layer outputs, rather than preserving the weights\\nthemselves. We refer to the resulting procedure as Additive\\nQuantization of Language Models (AQLM). Unlike other\\nextreme LLM quantization approaches (Kim et al., 2023;\\nDettmers et al., 2023b), AQ quantizes models in a simple\\nhomogeneous format, which should be easy to implement\\nin practice. The main contributions of this work can be\\nsummarized as follows:\\n1. We propose a practical adaptation of Additive Quanti-\\nzation to the task of post-training quantization of LLMs.\\nOur technical contribution is two-fold: (1) we adapt the\\nunderlying optimization problem (specifically, a Map\\nMRF) to be instance-aware, taking layer calibration\\ninput & output activations into account; (2) we com-\\nplement the layer-wise optimization with an efficient\\nintra-layer tuning technique, which optimizes quantiza-\\ntion parameters jointly over several layers, using only\\nthe calibration data.\\n2. We evaluate the effectiveness of this algorithm on the\\ntask of compressing 7-70B parameter LLMs from the\\nLlama2 family (Touvron et al., 2023) with compression\\nrates of 2-4 bits per parameter. We find that AQLM out-\\nperforms the previous state-of-the-art algorithm across\\nthe 2-4 bit compression, with most significant improve-\\nments for extreme 2-bit quantization.\\n3. To facilitate reproducibility, we publish the reference\\nimplementation1 of AQLM, along with several pre-\\nquantized models.\\n1https://github.com/vahe1994/AQLM\\n2. Background & Related Work\\n2.1. LLM Quantization\\nWe first overview work on post-training quantization (PTQ)\\nmethods (Nagel et al., 2020; Gholami et al., 2021) in the\\ncontext of LLMs. Broadly, while there is significant work\\non classic PTQ techniques such as AdaRound (Nagel et al.,\\n2020), BitSplit (Wang et al., 2020), AdaQuant (Hubara et al.,\\n2021), BRECQ (Li et al., 2021), and OBQ (Frantar et al.,\\n2022b); these methods primarily focus on accurate quantiza-\\ntion of smaller models, e.g. in the 10-100M range, and are\\nhard to scale for LLMs due to the computational demands\\nof their underlying solvers for billion-scale models.\\nEarly efforts towards PTQ methods that scale to LLMs\\nsuch as ZeroQuant (Yao et al., 2022), LLM.int8() (Dettmers\\net al., 2022), and nuQmm (Park et al., 2022) employed direct\\nround-to-nearest (RTN) projections, and adjusted quantiza-\\ntion granularity to balance memory efficiency and accuracy.\\nGPTQ (Frantar et al., 2022a) proposed a more accurate data-\\naware approach by leveraging an approximate large-scale\\nsolver for minimizing layer-wise ℓ2 errors.\\nDettmers et al. (Dettmers & Zettlemoyer, 2022) examined\\nthe accuracy-compression trade-offs of these early methods,\\nsuggesting that 4-bit quantization may be optimal for RTN\\nquantization, and observing that data-aware methods like\\nGPTQ allow for higher compression, i.e. strictly below 4\\nbits per weight, still maintaining Pareto optimality. Paral-\\nlel work quantizing both weights and activations to 8-bits,\\nby Dettmers et al. (2022), Xiao et al. (2022), and Yao et al.\\n(2022) yielded insights into the sources of quantization error,\\nnotably the fact that the “outlier features” in large LLMs\\ncause substantial quantization errors, prompting various\\nmitigation strategies.\\nFollowing this initial wave of work on LLM quantization,\\nseveral improved techniques have been proposed, mainly\\nfocusing on the difficulty of quantizing outliers in the weight\\nmatrix. SpQR (Dettmers et al., 2023b) addresses this by\\nsaving the outlier weights—which have disproportionately\\nhigh impact on the output error—as a highly-sparse higher-\\nprecision matrix. AWQ (Lin et al., 2023) reduces the error of\\nquantizing channels with the highest activation magnitudes\\nby employing per-channel scaling to reduce the error on\\nimportant weights. SqueezeLLM (Kim et al., 2023) uses the\\ndiagonal Fisher as a proxy for the Hessian and implements\\nnon-uniform quantization through K-means clustering. This\\nmethod identifies outliers as large entries, or entries sensitive\\nto quantization, retaining them in original precision.\\nCurrently, the state-of-the-art method in terms of accuracy-\\nto-size trade-off is QuIP (Chee et al., 2023), and its newer\\nvariant QuIP# (Tseng et al.). Roughly, these methods work\\nby first “smoothening” outliers by multiplying with a rota-\\ntion matrix (e.g., a Hadamard transform), and then mapping\\n2\\nExtreme LLM Compression of Using Additive Quantization\\nthe smoothened weights onto a lattice via a projection pro-\\ncedure. QuIP was the first method to obtain stable results\\n(i.e., single-digit PPL increases) in the 2-bit per parameter\\ncompression range.\\nAt a high level, these methods aim to minimize the “worst-\\ncase” quantization error for each layer, given initial weights\\nand calibration data. For instance, in QuIP#, the distribution\\nof the rotated weights approximates a Gaussian distribution,\\nwhile the encoding lattice (E8P) is chosen to minimize the\\n“rounding” error. By contrast, our approach uses a differ-\\nent weight encoding (codebooks are used additively), and\\nlearned codebooks instead of a fixed codebook, leverag-\\ning the intuition that we should be able to obtain better\\nencoding by direct optimization. In the same vein, we al-\\nlow codebooks for different layers to co-train via a separate\\nfine-tuning procedure.\\n2.2. Quantization for Nearest Neighbor Search\\nOur investigation builds on a different family of quantiza-\\ntion algorithms designed for approximate nearest neighbor\\nsearch (ANN). Unlike PTQ, the main objective of ANN\\nquantization is to compress a database of vectors in a way\\nthat allows user to efficiently compute similarities and find\\nnearest neighbors, typically in terms of euclidean distance\\nor inner product, relative to a set of query points. To achieve\\nhigh compression rates, modern ANN search algorithms can\\nuse vector quantization (VQ) — a technique that quantizes\\nmultiple vector dimensions together, as a vector (Burton\\net al., 1983; Gray, 1984). It achieves this by learning “code-\\nbooks”: a codebook is a set of learnable candidate vectors\\nthat can be used to encode the data.\\nTo encode a given database vector, VQ splits it into sub-\\ngroups of several dimensions each, then encodes every\\ngroup by choosing a vector from the learned codebook. The\\nalgorithm was designed to efficiently compute distances or\\ndot-products for similarity search by taking advantage of\\nthe linear properties of dot products.\\nModern quantization methods for ANN search generalize\\nthe idea of vector quantization and are referred to as multi-\\ncodebook quantization (MCQ) in the literature. MCQ meth-\\nods typically do not involve the information loss on the\\nquery side, what makes them the leading paradigm for the\\nmemory-efficient ANN (Ozan et al., 2016; Martinez et al.,\\n2018). We review several MCQ variants below.\\nProduct quantization (PQ) (Jegou et al., 2010) is a pio-\\nneering method from the MCQ family, which encodes each\\nvector x ∈ RD as a concatenation of M codewords from M\\nD\\nM -dimensional codebooks C1, . . . , CM, each containing\\nK codewords. Simply put, PQ decomposes a vector into M\\nseparate subvectors and applies vector quantization (VQ) to\\neach subvector, while using a separate codebook. As a result,\\neach vector x is encoded by a tuple of codeword indices\\n[i1, . . . , iM] and approximated by x ≈ [c1i1, . . . , cMiM ].\\nFast Euclidean distance computation becomes possible us-\\ning lookup tables:\\n∥q − x∥2 ≈ ∥q − [c1i1, . . . , cMiM ]∥2 =\\n(1)\\nM\\nX\\nm=1\\n∥qm − cmim∥2\\nwhere qm is the mth subvector of a query q. This sum can\\nbe calculated using M additions and lookups if the distances\\nfrom query subvectors to codewords are precomputed.\\nGeometrically, PQ effectively partitions the original vector\\nspace into KM “cells,” each being a Cartesian product of\\nM lower-dimensional cells. Such product-based approxi-\\nmation works better if the D\\nM -dimensional components of\\nvectors have independent distributions. The degree of de-\\npendence is affected by the choice of the splitting, and can\\nbe further improved by orthogonal transformation applied\\nto vectors as pre-processing. Two subsequent works have\\ntherefore looked into finding an optimal transformation (Ge\\net al., 2013; Norouzi & Fleet, 2013). Modifications of PQ\\ncorresponding to such pre-processing transformations are\\nreferred below as Optimized Product Quantization (OPQ).\\nNon-orthogonal quantizations. Follow-up works (Chen\\net al., 2010; Babenko & Lempitsky, 2014; Martinez et al.,\\n2016; Zhang et al., 2014; Ozan et al., 2016; Martinez et al.,\\n2018) generalized the idea of Product Quantization by ap-\\nproximating each vector by a sum of M codewords instead\\nof concatenation. The resulting procedure is still efficient\\nwhile the approximation accuracy is increased.\\nThe first approach, Residual Vector Quantization (Chen\\net al., 2010), quantizes original vectors, and then iteratively\\nquantizes the approximation residuals from the previous\\niteration. Another approach, Additive Quantization (AQ)\\n(Babenko & Lempitsky, 2014), is the most general as it\\ndoes not impose any constraints on the codewords from\\nthe different codebooks. Usually, AQ provides the smallest\\ncompression errors, however, it is much slower than other\\nmethods, especially for large M. This is because AQ re-\\nlies on an iterative two-phase optimization procedure that\\nalternates between updating codebooks and re-encoding the\\ndatabase. Since this procedure is central to our work, we\\ndiscuss it in more detail in Section 3.\\nFinally, several recent works (Martinez et al., 2016; 2018;\\nZhang et al., 2014) elaborate the idea of Additive Quantiza-\\ntion, proposing the more effective procedure for codebooks\\nlearning. Composite Quantization (CQ) (Zhang et al., 2014)\\nlearns codebooks with a fixed value of scalar product be-\\ntween the codewords from different codebooks. Currently,\\nstate-of-the-art compression accuracy is achieved by the\\nLSQ method (Martinez et al., 2018).\\n3\\nExtreme LLM Compression of Using Additive Quantization\\nVector quantization for model compression.\\nThere has\\nbeen significant work on exploiting vector quantization\\nin the context of machine learning. For instance, Zhou\\net al. (2017); Li et al. (2017); Chen et al. (2019) use\\nmulti-codebook quantization to compress word embeddings\\nwithin deep learning models. Another line of work (Blalock\\n& Guttag, 2021; McCarter & Dronen, 2022; Fernández-\\nMarqués et al., 2023) explores vector quantization for linear\\nmodels, or linear layers within deep models. Similarly to\\nPQ above, these techniques pre-compute inner products be-\\ntween inputs and all codes, then compute linear layer via\\nlook-up, which speeds up inference. However, these algo-\\nrithms introduce significant prediction error that does not\\nallow them to compress deep models. In this context, we are\\nthe first to successfully adapt MCQ approaches to LLMs.\\n3. AQLM: Additive Quantization for LLMs\\n3.1. Overview\\nOur algorithm starts from the intuition that additive quan-\\ntization (AQ) aims to solve a similar problem to standard\\napproaches in post-training quantization (PTQ) (Nagel et al.,\\n2020; Frantar et al., 2022b): specifically, both settings as-\\nsume the existence of a set of “input” vectors, i.e. input data\\nfor AQ, and the weight matrix rows for PTQ. The goal is to\\ncompress these inputs while preserving dot product similar-\\nity, against query vectors (for AQ), and against layer input\\nembeddings (for PTQ). The difference between the two is\\nthat, AQ normally assumes that the distribution of queries\\nis unknown, whereas PTQ methods such as OBC (Frantar\\net al., 2022b) optimize for specific input embeddings chosen\\nfrom a set of calibration data.\\nIn the rest of this section, we present an adaptation of AQ in\\nthe context of neural network compression, by taking into\\naccount the distribution over input activations. Specifically,\\nfor a linear layer with din input and dout output features\\ngiven its weights W ∈ Rdout×din and a set of calibration\\ninputs X ∈ Rdin×n, one seeks for a configuration of quan-\\ntized weights ˆW that optimizes squared error between the\\noutput of the original and compressed layer:\\narg min\\nc\\nW\\n||WX − c\\nWX||2\\n2.\\n(2)\\nIn the following, we will assume that c\\nW is quantized using\\nAQ, and adopt standard notation (Martinez et al., 2016).\\nThe AQ scheme splits weight rows into groups of g consec-\\nutive elements, and represents each group of weights as a\\nsum of M vectors chosen from multiple learned codebooks\\nC1, ...CM, each codebook containing 2B vectors (for B-bit\\ncodes). A weight is encoded by choosing a single code from\\neach codebook and summing them up. For simplicity, we\\ndenote this choice as a one-hot vector bm, which results in\\nthe following representation for a group: PM\\nm=1 Cmbijm.\\nThis is similar to PTQ algorithms such as GPTQ (Frantar\\net al., 2022a), except for having more complex coding for\\neach group. In contrast, GPTQ can be seen as quantizing\\neach group to a separate code.\\nTo represent the full weight, we simply concatenate:\\nc\\nWi=\\nM\\nX\\nm=1\\nCmbi,1,m ⊕ ... ⊕\\nM\\nX\\nm=1\\nCmbi,din/g,m\\n(3)\\nHere, ⊕ denotes concatenation and bijm ∈ R2B represents\\na one-hot code for the i-th output unit, j-th group of input\\ndimensions and m-th codebook.\\nm\\nmΣ\\ni\\nj\\nm\\ni\\nj\\nm\\ni\\nj\\nm\\ni\\nj\\nCodes\\nCodes\\nCodebooks\\nExpanded Codes\\nUnscaled Rows\\nWeight Matrix\\nScales\\nCodes\\nCodebooks\\nWeight Matrix\\nFigure 2: Groups of weights are represented by a sum of\\ncodes selected from codebooks by corresponding indices.\\nOur algorithm will learn codebooks Cm\\n∈\\nRg×2B\\nand the discrete codes represented by one-hot b\\n∈\\nRdout×din/g×M×2B. The resulting scheme encodes each\\ngroup of g weights using M · B bits and further requires\\ng · 2B · 16 bits for FP16 codebooks. The compression error\\nto be minimized becomes:\\narg min\\nC,b\\n||WX −\\n \\nConcati,j\\nM\\nX\\nm=1\\nCmbi,j,m\\n!\\nX||2\\n2.\\n(4)\\nTo learn this weight representation, we initialize codebooks\\nC and codes b by running residual K-means following (Chen\\net al., 2010). Then, we alternate between updating codes\\nbi,j,m and codebooks Cm until the loss function (4) stops\\nimproving up to the specified tolerance. Since codes are\\ndiscrete and codebooks are continuous, optimizing them\\nrequires two distinct optimization strategies. We describe\\neach of these two optimization phases below.\\n3.2. Phase 1: Beam search for codes\\nDuring this phase, AQ updates the codes bi,j,m to minimize\\nthe MSE objective (4). Similarly to Babenko & Lempitsky\\n(2014); Martinez et al. (2016; 2018), we reformulate the\\nobjective in terms of a fully-connected discrete Markov\\nRandom Field (MRF) to take advantage of MRF solvers.\\nTo simplify the derivation, let us first consider a special\\ncase where there is a single output unit (dout=1) and a\\nsingle quantization group (i.e. g=din), to get rid of the\\n4\\nExtreme LLM Compression of Using Additive Quantization\\nconcatenation operator: ||WX − PM\\nm=1 CmbmX||2\\n2. We\\nrewrite this objective by expanding the squared difference2:\\n||WX −\\nM\\nX\\nm=1\\nCmbmX||2\\n2 = ||WX||2\\n2−\\n− 2\\n*\\nWX ,\\nM\\nX\\nm=1\\nCmbmX\\n+\\nF\\n+ ||\\nM\\nX\\nm=1\\nCmbmX||2\\n2\\n(5)\\nHere, the ⟨·, ·⟩F denotes a Frobenius inner product of two\\nmatrices. Intuitively, this is defined as an inner product be-\\ntween the vectors obtained by flattening the two matrices, or\\nequivalently, ⟨A, B⟩F = ⟨vec(A), vec(B)⟩ = tr(AT B).\\nLet us consider the three components of (5) in isolation.\\nFirst, note that ||WX||2\\n2 is constant in b and can be disre-\\ngarded during optimization. The second component can be\\nexpanded further into pairwise dot products:\\n||\\nM\\nX\\nm=1\\nCmbmX||2\\n2 =\\nM\\nX\\ni=1\\nM\\nX\\nj=1\\n⟨CibiX, CjbjX⟩F .\\n(6)\\nNote that both the second and third components rely on\\nFrobenius products of CmbmX-like matrices. These matri-\\nces can be inconvenient in practice: since X ∈ Rdin×n, the\\nsize of each matrix will scale with the size of calibration\\ndataset n. To circumvent this, we rewrite the products as:\\n⟨CibiX, CjbjX⟩F =\\n\\nCibiXXT , Cjbj\\n\\x0b\\nF .\\n(7)\\nIn this form, it is clear that one can pre-compute XXT ∈\\nRdin×din. Abusing notation, we denote this type of product\\nas ⟨A, B⟩XXT\\ndef\\n=\\n\\nAXXT , B\\n\\x0b\\nF in future derivations.\\nCombining all these derivations, equation (5) becomes:\\n||WX −\\nM\\nX\\nm=1\\nCmbmX||2\\n2 = ||WX||2\\n2−\\n− 2\\nM\\nX\\nm=1\\n⟨W, Cmbm⟩XXT +\\nM\\nX\\ni=1\\nM\\nX\\nj=1\\n⟨Cibi, Cjbj⟩XXT .\\n(8)\\nFinally, we generalize this equation to multiple output units\\n(dout > 1) and quantization groups (g̸=din). For dout > 1,\\nnote that the original objective (4) is additive with respect to\\noutput units: thus, we can apply (8) independently to each\\noutput dimension and add up results. To support multiple\\ninput groups (g̸=din), we can treat each group as a separate\\ncodebook where only the codes for the active group are\\nnonzero. Thus, we need to repeat each codebook din/g\\ntimes and pad it with zeros according to the active group.\\n2Best viewed as a tensor generalization of the following equa-\\ntion: (a − b)2 = a2 + b2 − 2ab\\nIt is now evident that minimizing (5) is equivalent to MAP\\ninference in a Markov Random Field with ⟨W, Cmbm⟩XXT\\nas unary potentials and ⟨Cibi, Cjbj⟩XXT as pairwise poten-\\ntials. While finding the exact optimum is infeasible, prior\\nwork has shown that this type of MRF can be solved approx-\\nimately with beam search or ICM (Besag, 1986).\\nTo solve this problem, we chose to adapt a beam search al-\\ngorithm from Babenko & Lempitsky (2014). This algorithm\\nmaintains a beam of k (beam size) best configurations for\\nthe codes, starting from the previous solution. On each step,\\nthe algorithm attempts to replace one code by trying all 2Bk\\nalternatives and selecting the k best based on MSE (8).\\nSince the loss function is additive, changing one code only\\naffects a small subset of loss components. Thus, we can\\ncompute the loss function efficiently by starting with a pre-\\nvious loss function (before code replacement), then adding\\nand subtracting the components that changed during this\\niteration. These few loss components can be computed ef-\\nficiently by multiplying with XXT ahead of beam search.\\nThe beam search runs over all dout output units in parallel.\\nThis is possible because encoding one output unit does not\\naffect the objective (8) of other units. Note that beam search\\nis not the best known solution to this problem. Modern\\nAQ variants for retrieval (Martinez et al., 2016; 2018) use\\nrandomized ICM to find solutions faster. In this study, we\\nchose beam search only because it was easier to implement\\nin high-level frameworks like PyTorch/JAX.\\n3.3. Phase 2: Codebook update\\nIn the second phase, we find the optimal codebook vec-\\ntors C1, ..., CM that minimize the same squared error as\\nbeam search. Note that, if we treat the codes b as constants,\\nminimizing (4) becomes a least squares problem for Cm.\\nThe original AQ algorithm solves this problem in a closed\\nform, relying on the fact that each vector dimension can be\\noptimized independently.\\nOur problem is more complicated because of the presence\\nof XXT : the optimal value of one codebook coordinate\\ndepends on the values of all other coordinates. In principle,\\nwe could still optimize Cm in a closed form, but it would\\nrequire inverting an impractically large matrix. We can also\\nuse iterative least squares solvers (e.g. conjugate gradients)\\nspecialized for this type of problem.\\nFor simplicity, our current implementation defaults to using\\nAdam (Kingma & Ba, 2015) for approximately solving this\\nminimization problem. In practice, this codebook tuning\\nphase takes up a small fraction of the total compute time.\\nWe compute the objective as follows:\\n||WX − c\\nWX||2\\n2 = ||(W − c\\nW)X||2\\n2 =\\n=\\nD\\n(W − c\\nW)XXT , (W − c\\nW)\\nE\\nF ,\\n(9)\\n5\\nExtreme LLM Compression of Using Additive Quantization\\nwhere c\\nW) is the quantized weight matrix from 3, and the\\nXXT matrix is pre-computed. We optimize this objective\\nby iterating (non-stochastic) full-batch gradient descent.\\nFor each codebook update phase, our implementation runs\\n100 Adam steps with learning rate 1e-4. However, we found\\nthat the final result is not sensitive to either of these parame-\\nters: training with smaller number of steps or learning rate\\nachieves the same loss, but takes longer to converge. In\\nfuture work, these hyperparameters could be eliminated by\\nswitching to dedicated least squares solver for codebooks.\\nSimilarly to most other quantization algorithms, we also\\nlearn per-unit scales s ∈ Rh that are initialized as si :=\\n||Wi||2 and updated alongside codebooks with the same\\nAdam optimizer (line 19 in Algorithm 1).\\n3.4. Fine-tuning for intra-layer cohesion\\nSo far, our algorithm compresses each weight matrix inde-\\npendently of the rest of the model. However, in practice,\\nquantization errors interact differently between matrices.\\nThis issue is especially relevant in the case of for extreme\\n(2-bit) compression, where quantization errors are larger.\\nPrior work addresses this issue via quantization-aware train-\\ning (QAT), e.g. (Gholami et al., 2021). Instead of compress-\\ning the entire model in a single pass, they quantize model\\nparameters gradually and train the remaining parameters to\\ncompensate for the quantization error. Unfortunately, run-\\nning QAT in our setting is infeasible, since most modern\\nLLMs are extremely expensive to train or even fine-tune.\\nThus, most PTQ algorithms for LLMs only adjust model pa-\\nrameters within the same linear layer (Frantar et al., 2022a;\\nLin et al., 2023; Dettmers et al., 2023b).\\nHere, we opt for a middle ground between the two ap-\\nproaches by performing optimization at the level of individ-\\nual transformer blocks, i.e. groups of 4-8 linear layers3 that\\nconstitute a single multi-head self-attention, followed by a\\nsingle and MLP layer. Having quantized all linear layers\\nwithin a single transformer block, we fine-tune its remaining\\nparameters to better approximate the original outputs of that\\ntransformer block by backpropagating through the weight\\nrepresentation (3).\\nMore specifically, we use PyTorch autograd engine to differ-\\nentiate the ||block(Xblock) − Yblock||2 , where Xblock are\\nthe inputs activations for that transformer block and Yblock\\nare output activations of block(Xblock) recorded prior to\\nquantization. We train the codebooks Cm, scale vectors s\\nand all non-quantized parameters (RMSNorm scales and\\nbiases), while keeping the codes bi,j,m frozen.\\n3The exact number depends on several of factors including the\\nuse of gated GLU activations, group query attention and QKA\\nweight merging.\\nAlgorithm 1 Additive Quantization for LLM\\nRequire: model, data\\n1: Xblock := model.input_embeddings(data)\\n2: for i = 1, . . . , model.num_layers do\\n3:\\nblock := model.get_block(i)\\n4:\\nYblock := block(Xblock)\\n5:\\nfor layer ∈ linear_layers(block) do\\n6:\\nW := layer.weight\\n7:\\nX := layer_inputs(layer, Xblock)\\n8:\\nC, b, s := initialize(W)\\n// k-means\\n9:\\nwhile loss improves by at least τ do\\n10:\\nC, s := train_Cs_adam(XXT , W, C, b, s)\\n11:\\nb := beam_search(XXT , W, C, b, s)\\n12:\\nend while\\n13:\\n/* save for fine-tuning */\\n14:\\nlayer.weight := AQFormat(C, b, s)\\n15:\\nend for\\n16:\\nθ := trainable_parameters(block)\\n17:\\nwhile loss improves by at least τ do\\n18:\\nL := ||block(Xblock) − Yblock||2\\n2\\n19:\\nθ := adam(θ, ∂L\\n∂θ )\\n20:\\nend while\\n21:\\nYblock := block(Xblock)\\n22: end for\\nSimilarly to Section 3.3, we train these parameters using\\nAdam optimizer to minimize the mean squared error against\\nthe original block outputs (prior to quantization). This phase\\nuses the same calibration data as for the individual layer\\nquantization. The full procedure is summarized in Alg. 1.\\nWhile fine-tuning transformer blocks is more expensive than\\nindividual linear layers, it is still possible to quantize billion-\\nparameter models on a single GPU. Furthermore, since the\\nproposed algorithm only modifies the few remaining train-\\nable parameters, it uses relatively little VRAM for Adam\\noptimizer states. The fine-tuning procedure typically con-\\nverges after a few epochs, likely because it starts from a\\ngood initial guess. In practice, fine-tuning transformer lay-\\ners takes up a minority (10-30%) of the total calibration time.\\nHowever, the entire procedure takes longer than single-pass\\nquantization algorithms such as GPTQ.\\n4. Experiments\\nWe evaluate the AQLM algorithm in typical scenarios for\\npost-training quantization of modern LLMs. Our evalua-\\ntion is focused on the LLAMA 2 model family since it is\\na popular backbone for fine-tuned models or general LLM\\napplications, e.g. (Dettmers et al., 2023a). We organize the\\nexperiments as follows: in Section 4.1 we evaluate the full\\nAQ procedure for various LLAMA 2 models and quantiza-\\ntion bit widths, and Section 4.2 presents an ablation analysis\\nfor individual AQ components and implementation details.\\n6\\nExtreme LLM Compression of Using Additive Quantization\\nm\\nmΣ\\ni\\nj\\nm\\ni\\nj\\nm\\ni\\nj\\nm\\ni\\nj\\nCodes\\nCodes\\nCodebooks\\nExpanded Codes\\nUnscaled Rows\\nWeight Matrix\\nScales\\nFigure 3: AQLM compressed weight format. Horizontal and vertical axes are input features and output units, respectively.\\nDepth represents the codebook index. Reconstruction procedure, from left to right: i) compressed weight codes ii) zoom-in\\none weight group, each code is an index in its respective codebook iii) select codes from each codebook iv) add up codes as\\nin (3) v) multiply by scales (one scale per output dimension).\\nTable 1: Evaluation of quantized Llama 2 models for 2-2.1 bits per parameter. The table reports perplexity on Wiki-\\nText2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy\\ncolumn is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy.\\nSize Method Avg bits Wiki2↓\\nC4↓\\nWinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑\\n7B\\n–\\n16.00\\n5.12\\n6.63\\n67.25\\n78.45\\n56.69\\n69.32\\n40.02\\n62.35\\nAQLM\\n2.02\\n6.93\\n8.84\\n64.64\\n73.50\\n48.23\\n62.84\\n33.62\\n56.57\\nQuIP\\n2.020\\n222.54 145.03\\n51.78\\n54.62\\n27.25\\n26.01\\n19.37\\n35.81\\nQuIP#\\n2.02\\n8.22\\n11.01\\n62.43\\n71.38\\n42.94\\n55.56\\n28.84\\n52.23\\n13B\\n–\\n16.00\\n4.57\\n6.05\\n69.61\\n78.73\\n59.72\\n73.27\\n45.56\\n65.38\\nAQLM\\n1.970\\n5.70\\n7.59\\n69.46\\n75.08\\n53.44\\n66.96\\n38.48\\n60.68\\nQuIP\\n2.00\\n13.48\\n16.16\\n52.80\\n62.02\\n35.80\\n45.24\\n23.46\\n43.86\\nQuIP#\\n2.011\\n6.06\\n8.07\\n63.38\\n74.76\\n51.58\\n64.06\\n33.96\\n57.55\\n70B\\n–\\n16.00\\n3.12\\n4.97\\n76.95\\n81.07\\n63.99\\n77.74\\n51.11\\n70.17\\nAQLM\\n2.070\\n3.94\\n5.72\\n75.93\\n80.43\\n61.79\\n77.68\\n0.4793\\n68.75\\nQuIP\\n2.01\\n5.90\\n8.17\\n67.48\\n74.76\\n50.45\\n62.16\\n33.96\\n57.76\\nQuIP#\\n2.01\\n4.16\\n6.01\\n74.11\\n79.76\\n60.01\\n76.85\\n47.61\\n67.67\\n4.1. LLAMA 2 compression\\nTo evaluate the quality of AQ-quantized models, we report\\nperplexity on WikiText2 (Merity et al., 2016) and C4 (Raffel\\net al., 2020) validation sets. We also measure zero-shot ac-\\ncuracy on five tasks: WinoGrande (Sakaguchi et al., 2021),\\nPiQA (Tata & Patel, 2003), HellaSwag (Zellers et al., 2019),\\nARC-easy and ARC-challenge (Clark et al., 2018) via the\\nLM Evaluation Harness (Gao et al., 2021). We run all evalu-\\nations with the same hyperparameters as in GPTQ (Frantar\\net al., 2022a), except for the increased number of training\\ncalibration sequences.\\nWe consider three main targets in terms of compression\\nrange: 2-2.1 bits, 3-3.1 bits, and 4-4.1 bits per model pa-\\nrameter. The full number of bits for each algorithm in-\\ncludes scales, codebooks, zero points, but not the (non-\\nquantized) word embeddings and logits. We compare AQ\\nagainst GPTQ for 3&4 bits (Frantar et al., 2022a), SpQR for\\n3&4 bits (Dettmers et al., 2023b), QuIP in 2,3 & 4 bits (Chee\\net al., 2023) and QUIP# for 2&4 bits (Tseng et al.). While\\nGPTQ and SpQR technically support 2-bit quantization,\\nthese algorithms were designed for 3+ bits per parameter\\nand perform poorly in terms of accuracy in the 2-2.1 bit\\nrange. For QuIP, we omit results for the 7B model, as we\\ncould not achieve competitive performance in this one sce-\\nnario using the available4 implementations. For QuIP#, we\\nfocus on 2 and 4 bit because the available implementation\\ndoes not yet support 3-bit compression. While there are\\nseveral ways to modify QuIP to support 3 bits, it is unclear\\nwhich way is more natural. We calibrate each quantization\\nalgorithm using the same portion of RedPajama calibration\\ndataset (Computer, 2023), with a sequence length of 4096.\\nWe report the results in Tables 1 2 and 3 (for 2, 3 and 4\\nbits respectively), both in terms of perplexity and zero-shot\\naccuracy. To summarize, AQLM outperforms the previous\\nbest PTQ algorithms across all settings, but the margin of\\nimprovement varies significantly. For example, we observe\\nthe highest gains in perplexity in the “extreme” 2-2.1 bits per\\n4Unfortunately, there is no official implementation of the\\noriginal QuIP (non-#) for the LLAMA 2 model, see https:\\n//github.com/Cornell-RelaxML/QuIP/issues/2.\\n7\\nExtreme LLM Compression of Using Additive Quantization\\nTable 2: Evaluation of quantized Llama 2 models for 3-3.1 bits per parameter. The table reports perplexity on Wiki-\\nText2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy\\ncolumn is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy.\\nSize Method Avg bits Wiki2↓ C4↓\\nWinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑\\n7B\\n–\\n16.00\\n5.12\\n6.63\\n67.25\\n78.45\\n56.69\\n69.32\\n40.02\\n62.35\\nAQLM\\n3.04\\n5.46\\n7.10\\n68.35\\n76.61\\n54.36\\n67.76\\n38.65\\n61.15\\nGPTQ\\n3.00\\n8.28\\n10.72\\n60.30\\n71.65\\n45.60\\n56.48\\n31.23\\n53.05\\nSpQR\\n2.98\\n6.21\\n8.23\\n63.38\\n74.27\\n51.32\\n62.21\\n34.13\\n57.06\\nQuIP\\n3.00\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n13B\\n–\\n16.00\\n4.57\\n6.05\\n69.61\\n78.73\\n59.72\\n73.27\\n45.56\\n65.38\\nAQLM\\n3.026\\n4.83\\n6.37\\n67.64\\n77.80\\n58.22\\n73.48\\n43.60\\n64.15\\nGPTQ\\n3.00\\n5.87\\n7.90\\n64.17\\n75.14\\n53.70\\n68.14\\n39.16\\n60.06\\nSpQR\\n2.98\\n5.28\\n7.07\\n67.96\\n76.93\\n55.72\\n68.06\\n38.48\\n61.43\\nQuIP\\n3.00\\n5.12\\n6.79\\n69.93\\n76.88\\n57.07\\n70.41\\n41.47\\n63.15\\n70B\\n–\\n16.00\\n3.12\\n4.97\\n76.95\\n81.07\\n63.99\\n77.74\\n51.11\\n70.17\\nAQLM\\n3.01\\n3.36\\n5.17\\n77.19\\n81.28\\n63.23\\n77.61\\n50.00\\n69.86\\nGPTQ\\n3.00\\n4.41\\n6.26\\n73.72\\n78.73\\n59.79\\n73.65\\n44.20\\n66.02\\nSpQR\\n2.98\\n3.85\\n5.63\\n74.35\\n80.41\\n61.65\\n75.88\\n46.16\\n67.69\\nQuIP\\n3.01\\n3.87\\n5.67\\n74.59\\n79.98\\n60.73\\n73.19\\n46.33\\n66.96\\nTable 3: Evaluation of quantized Llama 2 models for 4-4.1 bits per parameter. The table reports perplexity on Wiki-\\nText2 (Merity et al., 2016) and C4 (Raffel et al., 2020), as well as accuracy for zero-shot tasks. The Average accuracy\\ncolumn is the mean of 5 zero-shot task accuracies. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average accuracy.\\nSize Method Avg bits Wiki2↓ C4↓ WinoGrande↑ PiQA↑ HellaSwag↑ ArcE↑ ArcC↑ Average accuracy↑\\n7B\\n–\\n16.00\\n5.12\\n6.63\\n67.25\\n78.45\\n56.69\\n69.32\\n40.02\\n62.35\\nAQLM\\n4.044\\n5.21\\n6.74\\n67.32\\n77.69\\n56.10\\n68.90\\n40.27\\n62.06\\nGPTQ\\n4.000\\n5.49\\n7.20\\n67.17\\n76.82\\n55.17\\n67.42\\n40.02\\n61.32\\nSpQR\\n3.980\\n5.28\\n6.87\\n66.54\\n77.42\\n55.97\\n68.14\\n39.85\\n61.58\\nQuIP\\n4.000\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nQuIP#\\n4.020\\n5.29\\n6.86\\n66.85\\n77.91\\n55.78\\n68.06\\n39.68\\n61.66\\n13B\\n–\\n16.00\\n4.57\\n6.05\\n69.61\\n78.73\\n59.72\\n73.27\\n45.56\\n65.38\\nAQLM\\n3.938\\n4.64\\n6.14\\n70.40\\n78.56\\n59.23\\n72.22\\n43.94\\n64.87\\nGPTQ\\n4\\n4.78\\n6.34\\n68.51\\n77.48\\n58.57\\n69.82\\n42.58\\n63.39\\nSpQR\\n3.980\\n4.69\\n6.20\\n68.82\\n78.13\\n59.11\\n72.90\\n44.62\\n64.72\\nQuIP\\n4.00\\n4.76\\n6.29\\n69.69\\n79.00\\n58.91\\n73.27\\n44.88\\n65.15\\nQuIP#\\n4.011\\n4.68\\n6.20\\n69.38\\n77.91\\n58.86\\n73.74\\n44.63\\n64.90\\n70B\\n–\\n16.00\\n3.12\\n4.97\\n76.95\\n81.07\\n63.99\\n77.74\\n51.11\\n70.17\\nAQLM\\n4.07\\n3.17\\n5.01\\n76.90\\n81.41\\n63.64\\n78.09\\n51.00\\n70.21\\nGPTQ\\n4.00\\n3.34\\n5.15\\n75.45\\n81.56\\n63.24\\n76.56\\n49.57\\n69.28\\nSpQR\\n3.97\\n3.24\\n5.07\\n76.48\\n81.12\\n63.70\\n76.05\\n50.34\\n69.54\\nQuIP\\n4.00\\n3.58\\n5.38\\n76.01\\n80.25\\n61.97\\n74.28\\n47.01\\n67.90\\nQuIP#\\n4.01\\n3.22\\n5.05\\n76.80\\n81.45\\n63.51\\n78.37\\n50.85\\n70.20\\nparameter range. One possible explanation for this is that\\nlayerwise fine-tuning (defined in Section 3.4) gives the most\\nbenefit in regions with lower compression bit width because\\nthese regions have higher error in the first place. Conversely,\\nwe observe that 3 and 4-bit quantizers are already fairly\\nclose to unquantized (FP16) model perplexity.\\n4.2. Ablation analysis\\nThe AQLM algorithm makes several design choices that\\nneed to be validated separately: the initialization, the alter-\\nnating optimization, the fine-tuning protocol, and the choice\\nof hyperparameters. Here, we study how each of these\\ncomponents affect the overall algorithm.\\n8\\nExtreme LLM Compression of Using Additive Quantization\\nInitialization. As we discuss in Section 3, we initialize\\nAQLM with residual K-means to obtain a good initial guess\\nfor both codes and codebooks. That is, we run K-means for\\nthe weight matrix, then subtract the nearest cluster from each\\nweight, and run K-means again M times. A simple baseline\\nwould be to initialize all codes uniformly at random. We\\ncompare the two initialization strategies for the problem of\\nquantizing a single linear layer within LLAMA 2 70B model\\nto 3 bits per parameter. We quantize groups of 8 consecutive\\nweights using 2 codebooks, 12 bit each. Each codebook\\ncontains 212 learnable values. As we can see in Figure 4,\\nAQLM with K-means initialization needs significantly fewer\\ntraining iterations to achieve the desired loss. The difference\\nis so drastic that we expect that running AQLM with a\\nrandom initialization would require extremely high runtimes\\nto accurately quantize the largest models.\\n0\\n250\\n500\\n750\\n1000\\n1250\\n1500\\n1750\\n2000\\nSteps\\n10\\n3\\n10\\n2\\n10\\n1\\n10\\n0\\nMSE\\nRandom\\nK-Means\\nFigure 4: MSE loss learning curves of AQLM trained on\\nthe self attention q_proj linear layer of 10-th block in the\\nLLAMA 2 70B model.\\nFine-tuning. Next, we validate the fine-tuning procedure.\\nWe compare the full block fine-tuning (default) against three\\nalternatives: i) no fine-tuning at all, ii) fine-tuning only non-\\nlinear layers (i.e. RMSNorm), but not the AQ parameters,\\nand iii) fine-tuning only the AQ parameters, but not the non-\\nlinear layers. Table 4 summarizes our results: fine-tuning\\nthe entire model or only AQ parameters achieves compet-\\nitive performance, while training only RMSNorm scales\\nis comparable to not fine-tuning at all. We attribute these\\nobservations to the fact that over 99% of quantized layer\\nparameters are contained in AQ codebooks Cm, whereas\\nthe remaining parameters are small 1-dimensional tensors.\\nThis validates the use of the AQ approach, as many compet-\\ning algorithms do not have learnable per-layer codebooks.\\nNotably, QuIP# uses a shared fixed lattice instead. We also\\nnote that, even without fine-tuning, AQLM is competitive\\nto previous state-of-the-art results.\\nTable 4: Ablation analysis of AQLM with different fine-\\ntuning restrictions on Llama-2 7B model at 2.02 bit width.\\nName\\nWiki2↓\\nC4↓\\nw/o\\n8.18\\n10.59\\nRMSnorm\\n8.31\\n10.46\\nAQ params\\n6.92\\n8.85\\nFull\\n6.93\\n8.84\\nNumber of samples. Finally, we verify our choice of cal-\\nibration hyperparameters. Traditionally, most PTQ algo-\\nrithms use several hundred calibration sequences (e.g. Fran-\\ntar et al. (2022a) has 128). In our experiments, we evaluate\\nboth AQLM and baselines with additional calibration data.\\nOur original motivation for that was to avoid potential over-\\nfitting when fine-tuning entire transformer blocks. To test\\nthis assumption, we run our algorithm with different calibra-\\ntion set sizes, varying from 128 to 4096 sequences. For each\\nsize, we report the average perplexity on WikiText2 over 3\\nruns, along with standard deviations. The results in Table 5\\ndemonstrate that increasing the number of samples leads\\nto gradual reduction in perplexity with seemingly dimin-\\nishing returns. Since the perplexity is still monotonically\\nimproving from 128 to 4096 samples, it is possible that\\nlarger sample sizes would yield further improvements.\\nTable 5: Wikitext2 PPL as a function of calibration set size\\nfor Llama 2 (7B) quantized to 2.3 bits with AQLM, averaged\\nover 3 runs. SD stands for adjusted standard deviation.\\n# of samples\\nAverage PPL\\nSD\\n128\\n6.994\\n0.127\\n256\\n6.584\\n0.031\\n512\\n6.455\\n0.005\\n1024\\n6.353\\n0.008\\n2048\\n6.297\\n0.018\\n4096\\n6.267\\n0.005\\n5. Conclusion and Future Work\\nWe presented AQLM, a variant of additive quantization (AQ)\\ntargeted to accurate LLM compression. Specifically, AQLM\\nadapts AQ to the layer-wise quantization problem by making\\nit instance-aware, by taking the layer input distributions into\\naccount into the codebook optimization. Moreover, we\\ncombine this approach with a block fine-tuning approach,\\nwhich allows us to further reduce quantization error across\\nlayers. To our knowledge, AQLM improves the current\\nstate-of-the-art results for LLM quantization in the regime\\nof 2 and 3 bits per weight.\\nIn terms of limitations, AQLM is more computationally-\\nexpensive relative to existing direct post-training quantiza-\\ntion methods, such as RTN or GPTQ. Moreover, as detailed\\nin the experiments, we have found it to be sensitive to the\\nchoice of parameters, such as the number of codebooks, or\\nthe initialization of centroids. Further, since it uses a more\\nsophisticated encoding, its implementation will be more\\ncomplex than direct quantization methods.\\nIn future work, we plan to provide an efficient GPU imple-\\nmentation of AQLM. Further, we intend to conduct a more\\ndetailed analysis of the importance of specific parameters\\non the quality of compression, as well as end-to-end joint\\nfine-tuning of codebook parameters, across the entire model.\\n9\\nExtreme LLM Compression of Using Additive Quantization\\n6. Acknowledgements\\nAuthors would like to thank Ruslan Svirschevski for his\\nhelp in solving technical issues with AQLM and baselines.\\nWe also thank Tim Dettmers for helpful discussions on the\\nstructure of weights in modern LLMs and size-accuracy\\ntrade-offs. Finally, authors would like to thank the com-\\nmunities of ML enthusiasts known as LocalLLaMA5 and\\nPetals community on discord6 for the crowd wisdom about\\nrunning LLMs on consumer devices.\\nReferences\\nBabenko, A. and Lempitsky, V. Additive quantization for\\nextreme vector compression. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npp. 931–938, 2014.\\nBesag, J. On the statistical analysis of dirty pictures. Jour-\\nnal of the Royal Statistical Society Series B: Statistical\\nMethodology, 48(3):259–279, 1986.\\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H.,\\nO’Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\\nlyzing large language models across training and scaling.\\narXiv preprint arXiv:2304.01373, 2023.\\nBlalock, D. and Guttag, J. Multiplying matrices without\\nmultiplying. In International Conference on Machine\\nLearning, pp. 992–1004. PMLR, 2021.\\nBurton, D., Shore, J., and Buck, J. A generalization of\\nisolated word recognition using vector quantization. In\\nICASSP ’83. IEEE International Conference on Acoustics,\\nSpeech, and Signal Processing, volume 8, pp. 1021–1024,\\n1983. doi: 10.1109/ICASSP.1983.1171915.\\nChee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit\\nquantization of large language models with guarantees,\\n2023.\\nChen, S., Wang, W., and Pan, S. J. Deep neural network\\nquantization via layer-wise optimization using limited\\ntraining data.\\nProceedings of the AAAI Conference\\non\\nArtificial\\nIntelligence,\\n33(01):3329–3336,\\nJul.\\n2019.\\ndoi:\\n10.1609/aaai.v33i01.33013329.\\nURL\\nhttps://ojs.aaai.org/index.php/AAAI/\\narticle/view/4206.\\nChen, Y., Guan, T., and Wang, C. Approximate nearest\\nneighbor search by residual vector quantization. Sensors,\\n10(12):11259–11273, 2010.\\n5https://www.reddit.com/r/LocalLLaMA/\\n6https://github.com/bigscience-workshop/\\npetals/\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\nquestion answering? try arc, the ai2 reasoning challenge.\\narXiv preprint arXiv:1803.05457, 2018.\\nComputer,\\nT.\\nRedpajama:\\nan open dataset for\\ntraining\\nlarge\\nlanguage\\nmodels,\\n2023.\\nURL\\nhttps://github.com/togethercomputer/\\nRedPajama-Data.\\nDettmers, T. and Zettlemoyer, L. The case for 4-bit pre-\\ncision: k-bit inference scaling laws.\\narXiv preprint\\narXiv:2212.09720, 2022.\\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\\nLLM.int8(): 8-bit matrix multiplication for transformers\\nat scale. Advances in Neural Information Processing\\nSystems 35: Annual Conference on Neural Information\\nProcessing Systems 2022, NeurIPS 2022, 2022.\\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\\nL. QLoRA: Efficient finetuning of quantized llms. arXiv\\npreprint arXiv:2305.14314, 2023a.\\nDettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,\\nD., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,\\nand Alistarh, D. Spqr: A sparse-quantized representation\\nfor near-lossless llm weight compression. arXiv preprint\\narXiv:2306.03078, 2023b.\\nFernández-Marqués, J., AbouElhamayed, A. F., Lane,\\nN. D., and Abdelfattah, M. S.\\nAre we there yet?\\nproduct\\nquantization\\nand\\nits\\nhardware\\naccelera-\\ntion.\\nArXiv, abs/2305.18334, 2023.\\nURL https:\\n//api.semanticscholar.org/CorpusID:\\n258967539.\\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\\nAccurate post-training quantization for generative pre-\\ntrained transformers. arXiv preprint arXiv:2210.17323,\\n2022a.\\nFrantar, E., Singh, S. P., and Alistarh, D.\\nOptimal\\nBrain Compression: A framework for accurate post-\\ntraining quantization and pruning.\\narXiv preprint\\narXiv:2208.11580, 2022b. Accepted to NeurIPS 2022, to\\nappear.\\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A.,\\nFoster, C., Golding, L., Hsu, J., McDonell, K., Muen-\\nnighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A.,\\nWang, B., Wang, K., and Zou, A. A framework for few-\\nshot language model evaluation, September 2021. URL\\nhttps://doi.org/10.5281/zenodo.5371628.\\nGe, T., He, K., Ke, Q., and Sun, J. Optimized product\\nquantization. IEEE transactions on pattern analysis and\\nmachine intelligence, 36(4):744–755, 2013.\\n10\\nExtreme LLM Compression of Using Additive Quantization\\nGholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,\\nand Keutzer, K.\\nA survey of quantization methods\\nfor efficient neural network inference. arXiv preprint\\narXiv:2103.13630, 2021.\\nGray, R. Vector quantization. IEEE ASSP Magazine, 1(2):\\n4–29, 1984. doi: 10.1109/MASSP.1984.1162229.\\nHubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry,\\nD. Accurate post training quantization with small cal-\\nibration sets. In International Conference on Machine\\nLearning (ICML), 2021.\\nJegou, H., Douze, M., and Schmid, C. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern\\nanalysis and machine intelligence, 33(1):117–128, 2010.\\nKim, S., Hooper, C., Gholami, A., Dong, Z., Li,\\nX., Shen, S., Mahoney, M. W., and Keutzer, K.\\nSqueezellm:\\nDense-and-sparse quantization.\\narXiv\\npreprint arXiv:2306.07629, 2023.\\nKingma, D. P. and Ba, J. Adam: A method for stochas-\\ntic optimization. International Conference on Learning\\nRepresentations (ICLR), 2015.\\nLi, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu,\\nF., Wang, W., and Gu, S. BRECQ: Pushing the limit of\\npost-training quantization by block reconstruction. In\\nInternational Conference on Learning Representations\\n(ICLR), 2021.\\nLi, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Perfor-\\nmance guaranteed network acceleration via high-order\\nresidual quantization, 2017.\\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., and\\nHan, S.\\nAwq: Activation-aware weight quantization\\nfor llm compression and acceleration. arXiv preprint\\narXiv:2306.00978, 2023.\\nMartinez, J., Clement, J., Hoos, H. H., and Little, J. J.\\nRevisiting additive quantization. In Computer Vision–\\nECCV 2016: 14th European Conference, Amsterdam, The\\nNetherlands, October 11-14, 2016, Proceedings, Part II\\n14, pp. 137–153. Springer, 2016.\\nMartinez, J., Zakhmi, S., Hoos, H. H., and Little, J. J. Lsq++:\\nLower running time and higher recall in multi-codebook\\nquantization. In Proceedings of the European Conference\\non Computer Vision (ECCV), pp. 491–506, 2018.\\nMcCarter, C. and Dronen, N.\\nLook-ups are not\\n(yet)\\nall\\nyou\\nneed\\nfor\\ndeep\\nlearning\\ninference.\\nArXiv,\\nabs/2207.05808,\\n2022.\\nURL\\nhttps:\\n//api.semanticscholar.org/CorpusID:\\n250491319.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\\nPointer sentinel mixture models.\\narXiv preprint\\narXiv:1609.07843, 2016.\\nNagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and\\nBlankevoort, T. Up or down? Adaptive rounding for\\npost-training quantization. In International Conference\\non Machine Learning (ICML), 2020.\\nNorouzi, M. and Fleet, D. J. Cartesian k-means. In Pro-\\nceedings of the IEEE Conference on computer Vision and\\nPattern Recognition, pp. 3017–3024, 2013.\\nOzan, E. C., Kiranyaz, S., and Gabbouj, M.\\nCom-\\npetitive quantization for approximate nearest neighbor\\nsearch.\\nIEEE Transactions on Knowledge and Data\\nEngineering, 28(11):2884–2894, 2016. doi: 10.1109/\\nTKDE.2016.2597834.\\nPark, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee,\\nD. nuQmm: Quantized matmul for efficient inference of\\nlarge-scale generative language models. arXiv preprint\\narXiv:2206.09557, 2022.\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\\nBai, J., and Chintala, S. PyTorch: An imperative style,\\nhigh-performance deep learning library. In Conference on\\nNeural Information Processing Systems (NeurIPS). 2019.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y., Li, W., and Liu, P. Exploring\\nthe limits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research, 21\\n(140):1–67, 2020.\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,\\nY. Winogrande: an adversarial winograd schema chal-\\nlenge at scale.\\nCommun. ACM, 64(9):99–106, 2021.\\ndoi: 10.1145/3474381.\\nURL https://doi.org/\\n10.1145/3474381.\\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow,\\nD., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M.,\\net al. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\\nTata, S. and Patel, J. M. PiQA: An algebra for querying pro-\\ntein data sets. In International Conference on Scientific\\nand Statistical Database Management, 2003.\\nTII UAE.\\nThe Falcon family of large language\\nmodels.\\nhttps://huggingface.co/tiiuae/\\nfalcon-40b, May 2023.\\n11\\nExtreme LLM Compression of Using Additive Quantization\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation lan-\\nguage models. arXiv preprint arXiv:2302.13971, 2023.\\nTseng, A., Chee, J., Sun, Q., Kuleshov, V., and Sa, C. D.\\nQuip#: Quip with lattice codebooks.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\\nis all you need. arXiv preprint arXiv:1706.03762, 2017.\\nWang, P., Chen, Q., He, X., and Cheng, J. Towards accurate\\npost-training network quantization via bit-split and stitch-\\ning. In International Conference on Machine Learning\\n(ICML), 2020.\\nXiao, G., Lin, J., Seznec, M., Demouth, J., and Han,\\nS. Smoothquant: Accurate and efficient post-training\\nquantization for large language models. arXiv preprint\\narXiv:2211.10438, 2022.\\nYao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and\\nHe, Y. Zeroquant: Efficient and affordable post-training\\nquantization for large-scale transformers. arXiv preprint\\narXiv:2206.01861, 2022.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\\nY. Hellaswag: Can a machine really finish your sentence?\\nIn Korhonen, A., Traum, D. R., and Màrquez, L. (eds.),\\nProceedings of the 57th Conference of the Association\\nfor Computational Linguistics, ACL 2019, Florence, Italy,\\nJuly 28- August 2, 2019, Volume 1: Long Papers, pp.\\n4791–4800. Association for Computational Linguistics,\\n2019. doi: 10.18653/v1/p19-1472. URL https://\\ndoi.org/10.18653/v1/p19-1472.\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\\net al. Opt: Open pre-trained transformer language models.\\narXiv preprint arXiv:2205.01068, 2022.\\nZhang, T., Du, C., and Wang, J. Composite quantization for\\napproximate nearest neighbor search. In International\\nConference on Machine Learning, pp. 838–846. PMLR,\\n2014.\\nZhou, S.-C., Wang, Y.-Z., Wen, H., He, Q.-Y., and Zou,\\nY.-H. Balanced quantization: An effective and efficient\\napproach to quantized neural networks. Journal of Com-\\nputer Science and Technology, 32(4):667–682, Jul 2017.\\nISSN 1860-4749.\\ndoi: 10.1007/s11390-017-1750-y.\\nURL https://doi.org/10.1007/s11390-017-\\n1750-y.\\n12\\n'}, 'http://arxiv.org/abs/2401.06104v1': {'title': 'Transformers are Multi-State RNNs', 'published_date': datetime.datetime(2024, 1, 11, 18, 35, 26), 'pdf_link': 'http://arxiv.org/pdf/2401.06104v1', 'summary': 'Transformers are considered conceptually different compared to the previous\\ngeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).\\nIn this work, we demonstrate that decoder-only transformers can in fact be\\nconceptualized as infinite multi-state RNNs - an RNN variant with unlimited\\nhidden state size. We further show that pretrained transformers can be\\nconverted into $\\\\textit{finite}$ multi-state RNNs by fixing the size of their\\nhidden state. We observe that several existing transformers cache compression\\ntechniques can be framed as such conversion policies, and introduce a novel\\npolicy, TOVA, which is simpler compared to these policies. Our experiments with\\nseveral long range tasks indicate that TOVA outperforms all other baseline\\npolicies, while being nearly on par with the full (infinite) model, and using\\nin some cases only $\\\\frac{1}{8}$ of the original cache size. Our results\\nindicate that transformer decoder LLMs often behave in practice as RNNs. They\\nalso lay out the option of mitigating one of their most painful computational\\nbottlenecks - the size of their cache memory. We publicly release our code at\\nhttps://github.com/schwartz-lab-NLP/TOVA.', 'pdf_text': 'Transformers are Multi-State RNNs\\nMatanel Oren∗,H\\nMichael Hassid∗,H,M\\nYossi AdiH,M\\nRoy SchwartzH\\nHThe Hebrew University of Jerusalem\\nMFAIR, AI at Meta\\nAbstract\\nTransformers are considered conceptually dif-\\nferent compared to the previous generation of\\nstate-of-the-art NLP models—recurrent neural\\nnetworks (RNNs). In this work, we demon-\\nstrate that decoder-only transformers can in\\nfact be conceptualized as infinite multi-state\\nRNNs—an RNN variant with unlimited hid-\\nden state size. We further show that pretrained\\ntransformers can be converted into finite multi-\\nstate RNNs by fixing the size of their hidden\\nstate. We observe that several existing trans-\\nformers cache compression techniques can be\\nframed as such conversion policies, and intro-\\nduce a novel policy, TOVA,1 which is sim-\\npler compared to these policies. Our experi-\\nments with several long range tasks indicate\\nthat TOVA outperforms all other baseline poli-\\ncies, while being nearly on par with the full (in-\\nfinite) model, and using in some cases only 1/8\\nof the original cache size. Our results indicate\\nthat transformer decoder LLMs often behave in\\npractice as RNNs. They also lay out the option\\nof mitigating one of their most painful com-\\nputational bottlenecks—the size of their cache\\nmemory. We publicly release our code.2\\n1\\nIntroduction\\nNot so long ago, transformers (Vaswani et al., 2017)\\nreplaced recurrent neural networks (RNNs; Elman,\\n1990) as the go-to architecture for NLP. Trans-\\nformers are considered conceptually different than\\nRNNs, as they have direct access to each token in\\nthe sequence, rather than RNNs that maintain a re-\\ncurring state of previous inputs. Recently, decoders\\nbecame a dominant transformer variant (Touvron\\net al., 2023a; Jiang et al., 2023). These typically\\ngenerate their output auto-regressively, where the\\ngeneration of each token depends on the key and\\n∗Equal contribuation\\n1Token Omission Via Attention.\\nLiterally “good” in\\nHebrew.\\n2https://github.com/schwartz-lab-NLP/TOVA\\nInfinite\\nkv0\\nkv1 kv2 q2\\nkv0\\nkv1 kv2 kv3 q3\\nkv0\\nqn\\nkv1 kv2 kv3\\nkvn\\nFinite\\nkv0\\nkv1 kv2 q2\\nkv0\\nq3\\nkv2 kv3\\nkv0\\nqn\\nkv9 kvn\\nFigure 1: Top: transformers can be thought of as infi-\\nnite multi-state RNNs (MSRNNs), with the key/value\\nvectors corresponding to a multi-state that dynamically\\ngrows infinitely (green elements). Bottom: transform-\\ners behave in many cases as finite MSRNNs (bottom),\\nwhich keep a fixed-size multi-state (here of size 2) by\\ndropping one state (red element) at each decoding step.\\nvalue computation of previous tokens.3\\nIn this work, we demonstrate that this auto-\\nregressivity aligns with the core principle of\\nRNNs—preserving a state from one step to the\\nnext one. Based on this observation, we formally\\nredefine decoder-only transformers as a form of\\nmulti-state RNNs (MSRNN)—a generalized ver-\\nsion of traditional RNNs. Importantly, as the num-\\nber of previous tokens grows with each decoding\\nstep, transformers correspond to MSRNNs with an\\ninfinite number of states (Fig. 1, top). We continue\\nby showing that transformers can be compressed\\ninto finite MSRNNs by limiting the number of to-\\nkens processed at each step (Fig. 1, bottom). We\\nthen consider previous work, which applied com-\\npression policies that effectively limit this capacity\\nin pretrained transformer-base LLMs (Zhang et al.,\\n2023; Xiao et al., 2023; Han et al., 2023). Our defi-\\nnition frames these works as converting pretrained\\ntransformers from infinite into finite MSRNNs.\\nWe continue by proposing TOVA, a simpler\\n3These previous computations are often cached for effi-\\nciency purposes, referred to as KV caching (Radford et al.,\\n2019; Pope et al., 2022). We note that the arguments we make\\nin this work apply similarly to non-cached implementations.\\narXiv:2401.06104v1  [cs.CL]  11 Jan 2024\\nyet more powerful MSRNN compression policy.\\nTOVA selects which tokens to keep in the multi-\\nstate based solely on their attention scores. We eval-\\nuate TOVA on four long range tasks. Our results\\nshow that it outperforms all existing policies, and\\nleads to minimal performance degradation com-\\npared to the infinite MSRNN, using, in some cases,\\nas little as 1/8–1/4 of the context.\\nWe finish by analyzing the tokens kept in mem-\\nory by our method. Unlike previous work (Xiao\\net al., 2023; Zhang et al., 2023), we observe that not\\nall recent tokens are important to keep in memory,\\nand some can be safely dropped. Moreover, we\\nshow the importance of keeping the very first token\\nin the sequence, and highlight other, perhaps sur-\\nprising important tokens such as possessive nouns.\\nOur results shed light on the behavior of trans-\\nformer decoder LLMs; while they are trained as\\ninfinite MSRNNs, they often perform in practice as\\nfinite MSRNNs. Our results also have practical ben-\\nefits. Our proposed method substantially reduces\\nmemory consumption during inference, leading to\\nup to 88% reduction in LLM cache size.\\n2\\nBackground\\nWe briefly introduce RNNs (Sec. 2.1) and trans-\\nformers (Sec. 2.2). Throughout this work, we as-\\nsume a model with a hidden dimension size d.\\n2.1\\nRNNs\\nRecurrent Neural Networks (RNNs; Elman, 1990)\\nare a family of deep learning architectures that\\nprocess sequential data in a recurrent manner. In\\nthe most general form, each layer l (often called a\\n“cell”) is modeled as a function fl\\nRNN that receives at\\ntime t two inputs: xl\\nt, a representation of the token\\nt, and hl\\nt−1, the hidden state from the previous time\\nstep. It then outputs two values: xl+1\\nt\\n, an updated\\ntoken representation, and hl\\nt, a new hidden state:\\nxl+1\\nt\\n, hl\\nt = fl\\nRNN(xl\\nt, hl\\nt−1)\\n(1)\\nhl\\nt is used for the recurrent computation over the\\nnext token xl\\nt+1, while xl+1\\nt\\nis used as input for\\nthe next layer. It is common, though not necessary,\\nto set xl+1\\nt\\n:= hl\\nt, i.e., the input for the following\\nlayer and the hidden state are the same.\\n2.2\\nTransformers\\nTransformers (Vaswani et al., 2017) also process\\nsequential data, but do so non-recurrently. A trans-\\nformer layer fl\\nTRANS takes as input a sequence of\\ntoken representations: Xl = (xl\\n1, ..., xl\\nt) ∈ Rt×d,\\nand returns a transformed representation:\\nXl+1 = fl\\nTRANS(Xl) = FFlWe can interpret each row of Hl\\nt as a single-state,\\nallowing us to think of Hl\\nt as a multi-state matrix.\\nBy defining g(t) = 1 for all t, MSRNN reduces to\\na standard (single-state) RNN.\\n3.2\\nTransformers are Infinite MSRNNs\\nConsider the case where g(t) = t, in which the\\nnumber of single-states equals the number of input\\ntokens in the corresponding time-step. In this setup,\\nwe can view the transformer as an MSRNN, where\\nHl\\nt = (Kl\\nt, V l\\nt ) and the layer computation is:\\n(Kl\\nt, V l\\nt ) =\\n\\x10\\x10Kl\\nt−1\\nkl\\nt\\n\\x11\\n,\\n\\x10V l\\nt−1\\nvl\\nt\\n\\x11\\x11\\n(6)\\nxl+1\\nt\\n= FFl`\\nkv0\\n0.2\\nkv1\\n0.1\\nkv2\\n0.1\\nkv3\\n0.2\\nkv4\\n0.4\\nkv0\\n0.2\\nkv1\\n0.05\\nkv2\\n0.1\\nkv3\\n0.1\\nkv4\\n0.2\\nkv5\\n0.35\\nkv0\\n0.15\\nkv1\\n---\\nkv2\\n0.15\\nkv3\\n0.1\\nkv4\\n0.05\\nkv5\\n0.25\\nkv6\\n0.3\\nkv0\\n0.15\\nkv1\\n---\\nkv2\\n0.15\\nkv3\\n0.1\\nkv4\\n---\\nkv5\\n0.15\\nkv6\\n0.2\\nkv7\\n0.25\\nkv0\\n0.2\\nkv1\\n---\\nkv2\\n0.15\\nkv3\\n---\\nkv4\\n---\\nkv5\\n0.15\\nkv6\\n0.1\\nkv7\\n0.15\\nkv8\\n0.25\\nTime\\nFigure 2: Illustration of the TOVA policy, which keeps a\\nfixed-size multi-state (green cells). For a given attention\\nlayer, at each decoding step, the state with the lowest\\nattention score is omitted (red cells, which become trans-\\nparent in the following steps).\\nby TOVA, indicating that perhaps a fixed recent\\nwindow is too strict. Further, some initial tokens\\nare kept for thousands of decoding steps, indicating\\nthat they are indeed important for successful decod-\\ning (Xiao et al., 2023; Han et al., 2023).\\nWe note that, much like H2O, TOVA can op-\\nerate head-wise or layer-wise. Unlike H2O, here\\nour preliminary results show a clear preference in\\nfavor of the layer-wise variant (App. A), which\\nwe therefore report. In the following, we show\\nthat TOVA dominates the other policies, obtaining\\nnear-similar results to the corresponding infinite\\nMSRNN (a regular pretrained transformer).\\n4\\nExperimental Setup\\nOur goal is to check whether any compression pol-\\nicy (finite MSRNN) can match the performance of\\nthe full, infinite MSRNN model. To address this\\nquestion, we evaluate the different compression\\npolicies (Sec. 3.3).9 Below we describe the bench-\\nmarks we use (Sec. 4.1) and the transformer LLM\\nfamilies we experiment with (Sec. 4.2).\\n4.1\\nLong Range Evaluation\\nOur experimental setup focuses on long range eval-\\nuation, which would trigger the different policies.\\nBelow we describe the three types of long-range\\nevaluations we employ: language modeling, long-\\nrange understanding, and generation of long texts.\\n9We highlight that our focus is on the capacity of off-\\nthe-shelf models, so we only consider policies that operate\\non pretrained LLMs and require no additional training. See\\nSec. 7 for approaches that do require training.\\nLanguage modeling\\nWe report perplexity on\\nthe PG-19 test set (Rae et al., 2020), a widely\\nused benchmark for evaluating long range language\\nmodels (So et al., 2022; Hutchins et al., 2022; Chen\\net al., 2023). PG-19 is composed of 100 full-length\\nbooks of average length of 70k tokens.\\nLong range understanding\\nWe use two test sets\\nfrom the ZeroSCROLLS benchmark (Shaham et al.,\\n2023), each focusing on a different aspect of long\\nrange understanding: long range summarization\\nand long range question answering (QA).\\nFor the former, we use SQuALITY (Wang et al.,\\n2022), a question focused summarization dataset.\\nFollowing Shaham et al. (2023), we report the\\ngeometric mean of ROUGE-1/2/L scores (based\\non the ground truth summary). For the latter, we\\nuse QASPER (Dasigi et al., 2021), a QA dataset\\nbased on Semantic Scholar Open Research Corpus\\n(S2ORC; Lo et al., 2020). As a QA task over long\\ntexts, this task can be considered a retrieval task,\\nas the model needs to retrieve the relevant infor-\\nmation from the text to answer the question. We\\nfollow Dasigi et al. (2021) and report F1 score. See\\nApp. C for the prompts used for both tasks.\\nText generation\\nWe feed the models with\\nprompts that solicit the generation of a long story.\\nWe sample 100 unique stories from each version\\nof the model, using different seeds. Given the com-\\nplexity of comparing two stories, we follow Chiang\\net al. (2023) and Zhou et al. (2023) and employ\\nGPT-4 as an evaluator. For each seed, we com-\\npare the two generated stories by asking GPT-4 to\\nevaluate which is better, reporting the average win\\nrate for each approach. We drop cases where the\\nmodel stops generating before reaching the mem-\\nory limit, as both stories are identical. To account\\nfor GPT-4’s positional bias (Wang et al., 2023), we\\npresent each pair of stories twice, alternating their\\npositions, and only consider a “win” if the same\\napproach is preferred in both cases. See App. C for\\nthe prompts used for generation and evaluation.\\n4.2\\nModels\\nWe experiment with three state-of-the-art trans-\\nformer decoder LLMs families: LLaMA-2 (Tou-\\nvron et al., 2023b), Mistral (Jiang et al., 2023) and\\nYi (01-ai, 2023). Each family offers a ∼7B param-\\neter version, which we use for evaluation.\\nFor language-modeling, we use the vanilla ver-\\nsions of the models.\\nFor the long range un-\\nderstanding tasks, we also consider three fine-\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nPerplexity\\nLLaMA 2 on PG-19\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nMistral on PG-19\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nYI on PG-19\\nBaseline\\nWindow\\nWindow+4\\nH2O\\nTopline (full context)\\nTOVA (ours)\\nFigure 3: Perplexity results for the PG-19 test set. TOVA outperforms all other policies in all multi-state sizes, while\\nmaintaining comparable results to the full context topline using 1/8–1/4 of the context size.\\ntuned versions: LLaMA-2-chat (Touvron et al.,\\n2023b), Mistral-Instruct (Jiang et al., 2023) and\\nneural-chat,10 which have been shown to excel in\\ninstruction-specific tasks. Lastly, for text genera-\\ntion, we use MythoLogic,11 a LLaMA-2-13B ver-\\nsion fine-tuned for story generation.\\nFor all models and tasks, we use a maximal input-\\nlength of 4,096 tokens.12 For the language mod-\\neling task, we split the texts into chunks of length\\n4,096, and apply efficient masking (see App. D).\\nFor the language understanding tasks, we truncate\\nthe end of the example (excluding prompt) if it ex-\\nceeds 4,096 tokens as done in Shaham et al. (2023).\\nAll experiments are done using bfloat16 floating-\\npoint precision over Nvidia V100 GPUs.\\n5\\nPretrained Transformers Act as Finite\\nMSRNNs\\nWe present our results of the different tasks: lan-\\nguage modeling (Sec. 5.1), long-range understand-\\ning (Sec. 5.2), and long text generation (Sec. 5.3).\\n5.1\\nLanguage Modeling\\nWe evaluate our base models over the language\\nmodeling task using the following policies: Win-\\ndow, Window+4, H2O and our TOVA policy.13 As\\na baseline, we run the models with a smaller se-\\nquence length, while not applying compression,\\nwhich corresponds to an infinite MSRNN with\\na full sequence length smaller than 4,096. As a\\ntopline, we use the models with the full training se-\\nquence length (4,096), again without compression.\\nWe examine multi-state sizes in exponential scales\\nof 2j for j ∈ {6, 7, . . . , 12} (212=4,096).\\nFigure 3 presents the perplexity results of the\\ndifferent policies. Our TOVA policy outperforms\\n10huggingface.co/Intel/neural-chat-7b-v3\\n11huggingface.co/Gryphe/MythoLogic-L2-13b\\n12Due to computational constraints, we defer extrapola-\\ntion experiments to future work, and approximate the infinite\\nMSRNN with a sequence length of 4,096 tokens.\\n13We ablate other policies using LLaMA-2-7B in App. A.\\nall other policies using all three models in all multi-\\nstate sizes. Specifically, our policy maintains re-\\nsults within 0.5 perplexity points of the topline\\nusing a quarter (LLaMA-2 and Yi) or even one\\neighth (Mistral) of the full context length. In con-\\ntrast, other policies require at least half of the full\\ncontext length to reach comparable results.\\nAs to the other policies, as observed by Han et al.\\n(2023) and Xiao et al. (2023), the Window policy\\nperforms quite poorly, while the Window+4 and\\nH2O policies obtain much better results, though\\nstill substantially lower than our TOVA. In light of\\nthese findings, we proceed to evaluate other tasks\\nusing two policies: Window+4 and TOVA.\\n5.2\\nLong Range Understanding\\nWe next evaluate instruction-tuned LLMs on\\nSQuALITY and QASPER.14 As a baseline, we\\nconsider a setup where the model is presented with\\na truncated version of the example according to\\nthe MSRNN capacity. E.g., when considering a\\nmulti-state size of 1,024 tokens, the baseline uses\\nthe example truncated to 1,024 tokens (including\\nthe prompt). We examine multi-state sizes in expo-\\nnential scales of 2j for j ∈ {8, 9, . . . , 12}.\\nLong range summarization\\nResults for SQuAL-\\nITY are presented in Fig. 4. Our TOVA policy\\nconsistently outperforms both baselines across all\\nmulti-state sizes and models. As in language mod-\\neling, using TOVA with a quarter (Mistral and Yi)\\nor even one eighth (LLaMA-2) of the full context\\nyields results within one point of the topline model.\\nLong range QA\\nFigure 5 shows the QASPER\\nresults. The gap between TOVA and the baselines\\nis large, in some cases reaching beyond 5 F1 points.\\nNonetheless, here TOVA needs half of the full con-\\ntext to perform within one F1 point of the topline.\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n17.0\\n17.5\\n18.0\\n18.5\\n19.0\\n19.5\\n20.0\\nROUGE\\nLLaMA 2-chat on SQuALITY\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n16.0\\n16.5\\n17.0\\n17.5\\n18.0\\n18.5\\n19.0\\n19.5\\n20.0\\nMistral-Instruct on SQuALITY\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n17.0\\n17.5\\n18.0\\n18.5\\n19.0\\n19.5\\n20.0\\n20.5\\nneural-chat on SQuALITY\\nBaseline\\nWindow+4\\nTopline (full context)\\nTOVA (ours)\\nFigure 4: Geometric mean of ROUGE-1/2/L for SQuALITY. TOVA achieves within one point of the topline using\\n1/8 − 1/4 of the multi-state size, while outperforming all other policies.\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\nF1\\nLLaMA 2-chat on QASPER\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\n27\\n30\\n33\\n36\\n39\\n42\\nMistral-Instruct on QASPER\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\n27\\n30\\n33\\nneural-chat on QASPER\\nBaseline\\nWindow+4\\nTopline (full context)\\nTOVA (ours)\\nFigure 5: F1 score over QASPER benchmark. TOVA outperforms both baselines, but requires a half of the full\\nmulti-state size for obtaining comparable results to the topline.\\n1024\\n512\\n256\\nMulti-state size\\n5%\\n88%\\n6%\\n10%\\n71%\\n19%\\n6%\\n47%\\n47%\\nTOVA wins\\nTie\\nTopline wins\\nFigure 6: GPT-4 preference over stories generated by\\nthe infinite MSRNN and a finite one using TOVA.\\n5.3\\nText Generation\\nFinally, we evaluate task generation. We first note\\nthat limiting the multi-state size makes the gener-\\nated text shorter: the average story length for the\\nfull model is 1,566 tokens. This value is kept for a\\nmulti-state size of 1,024, but drops to 1,503 with\\n512 tokens and to 1,361 with 256 tokens.\\nFigure 6 shows the evaluation results of the sto-\\nries using GPT-4. We observe that using a small\\nmulti-state size (256 tokens), our policy losses to\\nthe topline in 47% of cases, while winning or tying\\nin the remaining cases. This loss rate decreases\\nsubstantially to 19% with 512 tokens and further to\\nonly 6% with 1,024 tokens. Importantly, our policy\\n14Base LLMs numbers are reported in App. B.\\nis also preferred over the topline in 5–10% of the\\ncases in all multi-state sizes considered.\\n5.4\\nDiscussion\\nOur results indicate that transformer decoder\\nLLMs, which are infinite MSRNNs, often behave\\nempirically as finite MSRNNs: in 2/4 tasks, finite\\nMSRNNs using as little as 1/8–1/4 of the tokens yield\\ncomparable results to the corresponding infinite\\nMSRNN, despite being trained with the full context.\\nThe other two tasks, text generation and retrieval\\nQA, seem to require longer contexts, though still\\nmaintain comparable performance using one half\\nof the original context. This suggests that the con-\\nversion of a transformer into an RNN reintroduces\\nthe inherent challenges associated with RNNs, as\\nthey encounter difficulties with the retrieval of long\\nrange information (Hochreiter and Schmidhuber,\\n1997; Arjovsky et al., 2016).\\n6\\nAnalysis\\n6.1\\nWhich Tokens Matter?\\nWe have shown so far that pretrained transformer\\nLLMs, trained as infinite MSRNNs, empirically\\nbehave in many cases as finite MSRNNs. This ef-\\nfectively means that most of the tokens are dropped\\nfrom memory as generation progresses. This sec-\\ntion aims to shed light on this process, and charac-\\nterize the tokens frequently kept, and those dropped.\\nAs our TOVA policy presents the best approxima-\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nFigure 7: An illustration of the tokens kept by our policy\\nin the last layer of LLaMA-2-7B on one PG-19 example.\\nEach row represents a decoding step, and each column\\nis a token attended to.\\ntion of the full model, we analyze its results. The\\nanalysis below uses the LLaMA-2-7B model, with\\n31 instances from PG-19.\\nRecency is not all you need\\nWe first observe that,\\nmuch like most compression policies (Sec. 3.3),\\nTOVA preserves recent tokens. Figure 7 illustrates\\nthe tokens kept by TOVA in the final layer for\\none example from PG-19, using multi-state size\\nof 512.15 The figure shows a clear window trend,\\nwhich indicates the importance of recent tokens for\\ndecoding. Nonetheless, we also observe that many\\nolder tokens are kept. To quantify this, we compute\\nthe proportion of recent tokens out of the tokens\\nkept in the multi-state, averaging across the PG-\\n19 examples, layers, and positions. We find that\\nonly 73-76% of the tokens are recent, the rest being\\nolder. This suggests that while recent tokens are\\nimportant, they are far from sufficient. Importantly,\\nunlike previous work that handcrafted the recent\\nwindow, our method identifies it automatically. We\\nnext turn to investigate which tokens from the far\\nhistory tend to be kept. We study two dimensions:\\nthe position of the token, and its content.\\nFirst token matters\\nFigure 8 shows the number\\nof steps kept (averaged across layers and exam-\\nples) for the first 25 tokens. As observed by pre-\\nvious work (Han et al., 2023; Xiao et al., 2023),\\nwe find that the very first token is crucial for the\\nmodel: it is kept until the end of the sequence\\nacross all multi-state sizes. However, other initial\\ntokens (e.g., positions 2–4) are far less important.\\nWhile they are kept for longer than the next tokens,\\n15See App. E for the illustrations of all layers.\\n0\\n5\\n10\\n15\\n20\\n25\\nToken position\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\n4000\\nAvg #steps kept\\n256\\n512\\n1024\\n2048\\nFigure 8: The average number of steps a token is kept\\nin the multi-state when applying TOVA as a function of\\ntoken position. Different lines are different multi-state\\nsizes. The very first token is kept through the entire\\ncontext, while next tokens are dropped far earlier.\\nTag\\nMulti-state\\nsize\\n256\\n512\\n1024\\n2048\\nAvg.\\n249\\n481\\n897\\n1537\\nPOS\\n1134\\n1393\\n1736\\n2061\\n”\\n845\\n1101\\n1413\\n1774\\n$\\n329\\n724\\n1276\\n2123\\n)\\n379\\n670\\n1161\\n1558\\n.\\n350\\n645\\n1117\\n1677\\nNNPS\\n321\\n578\\n1042\\n1671\\n\\\\n\\n303\\n550\\n969\\n1538\\nTable 1: Mean number of steps tokens are kept in the\\nmulti-state with TOVA, grouped by POS-tags. Columns\\nrepresent the multi-state size. Here we report the tokens\\nkept the longest, see full table in App. F.\\nthey are dropped far faster than the first one.\\nNot all tokens are equally kept\\nAs indicated\\nby Fig. 7, some tokens last much longer than others.\\nTo study this phenomenon, we map each token to\\nits part-of-speech tag (POS-tag) using NLTK (Bird\\net al., 2009), and plot the tags that last longest\\nin Tab. 1.16 Our results show that, as observed\\nby previous work (Clark et al., 2019; Zhang et al.,\\n2023; Ge et al., 2023), punctuation and other spe-\\ncial symbols tend to be kept. However, we also\\nidentify other tokens that tend to stay longer, e.g.,\\npossessive nouns (POS) and proper nouns (NNPS).\\nStudying the role of these tokens is an exciting\\nresearch direction, which we defer to future work.\\n16We present the full table for all POS-tags in App. F.\\n6.2\\nIncreased Batch Size using TOVA\\nAs discussed in Sec. 2.2, caching the K, V ma-\\ntrices in transformer auto-regressive decoding is\\ncommon in current frameworks, as is trades-off de-\\ncoding speed for runtime memory. Importantly, the\\nmemory factor is determined by two elements: the\\nmodel size (e.g., number of layers, hidden dimen-\\nsion), and the batch size. As the former is fixed,\\ncaching effectively limits the inference batch-size.\\nWhen decoding transformers as finite MSRNNs,\\nthe cached K, V matrices are compressed. As a\\nresult, reducing the multi-state size to 1/m of the\\noriginal size can increase the batch size by a fac-\\ntor of m, dramatically enhancing hardware utiliza-\\ntion. As shown in Sec. 5, TOVA allows limiting\\nthe multi-state to 1/8-1/4 of the full LLMs context\\nlength without major loss in performance, therefore\\nenabling up to an eightfold increase in batch size.\\n7\\nRelated Work\\nTransformers and RNNs\\nSeveral works have\\ntried to bridge the gap between RNNs and trans-\\nformers. Hutchins et al. (2022) employed a hybrid\\ntransformer-RNN approach that attends to recent\\ntokens and to further hidden states simultaneously.\\nKatharopoulos et al. (2020) and Sun et al. (2023)\\nsubstituted the self-attention layer with a convolu-\\ntion layer that can be applied in a recurrent manner.\\nPeng et al. (2023) adjusted the self-attention layer\\nto maintain an RNN nature at inference time.\\nPerhaps most relevant to this work, ABC (Peng\\net al., 2022) presented transformer models with\\nbounded memory. They also showed that several\\ntransformer variants such as Linformer (Wang et al.,\\n2020) and Window attention can be interpreted\\nas instances of their framework. However, these\\nmodels typically treat the memory as a single state\\nand not as multi-state, meaning there is no explicit\\nmapping from tokens to states. Importantly, unlike\\nour approach, both ABC and the other works above\\nrequire a dedicated training procedure, and cannot\\noperate on existing transformer-based LLMs.\\nNew RNN variants\\nRecent work aimed to revive\\nRNNs in NLP. The most prominent work is S4 (Gu\\net al., 2022) and its successors (Gupta et al., 2022;\\nMehta et al., 2023; Gu and Dao, 2023), which ele-\\nvate state spaces to form linear RNNs. Other works\\nintroduced novel RNN variants that train effectively\\nwhile reducing inference cost (Merity, 2019; Orvi-\\neto et al., 2023; Yang et al., 2023).\\nLimited KV cache and window attention\\nWin-\\ndow attention (Beltagy et al., 2020; Zaheer et al.,\\n2020) is a simple way of constructing a transformer\\nwith limited cache requirement. As mentioned in\\nSec. 3.3, the H2O policy (Zhang et al., 2023) can\\nbe used to limit the cache size of transformers. A\\nvery recent followup work (Ge et al., 2023) showed\\nthat manually caching specific tokens like “.” and\\n“,” further boosts H2O performance. We showed\\nthat TOVA does so without manually selecting to-\\nkens (Sec. 6.1). Anagnostidis et al. (2023) intro-\\nduced a learned approach over LLMs that limits the\\ncache consumption of transformers. Lastly, Xiao\\net al. (2023) and Han et al. (2023) showed that the\\nWindow+i policy enables models to extrapolate\\nbeyond their training sequence length. The appli-\\ncation of TOVA for extending the context size of\\nLLMs is an intriguing topic for future research.\\nSimplifying transformers\\nPrevious work has\\nshown that many transformer attention heads can\\nbe pruned (Michel et al., 2019; Li et al., 2021).\\nHassid et al. (2022) showed that the dynamic at-\\ntention in transformers can be replaced with static\\nweights without major drop in performance. Sev-\\neral works replaced the attention mechanism in\\ntransformers with an efficient variant (Liu et al.,\\n2021; Lee-Thorp et al., 2022). We show that trans-\\nformer decoders can reduce to finite MSRNNs.\\n8\\nConclusion\\nIn this work, we redefined decoder transformers as\\na form of multi-state RNNs (MSRNN) with an infi-\\nnite multi-state size. We highlighted that limiting\\nthe number of token representations transformers\\ncan handle at each step is equivalent to compress-\\ning it from infinite to finite MSRNNs.\\nWe then introduced TOVA, a conceptually sim-\\nple compression method that selects which tokens\\nto keep using their attention scores. Our findings\\nhighlight its superior performance compared to ex-\\nisting compression policies. Moreover, we showed\\nthat in many cases, TOVA performs comparably to\\nthe infinite MSRNN model, while requiring 1/8–1/4\\nof the multi-state size. Notably, our results demon-\\nstrate that, although transformers are not trained as\\nsuch, they often function as finite MSRNNs.\\nOur findings shed light on the inter-working of\\ntransformers, and their connections to RNNs. They\\nalso have practical value—they can dramatically\\nreduce the LLM cache size by up to 88%.\\nLimitations\\nEvaluating models on long text generation is com-\\nputationally expensive and might limit others from\\nreproducing our results. Further, the evaluation of\\nsuch task is extremely complicated, even for hu-\\nmans. We therefore resort to GPT-4 to compare\\nthe output of our TOVA policy compared to the\\ntopline model (Sec. 5.3). We recognize that this is\\nfar from perfect, and will most likely not catch the\\nfull breadth of evaluating text quality. Finally, our\\nevaluation framework focuses on English tasks. It\\nis not unlikely that languages with more flexible\\nword order will make different use of the attention\\nmechanism, and thus potentially require a larger\\nmulti-state size.\\nEthics Statement\\nOur work has the potential to dramatically re-\\nduce the memory footprint of transformer LLMs,\\nthereby potentially increasing their adoption by\\nusers with limited hardware access.\\nThis work does not collect any new data, and\\nonly uses open source models, and public data col-\\nlected by other sources.\\nAcknowledgements\\nWe thank Miri Varshavsky Hassid for the great feed-\\nback and moral support. This work was supported\\nin part by NSF-BSF grant 2020793.\\nReferences\\n01-ai. 2023.\\n01-ai/yi-6b.\\nhttps://github.com/\\n01-ai/Yi.\\nSotiris Anagnostidis, Dario Pavllo, Luca Biggio,\\nLorenzo Noci, Aurelien Lucchi, and Thomas Hof-\\nmann. 2023. Dynamic context pruning for efficient\\nand interpretable autoregressive transformers.\\nIn\\nThirty-seventh Conference on Neural Information\\nProcessing Systems.\\nMartin Arjovsky, Amar Shah, and Yoshua Bengio. 2016.\\nUnitary evolution recurrent neural networks. In In-\\nternational conference on machine learning, pages\\n1120–1128. PMLR.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv:2004.05150.\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\\nural language processing with Python: analyzing text\\nwith the natural language toolkit. O’Reilly Media,\\nInc.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\\nYuandong Tian. 2023. Extending context window of\\nlarge language models via positional interpolation.\\narXiv:2306.15595.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing GPT-4 with 90%* Chat-\\nGPT quality.\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\\nChristopher D. Manning. 2019. What does BERT\\nlook at? an analysis of BERT’s attention. In Pro-\\nceedings of the 2019 ACL Workshop BlackboxNLP:\\nAnalyzing and Interpreting Neural Networks for NLP,\\npages 276–286, Florence, Italy. Association for Com-\\nputational Linguistics.\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\\nNoah A. Smith, and Matt Gardner. 2021. A dataset\\nof information-seeking questions and answers an-\\nchored in research papers. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 4599–4610, On-\\nline. Association for Computational Linguistics.\\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\\nnitive Science, 14(2):179–211.\\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang,\\nJiawei Han, and Jianfeng Gao. 2023. Model tells you\\nwhat to discard: Adaptive KV cache compression for\\nLLMs. In Workshop on Advancing Neural Network\\nTraining: Computational Efficiency, Scalability, and\\nResource Optimization (WANT@NeurIPS 2023).\\nAlbert Gu and Tri Dao. 2023.\\nMamba:\\nLinear-\\ntime sequence modeling with selective state spaces.\\narXiv:2312.00752.\\nAlbert Gu, Karan Goel, and Christopher Ré. 2022. Effi-\\nciently modeling long sequences with structured state\\nspaces. arXiv:2111.00396.\\nAnkit Gupta, Albert Gu, and Jonathan Berant. 2022. Di-\\nagonal state spaces are as effective as structured state\\nspaces. Advances in Neural Information Processing\\nSystems, 35:22982–22994.\\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng\\nJi, and Sinong Wang. 2023. LM-Infinite: Simple\\non-the-fly length generalization for large language\\nmodels. arXiv:2308.16137.\\nMichael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai,\\nIvan Montero, Noah A. Smith, and Roy Schwartz.\\n2022. How much does attention actually attend?\\nquestioning the importance of attention in pretrained\\ntransformers. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2022, pages 1403–\\n1416, Abu Dhabi, United Arab Emirates. Association\\nfor Computational Linguistics.\\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\\nshort-term memory. Neural computation, 9(8):1735–\\n1780.\\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan\\nDyer, and Behnam Neyshabur. 2022. Block-recurrent\\ntransformers. In Advances in Neural Information\\nProcessing Systems.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\\nde las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Re-\\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\\nthée Lacroix, and William El Sayed. 2023. Mistral\\n7b. arXiv:2310.06825.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\\npas, and François Fleuret. 2020. Transformers are\\nrnns: Fast autoregressive transformers with linear\\nattention. In International conference on machine\\nlearning, pages 5156–5165. PMLR.\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and\\nSantiago Ontanon. 2022. FNet: Mixing tokens with\\nFourier transforms. In Proceedings of the 2022 Con-\\nference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human\\nLanguage Technologies, pages 4296–4313, Seattle,\\nUnited States. Association for Computational Lin-\\nguistics.\\nJiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021.\\nDifferentiable subset pruning of transformer heads.\\nTransactions of the Association for Computational\\nLinguistics, 9:1442–1459.\\nHanxiao Liu, Zihang Dai, David So, and Quoc V Le.\\n2021. Pay attention to MLPs. In Advances in Neural\\nInformation Processing Systems, volume 34, pages\\n9204–9215. Curran Associates, Inc.\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\\nney, and Daniel Weld. 2020. S2ORC: The semantic\\nscholar open research corpus. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics, pages 4969–4983, Online. Asso-\\nciation for Computational Linguistics.\\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and\\nBehnam Neyshabur. 2023.\\nLong range language\\nmodeling via gated state spaces. In The Eleventh In-\\nternational Conference on Learning Representations.\\nStephen Merity. 2019. Single headed attention RNN:\\nStop thinking with your head. arXiv:1911.11423.\\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\\nAre sixteen heads really better than one?\\nIn Ad-\\nvances in Neural Information Processing Systems,\\nvolume 32. Curran Associates, Inc.\\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan\\nFernando, Caglar Gulcehre, Razvan Pascanu, and\\nSoham De. 2023. Resurrecting recurrent neural net-\\nworks for long sequences. arXiv:2303.06349.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Al-\\nbalak, Samuel Arcadinho, Stella Biderman, Huanqi\\nCao, Xin Cheng, Michael Chung, Leon Derczynski,\\nXingjian Du, Matteo Grella, Kranthi Gv, Xuzheng\\nHe, Haowen Hou, Przemyslaw Kazienko, Jan Ko-\\ncon, Jiaming Kong, Bartłomiej Koptyra, Hayden\\nLau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand\\nMom, Atsushi Saito, Guangyu Song, Xiangru Tang,\\nJohan Wind, Stanisław Wo´zniak, Zhenyuan Zhang,\\nQinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023.\\nRWKV: Reinventing RNNs for the transformer era.\\nIn Findings of the Association for Computational\\nLinguistics: EMNLP 2023, pages 14048–14077, Sin-\\ngapore. Association for Computational Linguistics.\\nHao Peng, Jungo Kasai, Nikolaos Pappas, Dani\\nYogatama, Zhaofeng Wu, Lingpeng Kong, Roy\\nSchwartz, and Noah A. Smith. 2022. ABC: Atten-\\ntion with bounded-memory control. In Proceedings\\nof the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 7469–7483, Dublin, Ireland. Association for\\nComputational Linguistics.\\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\\nJacob Devlin, James Bradbury, Anselm Levskaya,\\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and\\nJeff Dean. 2022. Efficiently scaling transformer in-\\nference. arXiv:2211.05102.\\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\\nshort, test long: Attention with linear biases enables\\ninput length extrapolation. In The Tenth International\\nConference on Learning Representations, ICLR 2022,\\nVirtual Event, April 25-29, 2022. OpenReview.net.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners.\\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\\nCompressive transformers for long-range sequence\\nmodelling. In International Conference on Learning\\nRepresentations.\\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-\\nrant, and Omer Levy. 2023.\\nZeroSCROLLS: A\\nzero-shot benchmark for long text understanding.\\narXiv:2305.14196.\\nDavid R. So, Wojciech Ma´nke, Hanxiao Liu, Zihang\\nDai, Noam Shazeer, and Quoc V. Le. 2022. Primer:\\nSearching for efficient transformers for language\\nmodeling. arXiv:2109.08668.\\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma,\\nYuqing Xia, Jilong Xue, Jianyong Wang, and\\nFuru Wei. 2023.\\nRetentive network:\\nA suc-\\ncessor to transformer for large language models.\\narXiv:2307.08621.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023a.\\nLLaMA:\\nOpen and efficient foundation language models.\\narXiv:2302.13971.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023b. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv:2307.09288.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. Advances in neural information processing\\nsystems, 30.\\nAlex Wang, Richard Yuanzhe Pang, Angelica Chen, Ja-\\nson Phang, and Samuel R. Bowman. 2022. SQuAL-\\nITY: Building a long-document summarization\\ndataset the hard way. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, pages 1139–1156, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,\\nBinghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\\nZhifang Sui. 2023. Large language models are not\\nfair evaluators. arXiv:2305.17926.\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\\nand Hao Ma. 2020. Linformer: Self-attention with\\nlinear complexity. arXiv:2006.04768.\\nGuangxuan Xiao,\\nYuandong Tian,\\nBeidi Chen,\\nSong Han, and Mike Lewis. 2023.\\nEfficient\\nstreaming language models with attention sinks.\\narXiv:2309.17453.\\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar\\nPanda, and Yoon Kim. 2023. Gated linear atten-\\ntion transformers with hardware-efficient training.\\narXiv:2312.06635.\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\\nLi Yang, et al. 2020. Big bird: Transformers for\\nlonger sequences. Advances in neural information\\nprocessing systems, 33:17283–17297.\\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong\\nChen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuan-\\ndong Tian, Christopher Ré, Clark Barrett, Zhangyang\\nWang, and Beidi Chen. 2023. H2o: Heavy-hitter ora-\\ncle for efficient generative inference of large language\\nmodels. arXiv:2306.14048.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\\nLuke Zettlemoyer, and Omer Levy. 2023. LIMA:\\nLess is more for alignment. arXiv:2305.11206.\\nA\\nAblation over policies\\nWe ablate all policies presented in Sec. 3.3 with the\\nlanguage modeling task, specifically we examine:\\nWindow, Window+i policies for i ∈ {1, 4}, H2O\\npolicy for both per layer and per head approaches\\nand our TOVA policy for both per layer and per\\nhead approaches. We also combine our approach\\nwith additionally fixing the first i tokens using i ∈\\n{1, 4}. We consider the same baseline policy as in\\nSec. 5.1. We use the LLaMA-7B as the backbone\\nmodel.\\nOur results are presented in Tab. 2. As shown\\nin Sec. 5.1 the Window policy fails, while the\\nWindow+1/4 policies maintain much better re-\\nsults (with Window+4 performs slightly better).\\nThe two H2O policies—head/layer produce similar\\nresults. Regarding our TOVA policies, the head ver-\\nsion performs worse than former policies in most\\nMulti-State sizes, while the layer version outper-\\nforms all other policies. We attribute this difference\\nto the more robust selection mechanism employed\\nby the layer version, which requires agreement\\namong all heads to determine the importance of spe-\\ncific tokens. Lastly, when we enhance our TOVA\\npolicy with the explicit preservation of i initial\\ntokens, the results remain relatively unchanged, im-\\nplying that our policy inherently retains the crucial\\ntokens.\\nB\\nLong Range Understanding with base\\nmodels\\nFig. 9 and Fig. 10 shows the results for base models\\nover the SQuALITY and QASPER benchmarks\\nrespectively.\\nC\\nPrompts\\nThe prompts used for the different evaluations\\nthrough this work are presented in Tab. 3. To eval-\\nuate the generated texts, using GPT-4, we use the\\ngpt-4-0613 version.\\nD\\nLanguage Modeling - Experimental\\nDetails\\nTo effectively parallelize the language modeling\\ntask for all tokens in the sequence, we modify the\\nattention mask to incorporate the different MSRNN\\npolicies presented in Sec. 3. Specifically, for Win-\\ndow and Window+i policies, we apply a static\\nmasking, as the reduced tokens are independent\\nwith respect to the attention computation. For H2O\\nand TOVA, we adjust the mask according to the\\nattention weights of the relevant layer.\\nE\\nTOVA illustration for the full model\\nFig. 11 and Fig. 12 shows illustrations for which\\ntokens are attended (X axis) at each step (Y axis)\\nfor every layer of LLaMA-2-7B, when applying\\nTOVA with multi-state size of 512, on next-token\\nprediction over one PG-19 example.\\nF\\nFull POS-tags analysis\\nThe full version of Tab. 1 presented in Tab. 4.\\nPolicy\\nMulti-state\\nsize\\n64\\n128\\n256\\n512\\n1024\\n2048\\n4096\\nBaseline\\n17.65\\n12.97\\n10.39\\n8.92\\n8.04\\n7.50\\n7.16\\nWindow\\n4812.27\\n4025.01\\n3275.58\\n2184.62\\n1001.29\\n240.17\\n7.16\\nWindow+1\\n10.20\\n8.97\\n8.22\\n7.76\\n7.50\\n7.33\\n7.16\\nWindow+4\\n10.28\\n8.98\\n8.19\\n7.73\\n7.46\\n7.30\\n7.16\\nH2O-head\\n10.22\\n8.97\\n8.21\\n7.75\\n7.49\\n7.32\\n7.16\\nH2O-layer\\n10.20\\n8.97\\n8.22\\n7.76\\n7.50\\n7.33\\n7.16\\nTOVA-head\\n11.13\\n9.55\\n8.69\\n7.90\\n7.52\\n7.27\\n7.16\\nTOVA-layer\\n9.53\\n8.32\\n7.71\\n7.41\\n7.25\\n7.17\\n7.16\\nTOVA-layer+1\\n9.53\\n8.31\\n7.71\\n7.41\\n7.25\\n7.17\\n7.16\\nTOVA-layer+4\\n9.63\\n8.33\\n7.72\\n7.41\\n7.25\\n7.17\\n7.16\\nTable 2: Perplexity over the PG-19 set using varying multi-state sizes (maximal number of states used). Our TOVA\\npolicy dominates the table in all multi-state sizes.\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n11.5\\n12.0\\n12.5\\n13.0\\n13.5\\nROUGE\\nLLaMA 2-base on SQuALITY\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n10.5\\n11.0\\n11.5\\nMistral-base on SQuALITY\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n9.0\\n9.5\\n10.0\\n10.5\\n11.0\\nYI-base on SQuALITY\\nBaseline\\nWindow+4\\nTopline (full context)\\nTOVA (ours)\\nFigure 9: Geometric mean of ROUGE-1/2/L for SQuALITY using base models.\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n18\\n21\\n24\\n27\\nF1\\nLLaMA 2-base on QASPER\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n15\\n18\\n21\\n24\\n27\\n30\\n33\\n36\\nMistral-base on QASPER\\n256\\n512\\n1024\\n2048\\n4096\\nMulti-state size\\n3\\n6\\n9\\n12\\n15\\n18\\n21\\n24\\n27\\n30\\n33\\n36\\n39\\nYI-base on QASPER\\nBaseline\\nWindow+4\\nTopline (full context)\\nTOVA (ours)\\nFigure 10: F1 score over QASPER benchmark using base models.\\nTask\\nPrompt\\nSQuALITY\\n{Story}\\nAnswer the question in a paragraph.\\nQuestion:\\n{Question}\\nAnswer:\\nQASPER\\n{Article}\\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the\\nquestion cannot be answered based on the information in the article, write \"unanswerable\". If\\nthe question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\".\\nQuestion:\\n{Question}\\nAnswer:\\nStory Generation\\n### Instruction:\\nWrite a very long story (at least 4,000 words). The story should include at least 20 named\\ncharacters, spans 3 countries and 9 cities, at least 10 chapters and should have a lot of plot twists.\\n### Response:\\nGPT- Evaluation\\nHelp me decide which response is better given the prompt:\\n{Prompt body for story generation}\\nWhich of the following responses is better (the responses are separated by ’————————’):\\nResponse (A):\\n{First Response}\\n————————\\nResponse (B):\\n{Second Response}\\nComparing these two responses, which response is better (A), (B) or (C) for equal quality?\\nplease select one and only one option, be as concisely as you can, using a single phrase.\\nTable 3: Prompts used for our experiments.\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 0\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 1\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 2\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 3\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 4\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 5\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 6\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 7\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 8\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 9\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 10\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 11\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 12\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 13\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 14\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 15\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 16\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 17\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 18\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 19\\nTOVA resulting attended tokens per step\\nFigure 11: Attended tokens with TOVA. Layers 0-19.\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 20\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 21\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 22\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 23\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 24\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 25\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 26\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 27\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 28\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 29\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 30\\n0\\n1024\\n2048\\n3072\\n4096\\nAttended tokens\\n0\\n1024\\n2048\\n3072\\n4096\\nStep\\nLayer 31\\nFigure 12: Attended tokens with TOVA. Layers 20-31.\\nTag\\nMulti-state\\nsize\\n256\\n512\\n1024\\n2048\\nAvg.\\n249\\n481\\n897\\n1537\\nPOS\\n1134\\n1393\\n1736\\n2061\\n”\\n845\\n1101\\n1413\\n1774\\n$\\n329\\n724\\n1276\\n2123\\n)\\n379\\n670\\n1161\\n1558\\n.\\n350\\n645\\n1117\\n1677\\nNNPS\\n321\\n578\\n1042\\n1671\\n\\\\n\\n303\\n550\\n969\\n1538\\nWP$\\n255\\n539\\n1121\\n1920\\nCD\\n301\\n537\\n940\\n1557\\nNN\\n270\\n527\\n983\\n1628\\nNNS\\n270\\n526\\n978\\n1618\\nNNP\\n270\\n517\\n951\\n1613\\nFW\\n253\\n511\\n903\\n1444\\n:\\n243\\n492\\n940\\n1570\\nJJ\\n240\\n480\\n918\\n1598\\nVBP\\n244\\n478\\n882\\n1504\\nJJS\\n220\\n475\\n953\\n1689\\nUH\\n233\\n474\\n870\\n1412\\nSYM\\n231\\n471\\n893\\n1482\\nWDT\\n223\\n462\\n903\\n1604\\nVBN\\n230\\n462\\n887\\n1549\\nEX\\n244\\n461\\n847\\n1461\\nRB\\n223\\n459\\n892\\n1566\\n,\\n236\\n453\\n840\\n1454\\nVBG\\n221\\n445\\n858\\n1523\\nRBS\\n210\\n441\\n878\\n1645\\nVBZ\\n219\\n440\\n844\\n1492\\nCC\\n217\\n437\\n862\\n1546\\nVBD\\n217\\n432\\n827\\n1493\\nVB\\n214\\n426\\n817\\n1457\\nPRP\\n217\\n424\\n794\\n1432\\nRP\\n207\\n417\\n811\\n1485\\nWRB\\n207\\n415\\n800\\n1502\\nWP\\n199\\n405\\n803\\n1506\\nJJR\\n195\\n403\\n782\\n1413\\nRBR\\n183\\n397\\n821\\n1566\\nPDT\\n181\\n391\\n756\\n1362\\nIN\\n190\\n385\\n760\\n1408\\nPRP$\\n189\\n383\\n745\\n1386\\nDT\\n190\\n379\\n734\\n1363\\nMD\\n177\\n378\\n754\\n1392\\nTO\\n182\\n368\\n734\\n1363\\nTable 4: Mean number of steps a token lasts, grouped\\nby POS-tags.\\n'}, 'http://arxiv.org/abs/2401.06102v1': {'title': 'Patchscope: A Unifying Framework for Inspecting Hidden Representations\\n  of Language Models', 'published_date': datetime.datetime(2024, 1, 11, 18, 33, 48), 'pdf_link': 'http://arxiv.org/pdf/2401.06102v1', 'summary': \"Inspecting the information encoded in hidden representations of large\\nlanguage models (LLMs) can explain models' behavior and verify their alignment\\nwith human values. Given the capabilities of LLMs in generating\\nhuman-understandable text, we propose leveraging the model itself to explain\\nits internal representations in natural language. We introduce a framework\\ncalled Patchscopes and show how it can be used to answer a wide range of\\nresearch questions about an LLM's computation. We show that prior\\ninterpretability methods based on projecting representations into the\\nvocabulary space and intervening on the LLM computation, can be viewed as\\nspecial instances of this framework. Moreover, several of their shortcomings\\nsuch as failure in inspecting early layers or lack of expressivity can be\\nmitigated by a Patchscope. Beyond unifying prior inspection techniques,\\nPatchscopes also opens up new possibilities such as using a more capable model\\nto explain the representations of a smaller model, and unlocks new applications\\nsuch as self-correction in multi-hop reasoning.\", 'pdf_text': 'Patchscopes: A Unifying Framework for Inspecting\\nHidden Representations of Language Models\\nAsma Gandeharioun * 1 Avi Caciularu * 1 Adam Pearce 1 Lucas Dixon 1 Mor Geva 1 2\\nAbstract\\nInspecting the information encoded in hidden rep-\\nresentations of large language models (LLMs) can\\nexplain models’ behavior and verify their align-\\nment with human values. Given the capabilities\\nof LLMs in generating human-understandable\\ntext, we propose leveraging the model itself\\nto explain its internal representations in natu-\\nral language. We introduce a framework called\\nPatchscopes and show how it can be used to\\nanswer a wide range of research questions about\\nan LLM’s computation. We show that prior in-\\nterpretability methods based on projecting repre-\\nsentations into the vocabulary space and interven-\\ning on the LLM computation, can be viewed as\\nspecial instances of this framework. Moreover,\\nseveral of their shortcomings such as failure in\\ninspecting early layers or lack of expressivity can\\nbe mitigated by a Patchscope. Beyond unify-\\ning prior inspection techniques, Patchscopes\\nalso opens up new possibilities such as using a\\nmore capable model to explain the representations\\nof a smaller model, and unlocks new applications\\nsuch as self-correction in multi-hop reasoning.\\n1. Introduction\\nThe question of what information is captured within the\\nhidden representations of large language models (LLMs) is\\nof key importance in control and understanding of modern\\ngenerative AI, and has drawn substantial attention recently\\n(Casper et al., 2022; Madsen et al., 2022; Patel & Pavlick,\\n2021; Nanda et al., 2023). To tackle this question, prior work\\nhas introduced a diverse array of interpretability methods,\\nwhich largely rely on three prominent approaches: training\\nlinear classifiers, called probes, on top of hidden represen-\\ntations (Belinkov & Glass, 2019; Belinkov, 2022; Alain &\\n*Equal contribution\\n1Google Research 2Tel Aviv Univer-\\nsity. Correspondence to: Asma Ghandeharioun <aghandehari-\\noun@google.com>.\\nPreprint.\\nStep 4: \\nRunning Execution\\n on Patched Target\\nAmazon `s\\nformer CEO  attended Oscars \\nf(  ) =\\ncat->cat; 135->135; hello->hello; ?\\nStep 1: \\nFeeding Source Prompt\\nto Source Model\\nStep 2: \\nTransforming \\nHidden State\\nStep 3: \\nFeeding Target Prompt \\nto Target Model\\nJeff Bezos\\nFigure 1. Illustration of our framework, showing a Patchscope\\nfor decoding what is encoded in the representation of “CEO” in\\nthe source prompt (left). We use a target prompt (right) comprised\\nof few-shot demonstrations of token repetitions, which encourages\\ndecoding the token identity given a hidden representation. Step 1:\\nRun the forward computation on the source prompt in the source\\nmodel. Step 2: Apply an optional transformation to the source hid-\\nden state at source layer. Step 3: Run the forward computation on\\nthe target prompt up to the target layer in the target model. Step 4:\\nPatch the target representation of “?” at the target layer with the\\ntransformed representation (from step 2), and continue the forward\\ncomputation from that layer onward. Note that the modularity of\\nPatchscopes allows designing a variety of methods as one can\\nconfigure the target prompt and model and the transformation.\\nBengio, 2017), projecting representations to the model’s\\nvocabulary space (nostalgebraist, 2020; Din et al., 2023;\\nBelrose et al., 2023), and intervening on the computation to\\nidentify if a representation is critical for certain predictions\\n(Meng et al., 2022a; Wallat et al., 2020; Wang et al., 2022;\\nConmy et al., 2023; Geva et al., 2023).\\nDespite the wide success of these methods, they each exhibit\\npractical shortcomings. First, probing relies on supervised\\ntraining for pre-defined classes, which is hard to scale when\\nthe feature of interest has a large number of classes or when\\nall the categories are not known a priori. Second, the ac-\\ncuracy of vocabulary projections substantially decreases in\\nearly layers and their outputs are often hard to interpret.\\narXiv:2401.06102v1  [cs.CL]  11 Jan 2024\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nLast, all the above methods are not expressive: they provide\\nclass probabilities or most likely tokens, as opposed to a\\nhigh-quality explanation in natural language.\\nIn this work, we argue that the advanced capabilities of\\nLLMs in generating human-like text can be leveraged\\nfor “translating” the information in their representations\\nfor humans. We introduce a modular framework, called\\nPatchscopes (see §3), that can easily be configured to\\nquery various information from LLM representations. Given\\na representation, we propose to decode specific information\\nfrom it by “patching” it into a separate inference pass that\\nencourages the extraction of that information, independently\\nof the original context.1 A configuration of our framework (a\\nPatchscope) can be viewed as an inspection tool geared\\ntowards a particular objective, as illustrated in Fig. 1.\\nWe show that many existing methods, including those that\\nrely on vocabulary projections and interventions, can be\\ncast as Patchscopes. Moreover, new configurations of\\nour framework introduce more effective tools in addressing\\nthe same questions, while mitigating several limitations of\\nprior approaches. Additionally, Patchscopes enables\\naddressing underexplored questions, such as fine-grained\\nanalysis of the input contextualization process and the extent\\nto which a more expressive model can be used to inspect\\nhidden representations of a smaller model.\\nWe conduct a series of experiments to evaluate the benefits\\nand opportunities introduced by Patchscopes, focusing\\non auto-regressive LLMs. First, we consider the problem of\\nestimating the model’s next-token prediction from its inter-\\nmediate representations (see §4.1). Across multiple LLMs,\\nwe show that using a few-shot token identity prompt2 leads\\nto substantial gains over vocabulary projection methods.\\nNext, we evaluate how well Patchscopes can decode\\nspecific attributes of an entity from its LLM representa-\\ntions, when these are detached from the original context\\n(see §4.2). We observe that, despite using no training data,\\nPatchscopes significantly outperforms probing in six\\nout of twelve commonsense and factual reasoning tasks, and\\nworks comparably well to all but one of the remaining six.\\nBeyond\\noutput\\nestimation\\nand\\nattribute\\ndecoding,\\nPatchscopes can address questions that are hard\\nto answer with existing methods.\\nIn §4.3, we apply\\nPatchscopes to study how LLMs contextualize input\\nentity names in early layers, where vocabulary projections\\nmostly fail and other methods only provide a binary\\nsignal of whether the entity has been resolved, at best\\n1While patching (or “activation patching”) by itself is not a new\\ntechnique, to the best of our knowledge, we are the first to propose\\nusing it for decoding information from hidden representations in a\\nconfigurable and expressive manner. See more details in §3.2.\\n2A prompt in the form of “tok1\\n→ tok1; tok2\\n→\\ntok2; . . . ; tokk” where toki refers to a random token.\\n(Youssef et al., 2023; Tenney et al., 2019). With a new\\nPatchscope, we are able to verbalize the gradual entity\\nresolution process. For example, we show in processing\\n“Alexander the Great” throughout the layers, the model\\nreflects different entities starting from “Great Britain”,\\nto “the Great Depression”, to finally resolving “Alexander\\nthe Great”. Then, in §4.4 we show how one can further\\nimprove Patchscope expressivity by using a stronger\\ntarget model, e.g., Vicuna 13B instead of Vicuna 7B.\\nLastly, we showcase the utility of Patchscopes for prac-\\ntical applications in §5. We show how it can be used to fix\\nlatent multi-hop reasoning errors, when the model is capa-\\nble of conducting each reasoning step correctly, but fails to\\nprocess their connection in-context. Building on top of the\\ndata provided by Hernandez et al. (2023b), we introduce a\\nmore complex task that requires two steps of factual reason-\\ning. We then show our proposed Patchscope improves\\naccuracy from a baseline 19.57% to 50%.\\nTo conclude, our work makes the following contributions:\\nWe propose Patchscopes, a general modular framework\\nfor decoding information from LLM hidden representations.\\nWe show that prominent interpretability methods can be\\nviewed as instances of Patchscopes, and new configu-\\nrations result in more expressive, robust across layers, and\\ntraining-data free alternatives that mitigate their shortcom-\\nings. In addition, novel configurations introduce unexplored\\npossibilities of stronger inspection techniques, as well as\\npractical benefits, such as correcting multi-hop reasoning.\\n2. Related Work\\nActivation patching is a causal intervention, commonly used\\nas a tool for studying if certain activations play a key role in\\na model’s computation (Geiger et al., 2021; Vig et al., 2020).\\nPatching has been used largely for localizing specific infor-\\nmation to specific layers and token positions (Goldowsky-\\nDill et al., 2023; Meng et al., 2022a;b; Stolfo et al., 2023),\\nand for finding paths explaining how information propa-\\ngates in the computation (Wang et al., 2022; Geva et al.,\\n2023; Hendel et al., 2023; Hanna et al., 2023; Lieberum\\net al., 2023). Despite certain limitations (Hase et al., 2023;\\nZhang & Nanda, 2023), patching remains a key tool for\\nmechanistic interpretability (Conmy et al., 2023).\\nGiven promising results from emerging interpretability ef-\\nforts that employ LLMs to generate human-like text for\\ninspection (e.g., Mousi et al., 2023; Slobodkin et al., 2023;\\nBills et al., 2023), we argue that using patching only for lo-\\ncalization purposes is myopic, and propose to use it for\\n“translating” LLM representations into natural language.\\nVery recently, patching has been used to study new prob-\\nlem setups (e.g., Pal et al., 2023; Hernandez et al., 2023b;\\nMerullo et al., 2022), all of which can be seen as different\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nTable 1. Patchscopes is a novel framework for inspection of hidden representations in language models. Many prior inspection\\nmethods with various objectives can be viewed as Patchscopes, as detailed in the “Configuration” column (see notation description\\nin §3). The rows highlighted in green show our new configurations that overcome several limitations of prior methods through more\\nexpressive inspection that is training data free and is more robust across layers. Additionally, the generality of this framework enables\\nnovel inspection possibilities that were unexplored before. When the target prompt (T) is not specified, it means the output would be\\ninvariant to the choice of T. When not specified, f ← I and M∗ = M.\\nInspection\\nObjective\\nExpressive\\nTraining\\nData\\nFree\\nRobust\\nAcross\\nLayers\\nConfiguration\\nFew-shot token identity Patchscope (§4.1)\\nËË\\nË\\nËË\\nℓ∗ ← ℓ,\\nT ← “tok1 → tok1; tok2 → tok2; . . . ; tokk”\\nLogit Lens (nostalgebraist, 2020), Embedding Space\\nAnalysis (Dar et al., 2023)\\nË\\nË\\né\\nℓ∗ ← L∗\\nTuned Lens (Belrose et al., 2023)\\nË\\nFor learning\\nmappings\\nË\\nℓ∗ ← L∗, f ← Affine\\nInspecting\\nOutput\\nDistribution\\nFuture Lens (Pal et al., 2023)\\nË\\nFor learning\\nmappings\\nËË\\nℓ∗ ← ℓ, f ← Linear, T ← Fixed or learned soft prompt\\nZero-shot feature extraction Patchscope (§4.2)\\nËË\\nË\\nËË\\nℓ∗ ← j′ ∈ [1, . . . , L∗], i∗ ← m,\\nT ← relation verbalization followed by x\\nLRE Attribute Lens (Hernandez et al., 2023b)\\nË\\nFor linear\\nrelation\\napprox.\\nËË\\nℓ∗ ← L∗, f ← Linear with additional variables, T ← S\\nFeature\\nExtraction\\nProbing (e.g., Belinkov & Glass, 2019; Belinkov,\\n2022; Alain & Bengio, 2017; Wang et al., 2023)\\né\\nFor training\\nprobe\\nË\\nN/A\\nEntity description Patchscope (§4.3)\\nËË\\nË\\nËË\\nℓ∗ ← ℓ, i∗ ← m,\\nT ← “subject1: description1, ..., subjectk: descriptionk , x”\\nX-model entity description Patchscope (§4.4)\\nËËË\\nFor learning\\nmappings\\nËË\\nM∗ ← a larger variant of M, ℓ∗ ← ℓ, i∗ ← m,\\nT ← “subject1: description1, ... , subjectk: descriptionk, x”\\nCausal Tracing (Meng et al., 2022a)\\né\\nË\\nËË\\nℓ∗ ← ℓ, T ← S + ϵ, ϵ ∼ N(0, σ)\\nEntity\\nResolution\\nAttention Knockout (Wang et al., 2022;\\nConmy et al., 2023; Geva et al., 2023)\\né\\nË\\nËË\\nℓ∗ ← Multiple, f ← 0, T ← S\\nInspection\\nApplication\\nEarly Exiting, e.g., Linear Shortcuts (Din et al., 2023)\\nË\\nFor learning\\nmappings\\nË\\nℓ∗ ← L∗, f ← Affine\\nCaption Generation, e.g., Linear Mapping\\n(Merullo et al., 2022)\\nË\\nFor learning\\nmappings\\nË\\nM∗ ← A language model of choice, ℓ∗ ← L∗, f ← Affine\\nconfigurations of our proposed framework (see §3.2).\\nInspecting the hidden representations of neural networks has\\ngradually became an active research area. Probing classifiers\\nare perhaps the most common among such efforts, ranging\\nfrom linear probes (e.g., Alain & Bengio, 2017; Belinkov &\\nGlass, 2019; Belinkov, 2022) to more recent variants like\\nGaussian Process probes (Wang et al., 2023). Since the\\nemergence of the Transformer architecture (Vaswani et al.,\\n2017), other inspection methods specific to natural language\\ndomain were born that use projections into the vocabulary\\nspace (e.g., Geva et al., 2022b; nostalgebraist, 2020; Belrose\\net al., 2023; Dar et al., 2023; Din et al., 2023), and more\\nrecent variants that extend them to vision transformers and\\nuse projections into the class embedding space (Vilas et al.,\\n2023). While various other latent inspection methods exist\\n(e.g., Zhou et al., 2018; Strobelt et al., 2017; Ghandeharioun\\net al., 2021; Kim et al., 2018), the above are the most rele-\\nvant to this work, and as we show in §3.2, many of which\\ncan be cast as different Patchscope instantiations.\\n3. Patchscopes\\nIn this section, we introduce Patchscopes and show how\\nit extends prior interpretability methods with new capabili-\\nties. While not limited to particular LLM architectures, this\\nwork focuses on auto-regressive transformer-based LLMs.\\n3.1. Framework Description\\nThe key idea in Patchscopes is to leverage the advanced\\ncapabilities of LLMs to generate human-like text for “trans-\\nlating” the information encoded in their own hidden repre-\\nsentations. Concretely, given a hidden representation ob-\\ntained from an LLM inference pass, we propose to decode\\nspecific information from it by “patching” it into a differ-\\nent inference pass (of the same or a different LLM) that\\nencourages the translation of that specific information.\\nNotably, the rest of the forward computation after patching\\ncan augment the representation with additional information,\\nhence, this approach does not guarantee that the patched\\nrepresentation itself stores all that information. However,\\ndispatching the representation from its original context (the\\nsource prompt) stops contextualization and guarantees that\\nno further information from the source prompt is incorpo-\\nrated to it in the post-patching computation. Therefore, our\\nframework reveals if specific information can be decoded\\nfrom the patched representation via the post-patching com-\\nputation, which is an implicit way to expose the information\\ncontextualized within it.\\nGiven an input sequence of n tokens S = ⟨s1, ..., sn⟩ and\\na model M with L layers, hℓ\\ni denotes the hidden repre-\\nsentation obtained at layer ℓ ∈ [1, . . . , L] and position\\ni ∈ [1, . . . , n], when running M on S. To inspect hℓ\\ni,\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nwe consider a separate inference pass of a model M∗ with\\nL∗ layers on a target sequence T = ⟨t1, . . . , tm⟩ of m to-\\nkens. Specifically, we choose a hidden representation ¯hℓ∗\\ni∗\\nat layer ℓ∗ ∈ [1, . . . , L∗] and position i∗ ∈ [1, . . . , m] in\\nthe execution of M∗ on T. Moreover, we define a map-\\nping function f(h; θ) : Rd 7→ Rd∗ parameterized by θ that\\noperates on hidden representations of M, where d and d∗\\ndenote the hidden dimension of representations in M and\\nM∗, respectively. This function can be the identity function,\\na linear or affine function learned on task-specific pairs of\\nrepresentations, or even more complex functions that incor-\\nporate other sources of data. The patching operation refers\\nto dynamically replacing the representation ¯hℓ∗\\ni∗ during the\\ninference of M∗ on T with f(hℓ\\ni). Namely, by applying\\n¯hℓ∗\\ni∗ ← f(hℓ\\ni), we intervene on the generation process and\\nmodify the computation after layer ℓ∗.\\nOverall, a Patchscope intervention applied to a represen-\\ntation determined by (S, i, M, ℓ), is defined by a quintuplet\\n(T, i∗, f, M∗, ℓ∗) of a target prompt T, a target position i∗\\nin this prompt, a mapping function f, a target model M∗,\\nand a target layer ℓ∗ of this model. Notably, it is possible\\nthat M and M∗ are the same model, S and T are the same\\nprompt, and f is the identity function I (i.e., I(h) = h).\\nIn the following sections, we show how this formulation\\ncovers prior interpretability methods and further extends\\nthem with new capabilities.\\n3.2. Patchscopes Encompasses Prior Methods\\nWe show how prominent interpretability methods can be\\ncast as Patchscope instances. See a summary in Tab. 1.\\nRecent methods inspect LLM representations by project-\\ning them to the output vocabulary space (Dar et al., 2023;\\nnostalgebraist, 2020; Din et al., 2023; Belrose et al., 2023).\\nFormally, an estimation of the output distribution is obtained\\nfrom the representation hℓ\\ni at position i and layer ℓ by:\\npℓ\\ni = softmax(WUf(hℓ\\ni)) ∈ R|V |,\\nwhere WU ∈ R|V |×d is the model’s unembedding matrix\\nand f is a simple mapping function, such as the identity\\nfunction or an affine mapping. We note that the operation\\napplied to f(hℓ\\ni) is the same computation applied by the\\nmodel to the last-layer representation for obtaining the next-\\ntoken prediction. Therefore, prior methods that inspect\\nrepresentations in the vocabulary space can be viewed as\\na class of Patchscopes with identical source and target\\nprompts (T = S) that maps representations from any source\\nlayer ℓ to the last target layer L∗. Differences between these\\nmethods lie in the choice of f; logit lens (nostalgebraist,\\n2020; Dar et al., 2023) applies the identity function, linear\\nshortcuts (Din et al., 2023) uses a linear mapping function,\\nand tuned lens (Belrose et al., 2023) trains an affine map-\\nping. Recently, Hernandez et al. (2023b) introduced LRE\\nAttribute Lens that builds f based on a relation linearity\\nassumption, and they showcase its effectiveness in attribute\\nextraction.\\nThis class of methods has proven to be effective for different\\napplications, for example, in improving inference efficiency\\nvia early exiting (Din et al., 2023). While the majority\\nof methods and applications in this category use a single\\nmodel (M∗ = M), Merullo et al. (2022) had demonstrated\\nsuccessful caption generation with a generative image model\\nas M and a language model as M∗.\\nAnother category of inspection methods intervene on the\\nLLM computation. Contemporary to our work, Pal et al.\\n(2023) have investigated whether it is possible to antici-\\npate multiple generated tokens ahead from a given hidden\\nrepresentation, rather than estimating just the next-token\\nprediction. Their method, called Future Lens, uses a tar-\\nget prompt that is different from the original prompt (i.e.,\\nT ̸= S) and is designed to decode subsequent tokens from\\ninformation encoded in a hidden representation hℓ\\ni. Ex-\\nample target prompts are “The multi-tokens present here\\nare ” and “Hello! Could you please tell me more about ”.\\nNotably, Future Lens can be cast as another Patchscope\\nwith M∗ = M and ℓ∗ = ℓ.\\nMore broadly, Patchscopes also covers recent mechanis-\\ntic interpretability methods that analyze internal processes\\nin LLMs with inference computation interventions. Specif-\\nically, causal tracing (Meng et al., 2022a) uses a source\\nprompt augmented with Gaussian noise as the target prompt.\\nIn addition, previous work have intervened on one or more\\ntarget layers during inference by patching zero vectors to\\nthe computation (Wang et al., 2022; Conmy et al., 2023;\\nGeva et al., 2023), namely, setting f(h) = 0.\\n3.3. Patchscopes Enables Novel Inspection Methods\\nPrior work has utilized specific patching configurations for\\ninterpretability, largely focusing on patching the same model\\nwhile using the same prompt (i.e., M∗ = M, T = S).\\nThe framing of Patchscopes introduces a wide range of\\nunexplored configurations that could potentially unlock new\\ninspection capabilities.\\nSpecifically, we observe that modifying the target prompt\\nenables an expressive decoding of any feature of our choice,\\ndetached from the source prompt computation. For instance,\\nwe can use the prompt “The capital of X is” to check if the\\ncapital city of a given country is extractable from the hidden\\nrepresentation of this country at specific layer. Similarly,\\na prompt like “Tell me facts about X” can be leveraged\\nto assess whether the model has resolved the entity name\\ncorresponding to a given description in a specific layer (see\\nFig. 1). Importantly, contrary to probing, this approach is\\nnot restricted by the number of classes of the chosen feature.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0.0\\n0.5\\n1.0\\nprecision@1\\nVicuna (13B)\\nLlama2 (13B)\\nPythia (12B)\\nGPT-J (6B)\\n10\\n20\\nlayer\\n0\\n10\\nsurprisal\\n10\\n20\\nlayer\\n10\\n20\\nlayer\\n10\\n20\\nlayer\\nLogit Lens\\nTuned Lens\\nToken Identity (Ours)\\nFigure 2. Precision@1 (↑ is better) and Surprisal (↓ is better) of\\nnext-token prediction estimation in LLaMA2 (13B), Vicuna (13B),\\nGPT-J (6B), and Pythia (12B). From layer 10 and upwards, the\\ntoken identity method (ours) consistently outperforms the rest of\\nthe baselines across all the models.\\nMoreover, patching the representation into a more capable\\nmodel could be useful in cases when the inspected model\\nis not expressive enough to answer particular queries (Her-\\nnandez et al., 2022; Singh et al., 2023; Schwettmann et al.,\\n2023). In the next section, we show that these new exten-\\nsions substantially outperform existing methods for query-\\ning specific information in the computation of LLMs.\\n4. Experiments\\nWe evaluate how well our framework allows decoding dif-\\nferent types of information from LLM representations, in-\\ncluding next-token predictions (§4.1) and specific attributes\\n(§4.2). Then, we demonstrate the new possibilities that\\nPatchscopes introduce, focusing on analyzing the con-\\ntextualization of entity names (§4.3) and leveraging stronger\\nmodels for inspection via cross-model patching (§4.4).\\nTab. 1 summarizes the new proposed Patchscopes and\\ntheir configurations compared to prior work.\\n4.1. Decoding of Next-Token Predictions\\nAs introduced in §3.2, let pL be the output probability dis-\\ntribution for some input, obtained by multiplying the final-\\nlayer last-position hidden representation hL by the unem-\\nbedding matrix WU ∈ R|V |×d. We wish to estimate pL\\nfrom intermediate representations hℓ s.t. ℓ < L. Particu-\\nlarly, we ask how early in the computation the model has\\nconcluded its final prediction from the given context. In our\\nexperiments, we consider multiple LLMs – LLaMA2 (13B)\\n(Touvron et al., 2023b), Vicuna (13B) (Chiang et al., 2023),\\nGPT-J (6B) (Wang & Komatsuzaki, 2021), and Pythia (12B)\\n(Biderman et al., 2023) (see more details in §A.1).\\nMethods\\nWe compare vocabulary projection methods\\n(§3.2) with a new Patchscope. Each method yields an\\nestimated output probability ˜pℓ by patching an intermedi-\\nate representation hℓ to the model’s final layer. Here, we\\nfocus on the common setting where M = M∗, and discuss\\nextensions to M ̸= M∗ in §4.4.\\n• Logit Lens: Following prior work (nostalgebraist, 2020;\\nGeva et al., 2022a), we define f as the identity function,\\nmeaning no change is applied to the patched representa-\\ntion. That is, f(h) := I(h).\\n• Tuned Lens: Motivated by Belrose et al. (2023); Din et al.\\n(2023), we employ an affine mapping function between\\nrepresentations at layer ℓ and the final layer L. Specifi-\\ncally, we feed the model examples from a training set T\\nand for each example s ∈ T obtain a pair (hℓ\\ns, hL\\ns ) of\\nhidden representations. Then, we fit linear regression to\\nfind a matrix Aℓ ∈ Rd×d and a bias vector bℓ ∈ Rd that\\nare numerical minimizers for P\\ns∈T ||Ahℓ\\ns − hL\\ns + b||2.\\nWe define f as:\\nf(hℓ) := Aℓhℓ + bℓ.\\n• Token Identity Patchscope: Unlike the previous\\nmethods, here we use a target prompt that is different\\nfrom the source prompt (T ̸= S) and is meant to en-\\ncourage the model to decode the token identity of the\\nhidden representation. Also, while the above methods\\nskip the computation between layers l and L, here we\\nmodify it such that all the information from the source\\nprompt computation is discarded, except for the patched\\nrepresentation. We craft a prompt with k demonstra-\\ntions representing an identity-like function, formatted\\nas “tok1 → tok1 ; tok2 → tok2 ; . . . ; tokk”. Fur-\\nther details and an experiment showing the robustness of\\nthe method to this selection are provided in §A.3. Note\\nthat this Patchscope does not require any training.\\nEvaluation\\nWe follow Din et al. (2023) and evaluate the\\nestimated prediction on the Pile evaluation set (see §A.2 for\\ndetails) using two metrics:\\n• Precision@1 (↑ is better): The portion of examples for\\nwhich the highest-probability token t in the estimated\\nprobability distribution matches the highest-probability\\ntoken in the original output distribution.\\nThat is, if\\narg maxt(˜pℓ\\nt) = arg maxt(pL\\nt ).\\n• Surprisal (↓ is better): The minus log-probability of the\\nhighest-probability token in the predicted distribution ˜pℓ\\naccording to pL, i.e., − log pL\\n˜t , where ˜t = arg maxt(˜pℓ\\nt).\\nResults\\nFig. 2 depicts the results.\\nAcross all the\\nmodels, from layer 10 and upwards, the token identity\\nPatchscope consistently outperforms the other baselines,\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nobtaining a gain of up to 98% in layers 18-22. This demon-\\nstrates the utility of leveraging the model’s decoding pro-\\ncedure for inspecting representations of different source\\nprompts, and shows that in most cases hidden representa-\\ntions in early layers carry out the prediction information\\nregardless of their context.\\nIn the first 10 layers, performance of all methods is sub-\\nstantially lower, with the token identity prompt performing\\non-par to logit lens and worse than tuned lens, in terms of\\nprecision. The advantage of tuned lens on this task might\\nbe attributed to the additional training of the mappings. The\\noverall low performance is expected, as the first layers con-\\ntextualize the input. In §4.3, we introduce a Patchscope\\nthat is geared towards unraveling this process.\\n4.2. Extraction of Specific Attributes\\nClassification probes are arguably the most commonly used\\nmethod for checking if certain attributes are encoded in\\nhidden representations (Belinkov, 2022; Belinkov & Glass,\\n2019). However, they need to be trained, and the range\\nof attribute classes needs to be known a priori. Here we\\nshow that repurposing Patchscopes for attribute extrac-\\ntion overcomes these limitations. First, it does not require\\ntraining. Second, it is not limited by a predefined set of\\nlabels, but rather can benefit from an open vocabulary. In\\naddition, by taking advantage of the model’s nonlinearities,\\nit is more flexible in capturing complex relations compared\\nto linear probes.\\nExperimental\\nSetup\\nConsider\\nfactual\\nand\\ncom-\\nmonsense knowledge represented as triplets (σ, ρ, ω)\\nof\\na\\nsubject\\n(e.g.,\\n“United States”),\\na\\nrela-\\ntion (e.g.,\\n“largest city of”),\\nand an object\\n(“New York City”). We investigate to what extent the\\nobject ω can be extracted from the last token representation\\nof the subject σ in an arbitrary input context. To this end,\\nwe conduct experiments on 8 commonsense and 25 factual\\nknowledge tasks curated by Hernandez et al. (2023b). This\\ndataset includes (σ, ρ, ω) triplets for different relations,\\nalong with prompt templates that verbalize them in natural\\nlanguage. We conduct experiments with GPT-J (6B) (Wang\\n& Komatsuzaki, 2021), filtering the data to keep only the\\nexamples where o appears in the the model’s continuation\\nof the prompt up to 20 tokens. For each example, we\\nsample 5 utterances from the WikiText-103 dataset (Merity\\net al., 2016) that include σ and use them as S. Lastly,\\nwe keep tasks with at least 15 samples, which results in\\n5 commonsense and 7 factual tasks with a total of 1,453\\ndatapoints. For more details, see §B.\\nMethods\\nWe devise a Patchscope for feature extrac-\\ntion and compare it with linear probing (K¨ohn, 2015; Gupta\\net al., 2015) as a baseline.\\n• Zero-shot Feature Extraction Patchscope: We craft\\nT as a general verbalization of ρ followed by a place-\\nholder for σ, such that i∗ = m. For example, we use\\nT ← “The largest city in x” with “x” as a\\nplaceholder for the subject. To extract the object from\\nthe entity representation in S, we patch the represen-\\ntation of token “x” at layer ℓ∗ with the representation\\nof “States” from layer ℓ, and consider if the gener-\\nated text includes ω. The remaining configurations of\\nthis Patchscope are f ← I, M∗ ← M, i ← the\\nlast token of σ in S. We consider all combinations of\\nℓ ∈ [1, . . . , L] × ℓ∗ ∈ [1, . . . , L∗]. Later in this section,\\nwe discuss the role of ℓ pertaining to attribute extraction.\\n• Logistic Regression Probe: Let Ω represent the range\\nof possible objects for a given relation. We use the set\\nof unique values of ω in the training set as a proxy for Ω.\\nWe train a logistic regression probe (K¨ohn, 2015; Gupta\\net al., 2015) for each layer that predicts ω ∈ Ω from\\nlast token representation of σ. Given that 6 out of 12\\ntasks have fewer than 40 datapoints, we use three-fold\\ncross-validation for training and evaluation of this base-\\nline. Note that we have excluded tasks where the probe\\nfails completely due to insufficient number of training\\nexamples (fewer than 15 datapoints).\\nEvaluation\\nWe measure the average attribute extraction\\naccuracy. For a given sample, the Patchscope is consid-\\nered correct if ∃ ℓ∗ ∈ [1, . . . , L∗] where the generated text\\nup to 20 tokens includes ω. For the probe, a prediction is\\ncorrect if the highest probability is assigned to ω.\\nResults\\nTab. 2 summarizes the results, averaged over ℓ ∈\\n[1, . . . , L]. We conduct a T-test with Bonferroni correction\\nto compare the two methods. Despite using no training data\\nand having no restrictions on the output, the Patchscope\\nachieves a significantly higher accuracy than the probe on 6\\nout of 12 tasks (p<1e−5), compared to only one task where\\nprobing is better. For the remaining 5 tasks, the difference\\nbetween the Patchscope and baseline is not significant.\\nPerformance Breakdown Across Source Layers\\nWe\\nstudy how the accuracy changes across the source layers\\nℓ ∈ [1, . . . , L]. Fig. 3 shows one commonsense and one\\nfactual reasoning task as exemplars. Results for all other\\ntasks follow similar trends and can be found in §B. Consid-\\nering the early layers, we observe that the Patchscope\\nconsistently outperforms the baseline. This further confirms\\nour hypothesis that prior methods are particularly limited in\\nsurfacing information in the early layers, which often cannot\\nbe decoded via linear functions. However, Patchscope\\nis able to extract such attributes significantly earlier. Note\\nthat this observation does not mean the attribute is explicitly\\nencoded in a representation, but that there is enough informa-\\ntion encoded such that the attribute can be extracted from the\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nTable 2. Comparing Zero-Shot Feature Extraction Patchscope\\nto a Logistic Regression Probe shows that despite using no training\\ndata, it has a significantly higher accuracy than baseline in 6 out\\nof 12 tasks. We use pairwise t-test with Bonferroni correction for\\ncomparing the accuracy of the two methods.\\n∗∗ and ∗ indicate\\np< 1e−5 and p<1e−4, respectively.\\nAccuracy (mean±std)\\nTask\\nLogistic\\nRe-\\ngression Probe\\nZero-shot\\nFeat. Ext.\\nPatchscope\\nCommonsense\\nFruit inside color\\n37.4 ± 6.6\\n38.0 ± 18.7\\nFruit outside color\\n35.5 ± 3.1\\n71.0 ± 13.3∗∗\\nObject superclass\\n65.6 ± 10.5∗\\n54.8 ± 11.3\\nSubstance phase\\n73.8 ± 3.7\\n91.9 ± 1.7∗∗\\nTask done by tool\\n10.1 ± 3.2\\n48.1 ± 13.2∗∗\\nFactual\\nCompany CEO\\n5.0 ± 2.6\\n47.8 ± 13.9∗∗\\nCountry currency\\n17.7 ± 2.2\\n51.0 ± 8.9∗∗\\nFood from country\\n5.1 ± 3.7\\n63.8 ± 11.3∗∗\\nPlays pos. in sport\\n75.9 ± 9.1\\n72.2 ± 7.2\\nPlays pro sport\\n53.8 ± 10.3\\n46.3 ± 14.2\\nProduct by co.\\n58.9 ± 7.2\\n63.2 ± 10.7\\nStar constellation\\n17.5 ± 5.3\\n18.4 ± 5.1\\nrepresentation alone, without its original context, using the\\nmodel’s computation. In the middle layers, Patchscope\\nworks similarly or better than the baseline.\\nInterestingly, we observe that almost all cases where\\nPatchscope performs worse than the baseline occur in\\nlater layers (see the gradual decline in Fig. 3). Our interpre-\\ntation is that given the language modeling training objective,\\nthe representations shift toward next-token prediction in\\nthe later layers. Therefore, the attribute of interest would\\nnot be as readily accessible via the model’s computation\\nin these layers. This interpretation is also aligned with re-\\ncent findings that show no decline in using linear relational\\nembedding in predicting ω only when the next token also\\nhappens to be ω (Hernandez et al., 2023b). We postulate\\nthat when the immediate next token is not the attribute of in-\\nterest, it does not necessarily mean the attribute information\\nis lost, but rather it may not be accessible on the surface.\\nWe hypothesize that using a Patchscope with a more\\nexpressive mapping f could improve attribute extraction\\naccuracy in the later layers, which we leave for future work.\\nFor additional analyses see §B.\\n4.3. Analyzing Entity Resolution in Early Layers\\nThe previous sections focused on analyzing the information\\nencoded in a single hidden state. Here we turn to consider a\\nmore global question of how LLMs resolve entity mentions\\nacross multiple layers. Concretely, given a subject entity\\nname, such as “the summer Olympics of 1996”, how does\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nAttribute Extraction Acc.\\ntask done by tool (commonsense)\\nZero-Shot Feat. Ext. Patchscope\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n10\\n20\\n30\\n40\\n50\\n60\\ncountry currency (factual)\\nLogistic Reg. Probe\\nFigure 3. Attribute extraction accuracy across source layers (ℓ).\\nLeft: Task done by tool (commonsense), 54 Source prompts, 12\\nClasses. Right: Country currency (factual), 83 Source prompts,\\n14 Classes. Zero-Shot Feature Extraction Patchscope works\\nconsistently better than Logistic Regression Probe across all layers.\\nThere is a decline in Patchscope accuracy in later ℓ as the\\nsource representations shift toward next-token prediction.\\nthe model contextualize the input tokens of the entity and at\\nwhich layer is it fully resolved?\\nAnswering these questions is hard with existing methods;\\nvocabulary projections focus on the output prediction and\\nfail to show clear patterns in early layers, and probing is\\nrestricted to outputs from a fixed number of classes, which\\nmay not be expressive enough to describe this process. Al-\\nternative approaches have studied this process indirectly\\nvia interventions (Meng et al., 2022a), showing that the\\nmodel constructs a subject representation at the last token\\nof the entity name. However, it is still unclear how this\\ncontextualization is performed.\\nWe analyze how LLMs contextualize input entity names\\nby leveraging Patchscopes. Particularly, we craft a tar-\\nget prompt for generating a description of a given subject,\\nand apply it to the hidden representation at the last subject\\nposition in the source prompt (where the model forms the\\nsubject representation (Geva et al., 2023; Hernandez et al.,\\n2023a)) across the early layers. This will allow us to see\\nhow the model describes the subject in each layer.\\nAnalysis Setting\\nWe use a few-shot target prompt\\ntemplate for decoding an entity description: “subject1:\\ndescription of subject1,..., subjectk:\\ndescription of subjectk, x”,\\nwhile patching\\nthe last position corresponding to x.\\nWe take the 200\\nmost popular and 200 least popular subject entities from\\nthe PopQA dataset (Mallen et al., 2023).\\nThe popular\\nentities should appear frequently in LLMs’ pre-training\\ndata, and are thus likely to be captured by the model,\\nwhile resolving the rare entities is expected to be more\\nchallenging (Kandpal et al., 2023; Mallen et al., 2023).\\nThen, for the source prompt we use the entity name, and for\\nthe target prompt we sample k = 3 random subject entities.\\nWe obtain a short (up to one sentence) description of every\\nsubject entity from Wikipedia. Our target prompt and more\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nTable 3. Illustrating entity resolution via qualitative examples. The\\nexpressive generations show that as we go through the layers, more\\ntokens from the context get integrated into the current representa-\\ntion, referred to as “Tokens Processed”. The “Explanation” column\\nexplains what the generation seems to be referring to and how that\\nrelates to the tokens processed. Here, M∗ ← M, ℓ∗ ← ℓ.\\nTokens Processed\\nℓ\\nGeneration\\nExplanation\\nM ← Vicuna 13B\\nS ← \"Diana, Princess of Wales\"\\n\"Wales\"\\n1-2\\n: Country in the United Kingdom\\nWales\\n\"Wales\"\\n3\\n: Country in Europe\\nWales\\n\"Princess\\nof Wales\"\\n4\\n: Title held by female sovereigns in their\\nown right or by queens consort\\nPrincess of Wales\\n(unspecific)\\n\"Princess\\nof Wales\"\\n5\\n: Title given to the wife of the Prince of\\nWales (and later King)\\nPrincess of Wales\\n(unspecific)\\n\"Diana,\\nPrincess\\nof Wales\"\\n6\\n: Diana, Princess of Wales (1961-1997),\\nthe first wife of Prince Charles, Prince of\\nWales, who was famous for her beauty\\nand humanitarian work\\nDiana,\\nPrincess of Wales\\nM ← Pythia 12B\\nS ← \"Alexander the Great\"\\n\"Great\"\\n1\\nBritain: Country in the European Union\\nGreat Britain\\n\"the Great\"\\n2\\nWall Street Crash of 1929: Financial\\ncrisis in the United States\\nthe Great\\nDepression\\n\"the Great\"\\n3\\nWall Street Bubble: The Great\\nDepression\\nthe Great\\nDepression\\n\"the Great\"\\n4\\nWall Street: Wall Street in New York\\nCity\\nWall Street\\n(related to the\\nGreat Depression)\\n\"Alexander\\nthe Great\"\\n5\\n: Ancient Greek ruler, and the first to rule\\nall of the then known world\\nAlexander\\nthe Great\\ntechnical details are provided in §C.1. For the analysis,\\nwe patch the last position representations from the first 10\\nlayers of Vicuna 13B to the target prompt and evaluate the\\ngenerated subject name and description. Specifically, the\\ngenerated descriptions are evaluated against the descriptions\\nfrom Wikipedia using RougeL (Lin, 2004). Evaluation\\nwith Rouge1 (Lin, 2004) and Sentence-Bert (Reimers &\\nGurevych, 2019) shows similar trends (see §C.2).\\nResults\\nTab. 3 illustrates the generated text for two subject\\nentities, one per model, when patching their representations\\nat different layers to the target prompt (for more examples\\nsee §C.3). For most entities, the contextualization process\\nis spread over the first layers, with the last subject token\\nencompassing from more distant positions across layers.\\nThis trend can be quantitatively observed by the similarity\\nbetween the generated descriptions and the descriptions\\nfrom Wikipedia, as measured by RougeL. See Fig. 4 where\\nM = M∗. For both models, similarity increases in the first\\n5 layers and then slowly decreases. This decrease could\\npotentially be attributed to contamination caused by the\\nrepresentation of the placeholder token “x” remaining in the\\nearly layers, when patching is applied to a later layer. Note\\nthat this potential issue is only applicable to multi-token\\ngeneration scenarios as future positions can still attend to the\\nplaceholder position in early layers, potentially interfering\\nwith the model’s ability to accurately generate descriptions\\n0\\n2\\n4\\n6\\n8\\nlayer\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nRougeL\\nM \\n 7B,   M* \\n 7B\\nM \\n 13B, M* \\n 13B\\nM \\n 7B,   M* \\n 13B\\npopular entities\\nrare entities\\nFigure 4. RougeL scores of the generated descriptions against de-\\nscriptions from Wikipedia, using the Vicuna models.\\nfor the patched token. See §C.3 for qualitative examples\\ncorroborating this interpretation. Notably, the scores for\\nrare, long-tail entities are significantly lower than the scores\\nof popular entities, as expected. Additional results for Pythia\\nare depicted in §C.2 show that the smaller model seems to\\noutperform the larger model, possibly because of the larger\\nmodel is biased toward output generation at the expense of\\ninput contextualization.\\nTaken together,\\nthis analysis shows the utility of\\nPatchscopes for inspecting the contextualization pro-\\ncess in early layers of LLMs.\\n4.4. Expressiveness from Cross-Model Patching\\nA possible avenue for improving inspection capabilities is to\\nexplain a given model with a model that is more expressive\\n(Bills et al., 2023). In the context of Patchscopes, this\\nmeans to patch a representation of M into a more expressive\\nmodel M∗. However, it is not clear if such an intervention\\nwould yield plausible results, due to possible discrepancies\\nbetween the two models resulting from different architec-\\ntures, optimization processes, and so on.\\nWe show that when patching across models from the same\\nfamily, such interventions are possible and can improve ex-\\npressiveness. Specifically, we consider patching representa-\\ntions across different sizes of Vicuna (M←7B, M∗←13B)\\nand Pythia (M←6.9B, M∗←12B), and measure how well\\nthe larger model estimates the next-token predictions and\\nentity resolution process of the smaller model.\\nNext-Token Prediction\\nWe repeat the experiment in §4.1,\\nusing the token identity Patchscope. To overcome dis-\\ncrepancies between the models, we learn affine mappings\\nbetween their layers (similarly to Tuned Lens). Fig. 5 de-\\npicts the Precision@1 scores for different combinations of\\nsource-target layers, showing that patching with a simple\\naffine Patchscope proves to be effective with precision\\nof up to 0.7 and 0.8 for Vicuna and Pythia, respectively.\\nSpecifically, patching representations to an early layer of the\\nlarger model seems to be the most effective. Furthermore, it\\nappears that there is a subtle matching among some layers of\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\ntarget layer\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nsource layer\\n0.7\\n0.7\\n0.5\\n0.5\\n0.4\\n0.3\\n0.2\\n0.2\\n0.7\\n0.7\\n0.6\\n0.6\\n0.5\\n0.4\\n0.4\\n0.3\\n0.7\\n0.7\\n0.6\\n0.6\\n0.6\\n0.5\\n0.4\\n0.3\\n0.7\\n0.6\\n0.6\\n0.6\\n0.6\\n0.6\\n0.5\\n0.4\\n0.7\\n0.6\\n0.5\\n0.6\\n0.6\\n0.7\\n0.6\\n0.6\\n0.7\\n0.5\\n0.5\\n0.5\\n0.6\\n0.7\\n0.7\\n0.7\\n0.6\\n0.5\\n0.5\\n0.6\\n0.6\\n0.7\\n0.7\\n0.7\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(a) Vicuna: M←7B, M∗ ←13B\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\ntarget layer\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nsource layer\\n0.8\\n0.6\\n0.6\\n0.5\\n0.3\\n0.3\\n0.2\\n0.2\\n0.8\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.3\\n0.8\\n0.8\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.8\\n0.8\\n0.8\\n0.7\\n0.7\\n0.6\\n0.5\\n0.4\\n0.7\\n0.8\\n0.7\\n0.7\\n0.7\\n0.7\\n0.6\\n0.5\\n0.7\\n0.7\\n0.7\\n0.7\\n0.7\\n0.7\\n0.7\\n0.5\\n0.7\\n0.7\\n0.7\\n0.7\\n0.7\\n0.7\\n0.7\\n0.6\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(b) Pythia: M←6.9B, M∗ ←12B\\nFigure 5. Next-token prediction estimation performance in Vicuna\\nand Pythia with cross-model Patchscopes, measured by Preci-\\nsion@1 (↑ is better).\\nthe models, with the diagonal consistently exhibiting higher\\nvalues. Similar trends are observed in the surprisal results\\nfor both models (see Fig. 12 in §D). Overall, these results\\nshow that, when M∗ and M are from the same model fam-\\nily, it is possible to leverage M∗ for decoding information\\nfrom the representations of M. Notably, our findings ex-\\ntend observations by Csisz´arik et al. (2021) from patching\\nrepresentations across deep convolutional neural networks\\nwith the same architecture but different initializations.\\nEntity Resolution in Early Layers\\nWe now show that us-\\ning a large model as M∗ can enhance the output expressivity.\\nTo this end, we repeat our entity resolution experiment in\\n§4.3 with Vicuna model family, setting M←7B, M∗←13B.\\nFig. 4 shows the cross-model patching results (orange line)\\ncompared to the same-model patching. The results show\\nthat cross-model patching from a smaller model to its larger\\nversion generally improves the ability to inspect the input\\ncontextualization, both for popular and rare entities. For\\nPythia, since the smaller model outperforms the larger one,\\ncross-model patching is not as effective (see §C.1).\\n5. Application: Self-Correction in Multi-Hop\\nReasoning\\nMulti-hop reasoning is a challenging problem. While a\\nlanguage model may be capable of correctly answering\\neach step independently, it could still fail at processing the\\nconnection between different steps, resulting in an incorrect\\nprediction. Recent attempts to improve multi-hop reasoning\\nrely on prompting the model to generate a step-by-step\\nanswer autoregressively (e.g., Wei et al., 2022; Yao et al.,\\n2023; Besta et al., 2024), possibly with an iterative process\\nof self-refinement (Madaan et al., 2023). While effective\\nin terms of performance, these methods incur additional\\ninference costs that could be avoidable.\\nIn this section, we show that Patchscopes can improve\\nmulti-hop reasoning performance without generating the\\nreasoning steps, particularly in cases when the model fails\\nat completing a multi-hop query despite being successful\\nin each reasoning step independently. Via Patchscopes,\\none can surgically operate on the model representations,\\nreroute model’s intermediate answer to one step of the rea-\\nsoning task, simplify the consequent reasoning step, and\\nultimately correct the final prediction.\\nExperimental Setup\\nFollowing the notation in §4.2,\\nlet τ1\\n=\\n(σ1, ρ1, ω1) represent the relation ρ1 be-\\ntween a subject entity σ1 and an object entity ω1. Let\\nτ2\\n=\\n(σ2, ρ2, ω2) represent another tuple such that\\nσ2 = ω1.\\nA multi-hop reasoning query pertaining to\\nτ1 and τ2 is a prompt composed of two parts: π1 is a\\nverbalization of σ1 and ρ1 from which ω1 can be inferred;\\nπ2 is a verbalization of ρ2, from which ω2 can be inferred\\nafter its concatenation with π1. For example, Let τ1 ←\\n(“‘Visual Basic”, “product of”, “Microsoft”)\\nand\\nτ2\\n←\\n(“Microsoft”,\\n“company CEO”,\\n“Satya Nadella”).\\nAn example verbalization of\\nthese tuples would result in π1\\n←“the company\\nthat created Visual Basic Script”,\\nand\\nπ2\\n←\\n“The current CEO of”.\\nThis leads to\\nsystematic generation of the multi-hop reasoning query\\n[π2][π1]\\n=“The current CEO of the company\\nthat created Visual Basic Script”.\\nBuild-\\ning on Hernandez et al. (2023b), we systematically generate\\nall valid multi-hop factual and commonsense reasoning\\nqueries where ω1 = σ2. We conduct experiments on Vicuna\\n(13B), focusing on samples where M accurately represents\\nboth τ1 and τ2 independently, that is, ω appears in the next\\n20 tokens M generates conditioned on the prompt π that\\nverbalizes σ and ρ. This process yields 1,104 multi-hop\\nreasoning samples, out of which 46 satisfy the above criteria\\nand are used for evaluation. For more details, see §E.\\nMethod and Evaluation\\nWe introduce a Chain-of-\\nThought (CoT) Patchscope to fix multi-hop reasoning\\nvia intervening on the computation graph and rerouting rep-\\nresentation likely to capture ω1 in place of σ2. Concretely, S\\nrefers to the formed query discussed above, and we use the\\nfollowing configuration: T ← S, M∗ ← M, i ← n, i∗ ←\\nthe token preceding π1. As a baseline, we evaluate a vanilla\\ngeneration of M conditioned on S without any intervention.\\nWe evaluate the outputs in terms of accuracy, similarly to\\n§4.2. For a sample S, the Patchscope is considered ac-\\ncurate if ∃ (ℓ, ℓ∗) : ℓ ∈ [1, . . . , L], ℓ∗ ∈ [1, . . . , L∗] where\\nthe autoregressive generation up to 20 tokens includes ω2.\\nResults\\nWhile the baseline accuracy is only 19.57%, the\\nPatchscope achieves 50% accuracy. Fig. 6 shows the\\ninteraction between ℓ and ℓ∗ and how it affects the success\\nrate. Patching representations from most source layers ℓ\\ninto early-to-mid ℓ∗ (6-16) is most effective in making the\\nright prediction. Our interpretation is that patching into late\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0 2 4 6 8 101214161820222426283032343638\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\n28\\n30\\n32\\n34\\n36\\n38\\nTarget Layer (\\n* )\\nSelf-correction in Multi-hop Reasoning\\n# samples: 73600\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\n0.12\\nFigure 6. The interaction between source (ℓ) and target (ℓ∗) layers\\nin self-correction. The majority of success cases correspond to\\nearly-to-mid ℓ∗. We observe higher cumulative success rate in the\\nlower right triangle which corresponds to ℓ∗ ≤ ℓ.\\nℓ∗ is not effective because ρ2 has already been processed\\nand the result has been copied to the last position, therefore\\nit cannot incorporate the proxy of σ2. We also observe that\\nℓ∗ ≤ ℓ is more successful on average.\\n6. Conclusion\\nWe present Patchscopes, a simple and effective frame-\\nwork that leverages the ability of LLMs to generate human-\\nlike text for decoding information from intermediate LLM\\nrepresentations. We show that existing interpretability meth-\\nods can be cast as specific instances of Patchscopes,\\nwhich cover only a small portion of all the possible config-\\nurations of the framework. Moreover, using new underex-\\nplored Patchscopes substantially improves our ability\\nto decode various types of information from the model’s in-\\nternal computation, such as the output prediction and knowl-\\nedge attributes, typically outperforming prominent methods\\nthat rely on projection to the vocabulary and probing. In ad-\\ndition, our framework enables new capabilities, such as ana-\\nlyzing the contextualization process of input tokens in the\\nvery early layers of the model, and is beneficial for practical\\napplications, such as multi-hop reasoning correction. This\\npaper only scratches the surface of the opportunities this\\nframework creates. Future work could study its application\\nacross different domains and modalities, investigate its vari-\\nants with simultaneous multi-token patching, and present\\nrecipes for task-specific and task-agonstic Patchscopes.\\nAcknowledgements\\nWe thank Amir Globerson for feedback on writing and pre-\\nsentation of results. We thank Ardavan Saeedi, Martin Wat-\\ntenberg, Ellie Pavlick, the AI Explorables team3 at Google\\nResearch, and Jasmijn Bastings for their helpful comments.\\n3https://pair.withgoogle.com/explorables\\nReferences\\nAlain, G. and Bengio, Y. Understanding intermediate layers\\nusing linear classifier probes.\\n5th International Con-\\nference on Learning Representations, Workshop Track\\nProceedings, 2017.\\nBelinkov, Y. Probing classifiers: Promises, shortcomings,\\nand advances. Computational Linguistics, 48(1):207–219,\\n2022.\\nBelinkov, Y. and Glass, J.\\nAnalysis methods in neural\\nlanguage processing: A survey. Transactions of the Asso-\\nciation for Computational Linguistics, 7:49–72, 2019.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBesta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-\\naninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,\\nNiewiadomski, H., Nyczyk, P., et al. Graph of thoughts:\\nSolving elaborate problems with large language models.\\nProceedings of the 38th AAAI Conference on Artificial\\nIntelligence, 2024.\\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,\\nH., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for analyz-\\ning large language models across training and scaling. In\\nInternational Conference on Machine Learning (ICML),\\n2023.\\nBills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L.,\\nGoh, G., Sutskever, I., Leike, J., Wu, J., and Saunders,\\nW. Language models can explain neurons in language\\nmodels. URL https://openaipublic. blob. core. windows.\\nnet/neuron-explainer/paper/index. html.(Date accessed:\\n14.05. 2023), 2023.\\nCasper, S., Rauker, T., Ho, A., and Hadfield-Menell, D.\\nSok: Toward transparent ai: A survey on interpreting\\nthe inner structures of deep neural networks. In First\\nIEEE Conference on Secure and Trustworthy Machine\\nLearning, 2022.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\nStoica, I., and Xing, E. P.\\nVicuna: An open-source\\nchatbot impressing GPT-4 with 90%* ChatGPT qual-\\nity, March 2023. URL https://lmsys.org/blog/\\n2023-03-30-vicuna/.\\nConmy, A., Mavor-Parker, A. N., Lynch, A., Heimersheim,\\nS., and Garriga-Alonso, A. Towards automated circuit dis-\\ncovery for mechanistic interpretability. In Thirty-seventh\\nConference on Neural Information Processing Systems,\\n2023.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nCsisz´arik, A., K˝or¨osi-Szab´o, P., Matszangosz, ´A. K., Papp,\\nG., and Varga, D. Similarity and matching of neural\\nnetwork representations. In Beygelzimer, A., Dauphin, Y.,\\nLiang, P., and Vaughan, J. W. (eds.), Advances in Neural\\nInformation Processing Systems, 2021. URL https:\\n//openreview.net/forum?id=aedFIIRRfXr.\\nDar, G., Geva, M., Gupta, A., and Berant, J. Analyzing\\ntransformers in embedding space. In Annual Meeting of\\nthe Association for Computational Linguistics, 2023.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\\net al. The Pile: An 800GB dataset of diverse text for\\nlanguage modeling. arXiv preprint arXiv:2101.00027,\\n2020.\\nGeiger, A., Lu, H., Icard, T., and Potts, C. Causal abstrac-\\ntions of neural networks. Advances in Neural Information\\nProcessing Systems, 34:9574–9586, 2021.\\nGeva, M., Caciularu, A., Dar, G., Roit, P., Sadde, S., Shlain,\\nM., Tamir, B., and Goldberg, Y. LM-debugger: An inter-\\nactive tool for inspection and intervention in transformer-\\nbased language models.\\nIn Che, W. and Shutova, E.\\n(eds.), Proceedings of the 2022 Conference on Empiri-\\ncal Methods in Natural Language Processing: System\\nDemonstrations, pp. 12–21, Abu Dhabi, UAE, Decem-\\nber 2022a. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.emnlp-demos.2. URL https:\\n//aclanthology.org/2022.emnlp-demos.2.\\nGeva, M., Caciularu, A., Wang, K., and Goldberg, Y. Trans-\\nformer feed-forward layers build predictions by promot-\\ning concepts in the vocabulary space. In Goldberg, Y.,\\nKozareva, Z., and Zhang, Y. (eds.), Proceedings of the\\n2022 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pp. 30–45, Abu Dhabi, United Arab\\nEmirates, December 2022b. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2022.emnlp-main.\\n3.\\nURL https://aclanthology.org/2022.\\nemnlp-main.3.\\nGeva, M., Bastings, J., Filippova, K., and Globerson, A. Dis-\\nsecting recall of factual associations in auto-regressive\\nlanguage models. In Bouamor, H., Pino, J., and Bali,\\nK. (eds.), Proceedings of the 2023 Conference on Em-\\npirical Methods in Natural Language Processing, pp.\\n12216–12235, Singapore, December 2023. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2023.\\nemnlp-main.751.\\nURL https://aclanthology.\\norg/2023.emnlp-main.751.\\nGhandeharioun, A., Kim, B., Li, C.-L., Jou, B., Eoff, B.,\\nand Picard, R. W.\\nDissect: Disentangled simultane-\\nous explanations via concept traversals. arXiv preprint\\narXiv:2105.15164, 2021.\\nGoldowsky-Dill, N., MacLeod, C., Sato, L., and Arora, A.\\nLocalizing model behavior with path patching. arXiv\\npreprint arXiv:2304.05969, 2023.\\nGupta, A., Boleda, G., Baroni, M., and Pad´o, S. Distribu-\\ntional vectors encode referential attributes. In Proceed-\\nings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing, pp. 12–21, 2015.\\nHanna, M., Liu, O., and Variengien, A. How does GPT-2\\ncompute greater-than?: Interpreting mathematical abili-\\nties in a pre-trained language model. In Thirty-seventh\\nConference on Neural Information Processing Systems,\\n2023. URL https://openreview.net/forum?\\nid=p4PckNQR8k.\\nHase, P., Bansal, M., Kim, B., and Ghandeharioun, A. Does\\nlocalization inform editing? surprising differences in\\ncausality-based localization vs. knowledge editing in lan-\\nguage models. arXiv preprint arXiv:2301.04213, 2023.\\nHendel, R., Geva, M., and Globerson, A. In-context learning\\ncreates task vectors. In The 2023 Conference on Empiri-\\ncal Methods in Natural Language Processing, 2023.\\nHernandez, E., Schwettmann, S., Bau, D., Bagashvili, T.,\\nTorralba, A., and Andreas, J.\\nNatural language de-\\nscriptions of deep features.\\nIn International Confer-\\nence on Learning Representations, 2022. URL https:\\n//openreview.net/forum?id=NudBMY-tzDr.\\nHernandez, E., Li, B. Z., and Andreas, J. Measuring and ma-\\nnipulating knowledge representations in language models.\\narXiv preprint arXiv:2304.00740, 2023a.\\nHernandez, E., Sharma, A. S., Haklay, T., Meng, K., Watten-\\nberg, M., Andreas, J., Belinkov, Y., and Bau, D. Linear-\\nity of relation decoding in transformer language models.\\narXiv preprint arXiv:2308.09124, 2023b.\\nKandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel,\\nC. Large language models struggle to learn long-tail\\nknowledge. In International Conference on Machine\\nLearning, pp. 15696–15707. PMLR, 2023.\\nKim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J.,\\nViegas, F., et al. Interpretability beyond feature attribu-\\ntion: Quantitative testing with concept activation vectors\\n(tcav). In International conference on machine learning,\\npp. 2668–2677. PMLR, 2018.\\nK¨ohn, A. What’s in an embedding? analyzing word embed-\\ndings through multilingual evaluation. In Proceedings of\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nthe 2015 Conference on Empirical Methods in Natural\\nLanguage Processing, pp. 2067–2073, 2015.\\nLieberum, T., Rahtz, M., Kram´ar, J., Irving, G., Shah, R.,\\nand Mikulik, V. Does circuit analysis interpretability\\nscale? evidence from multiple choice capabilities in chin-\\nchilla. arXiv preprint arXiv:2307.09458, 2023.\\nLin, C.-Y.\\nROUGE: A package for automatic evalua-\\ntion of summaries.\\nIn Text Summarization Branches\\nOut, pp. 74–81, Barcelona, Spain, July 2004. Asso-\\nciation for Computational Linguistics. URL https:\\n//aclanthology.org/W04-1013.\\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L.,\\nWiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang,\\nY., Gupta, S., Majumder, B. P., Hermann, K., Welleck,\\nS., Yazdanbakhsh, A., and Clark, P. Self-refine: Iter-\\native refinement with self-feedback. In Thirty-seventh\\nConference on Neural Information Processing Systems,\\n2023. URL https://openreview.net/forum?\\nid=S37hOerQLB.\\nMadsen, A., Reddy, S., and Chandar, S. Post-hoc inter-\\npretability for neural nlp: A survey. ACM Computing\\nSurveys, 55(8):1–42, 2022.\\nMallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D.,\\nand Hajishirzi, H.\\nWhen not to trust language mod-\\nels: Investigating effectiveness of parametric and non-\\nparametric memories. In Rogers, A., Boyd-Graber, J., and\\nOkazaki, N. (eds.), Proceedings of the 61st Annual Meet-\\ning of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pp. 9802–9822, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2023.acl-long.546.\\nURL https:\\n//aclanthology.org/2023.acl-long.546.\\nMeng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating\\nand editing factual associations in gpt. Advances in Neu-\\nral Information Processing Systems, 35:17359–17372,\\n2022a.\\nMeng, K., Sharma, A. S., Andonian, A. J., Belinkov, Y., and\\nBau, D. Mass-editing memory in a transformer. In The\\nEleventh International Conference on Learning Repre-\\nsentations, 2022b.\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\\nsentinel mixture models. In International Conference on\\nLearning Representations, 2016.\\nMerullo, J., Castricato, L., Eickhoff, C., and Pavlick, E. Lin-\\nearly mapping from image to text space. In The Eleventh\\nInternational Conference on Learning Representations,\\n2022.\\nMousi, B., Durrani, N., and Dalvi, F. Can llms facilitate\\ninterpretation of pre-trained language models? In Pro-\\nceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing. Association for Com-\\nputational Linguistics, December 2023. URL https:\\n//browse.arxiv.org/pdf/2305.13386.pdf.\\nNanda, N., Lee, A., and Wattenberg, M. Emergent lin-\\near representations in world models of self-supervised\\nsequence models.\\nIn Belinkov, Y., Hao, S., Jumelet,\\nJ., Kim, N., McCarthy, A., and Mohebbi, H. (eds.),\\nProceedings of the 6th BlackboxNLP Workshop: An-\\nalyzing and Interpreting Neural Networks for NLP,\\npp. 16–30, Singapore, December 2023. Association for\\nComputational Linguistics.\\ndoi:\\n10.18653/v1/2023.\\nblackboxnlp-1.2.\\nURL https://aclanthology.\\norg/2023.blackboxnlp-1.2.\\nnostalgebraist.\\ninterpreting gpt: the logit lens.\\nLess-\\nWrong, 2020.\\nURL https://www.lesswrong.\\ncom/posts/AcKRB8wDpdaN6v6ru/\\ninterpreting-gpt-the-logit-lens.\\nPal, K., Sun, J., Yuan, A., Wallace, B. C., and Bau, D.\\nFuture lens: Anticipating subsequent tokens from a single\\nhidden state. In Proceedings of the 27th Conference on\\nComputational Natural Language Learning (CoNLL), pp.\\n548–560, 2023.\\nPatel, R. and Pavlick, E. Mapping language models to\\ngrounded conceptual spaces. In International Conference\\non Learning Representations, 2021.\\nReimers, N. and Gurevych, I. Sentence-bert: Sentence em-\\nbeddings using siamese bert-networks. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natu-\\nral Language Processing. Association for Computational\\nLinguistics, 11 2019. URL https://arxiv.org/\\nabs/1908.10084.\\nSchwettmann, S., Chowdhury, N., Klein, S., Bau, D., and\\nTorralba, A. Multimodal neurons in pretrained text-only\\ntransformers. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision, pp. 2862–2867,\\n2023.\\nSingh, C., Hsu, A., Antonello, R., Jain, S., Huth, A., Yu, B.,\\nand Gao, J. Explaining black box text modules in natural\\nlanguage with language models. In XAI in Action: Past,\\nPresent, and Future Applications, 2023. URL https:\\n//openreview.net/forum?id=3BX9tM03GT.\\nSlobodkin, A., Goldman, O., Caciularu, A., Dagan, I.,\\nand Ravfogel, S.\\nThe curious case of hallucinatory\\n(un)answerability: Finding truths in the hidden states\\nof over-confident large language models. In Bouamor,\\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n2023 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pp. 3607–3625, Singapore, December\\n2023. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.220. URL https://\\naclanthology.org/2023.emnlp-main.220.\\nStolfo, A., Belinkov, Y., and Sachan, M.\\nA mechanis-\\ntic interpretation of arithmetic reasoning in language\\nmodels using causal mediation analysis. In Bouamor,\\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\\n2023 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pp. 7035–7052, Singapore, December\\n2023. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.435. URL https://\\naclanthology.org/2023.emnlp-main.435.\\nStrobelt, H., Gehrmann, S., Pfister, H., and Rush, A. M.\\nLstmvis: A tool for visual analysis of hidden state dy-\\nnamics in recurrent neural networks. IEEE transactions\\non visualization and computer graphics, 24(1):667–676,\\n2017.\\nTenney, I., Das, D., and Pavlick, E. Bert rediscovers the\\nclassical nlp pipeline. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics,\\npp. 4593–4601, 2019.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation lan-\\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. arXiv preprint arXiv:2307.09288,\\n2023b.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-\\ntention is all you need. Advances in neural information\\nprocessing systems, 30, 2017.\\nVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D.,\\nSinger, Y., and Shieber, S. Investigating gender bias in\\nlanguage models using causal mediation analysis. Ad-\\nvances in neural information processing systems, 33:\\n12388–12401, 2020.\\nVilas, M. G., Schauml¨offel, T., and Roig, G. Analyzing\\nvision transformers for image classification in class em-\\nbedding space. arXiv preprint arXiv:2310.18969, 2023.\\nWallat, J., Singh, J., and Anand, A.\\nBERTnesia: In-\\nvestigating the capture and forgetting of knowledge in\\nBERT.\\nIn Alishahi, A., Belinkov, Y., Chrupała, G.,\\nHupkes, D., Pinter, Y., and Sajjad, H. (eds.), Proceed-\\nings of the Third BlackboxNLP Workshop on Analyz-\\ning and Interpreting Neural Networks for NLP, pp. 174–\\n183, Online, November 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1.\\n17.\\nURL https://aclanthology.org/2020.\\nblackboxnlp-1.17.\\nWang,\\nB.\\nand\\nKomatsuzaki,\\nA.\\nGPT-J-6B:\\nA\\n6\\nBillion\\nParameter\\nAutoregressive\\nLanguage\\nModel.\\nhttps://github.com/kingoflolz/\\nmesh-transformer-jax, May 2021.\\nWang, K. R., Variengien, A., Conmy, A., Shlegeris, B., and\\nSteinhardt, J. Interpretability in the wild: a circuit for indi-\\nrect object identification in GPT-2 small. In The Eleventh\\nInternational Conference on Learning Representations,\\n2022.\\nWang, Z., Ku, A., Baldridge, J., Griffiths, T. L., and Kim,\\nB. Gaussian Process Probes (GPP) for uncertainty-aware\\nprobing. arXiv preprint arXiv:2305.18213, 2023.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\\nprompting elicits reasoning in large language models.\\nAdvances in Neural Information Processing Systems, 35:\\n24824–24837, 2022.\\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,\\nY., and Narasimhan, K. R. Tree of thoughts: Deliberate\\nproblem solving with large language models. In Thirty-\\nseventh Conference on Neural Information Processing\\nSystems, 2023.\\nYoussef, P., Koras¸, O., Li, M., Schl¨otterer, J., and Seifert,\\nC. Give me the facts! a survey on factual knowledge\\nprobing in pre-trained language models. In Findings of\\nthe Association for Computational Linguistics: EMNLP\\n2023, pp. 15588–15605, 2023.\\nZhang, F. and Nanda, N. Towards best practices of activation\\npatching in language models: Metrics and methods. arXiv\\npreprint arXiv:2309.16042, 2023.\\nZhou, B., Bau, D., Oliva, A., and Torralba, A. Interpreting\\ndeep visual representations via network dissection. IEEE\\ntransactions on pattern analysis and machine intelligence,\\n41(9):2131–2145, 2018.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nA. Next-Token Prediction Additional Details and Experimental Results\\nA.1. Models\\nWe use LLaMA2 (13B) (Touvron et al., 2023b), Vicuna (13B) (Chiang et al., 2023), GPT-J (6B) (Wang & Komatsuzaki,\\n2021), and Pythia (12B) (Biderman et al., 2023). LLaMA2 was pre-trained on 2T tokens from a mix of publicly available\\ndata. Vicuna is a LLaMA1 (Touvron et al., 2023a) model that was pre-trained on 1T tokens and fine-tuned on 70K\\nuser-shared conversations 4. The primary architectural differences between LLaMA2 and Vicuna (LLaMA1) include a\\ndifferent context length and grouped-query attention. Pythia and GPT-J were pre-trained using a deduplicated version of\\nThe Pile corpus (Gao et al., 2020), and for about 300B and 402B tokens, respectively.\\nA.2. Training and Evaluation Data\\nWe use 12,000 random samples from the Pile, partitioned into 10,000 examples for training the affine mappings, and 2,000\\nexamples for evaluation. In our pre-processing strategy, we introduce randomness in the patching positions by trimming the\\ninput sequence length of each example.\\nA.3. Additional Few-Shot Token Identity Prompts\\nIn this section, we provide additional details about the selection of the demonstrations for the token identity baseline, and\\nfurther evaluate the robustness of LLaMA2 (13B) (Touvron et al., 2023b) to various token identity prompts.\\nDemonstrations Construction\\nFor the demonstrations used in this experiment, we sample a random set of k = 3 tokens\\nfor all the models (where k was also randomly sampled from the interval [1, . . . , 10]).\\nRobustness to Additional token IDs’ Demonstrations\\nWe randomly generate five realizations of token IDs series of\\nvarying lengths, formatted as “tok1 → tok1 ; tok2 → tok2 ; . . . tokk”, similarly to the procedure from §4.1.\\nThe results are illustrated in Fig. 7, where a comprehensive overview of the evaluation metrics can be found in §4.1. The\\nresults indicate the stability of the token identity baseline across a range of token identity demonstrations, particularly\\nnotable in the upper layers of the model.\\n0\\n10\\n20\\n30\\n40\\nlayer\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nprecision@1\\n450, 41, 8355\\n2, 7509, 564, 23, 10532, 443\\n2426, 11\\n98\\n65, 32354, 435, 3\\n(a) Precision@1\\n0\\n10\\n20\\n30\\n40\\nlayer\\n0\\n5\\n10\\n15\\nsurprisal\\n450, 41, 8355\\n2, 7509, 564, 23, 10532, 443\\n2426, 11\\n98\\n65, 32354, 435, 3\\n(b) Surprisal\\nFigure 7. Next-token prediction results for LLaMA2, using various token identity demonstrations (the token IDs appear in the legend).\\nWe report precision@1 (↑ is better), and surprisal (↓ is better).\\nB. More Details on Attribute Extraction Experiments\\nDataset Details\\nWe start from factual and commonsense reasoning subsets introduced by Hernandez et al. (2023b). This\\ndataset includes 8 commonsense and 25 factual relations. For each data point, we sample 5 utterances from Wikitext-103\\ndataset (Merity et al., 2016) including s. We then truncate sampled text to a window of random length up to 20 tokens that\\ncontains s. This constitutes our source prompt, S. Note that for each model, we filter the data to samples for which the\\n4collected from www.sharegpt.com\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nTable 4. Comparison between Zero-Shot Feature Extraction Patchscope and a Logistic Regression Probe shows that despite using no\\ntraining data, it has a significantly higher accuracy than baseline in most tasks (p < 1e − 5). Pairwise t-statistics and corresponding\\np-values are included in the table.\\nAccuracy (mean±std)\\nTask\\nLogistic\\nRegression\\nProbe\\nZero-shot\\nFeature Extraction\\nPatchscope\\nt-statistic\\np-value\\nCommonsense\\nFruit inside color\\n37.41 ± 6.58\\n37.99 ± 18.67\\n0.126\\n0.901\\nFruit outside color\\n35.50 ± 3.09\\n71.00 ± 13.26∗∗\\n12.426\\n< 1e − 5\\nObject superclass\\n68.92 ± 10.69∗∗\\n55.71 ± 10.81\\n−5.25\\n< 1e − 4\\nSubstance phase\\n73.77 ± 3.74\\n91.92 ± 1.73∗∗\\n25.647\\n< 1e − 5\\nTask done by person\\n0 ± 0\\n62.96 ± 16.513∗∗\\n19.632\\n< 1e − 5\\nTask done by tool\\n10.14 ± 3.23\\n48.12 ± 13.23∗∗\\n18.231\\n< 1e − 5\\nWork location\\n0 ± 0\\n13.58 ± 9.37∗∗\\n7.45990\\n< 1e − 5\\nCompany CEO\\n4.99 ± 2.56\\n47.82 ± 13.89∗∗\\n16.700\\n< 1e − 5\\nCountry capital city\\n0 ± 0\\n61.61 ± 14.14∗∗\\n22.426\\n< 1e − 5\\nFactual\\nCountry currency\\n17.70 ± 2.20\\n50.95 ± 8.85∗∗\\n20.293\\n< 1e − 5\\nCountry largest city\\n0 ± 0\\n67.78 ± 11.47∗∗\\n30.427\\n< 1e − 5\\nFood from country\\n5.13 ± 3.66\\n63.80 ± 11.34∗∗\\n26.710\\n< 1e − 5\\nPerson father\\n0 ± 0\\n25.34 ± 8.42∗∗\\n15.482\\n< 1e − 5\\nPerson plays position in sport\\n75.89 ± 9.14\\n72.20 ± 7.21\\n−2.066\\n0.049\\nPerson plays pro sport\\n53.87 ± 10.28\\n46.28 ± 14.19\\n−2.020\\n0.054\\nProduct by company\\n58.91 ± 7.15\\n63.24 ± 10.74\\n1.757\\n0.091\\nStar constellation\\n17.54 ± 5.30\\n18.35 ± 5.06\\n−2.98\\n0.006\\nSuperhero archnemesis\\n0 ± 0\\n41.73 ± 18.72∗∗\\n11.47044\\n< 1e − 5\\nSuperhero person\\n0 ± 0\\n28.32 ± 14.05∗∗\\n10.37461\\n< 1e − 5\\nunderlying model correctly encodes the tuple. For experiments with zero-shot target prompt T that includes r followed\\nby s, we autoregressively generate the next 20 tokens, and only keep examples where o appears in the generation. For\\nexperiments with few-shot demonstrations in T, after generating the next 20 tokens, we do an additional post-processing\\nstep. If the demonstration template of an example is identified in the generation, all the following tokens would be dropped.\\nThe example is used for evaluation only if o appears in the post-processed truncated generation. To have a reasonable\\namount of data for training the classification probe baseline, tasks with fewer than 15 datapoints are dropped from the\\nanalysis. For GPT-J, 5 commonsense and 7 factual reasoning tasks remain after applying the above postprocessing steps.\\nResults on Additional Tasks\\nIn this section, we provide additional results on various factual and commonsense reasoning\\ntasks. For a comparison of Zero-Shot Feature Extraction Patchscope and Logistic Regression Probe across layers, see\\nFig. 8. Zero-Shot Feature Extraction Patchscope outperforms baseline consistently in early ℓ and in the majority of mid ℓ.\\nAs source representation shifts toward next-token prediction in later ℓ, Zero-Shot Feature Extraction Patchscope accuracy\\ndeclines gradually. Late ℓ∗ is less successful, perhaps due to the prolonged influence of the placeholder x representation in\\nthe computation. In addition, we provide t-statistic details in Tab. 4.\\nSource-Target Layer Interplay\\nFig. 9 visualizes the interaction between ℓ and ℓ∗. These heatmaps show attribute\\nextraction success rate for Zero-Shot Feature Extraction Patchscope for a fixed (ℓ, ℓ∗) combination. The lower left\\nquadrants show setups where both ℓ and ℓ∗ represent early to middle layers, and the success rate is maximal. The right half\\nof the heatmaps represent late ℓ, which achieve lower success rate due to token representations shifting toward next-token\\nprediction as discussed earlier. In addition, we notice lower success rate in the top half of the heatmaps which represent\\nlate ℓ∗. It is worth noting that in this task, the accuracy is not only based on the immediate next-token prediction, but\\nrather whether ω appears in the next 20 autoregressively generated tokens. The placeholder token x does still remain in\\nthe input, and its representation persists in the early layers in the target computation. This explains why lower attribute\\nextraction rate is observed in later ℓ∗ values. We leave it to future work to investigate adaptations to Patchscopes to\\ncontrol contamination from the placeholder tokens and make them more amenable to late ℓ∗ choices.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nsubstance phase (commonsense)\\n# Source prompts: 182 - # Classes: 3\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(a) Substance Phase\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n20\\n40\\n60\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\ntask done by tool (commonsense)\\n# Source prompts: 54 - # Classes: 12\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(b) Task Done By Tool\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n10\\n20\\n30\\n40\\n50\\n60\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\ncountry currency (factual)\\n# Source prompts: 83 - # Classes: 14\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(c) Country Currency\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n20\\n40\\n60\\n80\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nfood from country (factual)\\n# Source prompts: 27 - # Classes: 10\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(d) Food From Country\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\ncompany ceo (factual)\\n# Source prompts: 50 - # Classes: 12\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(e) Company CEO\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nfruit outside color (commonsense)\\n# Source prompts: 56 - # Classes: 6\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(f) Fruit Outside Color\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n30\\n40\\n50\\n60\\n70\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nproduct by company (factual)\\n# Source prompts: 219 - # Classes: 11\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(g) Product By Company\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nperson plays pro sport (factual)\\n# Source prompts: 131 - # Classes: 5\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(h) Person Plays Pro Sport\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n50\\n60\\n70\\n80\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nperson plays position in sport (factual)\\n# Source prompts: 383 - # Classes: 7\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(i) Person Plays Position in Sport\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n30\\n40\\n50\\n60\\n70\\n80\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nobject superclass (commonsense)\\n# Source prompts: 173 - # Classes: 8\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(j) Object Superclass\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nfruit inside color (commonsense)\\n# Source prompts: 39 - # Classes: 3\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(k) Fruit Inside Color\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n5\\n10\\n15\\n20\\n25\\n30\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nstar constellation (factual)\\n# Source prompts: 56 - # Classes: 10\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(l) Star Constellation\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n10\\n20\\n30\\n40\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nperson father (factual)\\n# Source prompts: 27 - # Classes: 7\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(m) Person Father\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n20\\n40\\n60\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nsuperhero archnemesis (factual)\\n# Source prompts: 28 - # Classes: 7\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(n) Superhero Archnemesis\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n20\\n40\\n60\\n80\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\ntask done by person (commonsense)\\n# Source prompts: 33 - # Classes: 10\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(o) Task Done By Person\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n10\\n20\\n30\\n40\\n50\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nwork location (commonsense)\\n# Source prompts: 19 - # Classes: 4\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(p) Work Location\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n10\\n20\\n30\\n40\\n50\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\nsuperhero person (factual)\\n# Source prompts: 45 - # Classes: 12\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(q) Superhero Person\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n20\\n40\\n60\\n80\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\ncountry capital city (factual)\\n# Source prompts: 94 - # Classes: 21\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(r) Country Capital City\\n0\\n5\\n10\\n15\\n20\\n25\\nSource Layer ( )\\n0\\n20\\n40\\n60\\n80\\nAttribute Extraction Acc.\\nAttribute Extraction Acc.\\ncountry largest city (factual)\\n# Source prompts: 91 - # Classes: 21\\nZero-Shot Feat. Ext. Patchscope\\nLogistic Reg. Probe\\n(s) Country Largest City\\nFigure 8. Feature extraction accuracy with respect to source layer (ℓ) across various factual and commonsense reasoning tasks. Zero-Shot\\nFeature Extraction Patchscope works consistently better than Logistic Regression Probe in early layers, and mostly in mid layers.\\nThere is a decline in Patchscope accuracy in later ℓ as the source representations shift toward next-token prediction.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nsubstance phase (commonsense)\\n# Source prompts: 182 - # Classes: 3\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n(a) Substance Phase\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\ntask done by tool (commonsense)\\n# Source prompts: 54 - # Classes: 12\\n0\\n10\\n20\\n30\\n40\\n50\\n(b) Task Done By Tool\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\ncountry currency (factual)\\n# Source prompts: 83 - # Classes: 14\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n(c) Country Currency\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nfood from country (factual)\\n# Source prompts: 27 - # Classes: 10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n(d) Food From Country\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\ncompany ceo (factual)\\n# Source prompts: 50 - # Classes: 12\\n0\\n10\\n20\\n30\\n40\\n50\\n(e) Company CEO\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nfruit outside color (commonsense)\\n# Source prompts: 56 - # Classes: 6\\n0\\n10\\n20\\n30\\n40\\n50\\n(f) Fruit Outside Color\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nproduct by company (factual)\\n# Source prompts: 219 - # Classes: 11\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n(g) Product By Company\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nperson plays pro sport (factual)\\n# Source prompts: 131 - # Classes: 5\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n(h) Person Plays Pro Sport\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nperson plays position in sport (factual)\\n# Source prompts: 383 - # Classes: 7\\n0\\n10\\n20\\n30\\n40\\n50\\n(i) Person Plays Position in Sport\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nobject superclass (commonsense)\\n# Source prompts: 173 - # Classes: 8\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n(j) Object Superclass\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nfruit inside color (commonsense)\\n# Source prompts: 39 - # Classes: 3\\n0\\n10\\n20\\n30\\n40\\n50\\n(k) Fruit Inside Color\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nstar constellation (factual)\\n# Source prompts: 56 - # Classes: 10\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n(l) Star Constellation\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nperson father (factual)\\n# Source prompts: 27 - # Classes: 7\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0\\n(m) Person Father\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nsuperhero archnemesis (factual)\\n# Source prompts: 28 - # Classes: 7\\n0\\n10\\n20\\n30\\n40\\n50\\n(n) Superhero Archnemesis\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\ntask done by person (commonsense)\\n# Source prompts: 33 - # Classes: 10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n(o) Task Done By Person\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nwork location (commonsense)\\n# Source prompts: 19 - # Classes: 4\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0\\n(p) Work Location\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\nsuperhero person (factual)\\n# Source prompts: 45 - # Classes: 12\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n(q) Superhero Person\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\ncountry capital city (factual)\\n# Source prompts: 94 - # Classes: 21\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n(r) Country Capital City\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\nSource Layer ( )\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18 20 22 24 26\\nTarget Layer (\\n* )\\nAttribute Extraction Acc.\\ncountry largest city (factual)\\n# Source prompts: 91 - # Classes: 21\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n(s) Country Largest City\\nFigure 9. The interaction between source and target layers in Zero-Shot Feature Extraction Patchscopeacross various factual and\\ncommonsense reasoning tasks. Each cell (ℓ, ℓ∗) in the heatmap shows the attribute extraction success rate where source and target layers\\nare fixed to ℓ and ℓ∗, respectively. Particularly, there is a higher success rate in the lower left quadrants, representing early to mid source\\nand target layer combinations. The right half of the heatmaps shows late source layers, where the source representation has shifted toward\\nnext-token prediction, leading to lower success rate in attribute extraction. The top half of the heatmaps shows late target layers. When the\\naccuracy is a function of more than a single next-token, the placeholder token representation still remains in the early layers, leading to\\nlower attribute extraction rate.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0\\n2\\n4\\n6\\n8\\nlayer\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nRouge1\\nM \\n 7B,   M* \\n 7B\\nM \\n 13B, M* \\n 13B\\nM \\n 7B,   M* \\n 13B\\npopular entities\\nrare entities\\n(a) Vicuna\\n0\\n2\\n4\\n6\\n8\\nlayer\\n0.0\\n0.1\\n0.2\\n0.3\\nRouge1\\nM \\n 6.9B, M* \\n 6.9B\\nM \\n 12B,  M* \\n 12B\\nM \\n 6.9B, M* \\n 12B\\npopular entities\\nrare entities\\n(b) Pythia\\nFigure 10. Rouge1 scores of the generated descriptions against descriptions from Wikipedia.\\n0\\n2\\n4\\n6\\n8\\nlayer\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nSBERT\\nM \\n 7B,   M* \\n 7B\\nM \\n 13B, M* \\n 13B\\nM \\n 7B,   M* \\n 13B\\npopular entities\\nrare entities\\n(a) Vicuna\\n0\\n2\\n4\\n6\\n8\\nlayer\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nSBERT\\nM \\n 6.9B, M* \\n 6.9B\\nM \\n 12B,  M* \\n 12B\\nM \\n 6.9B, M* \\n 12B\\npopular entities\\nrare entities\\n(b) Pythia\\nFigure 11. SBERT scores of the generated descriptions against descriptions from Wikipedia.\\nC. Additional Information and Results on the Entity Resolution Experiment\\nC.1. Experimental Setup\\nRecall () that we use a few-shot target prompt template for decoding an entity description:\\n“subject 1:\\ndescription of subject 1, ..., subject k:\\ndescription of subject k, x”, while patching\\nthe last position which corresponds to x. Specifically, we use the following target prompt, obtained randomly: “Syria:\\nCountry in the Middle East, Leonardo DiCaprio:\\nAmerican actor, Samsung:\\nSouth\\nKorean multinational major appliance and consumer electronics corporation, x”\\nand\\ntask the model to generate the completion after the patched representation in x. For the subject description, which is\\ncomposed of k = 3 random subject entities, we used the wptools5 python package for obtaining a description of every\\nsubject entity from Wikipedia.\\nC.2. Additional Quantitative Results\\nIn this section, we present the Rouge1 (Lin, 2004) and SBERT score (Reimers & Gurevych, 2019) results, as well as the\\nresults for the Pythia models (Biderman et al., 2023).6 In Fig. 10 and Fig. 11, we present the Rouge1 and the SBERT results,\\nrespectively, complementary to the RougeL results in Fig. 4 from §4.3. Note that for Pythia, the smaller model (6.9B)\\noutperforms the larger one (12B), and hence, our cross-model patching method does not have the potential to improve the\\ninspection of the smaller model, unlike the trends for Vicuna. The other\\n5https://github.com/siznax/wptools/\\n6We used the package from https://www.sbert.net/, with the sentence-transformers/all-MiniLM-L6-v2 model.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\ntarget layer\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nsource layer\\n1.1\\n1.9\\n4.0\\n4.4\\n4.8\\n5.8\\n6.7\\n7.0\\n1.3\\n1.9\\n2.5\\n2.7\\n3.1\\n3.9\\n4.5\\n5.2\\n1.5\\n2.3\\n2.8\\n2.6\\n2.8\\n3.6\\n4.4\\n4.7\\n1.4\\n2.8\\n3.3\\n2.8\\n2.4\\n2.7\\n3.2\\n3.9\\n1.5\\n3.6\\n4.3\\n3.7\\n2.8\\n1.9\\n2.1\\n2.7\\n1.8\\n4.8\\n6.1\\n5.4\\n4.1\\n2.4\\n1.9\\n1.8\\n2.1\\n5.3\\n3.9\\n3.3\\n2.7\\n1.8\\n1.7\\n1.5\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n(a) Vicuna: M←7B, M∗ ←13B\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\ntarget layer\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nsource layer\\n1.1\\n2.3\\n2.8\\n4.1\\n5.1\\n6.4\\n8.3\\n5.7\\n1.2\\n1.2\\n1.5\\n2.4\\n3.0\\n3.8\\n4.3\\n4.6\\n1.2\\n1.2\\n1.3\\n2.0\\n2.2\\n2.8\\n3.4\\n4.0\\n1.3\\n1.2\\n1.4\\n1.7\\n1.8\\n2.1\\n2.6\\n3.5\\n1.4\\n1.3\\n1.5\\n1.8\\n1.6\\n1.8\\n2.0\\n3.0\\n1.6\\n1.4\\n1.6\\n2.1\\n1.7\\n1.7\\n1.7\\n2.7\\n2.0\\n1.5\\n1.8\\n2.3\\n1.9\\n1.8\\n1.6\\n2.3\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n(b) Pythia: M←6.9B, M∗ ←12B\\nFigure 12. Next-token prediction estimation performance in Vicuna and Pythia with cross-model Patchscopes, measured by Surprisal\\n(↑ is better).\\nC.3. Additional Qualitative Results\\nIn this section, we provide more examples and discuss our observations about the gradual process of entity resolution.\\nAs shown in Tables 5 and 6, it is interesting to observe that the resolution process for the same input can look different\\nacross models, suggesting they assign different likelihoods to different entities, and weigh context differently. For example,\\nas Vicuna 13B processes “Will Smith”, it goes from “Smithsonian Museum” to the “Smith rock band” to the actor\\nand rapper, “Will Smith”. However, Pythia 12B starts with “Smith & Wesson weapon manufacturing company” before it\\nresolves the entity as the American actor, “Will Smith”.\\nWe also observe another phenomenon which we refer as placeholder contamination. This is the case where the remaining\\nrepresentation of the placeholder entity “x” in the early layers interferes with the model’s capability in generating descriptions\\nfor the patched token. For example, see Vicuna 13B response to “Paris Hilton” entity in Tab. 5. First, we see the\\ngradual process of going from “Rigatoni” pasta, to “Hilton Hotels”, to the socialite “Paris Hilton” in layers 1-6. But in\\nlayers 7 and onward, the generation seems to describe the placeholder token “x”rather than the “Paris Hilton” entity:\\n“Placeholder for a variable or concept”, “Variable representing any number of things or concepts” or “x is a placeholder”.\\nFor future, we would like to quantify these qualitative observations, study to what extend this contamination can be mitigated\\nwith a different placeholder choice, and why some models might be more susceptible to this contamination than others.\\nD. Additional Results on the Cross-Model Patching Experiment\\nIn this section, we present the Surprisal metric results of cross-model patching representations from Vicuna (7B) → Vicuna\\n(13B) (Chiang et al., 2023), and Pythia (6.9B) → Pythia (12B) (Biderman et al., 2023). We employ an affine mapping, as\\ndetailed in §4.1 and in §4.4, as a preliminary step before the patching representations from one model to another. In Fig. 12,\\nwe present the surprisal results, complementary to the precision@1 results in Fig. 5 from §4.4.\\nE. More Details on Multi-Hop Reasoning Experiment\\nData\\nTab. 7 summarizes information about the samples where the Vicuna (13B) correctly represents each reasoning step.\\nOut of 1,104 samples that require two steps of commonsense or factual reasoning, 46 satisfy the above criteria, with 8\\nunique relation combinations.\\nMethod\\nSee an illustration of CoT Patchscope in 13. In practice, we use the following configuration for CoT\\nPatchscope: S ← π1, T ← π2, i ← n, i∗ ← m. This is equivalent to S = T ← [π2][π1] and adjusting the attention\\nmask such that no token in S has visibility to π2 and no token in T has visibility to π1.\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nTable 5. Additional qualitative examples illustrating entity resolution via qualitative examples for Vicuna 13B.The expressive generations\\nshow that as we go through the layers, more tokens from the context get integrated into the current representation, referred to as “Tokens\\nCovered”. Here, M ← Vicuna 13B, M∗ ← M, ℓ∗ ← ℓ.\\nS\\nTokens Covered\\nℓ\\nGeneration\\nNotes/Entity Described by the\\nGeneration\\n\"Will Smith\"\\n\"Smith\"\\n1\\nsonian: National museum and research complex in Washington, D.C.\\nSmithsonian Museum\\n\"Smith\"\\n2\\nsonas: American rock band\\nAn amalgam of Smith band\\nand SONA band\\n\"Will Smith\"\\n3\\n: American actor and rapper\\nWill Smith\\n\"Queen Victoria\"\\n\"Victoria\"\\n1\\ns Secret: American multinational women’s lingerie, beauty and body-care products company\\nVictoria’s Secret\\n\"Victoria\"\\n2\\ns Secret: American lingerie and beauty products retailer\\nVictoria’s Secret\\n\"Queen Victoria\"\\n3\\n: Queen of the United Kingdom of Great Britain and Ireland from 1837 to 1901\\nQueen Victoria\\n\"Queen Victoria\"\\n4-8\\n: Queen of the United Kingdom of Great Britain and Ireland from 1837 until her death in 1901\\nQueen Victoria\\n\"Queen Victoria\"\\n9\\n: Queen of England (reigned from 1837-1901)\\nQueen Victoria\\n\"Queen Victoria\"\\n10\\n: 19th century British queen who ruled from 1837-1901\\nQueen Victoria\\n\"Paris Hilton\"\\n\"ton\"\\n1\\ny: a type of pasta\\nRigatontoni/Rigatony/Rigaton´e\\n\"Hilton\"\\n2\\nHotels: American multinational hospitality company\\nHilton Hotel\\n\"Hilton\"\\n3\\n: Hotel chain founded by Conrad Hilton\\nHilton Hotel\\n\"Hilton\"\\n4\\n: Hotel and resorts company\\nHilton Hotel\\n\"Paris Hilton\"\\n5-6\\n: Socialite and television personality\\nParis Hilton\\nN/A\\n7\\n: Placeholder for a variable or concept\\nPlaceholder contamination\\nN/A\\n8-9\\n: Variable representing any number of things or concepts\\nPlaceholder contamination\\nN/A\\n10\\n: x is a placeholder\\nPlaceholder contamination\\n\"James Bond\"\\n\"Bond\"\\n1\\n: Unit of measurement for the strength of a liquid\\nBond Unit\\n\"James Bond\"\\n2\\n: James Bond, the fictional British secret agent created by novelist Ian Fleming and portrayed by actor\\nDaniel Craig in the 2006 film ”Casino Royale,”\\nJames Bond\\n\"James Bond\"\\n3-5\\n: Fictional British secret agent\\nJames Bond\\n\"James Bond\"\\n6\\n: Code name for fictional British secret agent James Bond\\nJames Bond\\n\"James Bond\"\\n7\\n: Code name for a fictional British secret agent created by novelist Ian Fleming\\nJames Bond\\n\"James Bond\"\\n8\\n: Subject of a fictional British secret agent, created by novelist Ian Fleming and portrayed in a series\\nof films.\\nJames Bond\\nJames Bond\\n9\\n: Subject of the 007 novels and films\\nJames Bond\\nN/A\\n10\\n: Variable representing the number of the film in the James Bond series\\nPlaceholder contamination\\n\"Dwayne Johnson\"\\n\"Johnson\"\\n1-2\\n& Johnson: American multinational corporation in the fields of pharmaceuticals, medical devices, and\\nconsumer packaged goods\\nJohnson & Johnson\\n\"Dwayne Johnson\"\\n3-4\\n: American actor and professional wrestler, also known as ”The Rock”\\nDwayne Johnson\\n\"Dwayne Johnson\"\\n5\\n: Also known as ”The Rock,” American actor and professional wrestler\\nDwayne Johnson\\n\"Dwayne Johnson\"\\n6\\n: Former professional wrestler and actor\\nDwayne Johnson\\nN/A\\n7\\n: Abbreviation for the United States\\nPlaceholder contamination\\nN/A\\n8\\n: Variable representing any number of things or concepts\\nPlaceholder contamination\\nN/A\\n9\\n: Variable representing any number of other options.\\nPlaceholder contamination\\nN/A\\n10\\n: Variable representing any number of other options or possibilities.\\nPlaceholder contamination\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nTable 6. Additional qualitative examples illustrating entity resolution via qualitative examples for Pythia 12B.The expressive generations\\nshow that as we go through the layers, more tokens from the context get integrated into the current representation, referred to as “Tokens\\nCovered”. Here, M ← Pythia 12B, M∗ ← M, ℓ∗ ← ℓ.\\nS\\nTokens Covered\\nℓ\\nGeneration\\nNotes/Entity Described by the Generation\\n\"Will\\nSmith\"\\n\"Smith\"\\n1\\n& Wesson: American firearms manufacturer\\nSmith & Wesson\\n\"Will Smith\"\\n2\\n: American actor\\nWill Smith\\n\"George Washington\"\\n\"Washington\"\\n1\\n: Capital of the United States\\nWashington D.C.\\n\"Washington\"\\n2\\n: American capital city\\nWashington D.C.\\n\"George Washington\"\\n3\\n: American president\\nGeorge Washington\\n\"George Washington\"\\n4-6\\n: American revolutionary\\nGeorge Washington\\n\"George Washington\"\\n7\\n: American president\\nGeorge Washington\\n\"George Washington\"\\n8\\n: George Washington\\nGeorge Washington\\n\"George Washington\"\\n9\\n: George Washington, the first President of the United States\\nGeorge Washington\\n\"George Washington\"\\n10\\n: George Washington, Thomas Jefferson, Abraham Lincoln, and\\nTheodore Roosevelt: American presidents\\nGeorge Washington\\n\"Titanic\"\\n\"c\"\\n1\\nloud: Apple’s cloud-based storage service\\ncloud\\n\"Titanic\"\\n2\\n: Sinking ocean liner\\nTitanic\\n\"Titanic\"\\n3\\n: The ship that sank in the Atlantic Ocean in 1912\\nTitanic\\n\"Titanic\"\\n4-6\\n: British passenger ship\\nTitanic\\n\"Titanic\"\\n7-9\\n: The ship that sank\\nTitanic\\n\"Titanic\"\\n10\\n: Titanic, Titanic: British luxury passenger liner\\nTitanic\\n\"Saturday\\nNight Live\"\\n\"Live\"\\n1\\nNation: American concert promoter\\nLive Nation\\n\"Live\"\\n2\\nNation: American television network\\nLive Nation\\n\"Saturday Night Live\"\\n3\\n: American television program\\nSaturday Night Live\\n\"Saturday Night Live\"\\n4-10\\n: American television comedy show\\nSaturday Night Live\\n\"Nineteen Eighty-Four\"\\n\"Four\"\\n1\\nSeasons: American hotel chain\\nFour Seasons Hotel\\n\"Four\"\\n2\\nSeasons: The four seasons of the year\\nFour Seasons (of the year)\\n\"ighty-Four\"\\n3\\nLions: Chinese professional football club\\nCangzhou Mighty Lions F.C.\\n\"Nineteen Eighty-Four\"\\n4\\n: Number of the novel 1984 by George Orwell\\nNineteen Eighty-Four (Novel)\\n\"Nineteen Eighty-Four\"\\n5\\n: George Orwell’s novel, and the 1984 film.\\nNineteen Eighty-Four (Novel & Film)\\n\"Nineteen Eighty-Four\"\\n6\\n: George Orwell’s novel\\nNineteen Eighty-Four (Novel)\\n\"Nineteen Eighty-Four\"\\n7\\n: George Orwell: British novelist\\nNineteen Eighty-Four (Novel)\\n\"Nineteen Eighty-Four\"\\n8\\n: 1984 novel by George Orwell\\nNineteen Eighty-Four (Novel)\\n\"Nineteen Eighty-Four\"\\n9-10\\n: 1984 novel by George Orwell, and the 1984 film adaptation\\ndirected by Michael Radford.\\nNineteen Eighty-Four (Novel & Film)\\nPatchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models\\nThe current CEO of the company that created Visual Basic Script\\nThe current CEO of the company that created Visual Basic Script\\nis Satya Nadella\\nSource\\nTarget\\nFigure 13. An\\nillustration\\nof\\nCoT\\nPatchscope\\non\\na\\nsingle\\nexample.\\nIn\\nthis\\nexample,\\nπ1\\n←\\n“the company that created Visual Basic Script”, π2 ← “The current CEO of”, S = T ← [π2][π1] =\\n“The current CEO of the company that created Visual Basic Script”. Note that M = M∗ and f ← I. For\\nmore details about attention mask adjustments that are not visible on the plot, see §E.\\nTable 7. Sample statistics for the multi-hop reasoning experiment where M correctly represents both τ1 and τ2.\\nρ1\\nρ2\\n# Samples\\nCompany CEO\\nPerson Father\\n4\\nFood from Country\\nCountry Capital City\\n10\\nFood from Country\\nCountry Currency\\n3\\nFood from Country\\nCountry Language\\n9\\nFood from Country\\nCountry Largest City\\n11\\nPerson Father\\nPerson Father\\n1\\nPerson Father\\nPerson Mother\\n1\\nProduct by Company\\nCompany CEO\\n7\\nTotal\\n46\\n'}, 'http://arxiv.org/abs/2401.06088v1': {'title': 'Autocompletion of Chief Complaints in the Electronic Health Records\\n  using Large Language Models', 'published_date': datetime.datetime(2024, 1, 11, 18, 6, 30), 'pdf_link': 'http://arxiv.org/pdf/2401.06088v1', 'summary': \"The Chief Complaint (CC) is a crucial component of a patient's medical record\\nas it describes the main reason or concern for seeking medical care. It\\nprovides critical information for healthcare providers to make informed\\ndecisions about patient care. However, documenting CCs can be time-consuming\\nfor healthcare providers, especially in busy emergency departments. To address\\nthis issue, an autocompletion tool that suggests accurate and well-formatted\\nphrases or sentences for clinical notes can be a valuable resource for triage\\nnurses. In this study, we utilized text generation techniques to develop\\nmachine learning models using CC data. In our proposed work, we train a Long\\nShort-Term Memory (LSTM) model and fine-tune three different variants of\\nBiomedical Generative Pretrained Transformers (BioGPT), namely\\nmicrosoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.\\nAdditionally, we tune a prompt by incorporating exemplar CC sentences,\\nutilizing the OpenAI API of GPT-4. We evaluate the models' performance based on\\nthe perplexity score, modified BERTScore, and cosine similarity score. The\\nresults show that BioGPT-Large exhibits superior performance compared to the\\nother models. It consistently achieves a remarkably low perplexity score of\\n1.65 when generating CC, whereas the baseline LSTM model achieves the best\\nperplexity score of 170. Further, we evaluate and assess the proposed models'\\nperformance and the outcome of GPT-4.0. Our study demonstrates that utilizing\\nLLMs such as BioGPT, leads to the development of an effective autocompletion\\ntool for generating CC documentation in healthcare settings.\", 'pdf_text': \"Autocompletion of Chief Complaints in the\\nElectronic Health Records using Large Language\\nModels\\n1st K M Sajjadul Islam\\nComputer Science\\nMarquette University\\nsajjad.islam@marquette.edu\\n2nd Ayesha Siddika Nipu\\nComputer Science & Software Engineering\\nUniversity of Wisconsin-Platteville\\nnipua@uwplatt.edu\\n3rd Praveen Madiraju\\nComputer Science\\nMarquette University\\npraveen.madiraju@marquette.edu\\n4th Priya Deshpande\\nElectrical & Computer Engineering\\nMarquette University\\npriya.deshpande@marquette.edu\\nAbstract—The Chief Complaint (CC) is a crucial component\\nof a patient's medical record as it describes the main reason\\nor concern for seeking medical care. It provides critical in-\\nformation for healthcare providers to make informed decisions\\nabout patient care. However, documenting CCs can be time-\\nconsuming for healthcare providers, especially in busy emergency\\ndepartments. To address this issue, an autocompletion tool that\\nsuggests accurate and well-formatted phrases or sentences for\\nclinical notes can be a valuable resource for triage nurses. In this\\nstudy, we utilized text generation techniques to develop machine\\nlearning models using CC data. In our proposed work, we train\\na Long Short-Term Memory (LSTM) model and fine-tune three\\ndifferent variants of Biomedical Generative Pretrained Trans-\\nformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-\\nLarge, and microsoft/BioGPT-Large-PubMedQA. Additionally,\\nwe tune a prompt by incorporating exemplar CC sentences,\\nutilizing the OpenAI API of GPT-4. We evaluate the models'\\nperformance based on the perplexity score, modified BERTScore,\\nand cosine similarity score. The results show that BioGPT-Large\\nexhibits superior performance compared to the other models. It\\nconsistently achieves a remarkably low perplexity score of 1.65\\nwhen generating CC, whereas the baseline LSTM model achieves\\nthe best perplexity score of 170. Further, we evaluate and assess\\nthe proposed models' performance and the outcome of GPT-4.0.\\nOur study demonstrates that utilizing LLMs such as BioGPT,\\nleads to the development of an effective autocompletion tool for\\ngenerating CC documentation in healthcare settings.\\nIndex Terms—Chief Complaint, Electronic Health Record,\\nText Generation, Large Language Model, BioGPT, Prompt En-\\ngineering, LSTM\\nI. INTRODUCTION\\nA chief complaint (CC) is a brief statement that explains\\nwhy a patient is seeing a doctor. It is usually the second\\nthing asked during a medical history after identifying the\\npatient's demographic information [1]. When a patient seeks\\nmedical care, their CC is recorded several times. First, when\\nThis work is funded by Northwestern Mutual Data Science Institute\\n(NMDSI), Milwaukee, WI, USA.\\nthey register at a clinic or emergency department (ED),\\ntriage nurses and clerks create a record. Then, clinicians also\\ndocument the CC in various notes throughout the patient's\\ncare, including daily progress notes, discharge notes, transfer\\nnotes, and patient acceptance summary notes [2]. The limited\\ntime and information available during triage can sometimes\\nresult in an oversimplified or inaccurate CC, which may\\nnot fully capture the patient's symptoms or concerns. This\\ncan potentially impact the diagnostic process, as the treating\\nclinician may not have a complete understanding of the\\npatient's condition and may not order appropriate tests or\\ntreatments [3]. In addition, errors in CC's can also occur\\ndue to misspelled words, incorrect punctuation, or inaccurate\\nsymptom descriptions [4].\\nThe goal of this study is to employ Natural Language\\nProcessing (NLP) techniques to create an autocompletion tool\\nfor CC's in ED settings. A state-of-the-art (SOTA) NLP model\\nmay help triage nurses generate accurate CC's more efficiently.\\nThis study aims to\\n• Explore the potential of NLP techniques for autocom-\\npleting CC's in ED settings. This study will involve\\ndeveloping an NLP model capable of generating CC's.\\nThis generated CC will not only suggest accurate and\\nwell-formatted notes but also provide ideas to improve\\ntheir notes.\\n• Assess the impact of an autocompletion tool on the\\nefficiency and accuracy of triage in ED settings. This\\nstudy will compare the accuracy of CC's generated with\\nthe NLP model to those entered manually by triage\\nhealthcare providers.\\nAutocompletion provides word, phrase, or sentence sugges-\\ntions as a user types. The primary objective of this system is\\nto improve efficiency by reducing the number of keystrokes\\nrequired, while also elevating the quality of the content by\\narXiv:2401.06088v1  [cs.CL]  11 Jan 2024\\nminimizing typographical errors, promoting the adoption of\\nstandardized terminology, and facilitating the exploration of a\\nwider range of vocabulary [5]. This process works by analyz-\\ning previously entered words to make educated guesses about\\na subsequent word, phrase, or sentence. To complete a CC\\nautomatically, text generation techniques are employed which\\nis one of the primary tasks in Natural Language Generation\\n(NLG). NLG is a specialized area within the discipline of NLP\\nthat focuses on the development of systems with the ability to\\ngenerate both coherent and easily understandable text. NLG\\nis often regarded as a comprehensive term that incorporates\\na diverse array of tasks involving the transformation of input\\ndata into a textual sequence as output. These tasks include gen-\\nerating answers for users in a chatbot, translating languages,\\nsuggesting story ideas, or summarizing data analysis. Clinical\\ndocuments provide distinct issues in comparison to general-\\ndomain text due to the extensive utilization of acronyms and\\nnon-standard clinical terminology by healthcare professionals,\\nas well as the irregular structure and arrangement of these\\ndocuments [6]. Although Generative Pretrained Transformers\\n(GPT) models [7]–[9] demonstrate proficiency in generating\\ncoherent text for broad subject areas, their effectiveness may\\ndiminish when confronted with the complexities inherent in\\nclinical documentation.\\nCC's are free text that consists of one or more improper\\nsentences and medical acronyms [10]. General-purpose lan-\\nguage models may not be able to capture the context and fail\\nto show exemplary results on CC's. GPT-2 [8] has recently\\nadapted to the bio-medical domain. Biomedical Generative\\nPretrained Transformers (BioGPT) is such an adaptation that\\nhas been trained on a very large corpus of biomedical literature\\nand has shown to work well on many tasks, including text\\ngeneration [11]. Hence, we propose to employ BioGPT for\\nautocompletion of the CC.\\nII. BACKGROUND STUDY\\nA. Chief Complaint\\nThe ED in hospitals gets very crowded; it often has more\\npatients and fewer resources than other departments. Many\\nstudies show that when the ED is too crowded, the quality\\nof care for patients gets worse [12]. Long wait time at the\\npoint of triage in ED causes patient dissatisfaction [13].\\nPatients may have to wait a long time for treatment or to\\nleave the ED. Overcrowding can also lead to medical errors\\nand bad outcomes for patients [14]. The Emergency Nurses\\nAssociation (ENA) Triage curriculum stresses the significance\\nof CC in the decision-making process for emergency nurses\\n[15]. It is the first piece of information gathered during the\\ntriage assessment. Around 20% of patients who visit an ED\\nhave non-specific complaints and the majority of them are\\nelderly. Research conducted by retrospective chart analysis\\nindicates that these patients are at a higher risk of being\\nmisdiagnosed and require hospital admission [16]. A study\\nconducted by Nunez et al. (2006) demonstrated that the lack\\nof seriousness of the initial CC is a major factor in patients'\\nunscheduled return to ED [17]. An autocompletion tool for\\nCC can help alleviate these problems.\\nSeveral studies have been done with CC datasets. Tootooni\\net al. (2019) proposed a heuristic methodology for automati-\\ncally mapping free-text CC data into a structured list of CCs,\\nusing an NLP-based algorithm called Chief Complaint Mapper\\n(CCMapper) and to demonstrate its high performance and\\ncapability of incorporating new free-text CC data [18]. Chang\\net al. (2020) used the Bidirectional Encoder Representations\\nfrom Transformers (BERT) language model to learn contextual\\nembeddings for CC [19]. It predicts their provider-assigned\\nlabels with potential applications in automating the mapping\\nof free-text CC's to structured fields and developing a stan-\\ndardized ontology. Hsu et al. (2020) used NLP technologies,\\nincluding deep learning methods such as BERT, to classify\\nChinese CC's at emergency departments for the detection of\\ninfluenza-like illness, with the goal of developing a fast and\\neffective tool to assist physicians in making diagnoses and\\ncontrolling outbreaks [20].\\nB. Text Generation in Electronic Health Record\\nThe process of generating Electronic Health Records\\n(EHRs) presents significant challenges due to the complex\\ndiverse nature of medical data, the imperative for utmost accu-\\nracy, and the rigorous demands for privacy. Recent improve-\\nment in NLG is revolutionizing EHR generation in different\\nfields such as report generation from medical images [21],\\nmedical note generation from table data [22], medical topic to\\ntext generation [23], and so on. More focus has been given to\\nsynthetic EHR generation due to the scarcity of medical data\\n[24]–[27]. In their work, Lee et al. (2018) generate synthetic\\nCC's from discrete variables in EHRs, like age group, gender,\\nand discharge diagnosis [28].\\nRecent advancements in EHR generation have leveraged\\na range of methodologies, from Long Short-Term Memory\\n(LSTM) to transformer-based language modeling. In a study\\nby Liu et al. (2018), a novel transformer-based language mod-\\neling job was introduced. This work involved predicting the\\ncontent of medical notes, taking into consideration previous\\ndata from a patient's medical record [29]. Krishna et al. (2020)\\nprimarily used LSTM and BERT to generate semi-structured\\nclinical summaries (SOAP) notes from doctor-patient conver-\\nsations [30]. Ive et al. (2020) used a neural Transformer model\\nto generate artificial clinical documents for mental health\\nrecords [31]. Sirrianni et al. (2022) employed GPT-2 and GPT-\\nNeo for next-word prediction on dental medical notes that\\ninclude exam notes, emergency notes, trauma notes, etc [32].\\nC. Autocompletion in Electronic Health Record\\nOver the past few years, researchers have extensively in-\\nvestigated diverse techniques to enhance autocompletion tasks\\nin the medical domain. Spithourakis et al. (2016) developed\\nLSTM-based neural language models to improve word pre-\\ndiction and completion tasks [5]. They demonstrated superior\\nperformance on a clinical dataset. Yazdani et al. (2019) in-\\nvestigated the effectiveness of a tri-gram language model in\\nFig. 1. Process Flow of Current Study\\npredicting the next words while typing free texts [33]. Van et\\nal. (2020) explored the use of autocomplete and pre-trained\\nneural language models in semi-automated text simplification\\nfor the medical domain, using a new parallel dataset, and\\ncomparing the performance of four models and an ensemble\\nmodel [34].\\nIn the biomedical domain, the scarcity of large-scale an-\\nnotated data makes it essential to use pre-trained language\\nmodels, which can act as rich feature extractors and reduce\\nreliance on annotated samples [35]. These models also serve\\nas soft knowledge bases, capturing the domain's intricate\\nknowledge from vast unannotated texts. In the biomedical\\nfield, there has been a significant increase in the attention given\\nto PLM models such as clinical BERT and BioGPT in recent\\nyears. To the best of our knowledge, we have not come across\\nany research that specifically addresses autocompletion using\\nSOTA biomedical-based PLMs for CC datasets.\\nIII. METHODOLOGY\\nText generation has evolved significantly from its early\\ndays of statistical language models to neural networks. Joze-\\nfowicz et al. (2016) showed that training recurrent neural\\nnetwork (RNN) LMs on extensive datasets yields superior\\nperformance compared to other statistical language models,\\nsuch as meticulously optimized N-grams [36]. While neural\\nmodels have made impressive advancements in text genera-\\ntion, their performance is often hindered by the scarcity of\\nexpensive labeled data [37]. However, the inception of the\\nTransformer architecture [38], which is the foundation of pre-\\ntrained language models, marked a significant advancement.\\nPre-trained models have revolutionized the capabilities of text\\ngeneration exhibiting improved accuracy and fluency. There\\nexist two primary categories of pre-training models: BERT-\\nlike models [39]–[41] are primarily utilized for language\\nunderstanding tasks, while the GPT-like models [7], [11] are\\nprimarily employed for language generation tasks.\\nOur study suggests that LSTM and BioGPT, are the most\\nsuitable models for our tasks. LSTM model is widely recog-\\nTABLE I\\nSAMPLE OF CHIEF COMPLAINT DATASET\\nChief Complaint a\\nPredict\\nConsensus\\n“been feeling bad” last 2 weeks & switched\\nBP medications last week & worried about\\nBP PMHx: CHF, HTN, gout, 3 strokes, DM\\nN\\n-\\n“can't walk”, reports onset at <<TIME>>.\\noriented x2. aortic valve replacement in\\n<<DATE >>. wife reports episode of sim-\\nilar last week, hospitalized at <<HOSPI-\\nTAL>>for UTI, gout - pmhx: CVA (L side\\nresidual deficits)\\nY\\nN\\n“dehydration” Chest hurts, hips hurt, cramps\\nPMH- Hip replacement, gout, missed pain\\nclinic appt today, thinks he has a gout flair\\nup knee and foot pain\\nY\\nY\\naOnly CC column is employed in present work.\\nnized and commonly employed as a baseline [42] and BioGPT\\ndemonstrates impressive capabilities in NLG, especially in\\nthe medical domain [11]. Additionally, OpenAI API [43]\\nfrom the GPT-4.0 model, is utilized to develop a prompt by\\nimplementing few-shot (FS) technique. Figure 1 depicts the\\noverall flow of our study.\\nA. Dataset Description\\nOsborne et al. (2020) developed an algorithm for identifying\\ngout flares in ED patients using triage nurse CC notes [10]. In\\nthis work, the researchers have provided a de-identified version\\nof a clinical corpus which to the best of their knowledge,\\nis the first free-text CC clinical corpus available. The corpus\\nwas de-identified to adhere to Health Insurance Portability and\\nAccountability Act (HIPAA) Safe Harbor regulations. This\\nde-identification process involves fine-tuning named entity\\nrecognition algorithms using BERT [39] and ALBERT [40].\\nIn addition, potentially identifiable time information was elim-\\ninated, followed by a thorough manual review utilizing BRAT\\nsoftware [44] to guarantee the absence of personal information.\\nThe corpus was annotated to predict gout flare status based on\\na retrospective manual examination of CC's. A subset of these\\ncomplaints underwent review by rheumatologists, applying\\nGaffo criteria to confirm gout flare status, with annotator\\nagreement calculated for both the initial annotation and chart\\nreview phases. This publicly available corpus consists of\\n2 datasets: GOUT-CC-2019-CORPUS and GOUT-CC-2020-\\nCORPUS. In the corpus, there are in total of 8342 CC and\\neach observation has 3 fields: CC, predict, and consensus.\\nThe “Chief Complaint” field consists of freely written text\\nwith abundant abbreviations and acronyms. The “Predict” field\\nsignifies potential gout flare relevance (Y, N, U, -), while the\\n“Consensus” field indicates gout flare status based on chart\\nreview (Y, N, U, -). Here values are yes (Y), no (N), unknown\\n(U), or unmarked (-). For our purpose, we only employed CC\\ndata. The first 3 observations from the dataset are mentioned\\nin Table I.\\nFig. 2. Illustration of Preprocessing Steps with Example\\nB. Data Preprocessing\\nCC is a free text which consists of one or more improper\\nsentences. It is mostly written in abbreviated forms and en-\\nriched in medical acronyms. From our observation, we identify\\nthat a CC consists of 2 parts, the first part involves a patient's\\ncomplaint regarding their current health condition, and the\\nsecond part pertains to their past medical or personal history.\\nWe find several medical acronyms that describe past medical\\nor personal history such as PMH, PMHX, HX, PSHX, SHX,\\nand FHX. We split a CC into two parts based on past medical\\nor personal history. The complaint part consists of one or more\\nimproper sentences. We use the Python NLP library Stanza to\\nseparate sentences. After splitting each CC in sentences, we\\nfilter them based on the length. If a sentence contains less\\nthan 4 words, it is discarded from the dataset. For instance:\\n‘Denies nausea’, ‘24 weeks OB’, etc. are filtered from further\\nconsideration as these types of small sentences do not require\\nautocompletion and degrade model performance. We find a\\ntotal of 11770 sentences after splitting CC and filtering the\\nsmall sentences. The dataset is divided into three sets - train,\\nvalidation, and test; with a ratio of 80%, 10%, and 10%,\\nrespectively. The vocabulary size in the training set is 11565\\nand the median number of words per sentence is 9 which\\nindicates a higher level of diversity in the dataset. It is expected\\nthat the user will type 3 or 4 words initially which is 30% to\\n50% of the sentence. For every test sentence, 2 seed sequences\\nare generated by taking 30% and 50% from the beginning. A\\ndata preprocessing example is shown in Figure 2.\\nC. A Neural Network Approach\\nLSTM [45] is a type of RNN that has shown high-quality\\nperformance in NLP tasks [46], [47]. RNNs are specifically\\nengineered to effectively process sequential input by employ-\\nFig. 3. Framework of Proposed LSTM Model Architecture\\ning a hidden state that undergoes iterative updates at each\\nconsecutive step. LSTM networks possess unique gating mech-\\nanisms, enabling them to effectively capture and learn long-\\nterm dependencies. In LSTM, the model can selectively choose\\nwhich information to keep or forget from the previous state,\\nmaking it more capable of handling long-term dependencies\\nin the input data [48]. The ability of LSTMs to effectively\\nhandle sequential input and comprehend long-term contextual\\ninformation serves as a foundation of text generation, which is\\nthe iterative process of making predictions for the subsequent\\nword in a sequence.\\nFigure 3 illustrates our proposed LSTM model for text\\ngeneration. The first layer of the model is an Embedding layer\\nwhich is used to convert the input text data into dense word\\nvectors of 100 dimensions. This layer takes three arguments:\\nthe total number of unique words in the input corpus, the\\ndimensionality of the embedding space, and the maximum\\nlength of input sequences. The next layer is an LSTM layer\\nwith 100 LSTM cells, a type of RNN layer that processes\\ninput data to capture long-term dependencies in the text.\\nThe output of the LSTM layer is then passed to a Dense\\nlayer with the number of neurons and softmax activation\\nfunction. This layer generates the probability distribution of\\nthe next word in the sequence, given the input sequence.\\nWe enable Adam optimizer, a popular optimizer used for\\ngradient descent in deep learning, with a learning rate of\\n0.001. We utilize categorical cross-entropy loss function that is\\nwidely used for multi-class classification tasks. This model is\\ncapable of predicting subsequent words in a sequence. During\\ntraining, each sentence is prepended with an <sos> token and\\nappended with an <eos> token to signify the start and end.\\nHowever, the model struggles to accurately identify sentence\\nendings in its predictions. As a workaround, we apply an\\niterative approach to generate the next five words in any given\\nsequence, regardless of sentence boundaries.\\nD. A Transfer Learning Approach\\nBioGPT is a highly specialized generative pre-trained Trans-\\nformer language model that has been specifically designed and\\noptimized for the purpose of generating and analyzing biomed-\\nical texts [11]. The model architecture was derived from the\\nGPT-2 [8] model architecture and serves as its backbone.\\nIts training process involves utilizing a dataset including 15\\nmillion abstracts sourced from PubMed. The ultimate acquired\\nvocabulary size amounts to 42,384. The GPT-2 (medium)\\nmodel, serving as the foundation network, consists of 24\\nlayers, a hidden size of 1024, and 16 attention heads. This\\nconfiguration yields a total of 355 million parameters. On the\\nother hand, the BioGPT model has 347 million parameters.\\nThe difference comes solely from variations in the embedding\\nsize and output projection size, which are a consequence of\\nthe dissimilar vocabulary sizes. BioGPT also scaled to larger\\nsize. The BioGPT- Large model was built with the GPT-2\\nXL architecture, which represents the most extensive iteration\\nof GPT-2, having a total of 1.5 billion model parameters.\\nThe BioGPT models demonstrate exceptional performance on\\nfour benchmark datasets, namely BC5CDR, KD-DTI, DDI\\nend-to-end relation extraction job, and PubMedQA question\\nanswering test, surpassing previous SOTA approaches. In\\naddition, the model depicts better biomedical text-generation\\nproficiency in comparison to a standard GPT model trained\\non a general domain.\\nPretrained BioGPT can be adapted from downstream tasks\\nsuch as end-to-end relation extraction, question answering\\n(QA), and document classification by fine-tuning the model.\\nFor this work, we tailor the model specifically for text gen-\\neration. To fine-tune BioGPT, we utilize Raj-High Perfor-\\nmance Computer which is funded in part by the National\\nScience Foundation award CNS-1828649 “MRI: Acquisition\\nof iMARC: High Performance Computing for STEM Research\\nand Education in Southeast Wisconsin” [49].\\nPretrained BioGPT models are available in Hugging-\\nface directory. For text generation, we fine-tune ‘microsoft-\\n/biogpt’1, ‘microsoft/BioGPT-Large’2 and ‘microsoft/BioGPT-\\nLarge-PubMedQA’3. We exploit the tokenizer from the same\\nmodels and tokenize the input sequences by adding special\\ntokens <sos> (start of sentence) and <eos> (end of sentence)\\nat the beginning and end of each sentence, respectively.\\nSubsequently, padding is performed considering the maximum\\ntoken sequence (74 tokens) to make the dimension uniform\\nregardless of the input sequence. Additionally, Adam optimizer\\nis incorporated into the model's training pipeline.\\nBioGPT models possess the capacity to generate multiple\\nsequences for a single seed sequence. For each of the seed\\nsequences, we assign the number of return sequences to 5.\\nThe ‘generate’ function from huggingface includes additional\\n1https://huggingface.co/microsoft/biogpt\\n2https://huggingface.co/microsoft/BioGPT-Large\\n3https://huggingface.co/microsoft/BioGPT-Large-PubMedQA\\noptions such as do sample, top k, max length, top p, etc.,\\nwhich serve to regulate the output sequence. The boolean\\nflag do sample is utilized to decide whether or not to em-\\nploy sampling throughout the process of text generation. The\\nparameter top k is an integer that determines the number of\\nmost probable words to be taken into account while generating\\ntext. The variable max length is an integer that serves as a\\ncontrol parameter for determining the maximum length of the\\noutput text. The variable top p is a floating-point number that\\ndetermines the cumulative probability of selecting the most\\nfrequent words to be considered in the process of generating\\ntext.\\nE. Prompt Tuning: Few-Shot Technique\\nOpenAI provides API to access their latest GPT models\\n[43]. GPT models are trained on natural language and these\\nmodels can generate responses based on their input. This\\ninput is called prompt. Through the strategic creation of\\ntailored prompts, a diverse array of tasks can be effectively\\naccomplished. These tasks include drafting comprehensive\\ndocuments, skillfully composing computer code, conducting\\ninsightful analyses of texts, adeptly crafting conversational\\nagents, and proficiently translating languages. Essentially, cre-\\nating a prompt involves “programming” a GPT model, which\\nis often accomplished by providing guidelines or examples\\nthat show the model how to complete a task.\\nFor our task, we tune a prompt using the OpenAI API\\nof the GPT-4 model, which is the latest model at present.\\nFS prompting technique is incorporated to generate CC. Al-\\nthough LLMs exhibit impressive zero-shot performance, they\\nnevertheless fall short when applied to more challenging tasks.\\nFS technique involves providing the model with a limited\\nnumber of task demonstrations during the inference phase as\\na form of conditioning, without making any adjustments to\\nthe model's weights [9], [50]. In FS prompting technique, a\\nhandful of demonstrations are provided which lead the model\\ntowards better performance and facilitate contextual learning.\\nAccording to the OpenAI official API documentation, it is\\nrecommended to have 50-100 examples as training examples,\\nhowever, a minimum of 10 examples are required. [43]. We\\nchose 100 examples of varying structures from the training\\nCC dataset for our prompt. A sample code is shown in Figure\\n4.\\nIn the prompt development, we use OpenAI's chat com-\\npletions API endpoint, setting the parameter ‘temperature’ as\\n0.7 and ‘n’ as 5. Here ‘n’ means the number of sequences the\\nmodel will generate for each input sentence. The ‘temperature’\\ncontrols the randomness of the model. Higher temperature\\nmakes the model's output more diverse and random. With\\na higher temperature, the model may produce unusual or\\nunexpected responses. A lower temperature makes the model's\\noutput more deterministic. If the temperature is set to 0,\\nthe model will always pick the most probable next word.\\nThe outcomes are often neither overly random nor overly\\npredictable when the temperature is moderate.\\nFig. 4. Prompt Tuning Code Snippet\\nOne problem with LLM like GPT is ‘hallucination’: the\\ncreation of unreliable, irrelevant, or false information [43].\\nGPT-4 is less likely to hallucinate than GPT-3.5-turbo. By\\nproviding explicit instructions in the prompt, it is possible to\\nreduce hallucinations. In our proposed task, the model will\\nsuggest CC and there will be an expert in the loop. So there\\nis minimal impact of hallucination.\\nIV. RESULTS\\nThe assessment of NLG model output presents considerable\\ndifficulties due to the intrinsic uncontrolled nature of many\\nNLG tasks. In contrast to tasks with well-defined parameters\\nthat allow for definitive outputs, open-ended NLG tasks can\\nproduce a diverse array of valid and logically consistent\\noutputs, posing challenges for objective evaluation. Conse-\\nquently, conventional criteria for assessing accuracy may be\\ninadequate, thereby requiring human judgment to evaluate the\\nquality and relevancy of the generated content. Celikyilmaz\\net al. (2020) categorize the assessment approaches for NLG\\ninto three main groups: Human-Centric evaluation, Untrained\\nAutomatic Metrics, and Machine-Learned Metrics [51]. In\\norder to assess the performance of our models, we employ\\nvarious methodologies such as the perplexity measure [52],\\nBERTScore metric [53], and cosine similarity measure [54],\\n[55]. We also include a few examples of models' output for\\ndemonstration.\\nA. Perplexity Measure\\nPerplexity is the often employed metric for quantifying\\nprogress in language modeling [36], [56]. To evaluate the\\nmodels' performance, we use perplexity as an evaluation\\nmetric. The metric quantifies the degree of ambiguity or\\nperplexity exhibited by the model in its predictions of the\\nsubsequent word within a given sequence. A model's per-\\nformance is considered better when its perplexity score is\\nlower, and conversely, worse when the perplexity value is\\nhigher. The concept of perplexity is characterized by the\\nexponential value of the average negative log-likelihood of a\\nTABLE II\\nPERPLEXITY SCORE & EXECUTION TIME\\nModel\\nPerplexity\\nExecution Time a\\n(milliseconds)\\nLSTM\\n170 ± 30\\n3727.09\\nBioGPT\\n3.45 ± 0.05\\n9710.04\\nBioGPT-Large\\n1.65 ± 0.10\\n30899.77\\nBioGPT-Large-PubMedQA\\n2.20 ± 0.10\\n33584.21\\naExecution time measured on Raj-HPC [49]\\ngiven sequence. The perplexity of a tokenized sequence X,\\ndenoted as X = (x0, x1, . . . , xt), can be calculated using\\nEquation 1, where log pθ (xi | x<i) denotes the ith tokens' log-\\nlikelihood depending on the value of preceding tokens [52].\\nPPL(X) = exp\\n(\\n−1\\nt\\nt\\nX\\ni\\nlog pθ (xi | x<i)\\n)\\n(1)\\nTable II provides an overview of the perplexity scores\\nassociated with our various experimented models. From the\\ntable, we can see that the perplexity score for LSTM stands\\nnotably higher, with an overall score of 170. Hence LSTM\\nis eliminated from further assessment. BioGPT, BioGPT-\\nLarge, and BioGPT-Large-PubMedQA exhibit closely aligned\\nperformance, with perplexity rates of 3.45, 1.65, and 2.20,\\nrespectively. Given the superior performance of the fine-tuned\\nBioGPT models in comparison to the LSTM model, these\\nthree models are selected for further quantitative evaluation.\\nB. BERTScore Measure\\nThe BERTScore measure, introduced by Zhang et al. (2019),\\nis a recently developed method for evaluating the quality of\\nlanguage generation. It utilizes pre-trained BERT contextual\\nembeddings as its foundation [53]. The purpose of this system\\nis to measure the semantic similarity between two sentences\\nby using pairwise cosine similarity, rather than relying solely\\non basic string matching. In the present study, Clinical BERT\\nTABLE III\\nCOMPARISON OF BERTSCORE\\nModel\\nFBERT\\nAll 5 CC\\nTop 2 CC\\n30%\\n50%\\n30%\\n50%\\n0.95\\n0\\n0\\n0\\n0\\n0.90\\n0\\n1\\n0\\n6\\nBioGPT\\n0.80\\n61\\n309\\n489\\n866\\n0.70\\n939\\n804\\n674\\n303\\n<0.70\\n177\\n63\\n14\\n2\\n0.95\\n0\\n0\\n4\\n16\\n0.90\\n1\\n37\\n39\\n295\\nBioGPT-Large\\n0.80\\n449\\n771\\n893\\n810\\n0.70\\n685\\n361\\n240\\n55\\n<0.70\\n42\\n8\\n1\\n1\\n0.95\\n0\\n0\\n1\\n19\\n0.90\\n2\\n27\\n45\\n291\\nBioGPT-Large-\\n0.80\\n453\\n823\\n875\\n812\\nPubMedQA\\n0.70\\n675\\n314\\n254\\n54\\n<0.70\\n47\\n13\\n2\\n1\\n[41] embeddings are employed in place of the conventional\\nBERT embedding. The clinical BERT model has undergone\\npre-training on the clinical text and is accessible to the public.\\nThe procedure for computing BERTScore is implemented [53],\\nas outlined in Equations 2, 3 and 4. The tokenized reference\\nsentence x = <x1, ..., xk> is embedded into a sequence of\\nvectors, and similarly, the tokenized candidate sentence ˆx =\\n<ˆx1, ..., ˆxl> is transformed into contextual embedding.\\nRBERT = 1\\n|x|\\nX\\nxi∈x\\nmax\\nˆxj∈ˆx x⊤\\ni ˆxj\\n(2)\\nPBERT = 1\\n|ˆx|\\nX\\nˆxj∈ˆx\\nmax\\nxi∈x x⊤\\ni ˆxj\\n(3)\\nFBERT = 2 PBERT · RBERT\\nPBERT + RBERT\\n(4)\\nTable III presents the BERTScore values obtained from\\nthree BioGPT models. We evaluate the BERTScore in 2\\nscenarios by selecting the seed sequence as described in\\nsection III-B. For the first scenario, 30% of each test CC\\nis taken from the beginning as 30% seed sequence, and for\\nthe second scenario, we take 50% of each test CC from the\\nbeginning as 50% seed sequence. Each scenario is divided into\\n2 cases. For the first case, we consider all 5 generated CCs,\\nand for the other case, we consider only the best 2 performing\\nCCs. Overall we categorize our results into 4 major categories.\\n• Scenario 1: 30% seed sequence, All 5 generated CCs\\n• Scenario 2: 50% seed sequence, All 5 generated CCs\\n• Scenario 3: 30% seed sequence, Top 2 generated CCs\\n• Scenario 4: 50% seed sequence, Top 2 generated CCs\\nFor example, in Scenario 3 for BioGPT-Large, there are\\n39 reference test CC which achieved a BERTScore between\\n0.90 to 0.94. This means that there are 39 reference test\\nCCs, whose top two generated candidate CCs achieved a\\nBERTScore between 0.90 to 0.94, when 30% of the reference\\nCCs are given to BioGPT-Large as the seed sequence.\\nTABLE IV\\nCOMPARISON OF SIMILARITY\\nModel\\nSimilarity\\nAll 5 CC\\nTop 2 CC\\n(Cosine)\\n30%\\n50%\\n30%\\n50%\\n0.95\\n9\\n52\\n112\\n297\\n0.90\\n379\\n497\\n792\\n731\\nBioGPT\\n0.80\\n735\\n580\\n271\\n148\\n0.70\\n50\\n45\\n2\\n1\\n<0.70\\n4\\n3\\n0\\n0\\n0.95\\n62\\n265\\n305\\n660\\n0.90\\n613\\n627\\n695\\n445\\nBioGPT-Large\\n0.80\\n474\\n278\\n175\\n72\\n0.70\\n28\\n7\\n2\\n0\\n<0.70\\n0\\n0\\n0\\n0\\n0.95\\n55\\n248\\n291\\n644\\n0.90\\n606\\n643\\n685\\n472\\nBioGPT-Large-\\n0.80\\n490\\n276\\n198\\n60\\nPubMedQA\\n0.70\\n26\\n10\\n3\\n1\\n<0.70\\n0\\n0\\n0\\n0\\nC. Cosine Similarity Measure\\nTable IV presents the cosine similarity score between the\\nreference and candidate CC. In the current study, we employ\\nthe method of averaging word vectors [55] to compute the\\nsimilarity of sentences, as denoted by the following Equation\\n5. The Clinical BERT [41] is employed to generate word\\nembeddings. To analyze cosine similarity, we also categorize\\nour result into 4 major categories similar to IV-B.\\nSimilarity(xi, ˆxj) =\\nxi · ˆxj\\n∥xi∥ × ∥ˆxj∥\\n(5)\\nD. Execution Time Evaluation\\nTo evaluate the execution time of our models, we utilize the\\nfirst example from Table V. For each model, we generate 5\\noutput sequences. In the context of the LSTM model, the next\\n5 consecutive words are predicted for each sequence. Instead\\nof always selecting the word with the highest probability,\\nrandomness is introduced into the predictions for this model.\\nOn the other hand, BioGPT models are capable of predicting\\nthe end of the sentence. So we generate 5 full sequences\\nwith BioGPT models. As evident from Table II, there is a\\ndirect correlation between model size and execution time. For\\ninstance, the LSTM model has only 1,502,676 parameters and\\nit requires only 3727 milliseconds to generate 5 sequences.\\nIn contrast, BioGPT-Large has 1.5 billion parameters and\\nit demands 30899 milliseconds for the same task. Broadly\\nspeaking, a model's execution time is influenced by a myriad\\nof factors, encompassing model dimensions, parameter count,\\narchitectural design, and the intricacies of the assigned task.\\nV. DISCUSSIONS\\nThe language structures seen in clinical documentation\\nare complex and diverse as a result of the specific nature\\nof medical information and terminologies. In addition, the\\nacquisition of clinical text datasets poses a persistent challenge\\nTABLE V\\nEXAMPLE OF GENERATED CHIEF COMPLAINTS\\nExample\\nModel\\nCandidate CC\\n1\\n(Reference CC)\\nReports have chills, fever, cough, CP, sore throat, back and leg pain.\\nBioGPT\\nReports have chills, fever, malaise x 4 days\\nReports have chills, fever, chills, nausea, HA.\\nBioGPT-Large\\nReports have chills, fever, bodyaches, cough x1 week.\\nReports have chills, fever, dysuria, symptoms since last night.\\nBioGPT-Large-PubMedQA\\nReports have chills, fever, generalized malaise, diarrhea, and congestion since yesterday.\\nReports have chills, fever, fatigue, loss of appetite.\\nGPT-4\\nReports have chills, fever, fatigue, and sore throat x 4 days, tested negative for Covid-19\\n(Prompt)\\nReports have chills, fever, coughing and headaches for the past 3 days\\n2\\n(Reference CC)\\n1cm puncture wound to right hand interweb between 2nd and 3rd digit, tetanus UTD, denies pmh\\nBioGPT\\n1cm puncture wound to right hand interweb between thumb and hand.\\n1cm puncture wound to right hand interweb between wound to R middle finger.\\nBioGPT-Large\\n1cm puncture wound to right hand interweb between 2nd and 3rd digit, tetanus, denies PMH\\n1cm puncture wound to right hand interweb between 2nd and 3rd finger\\nBioGPT-Large-PubMedQA\\n1cm puncture wound to right hand interweb between 2nd and 3rd digit, swelling and pain to\\nwound.\\n1cm puncture wound to right hand interweb between 2nd and 3rd digit, tetanus not UTD\\nGPT-4\\n1cm puncture wound to right hand interweb between thumb and index finger, no signs of infection\\nbut pain is increasing.\\n(Prompt)\\n1cm puncture wound to right hand interweb between thumb and index finger, caused by a rusty\\nnail.\\n3\\n(Reference CC)\\nChronic back and L hip pain x “years” and R shoulder pain x 1 month.\\nBioGPT\\nChronic back and L hip pain x 2 years, denies pmh\\nChronic back and L hip pain, worse with ambulation x one week\\nBioGPT-Large\\nChronic back and L hip pain x1 year.\\nChronic back and L hip pain x1 week.\\nBioGPT-Large-PubMedQA\\nChronic back and L hip pain, denies trauma, no known falls\\nChronic back and L hip pain, radiating down R leg x1 year.\\nGPT-4\\nChronic back and L hip pain, exacerbated by movement, no relief with OTC pain medication.\\n(Prompt)\\nChronic back and L hip pain, worsening over last week, OTC meds provide no relief.\\n*No objective metric is reported in Table III and IV for GPT-4 prompt tuning output.\\ndue to the ethical considerations around patient privacy and the\\nunique nature of medical narratives. In our study, we found\\nthat there is a correlation between the size of a corpus and\\nthe perplexity score of a Language Model. Larger corpora\\ntend to yield higher scores, indicating improved performance\\n[36], [57]. Deep learning models tend to get advantages from\\nan increased quantity of training data. Typically, the efficacy\\nof training an LSTM model relies upon the availability of a\\nsubstantial volume of data, particularly for tasks of greater\\ncomplexity. This is because the model needs to learn more\\nnuanced patterns in the data to make accurate predictions.\\nInsufficient information within a short dataset may impede\\nthe model's ability to properly learn, resulting in inferior\\noutcomes. The performance of our baseline LSTM model is\\nsuboptimal, mostly attributed to the limited size of our corpus.\\nBased on the perplexity score presented in Table II, it\\ncan be observed that large BioGPT models exhibit a higher\\nlevel of performance compared to BioGPT. Tables III and\\nIV also demonstrate similar findings. In every scenario, large\\nmodels consistently outperform BioGPT in terms of scoring.\\nIn Table III Scenario 1, large models display approximately\\n450 reference test CCs, exceeding a BERTScore of 0.80. On\\nthe other hand, the BioGPT model manages only 61 reference\\ntest CCs. For Scenario 2, around 70% of the reference test CCs\\nfor large models reach a BERTScore of 0.80 or above, whereas\\nBioGPT shows results for less than 30% of the reference test\\nCCs. In Scenario 3, more than 80% of the reference test CCs\\nfor large models hit a BERTScore of 0.80 or more. Lastly,\\nin Scenario 4, the large models are excellent, with almost all\\nreference test CCs reaching a BERTScore of 0.80 or above.\\nWhen we select 50% seed sequence instead of 30%, all\\nour models achieve superior BERTScore. One of the plausible\\nreasons behind this is that it becomes easier to generate the\\nincomplete portion when more clues are given. Among all of\\nthe scenarios considered for large models, it can be observed\\nthat BERTScore performs less well in Scenario 1. Given that\\nwe are taking into account all five candidate CCs that have\\nbeen generated, it is also important to note that only 30% of\\nthe test reference CC is being utilized as input for the models.\\nOn the other hand, the models have exhibited exceptional\\nperformance in Scenario 4. This can be attributed to the fact\\nthat we have only focused on the top two performing candidate\\nCCs, with 50% seed sequence as input.\\nAccording to the data shown in Table IV, while utilizing the\\nsemantic cosine similarity measure, it is observed that large\\nmodels achieve a similarity score of 0.90 for 60% reference\\nCC in Scenario 1, and around 95% reference CC in Scenario\\n4. BioGPT models especially large models show promising\\nperformance in generating contextually similar CCs.\\nIn table V, for demonstration we provide a few examples of\\nmodels' output including GPT-4 prompt tuning. No objective\\nmetric is reported for prompt tuning. In the table, reference\\nCC is shown in the first row of every example. The models\\ngenerate the bold-face part and the first part of the reference\\nCC is given to the models as seed sequence. In example 1,\\nthe patient reports several symptoms such as chills, fever, etc.\\nOur BioGPT-Large model is able to generate a few related\\nsymptoms such as bodyaches, cough, etc. The model not\\nonly suggests related symptoms but also proposes a time. The\\nrecommendation of time will help triage nurses improve their\\nclinical notes. BioGPT predicts a few irrelevant symptoms\\nsuch as ‘chills’ which are already present in the sentence.\\nBioGPT-Large-PubMedQA generates some relevant symptoms\\nand a probable timeframe, which is quite similar to the\\noutput of BioGPT-Large model. In example 2, when 50% seed\\nsequence is given, both BioGPT-Large and BioGPT-Large-\\nPubMedQA are able to complete the phrase and suggest the\\nnext words almost similar to reference CC. However, BioGPT\\nfails to generate a meaningful CC sentence in this scenario. In\\nexample 3, the reference CC has 2 parts formed as a compound\\nCC. Each of our experimented models successfully predicts the\\nnext word ‘pain’. Though the BioGPT-Large model was able\\nto complete the phrase, it failed to generate the last part. Other\\nmodels could not capture the first phrase properly. Several CCs\\nconsist of multiple clauses and also include direct statements\\nmade by patients. Such a CC is - about 7wks pregnant per pt,\\npt thinks she's having a miscarriage, pt states, “last night I felt\\nlike I was bleeding more than spotting”. The performance of\\nour experimented models for these particular sorts of CC is\\ncomparatively inferior.\\nFor all of these 3 aforementioned examples, GPT-4 success-\\nfully generates meaningful long sentences. However, from our\\nobservation, it seems unable to capture the CC structure fully.\\nOverall, our fine-tuned BioGPT-Large model performs better.\\nThough our fine-tuned BioGPT-Large model works excel-\\nlently in the short term, it diverges in the long term. It's\\nnot uncommon for language models like BioGPT to perform\\nwell in generating short-term text, but struggle with generating\\nlonger sequences. This is because generating long sequences\\nrequires the model to maintain coherence and consistency over\\na larger context, which can be challenging even for SOTA\\nmodels. In the training set, the median number of words in\\na CC sentence is 9. It is expected that user input will be 3\\nor 4 words which is 30% to 50% of the CC sentence. As\\na result, suggesting the next 5 subsequent words will prevent\\ndivergence. If 5 words are not required to complete a sentence,\\nthe BioGPT-Large model holds the capability to predict the\\nend of a sentence; exhibit example 3 in Table V.\\nVI. CONCLUSION AND FUTURE WORK\\nTo conclude, we evaluate the performance of two different\\ntypes of language models, LSTM and BioGPT, for generating\\nCCs. Our results show that the BioGPT models outperform the\\nLSTM model in terms of perplexity score. We further evaluate\\nBioGPT models based on BERTScore and cosine similarity.\\nAmong all BioGPT models, BioGPT-Large achieves superior\\nperformance while generating more accurate and coherent CC.\\nIn addition, we identify that the performance of the LSTM\\nmodel is limited due to the small size of our training data.\\nIn the upcoming phase, we intend to conduct a Human-\\nCentric evaluation of our models' outputs, with insights from\\ndomain experts. Additionally, we will use a medical corpus\\nto ensure the accuracy of medical terminologies. Moreover,\\nwe aim to refine the date-time representation during post-\\nprocessing.\\nACKNOWLEDGMENT\\nWe extend our sincere gratitude to Dr. Nasim Yahyasoltani\\nand Kevin Chovanec from MU, as well as Ahnaf Farhan from\\nUTEP, for their invaluable suggestions during this work.\\nREFERENCES\\n[1] D. Chang, “Generating contextual text embeddings for emergency de-\\npartment chief complaints using bert,” 2019.\\n[2] M. M. Wagner, W. R. Hogan, W. W. Chapman, and P. H. Gesteland,\\n“Chief complaints and icd codes,” Handbook of biosurveillance, p. 333,\\n2006.\\n[3] S. Krishan and D. Gurpreet, “Misleading complaint,” https://psnet.ahrq.\\ngov/web-mm/misleading-complaint, (Accessed on 09/09/2023).\\n[4] S. Karagounis, I. N. Sarkar, and E. S. Chen, “Coding free-text chief\\ncomplaints from a health information exchange: A preliminary study,”\\nin AMIA Annual Symposium Proceedings, vol. 2020. American Medical\\nInformatics Association, 2020, p. 638.\\n[5] G. P. Spithourakis, S. E. Petersen, and S. Riedel, “Clinical text prediction\\nwith numerically grounded conditional language models,” arXiv preprint\\narXiv:1610.06370, 2016.\\n[6] S. A. Hasan and O. Farri, “Clinical natural language processing with\\ndeep learning,” Data Science for Healthcare: Methodologies and Appli-\\ncations, pp. 147–171, 2019.\\n[7] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving\\nlanguage understanding by generative pre-training,” 2018.\\n[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems, vol. 33, pp. 1877–1901, 2020.\\n[10] J. D. Osborne, T. O’Leary, A. Mudano, J. Booth, G. Rosas, G. Per-\\namsetty, A. Knighton, J. Foster, K. Saag, and M. I. Danila, “Gout\\nemergency department chief complaint corpora,” 2020.\\n[11] R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T.-Y. Liu,\\n“Biogpt: generative pre-trained transformer for biomedical text genera-\\ntion and mining,” Briefings in Bioinformatics, vol. 23, no. 6, 2022.\\n[12] Y. Tiwari, S. Goel, and A. Singh, “Arrival time pattern and waiting\\ntime distribution of patients in the emergency outpatient department\\nof a tertiary level health care institution of north india,” Journal of\\nemergencies, trauma, and shock, vol. 7, no. 3, p. 160, 2014.\\n[13] S. Shah, A. Patel, D. P. Rumoro, S. Hohmann, and F. Fullam, “Managing\\npatient expectations at emergency department triage,” Patient Experience\\nJournal, vol. 2, no. 2, pp. 31–44, 2015.\\n[14] E. B. Kulstad, R. Sikka, R. T. Sweis, K. M. Kelley, and K. H.\\nRzechula, “Ed overcrowding is associated with an increased frequency\\nof medication errors,” The American journal of emergency medicine,\\nvol. 28, no. 3, pp. 304–309, 2010.\\n[15] D. A. Travers and S. W. Haas, “Using nurses’ natural language entries\\nto build a concept-oriented terminology for patients’ chief complaints in\\nthe emergency department,” Journal of biomedical informatics, vol. 36,\\nno. 4-5, pp. 260–270, 2003.\\n[16] T. C. Sauter, G. Capaldo, M. Hoffmann, T. Birrenbach, S. C. Hautz,\\nJ. E. K¨ammer, A. K. Exadaktylos, and W. E. Hautz, “Non-specific\\ncomplaints at emergency department presentation result in unclear\\ndiagnoses and lengthened hospitalization: a prospective observational\\nstudy,” Scandinavian journal of trauma, resuscitation and emergency\\nmedicine, vol. 26, pp. 1–7, 2018.\\n[17] S. Nunez, A. Hexdall, and A. Aguirre-Jaime, “Unscheduled returns to\\nthe emergency department: an outcome of medical errors?” BMJ Quality\\n& Safety, vol. 15, no. 2, pp. 102–108, 2006.\\n[18] M. S. Tootooni, K. S. Pasupathy, H. A. Heaton, C. M. Clements, and\\nM. Y. Sir, “Ccmapper: An adaptive nlp-based free-text chief complaint\\nmapping algorithm,” Computers in Biology and Medicine, vol. 113, p.\\n103398, 2019.\\n[19] D. Chang, W. S. Hong, and R. A. Taylor, “Generating contextual\\nembeddings for emergency department chief complaints,” JAMIA open,\\nvol. 3, no. 2, pp. 160–166, 2020.\\n[20] J.-H. Hsu, T.-C. Weng, C.-H. Wu, and T.-S. Ho, “Natural language\\nprocessing methods for detection of influenza-like illness from chief\\ncomplaints,” in 2020 Asia-Pacific Signal and Information Processing\\nAssociation Annual Summit and Conference (APSIPA ASC).\\nIEEE,\\n2020, pp. 1626–1630.\\n[21] B. Jing, P. Xie, and E. Xing, “On the automatic generation of medical\\nimaging reports,” arXiv preprint arXiv:1711.08195, 2017.\\n[22] H.-Y. Wu, J. Zhang, J. Ive, T. Li, V. Gupta, B. Chen, and Y. Guo, “Med-\\nical scientific table-to-text generation with human-in-the-loop under the\\ndata sparsity constraint,” arXiv preprint arXiv:2205.12368, 2022.\\n[23] Y. Pan, Q. Chen, W. Peng, X. Wang, B. Hu, X. Liu, J. Chen, and\\nW. Zhou, “Medwriter: Knowledge-aware medical text generation,” in\\nProceedings of the 28th International Conference on Computational\\nLinguistics, 2020, pp. 2363–2368.\\n[24] J. Guan, R. Li, S. Yu, and X. Zhang, “Generation of synthetic elec-\\ntronic medical record text,” in 2018 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM).\\nIEEE, 2018, pp. 374–380.\\n[25] O. Melamud and C. Shivade, “Towards automatic generation of share-\\nable synthetic clinical notes using neural language models,” arXiv\\npreprint arXiv:1905.07002, 2019.\\n[26] A. Amin-Nejad, J. Ive, and S. Velupillai, “Exploring transformer text\\ngeneration for medical dataset augmentation,” in Proceedings of the\\nTwelfth Language Resources and Evaluation Conference, 2020, pp.\\n4699–4708.\\n[27] R. Tang, X. Han, X. Jiang, and X. Hu, “Does synthetic data generation\\nof llms help clinical text mining?” arXiv preprint arXiv:2303.04360,\\n2023.\\n[28] S. H. Lee, “Natural language generation for electronic health records,”\\nNPJ digital medicine, vol. 1, no. 1, p. 63, 2018.\\n[29] P. J. Liu, “Learning to write notes in electronic health records,” arXiv\\npreprint arXiv:1808.02622, 2018.\\n[30] K. Krishna, S. Khosla, J. P. Bigham, and Z. C. Lipton, “Generating soap\\nnotes from doctor-patient conversations using modular summarization\\ntechniques,” arXiv preprint arXiv:2005.01795, 2020.\\n[31] J. Ive, N. Viani, J. Kam, L. Yin, S. Verma, S. Puntis, R. N. Cardinal,\\nA. Roberts, R. Stewart, and S. Velupillai, “Generation and evaluation\\nof artificial mental health records for natural language processing,” NPJ\\ndigital medicine, vol. 3, no. 1, p. 69, 2020.\\n[32] J. Sirrianni, E. Sezgin, D. Claman, and S. L. Linwood, “Medical\\ntext prediction and suggestion using generative pretrained transformer\\nmodels with dental medical notes,” Methods of Information in Medicine,\\nvol. 61, no. 05/06, pp. 195–200, 2022.\\n[33] A. Yazdani, R. Safdari, A. Golkar, and S. R. Niakan Kalhori, “Words\\nprediction based on n-gram model for free-text entry in electronic health\\nrecords,” Health information science and systems, vol. 7, pp. 1–7, 2019.\\n[34] H. Van, D. Kauchak, and G. Leroy, “Automets: the autocomplete for\\nmedical text simplification,” arXiv preprint arXiv:2010.10573, 2020.\\n[35] B. Wang, Q. Xie, J. Pei, Z. Chen, P. Tiwari, Z. Li, and J. Fu, “Pre-\\ntrained language models in biomedical domain: A systematic survey,”\\nACM Computing Surveys, 2021.\\n[36] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu, “Explor-\\ning the limits of language modeling,” arXiv preprint arXiv:1602.02410,\\n2016.\\n[37] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\\nwith neural networks,” Advances in neural information processing\\nsystems, vol. 27, 2014.\\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\\nneural information processing systems, vol. 30, 2017.\\n[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805, 2018.\\n[40] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language representa-\\ntions,” arXiv preprint arXiv:1909.11942, 2019.\\n[41] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann,\\nand M. McDermott, “Publicly available clinical bert embeddings,” arXiv\\npreprint arXiv:1904.03323, 2019.\\n[42] G. Melis, C. Dyer, and P. Blunsom, “On the state of the art of evaluation\\nin neural language models,” arXiv preprint arXiv:1707.05589, 2017.\\n[43] “Openai\\napi,”\\nhttps://platform.openai.com/docs,\\n(Accessed\\non\\n09/08/2023).\\n[44] P. Stenetorp, S. Pyysalo, G. Topi´c, T. Ohta, S. Ananiadou, and J. Tsujii,\\n“Brat: a web-based tool for nlp-assisted text annotation,” in Proceedings\\nof the Demonstrations at the 13th Conference of the European Chapter\\nof the Association for Computational Linguistics, 2012, pp. 102–107.\\n[45] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\\n[46] H. Park, S. Cho, and J. Park, “Word rnn as a baseline for sentence\\ncompletion,” in 2018 IEEE 5th International Congress on Information\\nScience and Technology (CiSt).\\nIEEE, 2018, pp. 183–187.\\n[47] L. Yao and Y. Guan, “An improved lstm structure for natural language\\nprocessing,” in 2018 IEEE International Conference of Safety Produce\\nInformatization (IICSPI).\\nIEEE, 2018, pp. 565–569.\\n[48] T.-H. Wen, M. Gasic, N. Mrksic, P.-H. Su, D. Vandyke, and S. Young,\\n“Semantically conditioned lstm-based natural language generation for\\nspoken dialogue systems,” arXiv preprint arXiv:1508.01745, 2015.\\n[49] “Raj hpc— marquette’s high performance computing cluster,” https:\\n//www.marquette.edu/high-performance-computing/architecture.php,\\n(Accessed on 09/20/2023).\\n[50] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a\\nfew examples: A survey on few-shot learning,” ACM computing surveys\\n(csur), vol. 53, no. 3, pp. 1–34, 2020.\\n[51] A. Celikyilmaz, E. Clark, and J. Gao, “Evaluation of text generation: A\\nsurvey,” arXiv preprint arXiv:2006.14799, 2020.\\n[52] “Perplexity\\nmeasure,”\\nhttps://huggingface.co/docs/transformers/\\nperplexity, (Accessed on 09/09/2023).\\n[53] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “Bertscore:\\nEvaluating text generation with bert,” arXiv preprint arXiv:1904.09675,\\n2019.\\n[54] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,”\\nin The 7th international student conference on advanced science and\\ntechnology ICAST, vol. 4, no. 1, 2012, p. 1.\\n[55] M. Farouk, “Measuring sentences similarity: a survey,” arXiv preprint\\narXiv:1910.03940, 2019.\\n[56] H. K. Dam, T. Tran, and T. Pham, “A deep language model for software\\ncode,” arXiv preprint arXiv:1608.02715, 2016.\\n[57] D. Kauchak, “Improving text simplification language modeling using\\nunsimplified text data,” in Proceedings of the 51st annual meeting of\\nthe association for computational linguistics (volume 1: Long papers),\\npp. 1537–1546.\\n\"}, 'http://arxiv.org/abs/2401.06081v1': {'title': 'Improving Large Language Models via Fine-grained Reinforcement Learning\\n  with Minimum Editing Constraint', 'published_date': datetime.datetime(2024, 1, 11, 17, 58, 41), 'pdf_link': 'http://arxiv.org/pdf/2401.06081v1', 'summary': 'Reinforcement learning (RL) has been widely used in training large language\\nmodels~(LLMs) for preventing unexpected outputs, \\\\eg reducing harmfulness and\\nerrors. However, existing RL methods mostly adopt the instance-level reward,\\nwhich is unable to provide fine-grained supervision for complex reasoning\\ntasks, and can not focus on the few key tokens that lead to the incorrectness.\\nTo address it, we propose a new RL method named \\\\textbf{RLMEC} that\\nincorporates a generative model as the reward model, which is trained by the\\nerroneous solution rewriting task under the minimum editing constraint, and can\\nproduce token-level rewards for RL training. Based on the generative reward\\nmodel, we design the token-level RL objective for training and an\\nimitation-based regularization for stabilizing RL process. And the both\\nobjectives focus on the learning of the key tokens for the erroneous solution,\\nreducing the effect of other unimportant tokens. The experiment results on\\nmathematical tasks and question-answering tasks have demonstrated the\\neffectiveness of our approach. Our code and data are available at\\n\\\\url{https://github.com/RUCAIBox/RLMEC}.', 'pdf_text': 'Improving Large Language Models via Fine-grained Reinforcement\\nLearning with Minimum Editing Constraint\\nZhipeng Chen1,3∗, Kun Zhou2,3∗, Wayne Xin Zhao1,3†, Junchen Wan4,\\nFuzheng Zhang4, Di Zhang4 and Ji-Rong Wen1,2,3\\n1Gaoling School of Artificial Intelligence, Renmin University of China.\\n2School of Information, Renmin University of China.\\n3Beijing Key Laboratory of Big Data Management and Analysis Methods.\\n4Kuaishou Technology, Beijing, China.\\nzhipeng_chen@ruc.edu.cn,francis_kun_zhou@163.com,batmanfly@gmail.com\\nAbstract\\nReinforcement learning (RL) has been widely\\nused in training large language models (LLMs)\\nfor preventing unexpected outputs, e.g., reduc-\\ning harmfulness and errors. However, existing\\nRL methods mostly adopt the instance-level re-\\nward, which is unable to provide fine-grained\\nsupervision for complex reasoning tasks, and\\ncan not focus on the few key tokens that lead\\nto the incorrectness. To address it, we pro-\\npose a new RL method named RLMEC that\\nincorporates a generative model as the reward\\nmodel, which is trained by the erroneous solu-\\ntion rewriting task under the minimum edit-\\ning constraint, and can produce token-level\\nrewards for RL training. Based on the gen-\\nerative reward model, we design the token-\\nlevel RL objective for training and an imitation-\\nbased regularization for stabilizing RL pro-\\ncess. And the both objectives focus on the\\nlearning of the key tokens for the erroneous\\nsolution, reducing the effect of other unimpor-\\ntant tokens. The experiment results on math-\\nematical tasks and question-answering tasks\\nhave demonstrated the effectiveness of our ap-\\nproach. Our code and data are available at\\nhttps://github.com/RUCAIBox/RLMEC.\\n1\\nIntroduction\\nRecently, large language models (LLMs) have\\nshown remarkable performance on a variety of\\nscenarios and tasks (Zhao et al., 2023a; OpenAI,\\n2023; Google, 2023), including language gener-\\nation, question answering, and complex reason-\\ning tasks (Hermann et al., 2015; Talmor et al.,\\n2019; Zhang et al., 2023). To further improve\\nLLMs, researchers (Touvron et al., 2023; Bai et al.,\\n2023; OpenAI, 2023) have proposed supervised\\nfine-tuning (SFT) and reinforcement learning (RL)\\nmethods, which can better adapt LLMs to specific\\ndomains and downstream tasks after pre-training.\\n∗ Equal contributions.\\n† Corresponding author.\\n... 6 times greater than the prev-\\nious number. The answer is B\\n... 6 times greater than the prev-\\nious number. The answer is B\\nRLMEC\\nVanilla PPO\\nDiscriminative\\nReward Model\\nProblem: Look at this series: 1.25, 7.5, ____, 270, 1620, ...\\nSolution: ... 6 times greater than the previous number. The answer is C\\nGenerated Solution: ... than the previous number. The answer is B\\n... ...\\nGenerative\\nReward Model\\n... ...\\n×\\n√  √  √   ... ...  √  √ ×\\n... ...\\n \\n \\n \\nToken-Level Rewards:\\n...6 times greater than the prev-\\nious number. The answer is B\\n \\n \\nSentence-Level Rewards:\\n...6 times greater than the prev-\\nious number. The answer is B\\nFigure 1: The comparison of our generative reward\\nmodel and the traditional discriminative one in PPO.\\nRed background color and green background color de-\\nnote negative and positive rewards, respectively.\\nTypically, SFT methods (Ouyang et al., 2022; Long-\\npre et al., 2023) incorporate annotated input-output\\npairs (e.g., question and solution, instruction and re-\\nsponse) to train the LLM for learning the sequence-\\nto-sequence pattern. RL methods (Schulman et al.,\\n2017; Christiano et al., 2017) adopt a reward model\\nto measure the quality of the generated outputs\\nfrom the LLM, and then guide its training for max-\\nimizing and minimizing the expectation of generat-\\ning high-quality and low-quality ones, respectively.\\nAs RL methods are capable of directly reduc-\\ning the probability of specific LLMs for producing\\nunexpected outputs, it has been widely used in op-\\ntimizing LLMs towards better human alignment\\n(e.g., reducing harmfulness) and stronger reason-\\ning ability (Luo et al., 2023; Wang et al., 2023b)\\n(i.e., reducing errors). Generally, RL methods first\\nrequire to train a discrimination model that can dis-\\ntinguish desirable and undesirable outputs. Then,\\nthe reward model will be used to produce the re-\\narXiv:2401.06081v1  [cs.CL]  11 Jan 2024\\nward scores for the sampled outputs from the LLM,\\nand the LLM would be trained to strengthen and\\npunish the generation of high-score and low-score\\nones, respectively.\\nDespite the success, as existing RL methods\\nmostly utilize only the instance-level reward for\\neach sampled output, it is hard to provide accu-\\nrate fine-grained supervision on complex reasoning\\ntasks (e.g., mathematical reasoning). Concretely,\\nfor a complex question, the sampled outputs from\\nthe LLM are prone to have many overlapping parts,\\nbut with significant differences in few specific key\\nwords or steps (Yuan et al., 2023) that determine\\nthe correctness. First, as the unimportant parts\\nwould occupy a large amount supervision signals,\\nthe instance-level rewards can not focus on the im-\\nportant details related to correctness, leading to\\nunnecessary computation cost and the dominant\\nbut redundant supervision. Second, the correct\\nand incorrect outputs may own the overlapping\\npart but receive opposite optimization goals, which\\nmay lead to the optimization conflict issue in the\\ntokens within the overlaps. Recently, a series of\\nworks (Christiano et al., 2017; Zheng et al., 2023b)\\nhave been proposed to alleviate the above prob-\\nlems. However, as they mostly follow the instance-\\nlevel rewards design, they are still suffering the not\\naccurate fine-grained supervisions, limiting their\\nperformance on complex tasks.\\nTo address the above issues, we aim to seek a\\nnew RL method to provide fine-grained supervi-\\nsion signals for all the output tokens. In this work,\\nwe utilize a generative model as the reward model,\\nwhich can naturally produce the probability on all\\nthe tokens for fine-grained supervision. It is trained\\nby an erroneous solution rewriting task under the\\nconstraint of minimum editing distance, which cor-\\nrects the errors in the generated outputs from the\\nLLM, and minimizes the number of edited tokens.\\nIn this way, by comparing the original and rewrit-\\nten outputs, we can distinguish the important error-\\ncausing tokens from the unimportant ones, which\\nenables us to focus on learning to avoid causing\\nsimilar errors and remove redundant supervision.\\nSuch a way is like the error correction process of\\nprofessional teachers on student homework, who\\nlocate and correct the key errors, but do not modify\\nthe whole content too much. Besides, as the unim-\\nportant tokens contain the commonly overlapped to-\\nkens across sampled outputs, removing them from\\noptimization can also prevent the mentioned op-\\ntimization conflict issue. Figure 1 illustrates the\\nfunctionality of the rewriting model.\\nTo this end, we propose a novel method, Rein-\\nforcement Learning with Minimum Editing Con-\\nstraint (RLMEC), which employs a generative\\nmodel to provide token-level rewards for train-\\ning. Specifically, we first collect a small amount\\nof sampled outputs from the LLM and leverage\\nmature LLMs (e.g., Claude 2 (Anthropic, 2023))\\nto rewrite their contained errors with minimum\\nediting constraint, to obtain the data for training\\nthe rewriting model. Then, we utilize the rewrit-\\ning model to produce the token probabilities for\\ncomputing the token-level rewards, and train the\\nLLM using the proximal policy optimization frame-\\nwork (PPO) (Schulman et al., 2017). To stabilize\\nthe training process, we also add a weighted im-\\nitation learning loss that focuses on learning the\\nedited tokens in the rewritten outputs.\\nThe major novelty of this paper lies in the us-\\nage of a generative reward models with minimum\\nediting mechanism for reinforcement learning of\\nLLMs. Table 1 presents the difference between our\\nmethod and previous work. To evaluate the effec-\\ntiveness of our methods, we conduct the experiment\\non two types of complex reasoning tasks, i.e., ques-\\ntion answering (Aggarwal et al., 2021; Mihaylov\\net al., 2018a) and mathematical reasoning (Cobbe\\net al., 2021; Hendrycks et al., 2021c). In all the\\ntasks, our RLMEC mostly outperforms other com-\\npetitive SFT and RL methods, across 7B and 13B\\nLLMs. Moreover, our analysis experiments also\\nshow that our method is able to stabilize the RL\\ntraining process and reduce the erroneous steps in\\nthe sampled outputs of LLMs.\\n2\\nRelated Work\\nIn this section, we introduce the related work from\\nthe two aspects, i.e., reinforcement learning for\\nLLMs and LLMs for reasoning.\\nReinforcement Learning for LLMs. With the\\ndevelopment of the LLMs, reinforcement learn-\\ning (RL) (Christiano et al., 2017; Ziegler et al.,\\n2019) is widely utilized to further improve the\\nability of LLMs. Proximal Policy Optimization\\n(PPO) (Schulman et al., 2017) is the traditional al-\\ngorithm to employ RL. To provide fine-grained su-\\npervision signals, previous work (Mnih et al., 2016;\\nZheng et al., 2023b) utilizes the critic model to cal-\\nculate the reward of the current stage. Because of\\nthe instability of the training procedure of reinforce-\\nment learning, recent work (e.g., DPO (Rafailov\\net al., 2023), CoH (Liu et al., 2023a), FIGA (Guo\\net al., 2023), Quark (Lu et al., 2022), SLiC-\\nHF (Zhao et al., 2023c)) has utilized supervised-\\nfinetuning (SFT) to simulate the RL procedure.\\nThe main idea of the above work is to fuse the\\ninformation about the quality of the generated re-\\nsponses into the supervision signals. Moreover,\\nexisting work (Uesato et al., 2022; Luo et al., 2023;\\nWang et al., 2023b,a; Yang et al., 2023) has found\\nthat process-supervision signals can better guide\\nthe training process of LLMs. However, the fine-\\ngrained annotated data (e.g., PRM800k (Lightman\\net al., 2023)) is necessary for these methods and is\\ndifficult to collect. Besides, other methods (Swamy\\net al., 2024; Chen et al., 2024) improve the abil-\\nity of LLMs during self-play procedure. In this\\nwork, we proposed a new RL framework with gen-\\nerative reward model to directly provide the fine-\\ngrained token-level supervisions, which enable the\\nRL training to focus on few key tokens.\\nLLMs for Reasoning. Previous work utilizes two\\ncategories of methods (i.e., prompting and training)\\nto enhance the performance of LLMs in reason-\\ning tasks. For the prompting methods, Chain-of-\\nThought (CoT) (Wei et al., 2022; Kojima et al.,\\n2022) guides LLMs to generate the intermediate\\nreasoning steps before generating the final answer.\\nBased on CoT, previous work decomposes the prob-\\nlem into several sub-problems which can be easily\\nsolved by LLMs (Dua et al., 2022; Jiang et al.,\\n2023), utilizes the external tools to help LLMs\\nduring reasoning process (Gao et al., 2022; Yao\\net al., 2022; Schick et al., 2023), designs the spe-\\ncific agents to perform reasoning (Yin et al., 2023;\\nChen et al., 2023; Du et al., 2023), or post-process\\nthe generated response (Madaan et al., 2023; Wang\\net al., 2022; Zhao et al., 2023b). Besides, existing\\nwork also guides LLMs to perform reasoning in\\nthe specific structure, e.g., tree (Yao et al., 2023;\\nDing et al., 2023) or graph (Besta et al., 2023). For\\nthe training methods, previous work (Lewkowycz\\net al., 2022) has leveraged domain-specific data\\nto fine-tune the LLMs. Because of the limitation\\nof the training data, the data generated by teacher\\nmodel (e.g., GPT-4, Claude 2) is utilized to aug-\\nment the training data, e.g., the multiple perspec-\\ntives of the original problems (Yue et al., 2023; Luo\\net al., 2023; Yu et al., 2023) or the reasoning path\\nfrom teacher model (Gou et al., 2023). In this work,\\nwe focus on how to effectively train the LLMs via\\nfine-grained RL to improve their reasoning ability.\\n3\\nPreliminary\\nIn this work, we focus on improving the perfor-\\nmance of reinforcement learning (RL) algorithm\\non LLMs for complex reasoning tasks. Typically,\\ncomplex reasoning tasks require LLMs to perform\\nstep-by-step reasoning (e.g., chain-of-thought (Wei\\net al., 2022; Kojima et al., 2022)) for each question,\\nwhere LLMs progressively generate the solution\\nfor reaching the answer. In this way, LLMs are\\nprone to make mistakes in the intermediate key to-\\nkens, leading to totally wrong answer (Bang et al.,\\n2023; Zhang et al., 2023). Our goal is to optimize\\na pre-trained LLM using RL algorithm, to reduce\\nits errors and improve the performance on unseen\\ncomplex reasoning tasks.\\nFormally, we are given a collection of question-\\nsolution pairs, denoted as D = {⟨𝑞𝑖, 𝑠𝑖⟩}𝑛\\n𝑖=1, where\\neach question and solution are both composed by a\\nsequence of tokens, denoted as {𝑡0, · · · , 𝑡𝑚}. Then,\\nwe follow the proximal policy optimization (PPO)\\nframework (Schulman et al., 2017) for RL, and\\noptimize its designs about reward model and train-\\ning loss. In PPO, the LLM required to optimize\\nis named as the policy model, and its original pa-\\nrameters would be copied to compose the reference\\nmodel. During training, the reference model out-\\nputs the sampled solutions for the given question,\\ndenoted as ˆ𝑠, and then the policy model would learn\\nfrom the feedback of the external world. To reduce\\nthe cost of collecting feedback, a reward model is\\nadopted to simulate the feedback from the external\\nworld, which produces the reward 𝑅 ˆ𝑠 for the sam-\\npled output ˆ𝑠. Based on it, the parameters of the\\npolicy model will be optimized to maximize the\\nreward expectation of all the sampled outputs, and\\nthe target function is:\\nJ (𝜃) =\\n𝑛\\n∑︁\\n𝑖=1\\n𝑟(𝑞𝑖, ˆ𝑠𝑖) × 𝑅 ˆ𝑠𝑖, 𝑟(𝑞𝑖, ˆ𝑠𝑖) = 𝑃𝜃 (ˆ𝑠𝑖|𝑞𝑖)\\n𝑃𝜃′(ˆ𝑠𝑖|𝑞𝑖)\\n(1)\\nwhere 𝑟(𝑞𝑖, ˆ𝑠𝑖) is the coefficient of importance sam-\\npling, 𝜃 and 𝜃′ are the parameters of policy model\\nand reference model, respectively.\\n4\\nApproach\\nIn this section, we present our proposed RLMEC,\\na new reinforcement learning framework for im-\\nproving LLMs on complex reasoning tasks. In\\nRLMEC, we train a generative reward model to\\nProducing Rewards\\nGenerative Reward Model Training\\nRL with Fine-grained Supervision\\nGenerated Solution: ... sold \\n24/2=12 clips in April ...\\nFind first error...24/2=12... \\nThe first error ...\\nProblem: ... clips \\nto 48 in April ...\\nPolicy\\nModel\\nGenerated Solution: ... \\n24/2=12 clips in April...\\nGenerative\\nReward Model\\nGenerative\\nReward Model\\nTeacher \\nModel\\nGenerative\\nReward Model\\nToken-Level Rewards\\nRefine the...24/2=12 clips in April...\\nSubtask\\nError Locating\\nSubtask\\nSolution Rewriting\\n...48 clips in April... \\nDistillation\\nRefine the...24/2=12... \\nGenerative\\nReward Model\\n...48 clips in April... \\nRegularization\\n \\n \\n...-0.1,-0.1,0,0,...\\nRewriting Solution\\nFigure 2: The overview of our RLMEC. Based on the sampled LLM solutions that contain errors, we train the\\ngenerative reward model using the erroneous solution rewriting task and the distilled data with minimum editing\\nconstraint from the teacher model. Then, we perform RL training on the policy model (i.e., our LLM) with\\nfine-grained supervision using the token-level RL objective and the imitation-based regularization.\\nproduce token-level reward scores for the sampled\\noutputs from the policy model (i.e., the LLM), then\\noptimize the policy model via reinforcement learn-\\ning based on the fine-grained rewards. Figure 2\\nillustrates the overall framework of our RLMEC.\\n4.1\\nGenerative Reward Model Training\\nTo provide fine-grained supervised signals for RL,\\nwe train a generative model based on the sequence-\\nto-sequence loss as the reward model. For com-\\nplex reasoning tasks, the reward model would offer\\nestimations for all the output tokens about their\\ncorrectness. Therefore, we design an erroneous\\nsolution rewriting task with the constraint of min-\\nimum editing distance to train the reward model,\\nto enable it to focus on the key tokens leading to\\nwrong answer for punishing. To achieve it, we\\nadopt mature LLMs (e.g., Claude 2 (Anthropic,\\n2023), GPT-4 (OpenAI, 2023)) to rewrite the sam-\\npled wrong solutions from our LLM, and collect\\nthe data for distilling the knowledge into it.\\nErroneous Solution Rewriting Task. The erro-\\nneous solution rewriting task is to correct the er-\\nror tokens in the LLM generated solutions with-\\nout changing other unnecessary tokens. Formally,\\ngiven the question 𝑞, ground-truth solution 𝑠, and\\nthe LLM generated solution ˆ𝑠, we aim to rewrite ˆ𝑠\\ninto a correct solution ˜𝑠. To achieve it, we decom-\\npose it into two sub-tasks, i.e., error locating and\\nsolution rewriting. For the error locating subtask,\\nthe model requires to locate the first erroneous rea-\\nsoning step in ˆ𝑠. It is to find out the source of the\\nerror in ˆ𝑠, which would mislead the following steps\\ninto erroneous ones. Concretely, we split ˆ𝑠 into a\\nsequence of reasoning steps according to the full\\nstop or question mark: ˆ𝑠 = {𝑟0, 𝑟1, . . . , 𝑟𝑛}. Then,\\nthe reward model needs to find the first undesired\\nreasoning step 𝑟𝑡 based on the given question and\\nground-truth solution:\\n𝑅𝑀(𝑝𝐿, 𝑞, 𝑠, ˆ𝑠) → 𝑟𝑡,\\n(2)\\nwhere 𝑝𝐿 is the prompt to guide the model. Then,\\nfor the solution rewriting subtask, we leverage an-\\nother prompt 𝑝𝑅 to guide the reward model that\\nrewrites ˆ𝑠 into the correct ˜𝑠:\\n𝑅𝑀(𝑝𝑅, 𝑞, 𝑠, ˆ𝑠, 𝑟𝑡) → ˜𝑠.\\n(3)\\nIn this way, only the erroneous steps after 𝑟𝑡 in ˆ𝑠\\nare refined into correct ones.\\nDistillation with Minimum Editing Constraint.\\nTo train the reward model for fulfilling the above\\ntwo subtasks, we collect the data from a teacher\\nLLM (i.e., Claude 2 (Anthropic, 2023)) to distill\\nthe task knowledge into it. Concretely, we first\\nsample the generated solutions from our LLM, and\\nselect the wrong ones to compose the erroneous\\nsolution set {ˆ𝑠}. Then, we feed the given question\\n𝑞, ground-truth solution 𝑠, and the generated erro-\\nneous solution ˆ𝑠 into the teacher LLM, and add few\\nannotated exemplars into the prompt, to guide the\\ngeneration of the first error step 𝑟𝑡 and the correct\\nrewritten solution ˜𝑠. All the in-context exemplars\\nare human-crafted high-quality instances, and the\\nones for solution rewriting strictly obey the mini-\\nmum editing constraint with only very few revised\\ntokens. Therefore, we can obtain high-quality syn-\\nthetic distilled data for the two subtasks. Finally,\\nfollowing Eq. 2 and Eq. 3, we prepare the inputs\\nand outputs for the two subtasks, and merge them\\nfor training our reward model.\\n4.2\\nRL with Fine-grained Supervision\\nAfter training the generative reward model, we can\\nleverage it to produce fine-grained supervision for\\nthe RL training of the policy model (i.e., our LLM).\\nWe obtain the token-level rewards based on the gen-\\nerated probabilities from the reward model, and de-\\nsign the token-level RL objective with the imitation-\\nbased regularization for training our LLM.\\nToken-level Rewards Generation. After distilla-\\ntion from the teacher model, the generative reward\\nmodel is capable of producing the substitution prob-\\nabilities for all its subsequent tokens in the LLM\\ngeneration solution ˜𝑠. Owing to the minimum edit-\\ning constraint, the error tokens would receive lower\\nprobabilities to be replaced by other tokens, and\\nthe correct tokens would obtain higher probabilities.\\nTherefore, we can rely on the token probability to\\nassign the token-level rewards. Concretely, given\\nthe prompt 𝑝𝑅, question 𝑞, ground-truth solution\\n𝑠, and the sampled solution ˆ𝑠 from our LLM, we\\nconsider utilizing the token substitution probability\\nfrom the generative reward model as the rewards\\nfor all tokens in ˆ𝑠. To normalize the rewards for bet-\\nter indicating the token quality, we subtract them\\nfrom the median value of the probability (i.e., 0.5)\\nand then clip the extreme values as:\\n𝑅 ˆ𝑠,𝑡𝑗 = clip(𝑃𝑅𝑀 (𝑡 𝑗|𝑝𝑅, 𝑞, 𝑠, ˆ𝑠, 𝑡< 𝑗) − 0.5, 𝛼, 𝛽)\\n(4)\\nwhere 𝑃𝑅𝑀(𝑡 𝑗|𝑝𝑅, 𝑞, 𝑠, ˆ𝑠, 𝑡< 𝑗) is the predicted\\nprobability from the reward model for the 𝑗-th to-\\nken in ˆ𝑠, 𝛼 and 𝛽 denote the minimum and maxi-\\nmum thresholds for the reward value. For imple-\\nmentation, we employ 𝛼 = −0.1 and 𝛽 = 0 for the\\nnegative samples while adopt 𝛼 = 0 and 𝛽 = 0.5\\nfor the positive samples. In this way, for negative\\nsamples, the upper threshold 𝛽 = 0 would lead to\\n0 rewards for all the non-error tokens, hence the\\npolicy model can only focus on punishing the error\\ntokens. Otherwise, for positive samples, the lower\\nthreshold 𝛼 = 0 would assign 0 rewards for the\\nerror tokens, where the policy can concentrate on\\nlearning the correct tokens.\\nToken-level RL Objective. Given the token-level\\nrewards, we perform RL training on the policy\\nmodel to correct its behaviors for avoiding making\\nerrors. As mentioned in Section 3, we incorporate\\nthe PPO framework for RL, and revise its loss func-\\ntion to support the token-level rewards. Concretely,\\nwe aim to maximize the expectation that generates\\nthe desired correct tokens in the solution. Thus, the\\nMethods\\nNS\\nRL\\nTLS\\nRM\\nSFT (Ouyang et al., 2022)\\n✗\\n✗\\n✗\\n-\\nRFT (Yuan et al., 2023)\\n✗\\n✗\\n✗\\nDIS\\nCoH (Liu et al., 2023a)\\n✔\\n✗\\n✗\\n-\\nDPO (Rafailov et al., 2023)\\n✔\\n✗\\n✗\\n-\\nFIGA (Guo et al., 2023)\\n✔\\n✗\\n✔\\nDIS\\nPPO (Schulman et al., 2017)\\n✔\\n✔\\n✗\\nDIS\\nRLMEC\\n✔\\n✔\\n✔\\nGEN\\nTable 1: The difference between RLMEC and previous\\nwork. NS, RL, and TLS denote whether utilize negative\\nsamples, RL, and token-level supervision. RM denotes\\nthe type of the reward model. DIS and GEN denote\\nthe discriminative reward model and generative reward\\nmodel, respectively.\\ngradients to optimizate the policy model is:\\n∇J𝑅𝐿(𝜃) =\\n𝑛\\n∑︁\\n𝑖=1\\n∑︁\\n𝑡𝑗 ∈ ˆ𝑠𝑖\\n𝑟(𝑞𝑖, 𝑡 𝑗) × 𝑅 ˆ𝑠𝑖,𝑡𝑗 × ∇ log 𝑃𝜃 (𝑡 𝑗 |𝑞𝑖, 𝑡< 𝑗)\\n(5)\\nwhere 𝜃 is the parameters of the policy model,\\n𝑃𝜃 (𝑡 𝑗|𝑞𝑖, 𝑡< 𝑗) is the predicted probability of the\\n𝑗-th token by the policy model, and 𝑟(𝑞𝑖, 𝑡 𝑗) is the\\ncoefficient of the importance sampling in PPO as:\\n𝑟(𝑞𝑖, 𝑡 𝑗) = 𝑃𝜃 (𝑡 𝑗 |𝑞𝑖, 𝑡< 𝑗)\\n𝑃𝜃′ (𝑡 𝑗 |𝑞𝑖, 𝑡< 𝑗)\\n(6)\\nMoreover, inspired by existing work (Schulman\\net al., 2017; Chen et al., 2019) that clips the gradi-\\nents of RL, we design a simplified way that clips\\nthe coefficient of the gradient to reduce the vari-\\nance of the reward and prevent the large difference\\nbetween the policy and reference model:\\nmin(𝑟(𝑞𝑖, 𝑡 𝑗)×𝑅 ˆ𝑠𝑖,𝑡𝑗 , clip(𝑟(𝑞𝑖, 𝑡 𝑗), 1−𝜀, 1+𝜀)×𝑅 ˆ𝑠𝑖, 𝑗), (7)\\nwhere 𝜀 is a hyperparameter that controls the up\\nand low bound for the positive and negative re-\\nwards, respectively.\\nImitation-based Regularization. As the RL train-\\ning process is prone to be unstable, we also design\\na regularization inspired by imitation learning in\\nour approach. In the imitation-based regulariza-\\ntion, we sample the generated wrong outputs ˆ𝑠\\nfrom the policy model, and utilize our generative\\nreward model to rewrite it into a correct one ˜𝑠 for\\nlearning, where we first find the first undesired\\nreasoning step and then revise its subsequent to-\\nkens. As discussed before, the rewritten solution\\nmay contain only few error tokens that lead to the\\nwrong solution. Therefore, we consider focusing\\non these error tokens in ˆ𝑠, and identify them for\\ntargeted learning. Specifically, we leverage the Lev-\\nenshtein Distance algorithm (Levenshtein, 1965)\\nto find out the revised tokens in ˆ𝑠, and employ\\nthe token-level weights to emphasize them. The\\nLevenshtein Distance algorithm utilizes dynamic\\nprogramming (DP) to calculate the edit distance\\nbetween ˆ𝑠 and ˜𝑠, and the replaced and added tokens\\nare selected into the error token set T. Then, the\\ntoken-level weight is computed as:\\n𝑊 𝑗 =\\n(\\n𝛾,\\n𝑡 𝑗 ∈ T\\n𝜙 ∗ 𝛾,\\n𝑡 𝑗 ∉ T\\n(8)\\nwhere 𝛾 denotes the weight for emphasized tokens\\nin T, and 𝜙 is the penalty coefficient for unim-\\nportant tokens. Based on it, the gradients of the\\nimitation regularization are:\\n∇L𝐼𝑅(𝜃) = −\\n𝑛\\n∑︁\\n𝑖=1\\n∑︁\\n𝑡𝑗 ∈ ˜𝑠𝑖\\n∇ log 𝑃𝜃 (𝑡 𝑗|𝑞𝑖, 𝑡< 𝑗) × 𝑊𝑗\\n(9)\\nFinally, the policy model is optimized by the rein-\\nforcement learning objective and imitation-based\\nregularization.\\n4.3\\nSummary and Discussion\\nIn this part, we present the summary of our ap-\\nproach and discuss the difference between our\\nRLMEC and existing methods.\\nSummary. The procedure of RLMEC can be di-\\nvided into two parts, i.e., generative reward model\\ntraining and RL with fine-grained supervision. For\\nreward model training, we leverage a teacher model\\nto synthesize the examples for the error locat-\\ning and solution rewriting subtasks, to compose\\nthe dataset for distilling our generative reward\\nmodel the capability of erroneous solution rewrit-\\ning. Then, for RL training, we first generate the\\nrewards for all the tokens in the sampled solutions\\nfrom the LLM using Eq. 4, where we utilize the\\nthresholds 𝛼 and 𝛽 to control our model to focus on\\nkey error tokens. Based on the token-level rewards,\\nwe perform RL training using the PPO framework\\nwith the optimization function Eq. 5, and we design\\nthe reward clip strategy using Eq. 7 to stabilize the\\ntraining process. Besides, we also add the imitation-\\nbased regularization using Eq. 9, to further help our\\nLLM focus on learning key tokens.\\nDiscussion. In Tabel 1, we present the difference\\nbetween RLMEC and the existing work. Previ-\\nous work mostly adopts the instance-level reward\\nTask\\nTrain/Test\\nDataset\\nNum. Data\\nMath\\nTrain\\nMathInst\\n118088\\nTest\\nGSM8k\\n1319\\nMATH\\n5000\\nSVAMP\\n1000\\nMM\\n974\\nQA\\nTrain\\nECQA\\n7598\\nQASC\\n8134\\nTest\\nECQA\\n2194\\nQASC\\n926\\nOBQA\\n500\\nARC\\n2376\\nTable 2: Statistics of the dataset in mathematical prob-\\nlems and question-answering problems. MathInst and\\nMM denote MathInstruct and the mathematical task in\\nMMLU, respectively.\\nmodel, and only FIGA employs the token-level su-\\npervision but not using RL. As a comparison, our\\nproposed RLMEC combines RL and token-level\\nsupervision, hence it can benefit from more fine-\\ngrained RL training process and focus on punishing\\nerror tokens during training. To achieve it, we de-\\nsign the generative reward model trained by the\\nerroneous solution rewriting task to replace the dis-\\ncriminative reward model, whose produced token\\nprobability can be naturally used as token-level su-\\npervision. Besides, by comparing with supervised\\nfine-tuning methods (e.g., SFT and RFT), our ap-\\nproach can utilize the negative samples that will not\\nbe used by them, which extends the understanding\\nof failed examples and fully utilizes the data.\\n5\\nExperiment\\nIn this section, we conduct two challenge tasks (i.e.,\\nmathematical tasks and question-answering tasks)\\nto assess the effectiveness of RLMEC.\\n5.1\\nExperimental Settings\\nDatasets.\\nWe employ mathematical tasks and\\nquestion-answering tasks for evaluation. Success-\\nfully solving these tasks necessitates LLMs to pos-\\nsess domain-specific knowledge and engage in sys-\\ntematic, step-by-step reasoning to reach the ulti-\\nmate answer. The specifics of each dataset are\\ndelineated in Table 2.\\n• Mathematical tasks include GSM8k (Cobbe\\net al., 2021), MATH (Hendrycks et al., 2021c),\\nSVAMP (Patel et al., 2021) and the mathemati-\\ncal problems in MMLU (MM) (Hendrycks et al.,\\n2021b,a). We adopt MathInstruct (Yue et al., 2023)\\nas the training set in our experiment and elimi-\\nnate the code samples. MathInstruct contains the\\ntraining set of GSM8k and MATH. Therefore, for\\nLLMs, GSM8k and MATH are seen tasks, while\\nSVAMP and MM are unseen tasks.\\n• Question-answering tasks contain ECQA (Ag-\\ngarwal et al., 2021), QASC (Khot et al., 2020),\\nOpenbookQA (Mihaylov et al., 2018b) and ARC-\\nEasy (Clark et al., 2018). We merge the training set\\nof ECQA and QASC and adopt the mixture as the\\ntraining set in the experiment, Therefore, ECQA\\nand QASC are seen tasks for LLMs, while Open-\\nbookQA and ARC are unseen tasks for LLMs.\\nBaselines. For a more comprehensive assessment,\\nwe incorporate three categories of methods as base-\\nline approaches, including supervised fine-tuning,\\nalignment without reinforcement learning, and re-\\ninforcement learning.\\n• Supervised Fine-tuning leverages the annotated\\ndata to train LLMs to imitate the human desired\\nbehavior. We conduct the vanilla SFT (Ouyang\\net al., 2022) and the Rejection sampling Fine-\\nTuning (RFT) (Liu et al., 2023b; Yuan et al., 2023)\\nas the baseline methods.\\n• Alignment without Reinforcement Learning is\\nthe method to align LLMs to human preference\\nand prevent instability in reinforcement learning.\\nRepresentative methods, e.g., DPO (Rafailov et al.,\\n2023), CoH (Liu et al., 2023a), and FIGA (Guo\\net al., 2023) are conducted as the baseline.\\n• Reinforcement Learning is the traditional\\nmethod to guide LLMs to explore the world and\\nlearn from external feedback.\\nPPO (Schulman\\net al., 2017) is the classical algorithm to employ\\nreinforcement learning. We conduct the vanilla\\nPPO (Schulman et al., 2017) and Actor-Critic ver-\\nsion of PPO (PPO A2C) (Zheng et al., 2023b) in\\nthe experiment.\\nMoreover, we also report the performance of\\nother LLMs without domain adaption, including\\nLLaMA 2 (Touvron et al., 2023), Vicuna (Zheng\\net al., 2023a), and WizardLM (Xu et al., 2023).\\nBesides, because of the limitation of resources, we\\ndo not employ FIGA on mathematical tasks and do\\nnot employ PPO A2C on 13B LLMs.\\nImplementation Details. During the evaluation,\\nwe adopt Claude 2 (Anthropic, 2023) as the teacher\\nmodel in the experiment. For backbone LLMs, we\\nutilize the mixture dataset of ECQA and QASC\\nto fine-tune LLaMA 2 (Touvron et al., 2023) to\\nobtain the domain-adapted SFT backbone model\\nin QA tasks, and adopt MAmmoTH (Yue et al.,\\n2023) as the backbone model for mathematical\\ntasks. The backbone LLMs of the policy model\\nand the generative reward model are the same SFT\\nLLMs. In the training procedure, we employ 5 ×\\n10−6 as the learning rate for all tasks and train\\nLLMs for 1 epoch. Besides, we set 128 and 768 as\\nthe batch size for QA tasks and mathematical tasks.\\nFor the value of 𝜀, we leverage 0.3 and 0.4 for 7B\\nmodel and 13B model, respectively. Because the\\nLLMs have adapted to the corresponding domain\\nafter training, we adopt the 0-shot setting during\\nevaluation.\\n5.2\\nMain Results\\nThe evaluation results of RLMEC and the baseline\\nmethods are presented in Table 3.\\nFirst, RLMEC outperforms other baselines on\\nthe average accuracy of the question-answering\\ntasks and mathematical tasks. RLMEC demon-\\nstrates a strong capacity to further enhance the spe-\\ncific ability (e.g., reasoning ability) of LLMs. With\\nthe limited number of training data, compared with\\nthe previous methods (e.g., RFT, PPO), RLMEC\\nleverages both positive and negative samples to pro-\\nvide fine-grained supervision signals, which can\\nguide LLMs to focus on the mistakes and correct\\nthem.\\nSecond, RLMEC can prevent overfitting during\\nthe process of domain adaption. Previous methods\\n(e.g., SFT) utilize the data from the training set or\\ngenerated by LLMs to fine-tune the LLMs which\\nmight cause overfitting. We can observe that the\\nperformance of the LLMs decreases after SFT on\\nthe unseen tasks (e.g., OBQA and SVAMP) of the\\n7B LLM. In contrast, the performance of LLMs on\\nall of the unseen tasks is improved after RLMEC.\\nThe reason is that RLMEC makes LLMs focus on\\nmistakes rather than correct components and utilize\\nthe clip mechanism to avoid overfitting.\\nThird, RLMEC can better leverage the gener-\\nated response containing undesired components\\nthan other methods. Comparing the performance\\nof RLMEC and DPO, we can observe that RLMEC\\ncan enhance the reasoning ability of LLMs in both\\nscenarios, but DPO only works well on question-\\nanswer tasks. That is because RLMEC utilizes soft\\nMethods\\nQuestion-Answering Tasks\\nMathematical Tasks\\nECQA\\nQASC\\nOBQA\\nARC\\nAvg.\\nGSM8k\\nMATH\\nSVAMP\\nMM\\nAvg.\\n7B Parameters LLMs\\nLLaMA 2\\n55.97\\n39.74\\n48.40\\n52.48\\n49.15\\n11.22\\n4.80\\n29.70\\n28.44\\n18.54\\nVicuna\\n49.82\\n32.18\\n46.40\\n51.52\\n44.98\\n12.20\\n4.26\\n24.30\\n26.08\\n16.71\\nWizardLM\\n36.28\\n18.68\\n27.80\\n46.59\\n32.34\\n14.48\\n3.34\\n34.80\\n27.10\\n19.93\\nSFT LLM\\n71.88\\n55.40\\n52.00\\n56.27\\n58.89\\n51.02\\n10.48\\n47.80\\n38.50\\n36.95\\n+ SFT\\n70.65\\n55.94\\n51.60\\n56.99\\n58.80\\n50.34\\n11.04\\n47.20\\n38.40\\n36.75\\n+ RFT\\n72.24\\n58.64\\n55.20\\n57.15\\n60.81\\n49.66\\n10.80\\n48.30\\n39.01\\n36.94\\n+ RFT w/ GT\\n72.47\\n58.53\\n53.60\\n57.11\\n60.43\\n49.89\\n11.26\\n46.70\\n38.91\\n36.69\\n+ RFT w/ TD\\n73.11\\n58.21\\n54.20\\n57.53\\n60.76\\n51.86\\n11.04\\n49.40\\n38.19\\n37.62\\n+ RS w/ RD\\n72.47\\n59.29\\n54.60\\n57.03\\n60.85\\n51.78\\n11.24\\n48.70\\n40.76\\n38.12\\n+ CoH\\n71.06\\n54.86\\n51.40\\n56.61\\n58.48\\n50.11\\n10.94\\n48.60\\n38.50\\n37.04\\n+ DPO\\n72.47\\n58.53\\n55.40\\n55.26\\n60.42\\n34.19\\n5.38\\n25.80\\n32.58\\n24.49\\n+ FIGA\\n69.83\\n52.48\\n51.00\\n46.21\\n54.88\\n-\\n-\\n-\\n-\\n-\\n+ Vanilla PPO\\n72.88\\n50.22\\n43.40\\n56.27\\n55.69\\n48.97\\n10.64\\n44.90\\n38.60\\n35.78\\n+ PPO A2C\\n70.83\\n55.08\\n52.40\\n56.02\\n58.58\\n50.94\\n9.38\\n46.60\\n38.50\\n36.36\\n+ RLMEC\\n73.66\\n59.50\\n56.80\\n58.50\\n62.12\\n51.18\\n11.16\\n49.60\\n40.97\\n38.23\\n13B Parameters LLMs\\nLLaMA 2\\n61.53\\n45.46\\n57.90\\n64.31\\n57.30\\n21.23\\n6.58\\n34.40\\n34.39\\n24.15\\nVicuna\\n50.14\\n39.96\\n48.40\\n53.70\\n48.05\\n24.10\\n4.74\\n33.80\\n29.98\\n23.16\\nWizardLM\\n52.60\\n40.93\\n52.30\\n58.96\\n51.20\\n31.01\\n3.18\\n52.00\\n21.36\\n26.89\\nSFT LLM\\n76.12\\n59.40\\n60.80\\n62.46\\n64.70\\n56.63\\n12.74\\n53.50\\n41.27\\n41.04\\n+ SFT\\n75.89\\n57.87\\n63.40\\n62.50\\n64.92\\n55.88\\n13.62\\n58.00\\n41.27\\n42.19\\n+ RFT\\n75.71\\n60.48\\n61.00\\n64.06\\n65.31\\n55.80\\n13.62\\n54.10\\n41.68\\n41.30\\n+ RFT w/ GT\\n76.66\\n60.37\\n63.40\\n63.17\\n65.90\\n57.32\\n13.74\\n56.70\\n43.94\\n42.93\\n+ RFT w/ TD\\n76.71\\n61.56\\n61.80\\n64.14\\n66.05\\n58.15\\n13.98\\n58.80\\n41.58\\n43.13\\n+ RS w/ RD\\n76.62\\n62.20\\n63.20\\n63.17\\n66.30\\n57.39\\n14.34\\n56.20\\n42.81\\n42.96\\n+ CoH\\n76.62\\n60.37\\n59.80\\n63.93\\n65.18\\n57.31\\n13.10\\n54.00\\n42.30\\n41.68\\n+ DPO\\n78.26\\n61.45\\n62.20\\n63.80\\n66.43\\n44.20\\n4.38\\n39.70\\n32.14\\n30.11\\n+ FIGA\\n61.21\\n60.26\\n52.80\\n46.34\\n55.15\\n-\\n-\\n-\\n-\\n-\\n+ Vanilla PPO\\n76.34\\n57.99\\n61.80\\n62.29\\n64.61\\n53.45\\n11.76\\n55.10\\n43.12\\n40.86\\n+ RLMEC\\n79.49\\n64.15\\n65.60\\n65.19\\n68.61\\n58.15\\n14.00\\n60.00\\n45.07\\n44.31\\nTable 3: Experimental results on question answering tasks and mathematical tasks. Avg. is the average accuracy of\\nall sub-tasks. GT, TD, and RD denote ground truth, the data generated by the teacher model, and the data generated\\nby the rewriting model. The best are denoted in bold and the second-best are underlined.\\nrewards to indicate positive or negative responses,\\nwhile DPO collects the positive-negative response\\npairs to train LLMs which can be regarded as uti-\\nlizing the hard labels to identify the quality of gen-\\nerated responses. Given the quality of generated\\nresponses is difficult to evaluate, the process of col-\\nlecting response pairs is very hard in the challenge\\ntasks (e.g., mathematical tasks). On mathematical\\ntasks, the performance of DPO is even worse than\\nthe backbone LLM because of the low quality of\\nthe training data.\\nFinally, token-level supervision signals can fur-\\nther improve the performance of the policy model.\\nThe results of vanilla PPO, PPO A2C, and RLMEC\\npresent the importance of fine-grained supervision\\nsignals. Vanilla PPO utilizes outcome-supervision\\nsignals to train the LLMs, which do not conform to\\nreality because the generated response might con-\\ntain desired components and undesired components\\nat the same time. PPO A2C trains the critic model\\nMethods\\nECQA\\nARC\\nGSM8k\\nMM\\nTLS\\nRL\\nIR\\nAcc.\\nAcc.\\nAcc.\\nAcc.\\n✔\\n✔\\n✔\\n79.49\\n65.19\\n58.15\\n45.07\\n✗\\n✔\\n✔\\n78.81\\n64.52\\n58.38\\n44.45\\n✗\\n✗\\n✔\\n77.85\\n64.18\\n58.56\\n43.84\\n✔\\n✔\\n✗\\n74.34\\n61.32\\n7.35\\n20.12\\nTable 4: The results of ablation study on 13B LLMs.\\nTLS, RL, and IR denote token-level supervision, rein-\\nforcement learning, and imitation-based regularization.\\nto provide fine-grained supervision signals which\\nwill increase the requirement of the computation re-\\nsources. In RLMEC, the generative reward model\\nis competent to implement the functionality of the\\nreward model and the critic model in the PPO A2C\\nat the same time.\\n5.3\\nDetailed Analysis\\nTo further verify the effectiveness of RLMEC, we\\nconduct the ablation study and present the analysis\\n(a) Edit distance between refined responses and predictions.\\n(b) Accuracy of the refined response.\\nFigure 3: The comparison of the rewriting performance\\nof teacher model and generative reward model. TM\\nand GRM denote the response refined by the teacher\\nmodel and the generative reward model, respectively.\\nGT denotes the ground truth solution of the problems.\\nGRM\\nPM\\n7B PM\\n13B PM\\nQA\\nMath\\nQA\\nMath\\n7B GRM\\n62.12\\n38.23\\n66.40\\n43.74\\n13B GRM\\n61.32\\n37.46\\n68.61\\n44.31\\nTable 5: The comparison of the different scaling of\\nthe generative reward model. GRM and PM denote the\\ngenerative reward model and policy model, respectively.\\nof the editing distance and scaling of the generative\\nreward model, and the case study of supervision\\nsignals of different methods. Finally, we discuss\\nthe methods to simplify the RLMEC to further\\nreduce the requirement of the resources.\\nAblation Study. In the ablation study, we eval-\\nuate the effectiveness of token-level supervision,\\nreinforcement learning, and imitation-based reg-\\nularization. The results are presented in Table 4.\\nGiven the results on the question-answering tasks\\n(i.e., ECQA and ARC), we can observe that remov-\\ning any of the modules will hurt the performance\\nof the LLMs. In the mathematical tasks, without\\ntoken-level supervision and reinforcement learn-\\ning, LLMs overfit to the training set, which causes\\nthe improvement on the seen task (i.e., GSM8k)\\nand hurts the performance on the unseen task (i.e.,\\nMM). The evaluation results demonstrate the abil-\\nFigure 4: The performance of 7B LLMs on question-\\nanswering tasks during different training strategies. To\\nbetter present the difference, we smooth out the lines.\\nity of RLMEC to prevent overfitting and achieve\\nthe balance between seen tasks and unseen tasks.\\nBesides, imitation-based regularization is also an\\nimportant module in RLMEC. Without regulariza-\\ntion, LLMs learn to generate correct responses only\\nthrough token-level rewards. Because of the large\\nsearch space, it is very difficult for LLMs to find\\nthe correct behavior in the challenge tasks. In the\\nsetting of removing imitation-based regularization,\\nthe decreasing of performance on all of the tasks\\ncan verify our analysis.\\nEditing Distance of Generative Reward Model.\\nThe effectiveness of the generative reward model\\nwill influence the quality of the token-level rewards\\nand the refined response. Thus, we present the\\ncomparison of the teacher model and the genera-\\ntive reward model in Figure 3. We can observe that\\nboth the teacher model and the generative reward\\nmodel can significantly reduce the edit distance\\nand even perform slightly better than the teacher\\nmodel. That is because we utilize the two-stage\\nprompting strategy to distillate knowledge from the\\nteacher model and conduct the high-quality data\\nto fine-tune the generative reward model. Through\\nfine-tuning, it can adapt to the erroneous solution\\nrewriting task well. Moreover, the teacher model\\nand the generative reward model have shown simi-\\nlar performance on the accuracy of the refined re-\\nsponses, which verifies that the rewriting task can\\nbe easily learned by the LLMs with smaller param-\\neters. Besides, given the performance of RFT w/\\nTD and RFT w/ RD in Table 3, we can observe that\\nthe higher accuracy of the refined responses will\\nlead to higher performance in downstream tasks\\nthrough simply supervised fine-tuning.\\nScaling Analysis of Reward Model. To explore\\nthe influence of the scale of the generative reward\\nmodel, we conduct the experiment and present the\\nresults in Table 5. For both 7B and 13B LLMs, the\\nrewriting model trained from the same backbone\\nLLMs with the policy model performs better. The\\npotential reason might be that the policy model\\nand the rewriting model with the same backbone\\nmodel will have a similar distribution. In this situa-\\ntion, the rewriting model can provide appropriate\\nsupervision signals and better guide the training\\nprocess.\\nProblem\\nLook at this series: 1.25, 7.5, ____,\\n270, 1620, ... What number should fill\\nthe blank?\\nAnswer Choices: (A) 50 (B) 65 (C) 45\\n(D) 55 (E) 40\\nGround Truth\\nSolution\\nLet’s think about the multi-choice\\nquestion.\\nThis is a simple multiplication series.\\nEach number is 6 times greater than\\nthe previous number.\\nThe answer is C\\nGenerated So-\\nlution\\nLet’s think about the multi-choice\\nquestion step by step.\\nThis is a simple multiplication series.\\nEach number is 6 times greater than\\nthe previous number.\\nThe answer is B\\nRewards from\\nRLMEC\\nLet’s think about the multi-choice qu-\\nestion step by step .\\nThis is a simple multiplication series.\\nEach number is 6 times greater than\\nthe previous number .\\nThe answer is B\\nRewards from\\nPPO A2C\\nLet’s think about the multi-choice qu-\\nestion step by step .\\nThis is a simple multiplication series .\\nEach number is 6 times greater than\\nthe previous number .\\nThe answer is B\\nRewards from\\nVanilla PPO\\nLet’s think about the multi - choice qu\\nestion step by step .\\nThis is a simple multiplication series .\\nEach number is 6 times greater than\\nthe previous number .\\nThe answer is B\\nTable 6: The comparison of the reward of the gener-\\nated solution from different methods. We use different\\nbackground colors to indicate the reward. The color\\nchanging from red to green denotes the reward chang-\\ning from negative to positive .\\nAnalysis of the Supervision Signals. We present\\nthe case study about the reward from different meth-\\nods in Table 6. To better express the difference, we\\ndo not employ the clip mechanism in the case study.\\nFrom the results, we can observe that the reason-\\ning step of the generated solution is correct but the\\nfinal answer is error. In PPO A2C, the reward will\\nbe calculated by the reward model and the critic\\nmodel. The tokens generated earlier will receive\\na lower reward, which is contradictory to reality.\\nThat is because PPO A2C has assumed that the\\nprevious token will influence the last token. In\\nthis case, once the generated solution contains the\\nwrong answer, the rewards of the previous tokens\\nare likely lower than the last tokens. In contrast,\\nwe leverage the generative reward model to gen-\\nerate the reward in RLMEC. The reward of the\\ncurrent token is calculated based on the previous\\ntokens. Therefore, the rewriting model in RLMEC\\ncan better indicate whether the token is correct and\\nprovide high-quality token-level supervision sig-\\nnals. Besides, for the outcome-supervised method\\n(i.e., Vanilla PPO), the reward of each token is\\nequivalent and is based on whether the generated\\nsolution is correct. This method cannot describe the\\ncorrectness of the tokens in the generated response.\\nThe\\nSimplification\\nof\\nRLMEC.\\nAlthough\\nRLMEC has significantly reduced the cost of tradi-\\ntional RL methods, there are still several methods\\nto simplify RLMEC and achieve comparable per-\\nformance. First, given the results of RFT w/ TD\\nand RFT w/ RD in Table 3, we can find that simply\\nutilizing refined responses to train the LLMs can\\nalso achieve acceptable performance. Second, the\\nscaling analysis of the generative reward model has\\ndemonstrated that reducing the parameters of the\\nrewriting model will not largely hurt the perfor-\\nmance of RLMEC. Therefore, leveraging a smaller\\nrewriting model is a good choice in a scenario with\\nlimited computation resources. Finally, because\\nof the rapid speed to fit the training set through\\nRLMEC, we can also decrease the training step of\\nRLMEC to reduce the computation cost.\\n6\\nConclusion\\nIn this paper, we proposed RLMEC, a new rein-\\nforcement learning framework with minimum edit-\\ning constraint, to leverage fine-grained supervision\\nsignals to further improve the ability of LLMs. In\\nour RLMEC, we first trained the generative reward\\nmodel via the erroneous solution rewriting task un-\\nder the minimum editing constraint, with the help\\nof a teacher LLM. Then, we leveraged it to produce\\ntoken-level rewards, and devised the token-level RL\\nobjective and an imitation-based regularization for\\ntraining our LLM, which both focus on the learning\\nof the key tokens leading to errors in the solution.\\nExperimental results on mathematical tasks and\\nquestion-answering tasks have demonstrated the\\neffectiveness of RLMEC.\\nIn future, we will consider implementing our\\nRL method on more advanced LLMs to further\\nimprove their performance on complex reasoning\\ntasks. Besides, we will also evaluate the capacity\\nof our approach on enhancing human alignment\\nand reducing hallucination.\\nReferences\\nShourya Aggarwal, Divyanshu Mandowara, Vishwajeet\\nAgrawal, Dinesh Khandelwal, Parag Singla, and Di-\\nnesh Garg. 2021. Explanations for commonsenseqa:\\nNew dataset and models. In Proceedings of the 59th\\nAnnual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Confer-\\nence on Natural Language Processing, ACL/IJCNLP\\n2021, (Volume 1: Long Papers), Virtual Event, Au-\\ngust 1-6, 2021, pages 3050–3065.\\nAnthropic. 2023. Claude 2. Anthropic Blog.\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,\\nRunji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,\\nXuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang\\nWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian\\nYang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi\\nYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,\\nYichang Zhang, Zhenru Zhang, Chang Zhou, Jin-\\ngren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.\\nQwen technical report. CoRR, abs/2309.16609.\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\\nand Pascale Fung. 2023. A multitask, multilingual,\\nmultimodal evaluation of chatgpt on reasoning, hal-\\nlucination, and interactivity. CoRR, abs/2302.04023.\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\\nLehmann, Michal Podstawski, Hubert Niewiadom-\\nski, Piotr Nyczyk, and Torsten Hoefler. 2023. Graph\\nof thoughts: Solving elaborate problems with large\\nlanguage models. CoRR, abs/2308.09687.\\nMinmin Chen, Alex Beutel, Paul Covington, Sagar Jain,\\nFrancois Belletti, and Ed H. Chi. 2019. Top-k off-\\npolicy correction for a REINFORCE recommender\\nsystem. In Proceedings of the Twelfth ACM Interna-\\ntional Conference on Web Search and Data Mining,\\nWSDM 2019, Melbourne, VIC, Australia, February\\n11-15, 2019, pages 456–464.\\nZhipeng Chen, Kun Zhou, Beichen Zhang, Zheng\\nGong, Xin Zhao, and Ji-Rong Wen. 2023. Chat-\\ncot: Tool-augmented chain-of-thought reasoning on\\nchat-based large language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023, Singapore, December 6-10, 2023, pages 14777–\\n14790.\\nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,\\nand Quanquan Gu. 2024. Self-play fine-tuning con-\\nverts weak language models to strong language mod-\\nels. CoRR, abs/2401.01335.\\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan\\nMartic, Shane Legg, and Dario Amodei. 2017. Deep\\nreinforcement learning from human preferences. In\\nAdvances in Neural Information Processing Systems\\n30: Annual Conference on Neural Information Pro-\\ncessing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA, pages 4299–4307.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\\nAshish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. 2018. Think you have solved question an-\\nswering? try arc, the AI2 reasoning challenge. CoRR,\\nabs/1803.05457.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\\nNakano, Christopher Hesse, and John Schulman.\\n2021. Training verifiers to solve math word prob-\\nlems. CoRR, abs/2110.14168.\\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu,\\nMinghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\\nQingwei Lin, and Dongmei Zhang. 2023. Everything\\nof thoughts: Defying the law of penrose triangle for\\nthought generation. CoRR, abs/2311.04254.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B.\\nTenenbaum, and Igor Mordatch. 2023. Improving\\nfactuality and reasoning in language models through\\nmultiagent debate. CoRR, abs/2305.14325.\\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt\\nGardner. 2022. Successive prompting for decompos-\\ning complex questions. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, EMNLP 2022, Abu Dhabi, United\\nArab Emirates, December 7-11, 2022, pages 1251–\\n1265.\\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\\nham Neubig. 2022. PAL: program-aided language\\nmodels. CoRR, abs/2211.10435.\\nGoogle. 2023. Palm 2 technical report. Google.\\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\\nYujiu Yang, Minlie Huang, Nan Duan, and Weizhu\\nChen. 2023.\\nTora: A tool-integrated reasoning\\nagent for mathematical problem solving.\\nCoRR,\\nabs/2309.17452.\\nGeyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin\\nZhao, and Ji-Rong Wen. 2023. Beyond imitation:\\nLeveraging fine-grained quality signals for alignment.\\nCoRR, abs/2311.04072.\\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\\n2021a. Aligning ai with shared human values. Pro-\\nceedings of the International Conference on Learning\\nRepresentations (ICLR).\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\\nhardt. 2021b. Measuring massive multitask language\\nunderstanding. Proceedings of the International Con-\\nference on Learning Representations (ICLR).\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt. 2021c. Measuring mathematical\\nproblem solving with the MATH dataset. In Pro-\\nceedings of the Neural Information Processing Sys-\\ntems Track on Datasets and Benchmarks 1, NeurIPS\\nDatasets and Benchmarks 2021, December 2021, vir-\\ntual.\\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\\nand Phil Blunsom. 2015. Teaching machines to read\\nand comprehend. In Advances in Neural Information\\nProcessing Systems 28: Annual Conference on Neu-\\nral Information Processing Systems 2015, December\\n7-12, 2015, Montreal, Quebec, Canada, pages 1693–\\n1701.\\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Struct-\\ngpt: A general framework for large language model\\nto reason over structured data.\\narXiv preprint\\narXiv:2305.09645.\\nTushar Khot, Peter Clark, Michal Guerquin, Peter\\nJansen, and Ashish Sabharwal. 2020.\\nQASC: A\\ndataset for question answering via sentence compo-\\nsition. In The Thirty-Fourth AAAI Conference on\\nArtificial Intelligence, AAAI 2020, The Thirty-Second\\nInnovative Applications of Artificial Intelligence Con-\\nference, IAAI 2020, The Tenth AAAI Symposium on\\nEducational Advances in Artificial Intelligence, EAAI\\n2020, New York, NY, USA, February 7-12, 2020,\\npages 8082–8090.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. In NeurIPS.\\nV. I. Levenshtein. 1965. Binary codes capable of cor-\\nrecting deletions, insertions and reversals. Soviet\\nphysics. Doklady, 10:707–710.\\nAitor Lewkowycz, Anders Andreassen, David Dohan,\\nEthan Dyer, Henryk Michalewski, Vinay V. Ra-\\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,\\nGuy Gur-Ari, and Vedant Misra. 2022. Solving quan-\\ntitative reasoning problems with language models. In\\nNeurIPS.\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Har-\\nrison Edwards, Bowen Baker, Teddy Lee, Jan\\nLeike, John Schulman, Ilya Sutskever, and Karl\\nCobbe. 2023.\\nLet’s verify step by step.\\nCoRR,\\nabs/2305.20050.\\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023a.\\nChain of hindsight aligns language models with feed-\\nback. CoRR, abs/2302.02676.\\nTianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman,\\nMohammad Saleh, Peter J. Liu, and Jialu Liu. 2023b.\\nStatistical rejection sampling improves preference\\noptimization. CoRR, abs/2309.06657.\\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le,\\nBarret Zoph, Jason Wei, and Adam Roberts. 2023.\\nThe flan collection: Designing data and methods for\\neffective instruction tuning. In International Confer-\\nence on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA, pages 22631–22648.\\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang,\\nLianhui Qin, Peter West, Prithviraj Ammanabrolu,\\nand Yejin Choi. 2022. QUARK: controllable text\\ngeneration with reinforced unlearning. In NeurIPS.\\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\\nardmath: Empowering mathematical reasoning for\\nlarge language models via reinforced evol-instruct.\\nCoRR, abs/2308.09583.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\\nSean Welleck,\\nBodhisattwa Prasad Majumder,\\nShashank Gupta, Amir Yazdanbakhsh, and Peter\\nClark. 2023. Self-refine: Iterative refinement with\\nself-feedback. CoRR, abs/2303.17651.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018a. Can a suit of armor conduct elec-\\ntricity? A new dataset for open book question an-\\nswering. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing,\\nBrussels, Belgium, October 31 - November 4, 2018,\\npages 2381–2391.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018b. Can a suit of armor conduct elec-\\ntricity? A new dataset for open book question an-\\nswering. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing,\\nBrussels, Belgium, October 31 - November 4, 2018,\\npages 2381–2391.\\nVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi\\nMirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,\\nDavid Silver, and Koray Kavukcuoglu. 2016. Asyn-\\nchronous methods for deep reinforcement learning.\\nIn Proceedings of the 33nd International Conference\\non Machine Learning, ICML 2016, New York City,\\nNY, USA, June 19-24, 2016, volume 48 of JMLR\\nWorkshop and Conference Proceedings, pages 1928–\\n1937.\\nOpenAI. 2023. Gpt-4 technical report. OpenAI.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\\n2022. Training language models to follow instruc-\\ntions with human feedback. In NeurIPS.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\\n2021. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, NAACL-HLT 2021, Online,\\nJune 6-11, 2021, pages 2080–2094.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Ste-\\nfano Ermon, Christopher D. Manning, and Chelsea\\nFinn. 2023. Direct preference optimization: Your\\nlanguage model is secretly a reward model. CoRR,\\nabs/2305.18290.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\nCoRR, abs/2302.04761.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\\nRadford, and Oleg Klimov. 2017. Proximal policy\\noptimization algorithms. CoRR, abs/1707.06347.\\nGokul Swamy, Christoph Dann, Rahul Kidambi, Zhi-\\nwei Steven Wu, and Alekh Agarwal. 2024. A mini-\\nmaximalist approach to reinforcement learning from\\nhuman feedback. CoRR, abs/2401.04056.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowl-\\nedge.\\nIn Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\\npages 4149–4158.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023. Llama 2: Open foundation and fine-\\ntuned chat models. CoRR, abs/2307.09288.\\nJonathan Uesato, Nate Kushman, Ramana Kumar,\\nH. Francis Song, Noah Y. Siegel, Lisa Wang, An-\\ntonia Creswell, Geoffrey Irving, and Irina Higgins.\\n2022. Solving math word problems with process- and\\noutcome-based feedback. CoRR, abs/2211.14275.\\nPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai\\nLin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. 2023a.\\nMaking large language models better reasoners with\\nalignment. CoRR, abs/2309.02144.\\nPeiyi Wang, Lei Li, Zhihong Shao, R.X. Xu, Damai\\nDai, Yifei Li, Deli Chen, Y.Wu, and Zhifang Sui.\\n2023b. Math-shepherd: Verify and reinforce llms\\nstep-by-step without human annotations.\\nCoRR,\\nabs/2312.08935.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\\nLe, Ed H. Chi, and Denny Zhou. 2022.\\nSelf-\\nconsistency improves chain of thought reasoning in\\nlanguage models. CoRR, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\\nand Denny Zhou. 2022. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nNeurIPS.\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\\nJiang. 2023.\\nWizardlm: Empowering large lan-\\nguage models to follow complex instructions. CoRR,\\nabs/2304.12244.\\nShentao Yang, Shujian Zhang, Congying Xia, Yihao\\nFeng, Caiming Xiong, and Mingyuan Zhou. 2023.\\nPreference-grounded token-level guidance for lan-\\nguage model fine-tuning. CoRR, abs/2306.00398.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023.\\nTree of thoughts: Deliberate\\nproblem solving with large language models. CoRR,\\nabs/2305.10601.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. CoRR, abs/2210.03629.\\nZhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng\\nGuo, Junqi Dai, Xuanjing Huang, and Xipeng Qiu.\\n2023. Exchange-of-thought: Enhancing large lan-\\nguage model capabilities through cross-model com-\\nmunication. In Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Process-\\ning, EMNLP 2023, Singapore, December 6-10, 2023,\\npages 15135–15153.\\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\\nZhengying Liu, Yu Zhang, James T. Kwok, Zhenguo\\nLi, Adrian Weller, and Weiyang Liu. 2023. Meta-\\nmath: Bootstrap your own mathematical questions\\nfor large language models. CoRR, abs/2309.12284.\\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting\\nDong, Chuanqi Tan, and Chang Zhou. 2023. Scaling\\nrelationship on learning mathematical reasoning with\\nlarge language models. CoRR, abs/2308.01825.\\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao\\nHuang, Huan Sun, Yu Su, and Wenhu Chen. 2023.\\nMammoth: Building math generalist models through\\nhybrid instruction tuning. CoRR, abs/2309.05653.\\nBeichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin\\nZhao, Jing Sha, Shijin Wang, and Ji-Rong Wen.\\n2023.\\nEvaluating and improving tool-augmented\\ncomputation-intensive math reasoning.\\narXiv\\npreprint arXiv:2306.02408.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\\n2023a. A survey of large language models. CoRR,\\nabs/2303.18223.\\nXin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong,\\nZhipeng Chen, Yuanhang Zhou, Ji-Rong Wen, Jing\\nSha, Shijin Wang, Cong Liu, et al. 2023b. Jiuzhang\\n2.0: A unified chinese pre-trained language model\\nfor multi-task mathematical problem solving. In Pro-\\nceedings of the 29th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, pages 5660–\\n5672.\\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,\\nMohammad Saleh, and Peter J. Liu. 2023c. Slic-hf:\\nSequence likelihood calibration with human feed-\\nback. CoRR, abs/2305.10425.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023a. Judg-\\ning llm-as-a-judge with mt-bench and chatbot arena.\\nCoRR, abs/2306.05685.\\nRui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei\\nShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,\\nYuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi,\\nNuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,\\nZhangyue Yin, Rongxiang Weng, Wensen Cheng,\\nHaoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,\\nQi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023b.\\nSecrets of RLHF in large language models part I:\\nPPO. CoRR, abs/2307.04964.\\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\\nBrown, Alec Radford, Dario Amodei, Paul F. Chris-\\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\\nguage models from human preferences.\\nCoRR,\\nabs/1909.08593.\\nFigure 5: The position of the first error in the generated\\nsolution. The X-axis denotes how many reasoning steps\\nbetween the first error and the final answer, and the Y-\\naxis is the ratio of the corresponding problems in these\\nproblems.\\nA\\nDetails for the Prompt\\nWe present the template of the prompt for teacher\\nmodel distillation, and generative reward model\\ntraining and inference in Table 7 and Table 8, re-\\nspectively. In practice, the information (i.e., ques-\\ntion 𝑞, Ground-Truth Solution 𝑠 and Generated\\nErroneous Solution ˆ𝑠) should be filled into the cor-\\nresponding curly brackets. For error locating task,\\nto better guide teacher model and generative reward\\nmodel to figure out the first undesired step, we uti-\\nlize the index to format the ground-truth solution.\\nThe formatted solution is as follows,\\n[0] The First Reasoning Step 𝑟0\\n[1] The Second Reasoning Step 𝑟1\\n· · ·\\n[𝑛] The Last Reasoning Step 𝑟𝑛\\nFor the generative reward model, the training\\ninstruction and inference prompt are similar. The\\ntarget output of the training procedure (i.e., the\\nbold sentence in the table) will be removed during\\ninference.\\nB\\nPerformance Analysis of RLMEC.\\nTo present the complete performance of RLMEC,\\nwe conduct experiments about the accuracy of the\\ntraining set during the training procedure and the\\nposition of the first error in the generated response\\nafter training. The results are shown in Figure 4 and\\nFigure 5, respectively. For the change of accuracy\\nduring training, we can observe that RLMEC can fit\\nthe training set better and faster than other methods\\n(e.g., RFT and DPO). That is because our methods\\nfocus on the mistakes in the generated response and\\nguide LLMs to correct these errors, which is more\\nefficient. Moreover, the experiment on the position\\nof the first error can also verify the effectiveness of\\nRLMEC. Compared with the backbone LLMs, the\\nfirst error appears later after RLMEC. For example,\\nafter RLMEC, the number of problems where the\\nfirst error occurs before the final answer 7 steps (i.e.,\\nthe third column on the right) has increased, while\\nthe number of problems where the first error occurs\\nmore than 7 steps has decreased. The reason is that\\nLLMs focus on the mistakes and learn to correct the\\nearly errors during RLMEC. In the ideal situation,\\nall of the mistakes will be corrected through further\\ntraining. In contrast, after training through other\\nmethods, the position of the first error is irregular,\\nwhich means that these methods do not consider the\\nmistakes in the generated response and guide LLMs\\nto learn to generate the correct solution without\\npurposiveness.\\nC\\nCase Study\\nTo further demonstrate the effectiveness of\\nRLMEC, we present the case study about the per-\\nformance of the LLMs trained by different methods\\nin Table 9 and Table 10. In both cases, our proposed\\nRLMEC can help the LLMs to focus on the previ-\\nous errors and correct the errors in the next time\\ngeneration. Concretely, in the question-answering\\ntasks, the keywords of the problem are “even if they\\nget it”. After being trained through RLMEC, the\\nLLMs can understand the meaning of the problem,\\nfigure the key point, and reach the correct answer.\\nHowever, through other methods, the LLM is still\\nunable to grasp the key works in the problem and\\ngenerate the answer about the emotion of losing the\\njob. Moreover, for mathematical problem, the LLM\\nhave made the mistake in calculating “12−15”. The\\nLLM trained by baseline methods still make simi-\\nlar mistakes. This case has shown that it is difficult\\nfor the previous methods to generate the supervised\\nsignals which can directly indicate the mistakes in\\nthe generated content and guide the LLMs to cor-\\nrect the errors. In contrast, RLMEC leverages the\\ngenerative reward model to provide the token-level\\nsupervised signals and guide the LLMs to focus\\non the mistakes. Therefore, through RLMEC, the\\nLLMs can correct the previous errors and obtain\\nthe correct answer.\\nError\\nLocating\\nGiven the problem, correct solution and the prediction from language models. The method in prediction\\nmight be different with correct solution, but it is also correct. You need to identify which step of the\\nprediction is the first wrong step, and write down the label of the first wrong step.\\nProblem: {Problem 𝑞}\\nCorrect solution: {Formatted Ground-Truth Solution 𝑠}\\nPrediction: {Generated Erroneous Solution ˆ𝑠}\\nWhich step of prediction is error? Only write down the label of the first wrong step. If the prediction is\\ncorrect, you need to write down correct. You should not write down any other words.\\nSolution\\nRewriting\\nGiven the problem and the correct solution, you need to correct the mistakes in prediction to get the\\ncorrect answer. You should make minimal modifications.\\nProblem: {Problem 𝑞}\\nCorrect solution: {Generated Erroneous Solution ˆ𝑠}\\nPrediction: {Generated Erroneous Solution ˆ𝑠}\\nCorrect prediction:\\nTable 7: The prompt for the teacher model distillation.\\nError\\nLocating\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nGiven the problem, correct solution and the prediction from language models. The method in prediction\\nmight be different with correct solution, but it is also correct. You need to identify which step of the\\nprediction is the first wrong step, and write down the label of the first wrong step.\\n### Input:\\nProblem: {Question 𝑞}\\nCorrect solution: {Formatted Ground-Truth Solution 𝑠}\\nPrediction: {Generated Erroneous Solution ˆ𝑠}\\n### Response:\\nThe first error step is [{First Undesired Reasoning Step 𝑟𝑡}]\\nSolution\\nRewriting\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nGiven the problem and the correct solution, you need to correct the mistakes in prediction to get the\\ncorrect answer. You should make minimal modifications.\\n### Input:\\nProblem: {Question 𝑞}\\nCorrect solution: {Ground-Truth Solution 𝑠}\\nPrediction: {Generated Erroneous Solution ˆ𝑠}\\n### Response:\\nCorrect prediction:{refined solution ˜𝑠}\\nTable 8: The instruction for the generative reward model training. The bold sentence will be utilized to optimize\\nthe generative reward model in cross entropy loss. The prompt for inference is the same as the training instruction\\nwithout the bold part.\\nProblem\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nApplying for a job can make someone feel what sort of emotion, even if they get it?\\nOptions:\\nworking hard\\nfrustration\\nrejection\\ndefeat\\nstress\\n### Response: Let’s think step by step.\\nGround-\\nTruth\\nSolution\\nStress is a feeling of tension about uncertainty.\\nApplying for a job has an uncertain outcome.\\nThe answer is stress\\nGenerated\\nSolution\\nBefore\\nTraining\\nApplying for a job is followed by attending an interview.\\nAttending an interview gives rejection or acceptance.\\nRejection is an emotion.\\nThe answer is rejection\\nGenerated\\nSolution Af-\\nter RLMEC\\nApplying for a job can make someone feel stress even if they get it.\\nStress is an emotion.\\nThe answer is stress\\nGenerated\\nSolution\\nAfter RFT\\nApplying for a job is followed by attending an interview.\\nAttending an interview gives rejection or acceptance.\\nRejection is an emotion.\\nThe answer is rejection\\nGenerated\\nSolution\\nAfter DPO\\nFrustration is the feeling of being upset or annoyed as a result of being unable to change or achieve\\nsomething.\\nApplying for a job can make someone feel frustration even if they get it.\\nThe answer is frustration\\nGenerated\\nSolution\\nAfter PPO\\nA2C\\nApplying for a job is followed by attending an interview.\\nAttending an interview gives an opportunity to sell one’s skills and abilities to the employer.\\nEmployer decides the suitability of the candidate based on his skills and abilities.\\nIf the employer rejects the candidate, he feels frustrated.\\nThe answer is frustration\\nTable 9: The case study for question-answering tasks.\\nProblem\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nA waiter had 12 customers. While 15 customers left he got 14 new customers. How many customers\\ndoes he still have?\\n### Response: Let’s think step by step.\\nGenerated\\nSolution\\nBefore\\nTraining\\nThe waiter has 12-15=3 customers left.\\nHe got 14+3=17 customers.\\nThe answer is 17\\nGenerated\\nSolution Af-\\nter RLMEC\\nThe waiter had 12 customers + 14 customers = 26 customers.\\nBut since 15 customers left, he now has 26 customers - 15 customers = 11 customers.\\nThe answer is 11\\nGenerated\\nSolution\\nAfter RFT\\nThe waiter has 12-15=3 customers left.\\nHe has 14+3=17 customers now.\\nThe answer is 17\\nGenerated\\nSolution\\nAfter PPO\\nA2C\\nThe waiter has 12-15=3 customers left.\\nHe got 14+3=17 customers.\\nThe answer is 17\\nTable 10: The case study for mathematical tasks.\\n'}, 'http://arxiv.org/abs/2401.06072v1': {'title': 'Chain of History: Learning and Forecasting with LLMs for Temporal\\n  Knowledge Graph Completion', 'published_date': datetime.datetime(2024, 1, 11, 17, 42, 47), 'pdf_link': 'http://arxiv.org/pdf/2401.06072v1', 'summary': \"Temporal Knowledge Graph Completion (TKGC) is a challenging task of\\npredicting missing event links at future timestamps by leveraging established\\ntemporal structural knowledge. Given the formidable generative capabilities\\ninherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize\\ntemporal link prediction as an event generation task within the context of a\\nhistorical event chain. We employ efficient fine-tuning methods to make LLMs\\nadapt to specific graph textual information and patterns discovered in temporal\\ntimelines. Furthermore, we introduce structure-based historical data\\naugmentation and the integration of reverse knowledge to emphasize LLMs'\\nawareness of structural information, thereby enhancing their reasoning\\ncapabilities. We conduct thorough experiments on multiple widely used datasets\\nand find that our fine-tuned model outperforms existing embedding-based models\\non multiple metrics, achieving SOTA results. We also carry out sufficient\\nablation experiments to explore the key influencing factors when LLMs perform\\nstructured temporal knowledge inference tasks.\", 'pdf_text': 'Chain of History: Learning and Forecasting with LLMs for Temporal\\nKnowledge Graph Completion\\nRuilin Luo1∗, Tianle Gu1∗, Haoling Li1∗, Junzhe Li2, Zicheng Lin1, Jiayi Li3, Yujiu Yang1†\\n1Tsinghua Shenzhen International Graduate School, Tsinghua University\\n2School of Computer Science, Peking University\\n3Baidu Inc.\\n{lrl23,gtl23,li-hl23}@mails.tsinghua.edu.cn\\nAbstract\\nTemporal\\nKnowledge\\nGraph\\nCompletion\\n(TKGC) is a challenging task of predict-\\ning missing event links at future timestamps\\nby leveraging established temporal structural\\nknowledge. Given the formidable generative\\ncapabilities inherent in LLMs (LLMs), this pa-\\nper proposes a novel approach to conceptual-\\nize temporal link prediction as an event gen-\\neration task within the context of a historical\\nevent chain. We employ efficient fine-tuning\\nmethods to make LLMs adapt to specific graph\\ntextual information and patterns discovered in\\ntemporal timelines.\\nFurthermore, we intro-\\nduce structure-based historical data augmen-\\ntation and the integration of reverse knowledge\\nto emphasize LLMs’ awareness of structural\\ninformation, thereby enhancing their reason-\\ning capabilities. We conduct thorough experi-\\nments on multiple widely used datasets and find\\nthat our fine-tuned model outperforms existing\\nembedding-based models on multiple metrics,\\nachieving SOTA results. We also carry out suf-\\nficient ablation experiments to explore the key\\ninfluencing factors when LLMs perform struc-\\ntured temporal knowledge inference tasks.\\n1\\nIntroduction\\nKnowledge Graphs (KGs), as meticulously struc-\\ntured repositories of deterministic knowledge,\\nfind application across various domains, includ-\\ning recommender systems (Qin et al., 2024),\\nquestion-answering (Liu et al., 2023), and more\\nrecently, the eagerly anticipated field of Retrieval-\\naugmented Generation (RAG) (Sun et al., 2023;\\nFeng et al., 2023). In contrast, Temporal Knowl-\\nedge Graphs (TKGs), enriched with timestamped\\ninformation, offer a more precise archival of knowl-\\nedge (Cai et al., 2023). However, this precision\\nsimultaneously results in a more generalized and\\ncomplex landscape for reasoning. The recent surge\\n∗ Equal\\ncontribution.\\n†\\nCorresponding\\nauthor:\\nyang.yujiu@sz.tsinghua.edu.cn.\\nFinetune\\nInference\\n296: Japan, Make_a_visit, North_Korea\\n297: Japan, Make_a_visit, South_Korea\\n298: Japan, Make_a_visit, Philippines\\n303: Japan, Make_a_visit, North_Korea\\n305: Japan, Make_a_visit, ? \\nThe missing entity is North_Korea.\\nLLM\\n304: Japan, Make_a_visit, North_Korea\\nFigure 1: LLM undergoes fine-tuning on known data\\nand subsequently utilizes the chain of known factual\\ninformation to generate the next event.\\nin the application of powerful generative capabili-\\nties inherent in LLMs has demonstrated promising\\nperformance in tasks beyond natural language pro-\\ncessing (Tang et al., 2023). Nevertheless, unlike\\ntasks in static scenarios, temporal reasoning man-\\ndates the model to capture potential knowledge\\nevolution patterns from historical data. Thus, can\\nLLMs exhibit comparable proficiency in generat-\\ning events for the next time? We posit that delving\\ninto TKGs, structures inherently characterized by\\ntextual and temporal sequences, holds the potential\\nto furnish an initial response to this inquiry.\\nA Temporal Knowledge Graph (TKG) stores sev-\\neral facts in the form of quadruple (eh, r, et, tT ),\\nwhich represents that eh has a directional edge\\nr into et at timestamp tT .\\nGiven a series of\\nseen facts denoted as F = {(s, p, o, ts)|s, o ∈\\nS, p ∈ P, ts < T}, TKGC under extrapolative\\nsetting requires a forecast on invisible links in\\nthe form of (si, pj, ?, tk), in which tk ≥ T (Zhu\\net al., 2021). Furthermore, a reverse inference\\n(?, pj, ol, tk) should also be taken into consider-\\nation. In static knowledge graph completion tasks,\\ndue to the determinism of textual knowledge and\\nthe relatively simple semantic structure of a single\\ngraph, many methods resort to language models\\narXiv:2401.06072v1  [cs.AI]  11 Jan 2024\\nto improve performance and generalization (Wang\\net al., 2022a,b). However, the innate evolutionary\\ncharacteristics of TKGs make the absolute repre-\\nsentation of text embeddings appear limited. There-\\nfore, previous embedding-based methods more con-\\ncentrate on learning the evolvement of entities us-\\ning relational graph encoder and recurrent networks\\nsuch as RNN and GRU (Chung et al., 2014; Jin\\net al., 2020; Li et al., 2021a).\\nSince the LLMs have demonstrated their pow-\\nerful generative capabilities, we emphasize that\\nTKGC can be seen as a generative sequel to LLMs\\nwithin the chain of historical events. The swift evo-\\nlution of instruction-tuning techniques for LLMs\\nallows for the adaptation of LLMs to this sequence\\nprediction task at a minimal cost. We construct\\ninstructions on known temporal facts by selecting\\nqueries in the form of (si, pj, ?, tk) and the his-\\ntory associated with them. In the fine-tuning pro-\\ncess, we inject proprietary knowledge of specific\\nTKGs into the model, along with the exploration of\\nvarious contextualized reasoning rules within the\\nhistorical chain.\\nIn this study, we utilize LLMs to complete event\\ngeneration on the history chain to aid the TKGC\\ntask as shown in Fig. 1. During fine-tuning, we par-\\ntition the known data into an input section and a su-\\npervised labeling segment, guiding LLMs in adapt-\\ning the mapping relationship between the textual\\ninformation of the specific TKG and the intricate\\nlogic inherent in temporal events. Furthermore, we\\npropose to use structural information on TKGs for\\nhistorical data augmentation to explore the ability\\nof LLMs to perceive graph information. In addi-\\ntion, we explore different means of reverse data\\nincorporation to mitigate the curse of reversal (Lv\\net al., 2023) problem in structured knowledge rea-\\nsoning.\\nWe conduct comprehensive experiments on\\nwidely used TKGC datasets,\\nincluding the\\nICEWS (Li et al., 2021a) series from news and the\\ncommonsense dataset YAGO (Mahdisoltani et al.,\\n2015). Significantly, we report the Hits@n met-\\nric under the most stringent raw setting and the\\nmore popular time-aware filtered setting, achiev-\\ning highly competitive results. We also provide\\nthe 8-shot in-context learning (ICL) performance\\nof several open-source models as a comparative\\nreference. Furthermore, we conduct thorough ab-\\nlation experiments to validate the effectiveness of\\nstructure-based historical data augmentation meth-\\nods and the introduction of reverse logic. Addition-\\nally, we investigate the impact of historical chain\\nlength, model size, and the performance of LLMs\\nlike GPT-4, aiming to uncover key factors influ-\\nencing temporal structural information reasoning\\nusing LLMs.\\n2\\nRelated Work\\nTemporal Knowledge Graph Completion in-\\nvolves two essential reasoning settings: interpo-\\nlation and extrapolation. Interpolation-based TKG\\nreasoning addresses the challenge of filling in miss-\\ning links within observed timestamps. Many static\\nknowledge graph embedding models have incor-\\nporated timestamp information to tackle this is-\\nsue (García-Durán et al., 2018; Goel et al., 2020).\\nTTransE (Leblay and Chekol, 2018) introduces\\ntime-based encoding through translation operations.\\nAdditionally, TNTComplEx (Lacroix et al., 2020)\\nand TuckERTNT (Shao et al., 2022) propose com-\\nplex decomposition and TuckER decomposition of\\nfour-order tensors, respectively, to augment model\\nexpressiveness under temporal conditions. How-\\never, the interpolation setting has limitations, as it\\ncannot infer missing information in future times-\\ntamps, thereby restricting its applicability.\\nExtrapolative reasoning in TKGC, involving the\\nprediction of facts for future timestamps, represents\\na more challenging yet valuable task. Recent works\\nhave concentrated on leveraging multi-relational\\ngraph convolutional networks (Li et al., 2021a; Jin\\net al., 2020). GHT (Sun et al., 2022) introduces\\na transformer that incorporates graph Hawkes\\ntransformation, aiming to model the evolution of\\nentity representations over time.\\nxERTE (Han\\net al., 2021a) captures query-related subgraph in-\\nformation through dynamic pruning operations.\\nTANGO (Han et al., 2021b) adopts neural ordi-\\nnary differential equations to model the temporal\\nrepresentation of entities. TITer (Sun et al., 2021)\\nstands out as the first model to utilize temporal-\\npath-based reinforcement learning for TKG rea-\\nsoning. Furthermore, TLogic (Liu et al., 2022)\\nenhances interpretability by extracting temporal\\nlogic rules through random exploration of time.\\nLLMs-as-Predictors LLM has demonstrated\\npromising potential in the field of graph machine\\nlearning, manifesting itself in two primary forms:\\nLLM as an enhancer and LLM as a predictor. (Chen\\net al., 2023) The former harnesses LLMs to en-\\nrich nodes’ textual attributes by leveraging their\\nextensive knowledge base. Enriched information\\nis then utilized in conjunction with Graph Neural\\nNetworks (GNNs) to generate predictions. How-\\never, such an approach only utilizes the pre-trained\\nknowledge of LLMs and ignores the reasoning abil-\\nity of them.\\nHence, some studies transform graph struc-\\nture information into sequential representations\\nand\\nutilize\\nLLMs\\nas\\nstandalone\\npredictors.\\nGraph4GPT (Guo et al., 2023) uses InstructGPT-\\n3 (Ouyang et al., 2022) to conduct an empirical\\nstudy to assess LLMs’ capabilities in graph under-\\nstanding, and GraphLLM (Chai et al., 2023) uses\\nLLaMA2 for the graph reasoning task, but these\\nwork ignore LLM’s ability to TKGC. Most rele-\\nvant to our work, (Lee et al., 2023a) uses ICL with\\nLLMs for TKGC, which may not fully exploit the\\nextensive learning capabilities of LLMs.\\nParameter-Efficient Fine-tuning Transformer-\\nbased pre-trained models has emerged as the cor-\\nnerstone of NLP tasks, and fine-tune them with\\nfull parameter updates has become a prevalent\\napproach.\\nHowever, recent studies have intro-\\nduced several more efficient fine-tuning techniques,\\nwhich can be seen as Parameter-Efficient Fine-\\ntuning (PEFT) (He et al., 2022) methods. These\\nmethods include the addition of adapters (Re-\\nbuffi et al., 2017; Houlsby et al., 2019; Bapna\\net al., 2019), which entail the insertion of small\\ntrainable feed-forward networks between fixed\\npre-trained models.\\nAdditionally, low-rank up-\\ndates (Hu et al., 2021) have been proposed as an\\nalternative, wherein the fine-tuning process lever-\\nages low-dimensional representations. Moreover,\\nprompt tuning (Lester et al., 2021) and prefix tun-\\ning (Li and Liang, 2021) have been developed,\\nwhich involve augmenting the model’s input or\\nactivations with learnable parameters.\\n3\\nPreliminary\\nDefinition 3.1. (TKGC) A TKG is defined as a\\nsequence G = {G1, · · · , Gt, · · · , Gn} comprising\\nstatic KGs. Here, each static KG denoted as Gt\\ncontains factual triplets at timestamp t. A single\\nstatic KG is formulated as {E, R, T }, in which\\nE, R and T = {si, pj, ok} respectively represent\\nentities, relations and triplets within it. TKGC in-\\nvolves bidirectional prediction of query quadruples,\\nspecifically, (si, pj, ?, ts) and (ok, p−1\\nj , ?, ts).\\nDefinition 3.2. (Fine-tuning) Given a pre-\\ntrained LLM denoted as M with parame-\\nters θ, and a dataset comprising n instances\\n{Queryi, Responsei}, the fine-tune processing\\naims to minimize the following loss function:\\nθ⋆ = arg min\\nθ′\\nn−1\\nX\\ni=0\\nL\\nStrategy\\nPrompt\\nOrdinary\\n280: [Japan, Make_a_visit, China]\\n281: [Japan, Make_a_visit, Vietnam]\\n· · ·\\n304: [Japan, Make_a_visit, Kiichi_Miyazawa]\\nQuery: 305: [Japan, Make_a_visit, ]\\nText-aware\\n280: [Japan, reverse Make_a_visit, China]\\n281: [Japan, reverse Make_a_visit, Vietnam]\\n· · ·\\n304: [Japan, reverse Make_a_visit, Kiichi_Miyazawa]\\nQuery: 305: [Japan, reverse Make_a_visit, ]\\nPosition-aware\\n280: [China, Make_a_visit, Japan]\\n281: [Vietnam, Make_a_visit, Japan]\\n· · ·\\n304: [Kiichi_Miyazawa, Make_a_visit, Japan]\\nQuery: 305: [ , Make_a_visit, Japan]\\nTable 1: Prompts for query (Japan, Make_a_visit−1, ?,\\n305).\\nmeet the specified value, we then sequentially in-\\ncorporate facts from He and Hr. ii) Data close to\\nthe current timestamp is introduced with priority.\\nBy following these two criteria, we aim to select\\nthe most relevant knowledge to inspire forecasting\\ncapabilities in LLMs.\\n4.2\\nIntroduction of Reverse Logic\\nSimilar to reasoning on static KGs, we require the\\nmodel to also possess the capability of reverse in-\\nference on TKG (Li et al., 2021a). However, re-\\ncent research indicates that LLM’s reasoning has\\nencountered the issue of reversal curse (Qi et al.,\\n2023; Berglund et al., 2023; Lv et al., 2023). In this\\nproblem, models often succeed in correctly deduc-\\ning questions like ’Who is Tom Cruise’s mother?’\\nbut struggle to answer ’Who is the son of Mary Lee\\nPfeiffer?’. We believe that this phenomenon also\\nexists in structured knowledge reasoning. We pro-\\npose using three prompt strategies to incorporate\\nreverse quadruples during the fine-tuning phase to\\nalleviate this issue, and explore the performance\\npatterns in the context of structured knowledge rea-\\nsoning scenarios.\\nAs demonstrated in Tbl. 1, the most ordinary\\nconstruction is to treat the structure of backward\\ninferences as forward inferences. The text-aware\\nprompt leverages reverse to indicate reverse rea-\\nsoning, and the position-aware prompt follows the\\norder of backward inference, providing different\\nhead entities in the historical records.\\n4.3\\nInstruction-tuning in TKGC\\nInstruction Tuning (Wei et al., 2021) achieves re-\\nmarkable zero-shot generalization results by train-\\ning LLMs on different tasks with instructions.\\nWhile prior work has demonstrated the effective-\\nness of fine-tuning LLMs via full-parameter up-\\ndates, this approach presents considerable chal-\\nlenges at large scale. Hence, we apply the Low-\\nRank Adaptation (LoRA) (Hu et al., 2021) method\\ndue to its demonstrated effectiveness for LLaMA-\\nstyle models. This method, founded on the plugin\\nencapsulation strategy of PEFT, furnishes us with\\nlightweight task-specific plugins.\\nThe LLM M generates a sequence of tokens\\nˆR = { ˆr1, ˆr2, ... ˆrn}, where response R we need\\nmust be extracted and consists of a set of consec-\\nutive tokens. Similarly to most fine-tuning LLMs\\nprocess using LoRA, the parameter update for a\\npre-trained weight matrix W0 ∈ Rd×k is specified\\nby product of two low-rank matrices WA and WB:\\nδW = WAWB\\n(2)\\nwhere WA ∈ Rd×r and WB ∈ Rr×k are matrices\\nof trainable parameters and rank r ≪ min(d, k).\\nTherefore, the forward pass for h = W0x is altered\\nas :\\nh = W0x + δWx = W0x + WAWBx\\n(3)\\nWe employ cross-entropy loss which constrains\\nthe similarity between estimated and ground-truth\\ntokens, to fine-tune LLMs by LoRA, which can be\\npresented as\\nL = CE( ˆR, ˜R)\\n(4)\\nwhere ˆR is the temporal knowledge graph com-\\npletion predicted by LLM M and ˜R is the given\\nlabel.\\n4.4\\nPredict with LLMs\\nThe instructions constructed are fed into the trained\\nLLMs for prediction. The response is obtained by\\nbeam search, which is a decoding strategy that\\nmaintains k beams of possible generated responses\\nat each time step t. The generation of response is\\nupdated as follows: for each generated response,\\nthe k tokens with the highest probabilities are se-\\nlected based on Eq. 5. This results in k × k new\\nresponse candidates. The next k beams of response\\nare obtained by selecting the top k responses with\\nthe highest probabilities from the generated re-\\nsponse candidates. The highest probability is deter-\\nmined by the product of probabilities of | ˆR| tokens\\nthat constitute the response, where | ˆR| represents\\nthe length of the current response.\\nrt = argmaxrP(r|r1:t−1)\\n(5)\\nIn this context, the single step setting is em-\\nployed, wherein for each test query in the test\\ndataset, the model can access the ground truth from\\npast timestamps. Consequently, after the prediction\\nfor this step is completed, the ground truth from\\nthe current timestamp is added to the history of the\\nnext timestamp before its execution.\\n5\\nExperiments\\n5.1\\nDatasets\\nIn our experimental setup, we utilize the ICEWS14\\ndataset (García-Durán et al., 2018), ICEWS18\\ndataset (Li et al., 2021a), ICEWS05-15 dataset (Li\\net al., 2021b), and YAGO dataset (Mahdis-\\noltani et al., 2015) as benchmarks for evalua-\\ntion. The specific statistics are listed in Tbl. 2.\\nWe employ partition criteria widely accepted\\nin prior studies (Han et al., 2021a) and estab-\\nlish instruction-tuning data on the validation set.\\nSpecifically, for the ordered timestamp set T =\\n{t1\\ntrain, t2\\ntrain, · · · , tn\\ntrain, t1\\nval, · · · , tm\\nval}, compris-\\ning training and validation sets, when gathering\\nhistorical data for timestamp ti\\nval, we observe only\\nfacts within the range t < ti\\nval. In the context\\nof testing under a single-step setup (Trivedi et al.,\\n2017), for a query at timestamp tq, we construct a\\nground-truth chain of history based on facts preced-\\ning timestamp tq, serving as the input to the model.\\nDatasets\\nEntity\\nRelation\\nTrain\\nValid\\nTest\\nInterval\\nICEWS14\\n6869\\n230\\n74845\\n8514\\n7371\\n1 day\\nICEWS05-15\\n10094\\n251\\n368868\\n46302\\n46159\\n1 day\\nICEWS18\\n23033\\n256\\n373018\\n45995\\n49545\\n1 day\\nYAGO\\n10623\\n10\\n161540\\n19523\\n20026\\n1 year\\nTable 2: Statistics of leveraged datasets.\\n5.2\\nBaseline Models\\nThe models selected for comparative analysis pri-\\nmarily fall into two categories: embedding-based\\nmethods and LLM-based approaches. Within the\\nrealm of embedding-based methods, we present\\nthe performance evaluations of RE-NET (Jin et al.,\\n2020), RE-GCN (Li et al., 2021a), TiRGN (Li et al.,\\n2022), xERTE (Han et al., 2021a), TANGO (Han\\net al., 2021b), Timetraveler (Sun et al., 2021) and\\nTiRGN (Li et al., 2022). Regarding LLM-based\\napproaches, to align with our model settings and\\nconsider the choices made in prior studies (Lee\\net al., 2023b), we focus on the effects of 8-shot\\nin-context learning for Llama-2-7b (Touvron et al.,\\n2023), Vicuna-7b (Vicuna, 2023), and GPT-NeoX-\\n20B (Black et al., 2022). In addition to these, we\\nalso include the rule-based method TLogic (Liu\\net al., 2022) in our comparison.\\n5.3\\nEvaluation Protocol\\nWe acknowledge that, at the metric level, notable\\ndistinctions exist between LLM-based methods and\\nembedding-based approaches. The latter proves ad-\\nvantageous as it can furnish a precise ranking of\\nall entities in the graph for a query presented in the\\nform of (s, q, ?), facilitating the calculation of met-\\nrics like Mean Reciprocal Rank (Chao et al., 2021;\\nYu et al., 2022). However, for LLM-based methods,\\nwe can only furnish the ranking of a predetermined\\nnumber of candidates, relying on the probabilities\\nof output paths from the open-source model (Lee\\net al., 2023b). This is in contrast to obtaining the\\nranking of all entities in the graph. This constraint\\nstems from the inability to compel the model to\\nremember all entities directly, and it introduces im-\\npractical search costs. Consequently, we choose\\nto report relatively accurate Hits@1, Hits@3, and\\nHits@10 (Sun et al., 2019). Furthermore, we align\\nwith the perspective outlined in (Ding et al., 2021;\\nJain et al., 2020) that directly excluding all other\\nvalid candidates to a specific query in a filtering set-\\nting is not entirely reasonable. Additionally, given\\nthat the proprietary LLMs we employ for compar-\\nison lack the opportunities to output ranking lists,\\nwe report raw metrics without loss of generality.\\n5.4\\nMain Results\\nAs shown in Tbl. 3, Llama-2-7b-CoH achieves\\nstate-of-the-art performance in multiple metrics\\nunder raw setting. Significantly, on the ICEWS14\\nand ICEWS18 datasets, Llama-2-7b-CoH shows\\nan improvement of 5.0% and 6.3% in the Hits@1\\nmetric compared to the current best models. We\\nobserve that on the YAGO dataset, the 8-shot ICL\\nperformance of GPT-NeoX-20B, Llama-2-7b, and\\nvicuna-7b is not significantly worse than Llama-\\n2-7b-CoH. However, there is a noticeable gap on\\nthe ICEWS14 series datasets, even falling behind\\nembedding-based models. We also report the met-\\nrics under the time-aware filtered setting in Tbl. 4,\\nwhere Llama-2-7b-CoH outperforms the previous\\nbest-performing TiRGN model by 4.1 percentage\\npoints in the Hits@1 and also exhibits a substantial\\nadvantage on other datasets. The relative perfor-\\nmance of the model remains generally consistent\\nunder both settings.\\nDatasets\\nYAGO\\nICEWS14\\nICEWS05-15\\nICEWS18\\nModel\\nHits@1\\nHits@3\\nHits@10\\nHits@1\\nHits@3\\nHits@10\\nHits@1\\nHits@3\\nHits@10\\nHits@1\\nHits@3\\nHits@10\\nRE-NET (Jin et al., 2020)\\n0.404\\n0.530\\n0.629\\n0.293\\n0.431\\n0.575\\n0.334\\n0.478\\n0.611\\n0.192\\n0.323\\n0.483\\nRE-GCN (Li et al., 2021a)\\n0.499\\n0.663\\n0.779\\n0.297\\n0.441\\n0.586\\n0.336\\n0.487\\n0.658\\n0.193\\n0.331\\n0.494\\nxERTE (Han et al., 2021a)\\n0.506\\n0.719\\n0.828\\n0.312\\n0.453\\n0.570\\n0.347\\n0.497\\n0.633\\n0.206\\n0.330\\n0.458\\nTANGO† (Han et al., 2021b)\\n0.409\\n0.554\\n0.637\\n0.151\\n0.272\\n0.431\\n0.311\\n0.476\\n0.622\\n0.178\\n0.314\\n0.460\\nTimetraveler (Sun et al., 2021)\\n0.494\\n0.675\\n0.790\\n0.313\\n0.451\\n0.571\\n0.341\\n0.494\\n0.667\\n0.210\\n0.325\\n0.437\\nTiRGN (Li et al., 2022)\\n0.509\\n0.710\\n0.864\\n0.313\\n0.468\\n0.612\\n0.358\\n0.535\\n0.690\\n0.202\\n0.350\\n0.514\\nTLogic (Han et al., 2021b)\\n0.454\\n0.703\\n0.782\\n0.322\\n0.470\\n0.603\\n0.345\\n0.525\\n0.673\\n0.205\\n0.339\\n0.484\\nGPT-NeoX-20B-ICL (Black et al., 2022)\\n0.520\\n0.722\\n0.870\\n0.295\\n0.406\\n0.475\\n0.348\\n0.497\\n0.586\\n0.177\\n0.290\\n0.385\\nLlama-2-7b-ICL (Touvron et al., 2023)\\n0.517\\n0.725\\n0.868\\n0.275\\n0.391\\n0.453\\n0.353\\n0.490\\n0.563\\n0.177\\n0.295\\n0.364\\nVicuna-7b-ICL (Vicuna, 2023)\\n0.514\\n0.714\\n0.868\\n0.270\\n0.386\\n0.453\\n0.347\\n0.483\\n0.563\\n0.172\\n0.288\\n0.364\\nLlama-2-7b-CoH\\n0.527\\n0.747\\n0.874\\n0.338\\n0.462\\n0.587\\n0.370\\n0.531\\n0.699\\n0.219\\n0.361\\n0.520\\nTable 3: Temporal forecasting with raw metrics Hits@1, Hits@3 and Hits@10. The best results are highlighted\\nin bold and the second-rank results are underlined. The results of the model with † are derived from (Han et al.,\\n2021b), while other models have been reproduced by us.\\nDatasets\\nYAGO\\nICEWS14\\nICEWS05-15\\nICEWS18\\nModel\\nHits@1\\nHits@3\\nHits@10\\nHits@1\\nHits@3\\nHits@10\\nHits@1\\nHits@3\\nHits@10\\nHits@1\\nHits@3\\nHits@10\\nRE-NET† (Jin et al., 2020)\\n0.586\\n0.715\\n0.868\\n0.301\\n0.440\\n0.582\\n0.336\\n0.488\\n0.627\\n0.197\\n0.326\\n0.485\\nRE-GCN† (Li et al., 2021a)\\n0.788\\n0.843\\n0.886\\n0.313\\n0.470\\n0.613\\n0.366\\n0.527\\n0.671\\n0.215\\n0.354\\n0.515\\nxERTE† (Han et al., 2021a)\\n0.801\\n0.880\\n0.898\\n0.327\\n0.457\\n0.573\\n0.378\\n0.523\\n0.639\\n0.210\\n0.335\\n0.465\\nTANGO‡ (Han et al., 2021b)\\n0.590\\n0.646\\n0.677\\n0.272\\n0.408\\n0.550\\n0.344\\n0.499\\n0.640\\n0.191\\n0.318\\n0.462\\nTimetraveler† (Sun et al., 2021)\\n0.801\\n0.900\\n0.903\\n0.327\\n0.465\\n0.584\\n0.383\\n0.527\\n0.649\\n0.221\\n0.335\\n0.448\\nTiRGN (Li et al., 2022)\\n0.839\\n0.907\\n0.923\\n0.328\\n0.481\\n0.622\\n0.379\\n0.544\\n0.698\\n0.220\\n0.366\\n0.522\\nTLogic‡ (Han et al., 2021b)\\n0.740\\n0.789\\n0.791\\n0.336\\n0.483\\n0.612\\n0.362\\n0.531\\n0.674\\n0.205\\n0.340\\n0.485\\nGPT-NeoX-20B-ICL (Black et al., 2022)\\n0.792\\n0.890\\n0.909\\n0.295\\n0.406\\n0.475\\n0.367\\n0.503\\n0.587\\n0.192\\n0.300\\n0.389\\nLlama-2-7b-ICL (Touvron et al., 2023)\\n0.767\\n0.852\\n0.868\\n0.286\\n0.397\\n0.453\\n0.353\\n0.490\\n0.563\\n0.177\\n0.294\\n0.364\\nVicuna-7b-ICL (Vicuna, 2023)\\n0.747\\n0.840\\n0.868\\n0.281\\n0.391\\n0.453\\n0.347\\n0.483\\n0.563\\n0.172\\n0.288\\n0.364\\nLlama-2-7b-CoH\\n0.880\\n0.929\\n0.931\\n0.349\\n0.470\\n0.591\\n0.386\\n0.541\\n0.699\\n0.223\\n0.360\\n0.518\\nTable 4: Temporal forecasting with time-aware filtered metrics Hits@1, Hits@3 and Hits@10. The best results are\\nhighlighted in bold and the second-rank results are underlined. The results of the model with † are derived from (Li\\net al., 2022), and results with ‡ are taken from (Lee et al., 2023b).\\n6\\nAnalysis\\n6.1\\nEffective Stucture-based Augmentation\\nTo assess the efficacy of the structure-augmented\\nhistory modeling strategy, we conduct comprehen-\\nsive ablation experiments on all used datasets, em-\\nploying Hits@1 as the evaluation criterion. For\\ncomparison, we exclude entity-augmented and\\nrelation-augmented histories during both the fine-\\ntuning and inference phases, relying solely on\\nschema-matching history for predictive determina-\\ntion. The results of the ablation studies are depicted\\nin Tbl. 5, enabling a clear analysis that structure-\\naugmented history is beneficial for both forward\\nand backward inference.\\nIllustrating with a practical case, when reason-\\ning about the quadruple (Economist (United King-\\ndom), Criticize or denounce, ?, 6960), due to\\nschema-matching history capturing only a histor-\\nical fact (Economist (United Kingdom), Criticize\\nor denounce, Silvio Berlusconi, 120), this leads to\\nan incorrect inference of Afghanistan. However,\\nthe entity-augmented history contains multiple in-\\nstances of Economist (United Kingdom) linked\\nthrough the Make statement relation to United King-\\ndom. This similar behavior guides the model to\\noutput the correct answer United Kingdom. Thus,\\nsupplementation enhances to some extent the ex-\\npression of structured information related to the\\ncentral node, thereby aiding LLM in making more\\naccurate predictions beyond simply relying on the\\nground truth history.\\n6.2\\nEffect of Introducing Reverse Logic\\nWe conduct a comprehensive ablation experiment\\nfor the introduction of reverse quadruples in the\\nfine-tuning phase. Considering the difficulty of\\nICEWS18 dataset, we set the length of the history\\nchain to 30, and we set this value to 10 on the other\\ndatasets. We use the ordinary prompt as a compari-\\nson to verify the effect of the reverse data introduc-\\ntion. The results are demonstrated in Tbl. 6, where\\nLlama-2-7b-CoH (w/o rq) indicates that no reverse\\nquadruples are added during the fine-tuning phase.\\nWe can see that all the results show an upward trend\\nexcept for a slight dip in the forward inference on\\nthe ICEWS18 dataset. Therefore, we can argue\\nthat the inclusion of reverse logic in the fine-tuning\\nstage is not only beneficial to alleviate the curse\\nof reversal in structured knowledge reasoning, but\\nalso largely harmless to forward reasoning.\\nWe still give a comparison of three proposed\\nDatasets\\nICEWS14\\nICEWS05-15\\nICEWS18\\nYAGO\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nLlama-2-7b-CoH w/o aug\\n0.353\\n0.297\\n0.325\\n0.400\\n0.357\\n0.379\\n0.226\\n0.196\\n0.211\\n0.555\\n0.491\\n0.523\\nLlama-2-7b-CoH\\n0.370\\n0.308\\n0.339\\n0.408\\n0.359\\n0.383\\n0.236\\n0.204\\n0.220\\n0.560\\n0.491\\n0.526\\n∆\\n4.8%\\n3.7%\\n4.3%\\n2.0%\\n0.6%\\n1.1%\\n4.4%\\n4.1%\\n4.3%\\n0.9%\\n0.0%\\n0.6%\\nTable 5: Ablations on the structure-based history augmentation. We also report Hits@1 metric. The strategy has\\nachieved comprehensive improvement in bi-directional forecasting.\\nDatasets\\nICEWS14\\nICEWS05-15\\nICEWS18\\nYAGO\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nLlama-2-7b-CoH w/o rq\\n0.367\\n0.298\\n0.333\\n0.396\\n0.343\\n0.369\\n0.238\\n0.188\\n0.213\\n0.560\\n0.489\\n0.524\\nLlama-2-7b-CoH\\n0.370\\n0.308\\n0.339\\n0.408\\n0.359\\n0.383\\n0.236\\n0.204\\n0.220\\n0.560\\n0.491\\n0.526\\n∆\\n0.8%\\n3.4%\\n1.8%\\n3.0%\\n4.7%\\n3.8%\\n0.8%\\n8.5%\\n3.3%\\n0.0%\\n0.4%\\n0.4%\\nTable 6: Ablations on the incorporation of reciprocal quadruples when fine-tuning. We report Hits@1 on four\\ndatasets. Rising and falling trends are indicated by green and red respectively. In order to more clearly observe\\ndifferences, we use historical chain with a length of 30 on the ICEWS18 dataset, while for other datasets, this value\\nis set to 10.\\nStrategy\\nYAGO\\nICEWS14\\nICEWS05-15\\nICEWS18\\nOrdinary\\n0.526\\n0.339\\n0.383\\n0.209\\nText-aware\\n0.525\\n0.333\\n0.382\\n0.214\\nPosition-aware\\n0.525\\n0.330\\n0.381\\n0.213\\nTable 7: Overall Hits@1 metrics for three utilized\\nprompt strategies under the raw setting.\\nprompt styles in Tbl. 7. We observe that ordinary\\nand text-aware strategies always lead to better re-\\nsults, so we believe that consistency in preserv-\\ning the inflectional position of different structured\\nquadruples during fine-tuning is more critical.\\n6.3\\nExploration on History Length\\nThe length of the historical chain L significantly in-\\nfluences prediction outcomes, reflecting the amount\\nof information provided to the LLMs. We conduct\\nexperiments with varying history lengths (L =\\n10, 20, 30, 50), while maintaining other settings\\nconstant. We choose the ordinary prompt for in-\\ncorporating reverse quadruples and harness entity-\\naugmented and relation-augmented quadruples to\\nenrich historical facts.\\nAs illustrated in Fig. 2, except the ICEWS14\\ndataset, on other datasets, the Hits@1 metric ex-\\nhibits an upward trend followed by stabilization as\\nL increases. We calculate the average length of\\nschema-matching history for each query in the test\\nsets of four datasets. For the ICEWS14 dataset, this\\nvalue is 30.05, significantly lower than the other\\ndatasets. On the ICEWS05-15 dataset, this value is\\n56.95. Consequently, an excessively long required\\nhistory length may negatively impact the reason-\\ning of LLM due to interference from numerous\\nhistorical quadruples used for padding. However,\\neven with a smaller input cost (i.e., smaller L) on\\n10\\n20\\n30\\n40\\n50\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\n0.45\\n0.5\\n0.55\\nDataset\\nICEWS14\\nICEWS05-15\\nICEWS18\\nYAGO\\nHistory Length\\nHits@1\\n0.339\\n0.343\\n0.339\\n0.338\\n0.383\\n0.39\\n0.391\\n0.393\\n0.209\\n0.218\\n0.22\\n0.225\\n0.526\\n0.527\\n0.527\\n0.527\\nFigure 2: The evolution pattern of the Hits@1 met-\\nric across four utilized datasets concerning the history\\nlength L.\\nModel\\nYAGO\\nICEWS14\\nICEWS05-15\\nICEWS18\\nLlama-2-7b-CoH\\n0.527\\n0.343\\n0.390\\n0.218\\nLlama-2-13b-CoH\\n0.526\\n0.343\\n0.392\\n0.210\\nTable 8: Overall Hits@1 metrics on different model\\nsizes.\\nthe ICEWS14 dataset, significant effectiveness is\\nalready achievable.\\n6.4\\nHow Model Size Affects Results\\nIn this section, we explore how model size of LLMs\\naffects performance in TKGC. We choose Llama-\\n2-13b as a comparison and consider leveraging\\nhistory chain with L = 20, and both add inverse\\nquadruples and structure-based augmentation data\\nfor fine-tuning. The results, as shown in Tbl. 8, de-\\npict that Llama-2-13b-CoH does not achieve a very\\nsignificant improvement in effectiveness. Specif-\\nically, Hits@1 on ICEWS18 dataset decreases by\\n3.7% compared to Llama-2-7b-CoH. We speculate\\nthat in temporal reasoning tasks, the model size\\nof LLMs is not the dominant factor, but rather de-\\nDatasets\\nICEWS14\\nICEWS05-15\\nICEWS18\\nYAGO\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nForward\\nBackward\\nOverall\\nGPT-3.5-turbo\\n0.260\\n0.158\\n0.209\\n0.157\\n0.177\\n0.167\\n0.079\\n0.070\\n0.075\\n0.496\\n0.441\\n0.481\\nGPT-4 (OpenAI, 2023)\\n0.298\\n0.233\\n0.266\\n0.293\\n0.260\\n0.277\\n0.096\\n0.092\\n0.094\\n0.510\\n0.484\\n0.497\\nQwen-72B-Chat (Bai et al., 2023)\\n0.279\\n0.216\\n0.248\\n0.357\\n0.343\\n0.350\\n0.159\\n0.148\\n0.154\\n0.499\\n0.463\\n0.481\\nTable 9: The performance of some powerful commercial models on 1000 randomly selected test samples in each\\ndataset.\\npends more on the model input.\\n6.5\\nPerformance of Commercial LLMs\\nIn this section, we test the effectiveness of three\\npowerful commercial LLMs on the TKGC task,\\naiming to explore the performance differences after\\nmulti-task instruction fine-tuning and Reinforce-\\nment Learning from Human Feedback (RLHF). We\\nprovide the same 8-shot ICL prompt samples for\\neach of the three models on different datasets, as\\ndetailed in the appendix. For the test data, we ran-\\ndomly select 1000 queries for forward reasoning\\nand 1000 queries for backward reasoning on each\\ndataset. Since these models do not provide output\\nprobabilities, we only present the most accurate\\nexact match metric, equivalent to the Hits@1 met-\\nric under the raw setting. After confirming that\\nthere are no fine-tuning on TKGC task and related\\ndatasets in the available technical reports (OpenAI,\\n2023; Bai et al., 2023), we consider this compari-\\nson to be relatively fair.\\nThe evaluation results are shown in Tbl. 9.\\nFirstly, we can observe that Qwen-72B-Chat is able\\nto achieve performance comparable to or surpass\\nGPT-4. In contrast, the performance of GPT-3.5-\\nturbo is not satisfactory. We are currently observing\\nthat the few-shot capabilities of Qwen-72B-Chat on\\nthe MMLU evaluation set are approaching those of\\nGPT-4 and surpassing the performance of GPT-3.5-\\nturbo. This eliminates a significant bias in terms of\\nlanguage tendency. On the other hand, we demon-\\nstrate that chat models, carefully fine-tuned and\\napplying RLHF, exhibit superior performance in\\nTKGC tasks. However, when we compare the re-\\nsults of Tbl. 9 and Tbl. 3, we can observe that the\\n8-shot ICL capability of commercial LLMs is still\\nsignificantly lower on the ICEWS series dataset\\ncompared to the capabilities of Llama-2-7b-CoH,\\nwhile the difference is not substantial on the YAGO.\\nThis is because YAGO is a dataset biased towards\\ncommon knowledge, and therefore, commercial\\nLLMs may already be familiar with a considerable\\nnumber of rules. However, the reasoning in the\\nICEWS series news dataset emphasizes the inter-\\naction and evolutionary information of nodes in\\nthe graph rather than relying on textual features.\\nThis results in commercial LLMs underperforming\\nin ICL, as they struggle to effectively capture the\\nevolutionary patterns along historical chains and\\nutilize augmented structure-based knowledge.\\n7\\nConclusion\\nIn this article, we model the TKGC task as a pro-\\ncess of event generation on the chain of historical\\nfacts, leveraging fine-tuning of LLMs to enhance\\ntheir ability to generate temporal events. We ex-\\nplore the capability of LLMs to perceive graph\\nstructures through structure-based historical data\\naugmentation and introduce reverse quadruples\\ninto the fine-tuning process to alleviate the curse of\\nreversal problem. The majority of our results un-\\nder both raw and filtered settings surpass those of\\nthe current best models. Furthermore, we conduct\\nextensive ablation experiments to validate the effec-\\ntiveness of data augmentation strategies and the in-\\ntroduction of reverse logic. We also investigate the\\nimpact of factors such as historical data length and\\nmodel size on the TKGC task. We find that longer\\nhistorical awareness often assists LLMs in making\\naccurate inferences, but it gradually reaches satu-\\nration. However, larger models do not necessar-\\nily provide greater benefits for temporal structural\\nknowledge reasoning. We also test the 8-shot ICL\\nperformance of some powerful commercial mod-\\nels on this task, and the results indicate that they\\nstill fall short of the ideal performance. We believe\\nthat our analysis can offer more powerful tools and\\neffective insights for LLMs in performing tasks\\nrelated to temporal knowledge graphs.\\nReferences\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,\\nRunji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,\\nXuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang\\nWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian\\nYang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi\\nYuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,\\nYichang Zhang, Zhenru Zhang, Chang Zhou, Jin-\\ngren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.\\nQwen technical report. CoRR, abs/2309.16609.\\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat.\\n2019. Simple, scalable adaptation for neural machine\\ntranslation.\\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita\\nBalesni, Asa Cooper Stickland, Tomasz Korbak, and\\nOwain Evans. 2023.\\nThe reversal curse: Llms\\ntrained on \"a is b\" fail to learn \"b is a\".\\nCoRR,\\nabs/2309.12288.\\nSid Black, Stella Biderman, Eric Hallahan, Quentin\\nAnthony, Leo Gao, Laurence Golding, Horace\\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\\nMichael Pieler, USVSN Sai Prashanth, Shivanshu\\nPurohit, Laria Reynolds, Jonathan Tow, Ben Wang,\\nand Samuel Weinbach. 2022. Gpt-neox-20b: An\\nopen-source autoregressive language model. CoRR,\\nabs/2204.06745.\\nBorui Cai, Yong Xiang, Longxiang Gao, He Zhang,\\nYunfeng Li, and Jianxin Li. 2023. Temporal knowl-\\nedge graph completion: A survey. In Proceedings\\nof the Thirty-Second International Joint Conference\\non Artificial Intelligence, IJCAI 2023, 19th-25th Au-\\ngust 2023, Macao, SAR, China, pages 6545–6553.\\nijcai.org.\\nZiwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han,\\nXiaohai Hu, Xuanwen Huang, and Yang Yang. 2023.\\nGraphllm: Boosting graph reasoning ability of large\\nlanguage model.\\nLinlin Chao, Jianshan He, Taifeng Wang, and Wei Chu.\\n2021.\\nPairre: Knowledge graph embeddings via\\npaired relation vectors. In ACL/IJCNLP(1), pages\\n4360–4369.\\nZhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi\\nWen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin,\\nWenqi Fan, Hui Liu, and Jiliang Tang. 2023. Explor-\\ning the potential of large language models (llms) in\\nlearning on graphs.\\nJunyoung Chung, Çaglar Gülçehre, KyungHyun Cho,\\nand Yoshua Bengio. 2014. Empirical evaluation of\\ngated recurrent neural networks on sequence model-\\ning. CoRR, abs/1412.3555.\\nZifeng Ding, Zhen Han, Yunpu Ma, and Volker Tresp.\\n2021. Temporal knowledge graph forecasting with\\nneural ODE. CoRR, abs/2101.05151.\\nChao Feng, Xinyu Zhang, and Zichu Fei. 2023.\\nKnowledge solver: Teaching llms to search for do-\\nmain knowledge from knowledge graphs. CoRR,\\nabs/2309.03118.\\nAlberto García-Durán, Sebastijan Dumancic, and Math-\\nias Niepert. 2018. Learning sequence encoders for\\ntemporal knowledge graph completion. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing, Brussels, Belgium,\\nOctober 31 - November 4, 2018, pages 4816–4821.\\nAssociation for Computational Linguistics.\\nRishab Goel, Seyed Mehran Kazemi, Marcus A.\\nBrubaker, and Pascal Poupart. 2020. Diachronic em-\\nbedding for temporal knowledge graph completion.\\nIn The Thirty-Fourth AAAI Conference on Artificial\\nIntelligence, AAAI 2020, The Thirty-Second Innova-\\ntive Applications of Artificial Intelligence Conference,\\nIAAI 2020, The Tenth AAAI Symposium on Educa-\\ntional Advances in Artificial Intelligence, EAAI 2020,\\nNew York, NY, USA, February 7-12, 2020, pages\\n3988–3995. AAAI Press.\\nJiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi\\nHe, and Shi Han. 2023. Gpt4graph: Can large lan-\\nguage models understand graph structured data ? an\\nempirical evaluation and benchmarking.\\nZhen Han, Peng Chen, Yunpu Ma, and Volker Tresp.\\n2021a. Explainable subgraph reasoning for forecast-\\ning on temporal knowledge graphs.\\nIn 9th Inter-\\nnational Conference on Learning Representations,\\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\nOpenReview.net.\\nZhen Han, Zifeng Ding, Yunpu Ma, Yujia Gu, and\\nVolker Tresp. 2021b. Learning neural ordinary equa-\\ntions for forecasting future links on temporal knowl-\\nedge graphs. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, EMNLP 2021, Virtual Event / Punta Cana, Do-\\nminican Republic, 7-11 November, 2021, pages 8352–\\n8364. Association for Computational Linguistics.\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\\nKirkpatrick, and Graham Neubig. 2022. Towards a\\nunified view of parameter-efficient transfer learning.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-efficient transfer learning for nlp.\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nPrachi Jain, Sushant Rathi, Mausam, and Soumen\\nChakrabarti. 2020. Temporal knowledge base com-\\npletion: New algorithms and evaluation protocols. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2020, Online, November 16-20, 2020, pages 3733–\\n3747. Association for Computational Linguistics.\\nWoojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren.\\n2020. Recurrent event network: Autoregressive struc-\\nture inferenceover temporal knowledge graphs. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2020, Online, November 16-20, 2020, pages 6669–\\n6683. Association for Computational Linguistics.\\nTimothée Lacroix, Guillaume Obozinski, and Nicolas\\nUsunier. 2020. Tensor decompositions for temporal\\nknowledge base completion. In 8th International\\nConference on Learning Representations, ICLR 2020,\\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\nview.net.\\nJulien Leblay and Melisachew Wudage Chekol. 2018.\\nDeriving validity time in knowledge graph. In Com-\\npanion Proceedings of the The Web Conference 2018,\\npages 1771–1776.\\nDong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred\\nMorstatter, and Jay Pujara. 2023a. Temporal knowl-\\nedge graph forecasting without knowledge using in-\\ncontext learning. arXiv preprint arXiv:2305.10613.\\nDong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred\\nMorstatter, and Jay Pujara. 2023b. Temporal knowl-\\nedge graph forecasting without knowledge using in-\\ncontext learning. In Proceedings of the 2023 Confer-\\nence on Empirical Methods in Natural Language Pro-\\ncessing, EMNLP 2023, Singapore, December 6-10,\\n2023, pages 544–557. Association for Computational\\nLinguistics.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\\nThe power of scale for parameter-efficient prompt\\ntuning.\\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\\nOptimizing continuous prompts for generation. In\\nProceedings of the 59th Annual Meeting of the Asso-\\nciation for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language\\nProcessing (Volume 1: Long Papers), pages 4582–\\n4597, Online. Association for Computational Lin-\\nguistics.\\nYujia Li, Shiliang Sun, and Jing Zhao. 2022. Tirgn:\\nTime-guided recurrent graph network with local-\\nglobal historical patterns for temporal knowledge\\ngraph reasoning. In Proceedings of the Thirty-First\\nInternational Joint Conference on Artificial Intelli-\\ngence, IJCAI 2022, Vienna, Austria, 23-29 July 2022,\\npages 2152–2158. ijcai.org.\\nZixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng\\nGuo, Huawei Shen, Yuanzhuo Wang, and Xueqi\\nCheng. 2021a. Temporal knowledge graph reason-\\ning based on evolutional representation learning. In\\nSIGIR ’21: The 44th International ACM SIGIR Con-\\nference on Research and Development in Information\\nRetrieval, Virtual Event, Canada, July 11-15, 2021,\\npages 408–417. ACM.\\nZixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng\\nGuo, Huawei Shen, Yuanzhuo Wang, and Xueqi\\nCheng. 2021b. Temporal knowledge graph reason-\\ning based on evolutional representation learning. In\\nSIGIR ’21: The 44th International ACM SIGIR Con-\\nference on Research and Development in Information\\nRetrieval, Virtual Event, Canada, July 11-15, 2021,\\npages 408–417. ACM.\\nPei Liu, Bing Qian, Qi Sun, and Longgang Zhao. 2023.\\nPrompt-wnqa: A prompt-based complex question an-\\nswering for wireless network over knowledge graph.\\nComput. Networks, 236:110014.\\nYushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell\\nJoblin, and Volker Tresp. 2022.\\nTlogic: Tempo-\\nral logical rules for explainable link forecasting on\\ntemporal knowledge graphs. In Thirty-Sixth AAAI\\nConference on Artificial Intelligence, AAAI 2022,\\nThirty-Fourth Conference on Innovative Applications\\nof Artificial Intelligence, IAAI 2022, The Twelveth\\nSymposium on Educational Advances in Artificial In-\\ntelligence, EAAI 2022 Virtual Event, February 22 -\\nMarch 1, 2022, pages 4120–4127. AAAI Press.\\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\\nShirui Pan. 2023. Reasoning on graphs: Faithful and\\ninterpretable large language model reasoning. CoRR,\\nabs/2310.01061.\\nAng Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan\\nChen, Ji-Rong Wen, and Rui Yan. 2023. Are we\\nfalling in a middle-intelligence trap?\\nan analy-\\nsis and mitigation of the reversal curse.\\nCoRR,\\nabs/2311.07468.\\nFarzaneh Mahdisoltani, Joanna Biega, and Fabian M.\\nSuchanek. 2015. YAGO3: A knowledge base from\\nmultilingual wikipedias. In Seventh Biennial Con-\\nference on Innovative Data Systems Research, CIDR\\n2015, Asilomar, CA, USA, January 4-7, 2015, Online\\nProceedings. www.cidrdb.org.\\nOpenAI. 2023.\\nGPT-4 technical report.\\nCoRR,\\nabs/2303.08774.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback.\\nChengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang,\\nJinyang Li, Jinwang Wu, and Yuanjun Laili. 2023.\\nAn investigation of llms’ inefficacy in understand-\\ning converse relations. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, EMNLP 2023, Singapore, Decem-\\nber 6-10, 2023, pages 6932–6953. Association for\\nComputational Linguistics.\\nYingrong Qin, Chen Gao, Shuangqing Wei, Yue Wang,\\nDepeng Jin, Jian Yuan, Lin Zhang, Dong Li, Jianye\\nHao, and Yong Li. 2024. Learning from hierarchical\\nstructure of knowledge graph for recommendation.\\nACM Trans. Inf. Syst., 42(1):18:1–18:24.\\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\\nVedaldi. 2017. Learning multiple visual domains\\nwith residual adapters.\\nPengpeng Shao, Dawei Zhang, Guohua Yang, Jian-\\nhua Tao, Feihu Che, and Tong Liu. 2022. Tucker\\ndecomposition-based temporal knowledge graph\\ncompletion. Knowl. Based Syst., 238:107841.\\nHaohai Sun, Shangyi Geng, Jialun Zhong, Han Hu, and\\nKun He. 2022. Graph hawkes transformer for extrap-\\nolated reasoning on temporal knowledge graphs. In\\nProceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP\\n2022, Abu Dhabi, United Arab Emirates, December\\n7-11, 2022, pages 7481–7493. Association for Com-\\nputational Linguistics.\\nHaohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, and\\nKun He. 2021. Timetraveler: Reinforcement learning\\nfor temporal knowledge graph forecasting. In Pro-\\nceedings of the 2021 Conference on Empirical Meth-\\nods in Natural Language Processing, EMNLP 2021,\\nVirtual Event / Punta Cana, Dominican Republic, 7-\\n11 November, 2021, pages 8306–8319. Association\\nfor Computational Linguistics.\\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\\nWang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,\\nand Jian Guo. 2023.\\nThink-on-graph: Deep and\\nresponsible reasoning of large language model with\\nknowledge graph. CoRR, abs/2307.07697.\\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\\nTang. 2019. Rotate: Knowledge graph embedding by\\nrelational rotation in complex space. In ICLR.\\nJiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su,\\nSuqi Cheng, Dawei Yin, and Chao Huang. 2023.\\nGraphgpt: Graph instruction tuning for large lan-\\nguage models. CoRR, abs/2310.13023.\\nYijun Tian, Huan Song, Zichen Wang, Haozhu Wang,\\nZiqing Hu, Fang Wang, Nitesh V. Chawla, and Pan-\\npan Xu. 2023. Graph neural prompting with large\\nlanguage models. CoRR, abs/2309.15427.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. CoRR,\\nabs/2302.13971.\\nRakshit Trivedi, Hanjun Dai, Yichen Wang, and\\nLe Song. 2017. Know-evolve: Deep temporal reason-\\ning for dynamic knowledge graphs. In Proceedings\\nof the 34th International Conference on Machine\\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11\\nAugust 2017, volume 70 of Proceedings of Machine\\nLearning Research, pages 3462–3471. PMLR.\\nVicuna. 2023. Vicuna: An open-source chatbot im-\\npressing gpt-4 with 90%* chatgpt quality. https:\\n//vicuna.lmsys.org/.\\nLiang Wang, Wei Zhao, Zhuoyu Wei, and Jingming\\nLiu. 2022a. Simkgc: Simple contrastive knowledge\\ngraph completion with pre-trained language models.\\nIn Proceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), ACL 2022, Dublin, Ireland, May\\n22-27, 2022, pages 4281–4294. Association for Com-\\nputational Linguistics.\\nXintao Wang, Qianyu He, Jiaqing Liang, and Yanghua\\nXiao. 2022b. Language models as knowledge em-\\nbeddings. CoRR, abs/2206.12617.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\\nguage models are zero-shot learners. arXiv preprint\\narXiv:2109.01652.\\nWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,\\nand William Yang Wang. 2018. One-shot relational\\nlearning for knowledge graphs. In Proceedings of the\\n2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium, October 31\\n- November 4, 2018, pages 1980–1990. Association\\nfor Computational Linguistics.\\nLong Yu, Zhicong Luo, Huanyong Liu, Deng Lin,\\nHongzhu Li, and Yafeng Deng. 2022.\\nTriplere:\\nKnowledge graph embeddings via tripled relation\\nvectors. CoRR, abs/2209.08271.\\nCunchao Zhu, Muhao Chen, Changjun Fan, Guangquan\\nCheng, and Yan Zhang. 2021. Learning from history:\\nModeling temporal knowledge graphs with sequen-\\ntial copy-generation networks. In Thirty-Fifth AAAI\\nConference on Artificial Intelligence, AAAI 2021,\\nThirty-Third Conference on Innovative Applications\\nof Artificial Intelligence, IAAI 2021, The Eleventh\\nSymposium on Educational Advances in Artificial In-\\ntelligence, EAAI 2021, Virtual Event, February 2-9,\\n2021, pages 4732–4740. AAAI Press.\\nA\\nPrompt for TKGC\\nIn this section, we provide a comprehensive de-\\nsign for the prompt, including versions that utilize\\nonly entity text (Tbl. 10) and versions identified by\\nnumber id (Tbl. 11).\\nSection\\nPrompt\\nInstruction\\nGiven contexts consisting of multiple quadruplets in the form of {time}: [{subject},\\n{relation}, {object}], please predict the missing entity in the query quadruplet {time}:\\n[{subject}, {relation}, ] in the end.\\nInput\\n295: [[Victor_Ponta, Make_statement, Romania]\\n296: [Victor_Ponta, Make_statement, North_Atlantic_Treaty_Organization]\\n296: [Victor_Ponta, Make_statement, Romania]\\n300: [Victor_Ponta, Make_statement, Viorel_Hrebenciuc]\\n301: [Victor_Ponta, Make_statement, Romania]\\n302: [Victor_Ponta, Make_statement, Romania]\\n303: [Victor_Ponta, Make_statement, National_Liberal_Party_(Romania)]\\n303: [Victor_Ponta, Make_statement, Romania]\\n304: [Victor_Ponta, Make_statement, Romania]\\n307: [Victor_Ponta, Make_statement, Representatives_(Romania)]\\nQuery:\\n308: [Victor_Ponta, Make_statement, ]\\nOutput\\nThe missing entity of query quadruplet is Romania.\\nTable 10: Prompt design using text only.\\nSection\\nPrompt\\nInstruction\\nGiven contexts consisting of multiple quadruplets in the form of {time}: [{subject},\\n{relation}, {object}], please predict the missing entity in the query quadruplet {time}:\\n[{subject}, {relation}, ] in the end.\\nInput\\n295: [[Victor_Ponta, Make_statement, 0.Romania]\\n296: [Victor_Ponta, Make_statement, 1.North_Atlantic_Treaty_Organization]\\n296: [Victor_Ponta, Make_statement, 0.Romania]\\n300: [Victor_Ponta, Make_statement, 2.Viorel_Hrebenciuc]\\n301: [Victor_Ponta, Make_statement, 0.Romania]\\n302: [Victor_Ponta, Make_statement, 0.Romania]\\n303: [Victor_Ponta, Make_statement, 3.National_Liberal_Party_(Romania)]\\n303: [Victor_Ponta, Make_statement, 0.Romania]\\n304: [Victor_Ponta, Make_statement, 0.Romania]\\n307: [Victor_Ponta, Make_statement, 4.Representatives_(Romania)]\\nQuery:\\n308: [Victor_Ponta, Make_statement, ]\\nOutput\\nThe missing entity of query quadruplet is 0.Romania.\\nTable 11: Prompt design using text and id.\\n'}, 'http://arxiv.org/abs/2401.06059v1': {'title': 'Investigating Data Contamination for Pre-training Language Models', 'published_date': datetime.datetime(2024, 1, 11, 17, 24, 49), 'pdf_link': 'http://arxiv.org/pdf/2401.06059v1', 'summary': \"Language models pre-trained on web-scale corpora demonstrate impressive\\ncapabilities on diverse downstream tasks. However, there is increasing concern\\nwhether such capabilities might arise from evaluation datasets being included\\nin the pre-training corpus -- a phenomenon known as \\\\textit{data contamination}\\n-- in a manner that artificially increases performance. There has been little\\nunderstanding of how this potential contamination might influence LMs'\\nperformance on downstream tasks. In this paper, we explore the impact of data\\ncontamination at the pre-training stage by pre-training a series of GPT-2\\nmodels \\\\textit{from scratch}. We highlight the effect of both text\\ncontamination (\\\\textit{i.e.}\\\\ input text of the evaluation samples) and\\nground-truth contamination (\\\\textit{i.e.}\\\\ the prompts asked on the input and\\nthe desired outputs) from evaluation data. We also investigate the effects of\\nrepeating contamination for various downstream tasks. Additionally, we examine\\nthe prevailing n-gram-based definitions of contamination within current LLM\\nreports, pinpointing their limitations and inadequacy. Our findings offer new\\ninsights into data contamination's effects on language model capabilities and\\nunderscore the need for independent, comprehensive contamination assessments in\\nLLM studies.\", 'pdf_text': 'Investigating Data Contamination for Pre-training\\nLanguage Models\\nMinhao Jiang1, Ken Ziyu Liu2, Ming Zhong1, Rylan Schaeffer2,\\nSiru Ouyang1, Jiawei Han1, Sanmi Koyejo2\\n1University of Illinois Urbana-Champaign 2Stanford University\\nminhaoj2@illinois.edu\\nAbstract\\nLanguage models pre-trained on web-scale corpora demonstrate impressive capa-\\nbilities on diverse downstream tasks. However, there is increasing concern whether\\nsuch capabilities might arise from evaluation datasets being included in the pre-\\ntraining corpus — a phenomenon known as data contamination — in a manner that\\nartificially increases performance. There has been little understanding of how this\\npotential contamination might influence LMs’ performance on downstream tasks.\\nIn this paper, we explore the impact of data contamination at the pre-training stage\\nby pre-training a series of GPT-2 models from scratch. We highlight the effect of\\nboth text contamination (i.e. input text of the evaluation samples) and ground-truth\\ncontamination (i.e. the prompts asked on the input and the desired outputs) from\\nevaluation data. We also investigate the effects of repeating contamination for\\nvarious downstream tasks Additionally, we examine the prevailing n-gram-based\\ndefinitions of contamination within current LLM reports, pinpointing their limita-\\ntions and inadequacy. Our findings offer new insights into data contamination’s\\neffects on language model capabilities and underscore the need for independent,\\ncomprehensive contamination assessments in LLM studies.\\n1\\nIntroduction\\nThe performance of large language models (LLMs) has been attributed primarily to their immense\\nsize and the increasing scale of pre-training data from large text corpora [29, 2, 27, 6, 1, 33, 34].\\nNevertheless, a critical aspect that remains under-explored is the potential contamination of the\\npre-training corpus with evaluation data. This oversight presents challenges in accurately assessing\\nthe LLMs’ capabilities among other scientific analyses of their behaviors. The importance of\\ncontamination analysis in the pre-training corpus has been recognized since pre-trained language\\nmodels were first introduced [7, 29, 6]; however, the lack of public access to most pre-training corpora\\ntoday complicates efforts to comprehensively understand and identify the impact of contamination on\\na model’s performance and behaviors.\\nRecent LLM reports [29, 2, 6, 27, 34, 11] have investigated the contamination of evaluation data in\\nthe pre-training corpora from various perspectives. Some of these studies offer limited details on their\\ncontamination investigations, especially for closed-source models [29, 27]. Others include attempts\\n[29, 27, 2, 6, 34] to investigate the data contamination on the evaluation level, where an evaluation\\ndataset is post-hoc categorized into contaminated and non-contaminated chunks based on a proposed\\ncontamination definition and the model is evaluated on them separately, to demonstrate that the model\\nis insusceptible to data contamination if the model performs similarly on these chunks. However,\\nthis line of work has not adequately analyzed contamination on the pre-training level, where the\\npre-training corpus is deliberately altered to study the effects of contamination on evaluation.\\nEvaluation data can be leaked into the pre-training corpus in various formats. Predominantly, it is\\nthe textual component of the evaluation dataset (i.e. input text). This aspect has been the primary\\narXiv:2401.06059v1  [cs.CL]  11 Jan 2024\\nfocus of many existing studies (e.g. [34, 6]). There are also many cases where the pre-training corpus\\nmight contain ground truth information of the evaluation data. Here, we consider ground truth of\\nthe evaluation samples to be their raw texts plus the prompts on such texts and the corresponding\\nanswers. Intuitively, contamination involving the ground truth may have different impacts on the\\nmodels’ performance than simple text contamination, but its effects have been under-explored.\\nAnother recent line of work focuses on the detection of data contamination from the pre-training\\ncorpus through the lens of membership inference attacks [21, 22, 10, 28, 31], which involves\\ndetermining whether the given text is in the pre-training data of a black-box model. While relevant,\\nthe detection of contamination does not necessarily offer a direct understanding of their effects during\\nevaluation. The recent works [11, 17] represent a step forward as they implement various methods,\\nincluding embedding-based search and syntax-based similarity analysis, to both detect and filter\\ncontamination from pre-training corpora, although they primarily focus on code-based data.\\nThis paper investigates the effects of contamination of pre-training data for language models via\\nleakage of evaluation datasets. We pre-train from scratch a series of GPT-2 models [29] and consider\\nvarious mechanisms of contamination of evaluation data in the pre-training corpus. Specifically, we\\nask and answer three research questions:\\n1. RQ1: How are language models affected by the deliberate addition of various forms of con-\\ntamination on the pre-training corpus? To answer this, we introduce intentional contamination\\n(with and without the ground truth) into the pre-training corpus (§4.1). We then pre-train GPT-2-\\nsmall models from scratch on these variously contaminated corpora to evaluate and compare their\\nperformance. We further extend the experiments with GPT-2-large models to evaluate the effects\\nof data contamination on larger models (§4.4).\\n2. RQ2: How do the number of repetitions of evaluation data in the pre-training corpus affect\\nperformance? In practice, how often a piece of evaluation data has appeared during pre-training\\nand its ramifications are also unclear. We investigate this by injecting the evaluation data into the\\npre-training corpus multiple times and provide detailed empirical analyses (§4.2).\\n3. RQ3: How effective are the n-gram-based contamination definitions used in recent LLM\\nreports? We systematically filter out different proportions of contaminated training documents,\\nas described by these definitions, and pre-train the same model on these cleansed corpora (§4.3).\\nAdditionally, we critically evaluate the methods used in current LLM reports for assessing\\ndata contamination at the evaluation level (§4.5). These reports often posit that the models\\nexhibit robustness against data contamination, and our discussion aims to elucidate the potential\\nshortcomings of such claims.\\nWe evaluate our experiments on several commonly used public datasets to observe the performance\\ndifferences quantitatively. Our analyses provide a new perspective on understanding data contamina-\\ntion in the pre-training of language models. The contributions are summarized as follows:\\n• We empirically investigate the effects of data contamination in the pre-training corpus due to\\nevaluation data leakage in language models by pre-training language models from scratch to\\nevaluate different mechanisms of data contamination.\\n• We identify the importance of considering the data contamination with ground truths from the\\nevaluation dataset. Surprisingly, we observed that the effects of increasing the number of repetitions\\nof contamination on the model performance can be U-shaped.\\n• We critically analyze the n-gram data contamination definitions from existing LLM reports and\\nfurther compare the empirical results by filtering the pre-training data with these definitions. Our\\nfindings suggest that they are insufficient and inadequate to identify contamination.\\n2\\nContamination Definitions\\nNumerous studies on large language models (LLMs) have explored and investigated the concept of\\ndata contamination and demonstrated the robustness of these models against potential contamination\\nin their evaluation datasets [29, 2, 6, 27, 33, 34, 11]. Most definitions proposed in the existing\\nstudies are based on n-gram duplication between pre-training data and evaluation data. For instance,\\nPaLM [6] divides the evaluation data into two categories—“clean” and “contaminated”—based on\\nwhether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the\\n2\\npre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered\\ncontaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample\\nand the training set, and the contamination percentage of an evaluation sample is defined to be the\\npercentage of tokens contaminated; the evaluation data are then divided into 4 buckets—“Clean”,\\n“Not Clean”, “Not Dirty”, and “Dirty”—based on the contamination percentage of each evaluation\\nsample. While intuitive, these contamination definitions primarily revolve around n-gram or token\\noverlaps, which only target direct duplications present in both training and evaluation datasets and\\nmight provide both high false positive rate (since many semantically different texts have overlaps)\\nand false negative rate (since simple paraphrasing can evade detection [36]). Moreover, investigations\\nrelying on these definitions have predominantly centered on evaluation level analysis; in our work,\\nwe focus on pre-training level analysis as described in §1.\\nIn our experiments, we follow PaLM [6] and Llama 2’s [34] definitions as well as a direct n-gram\\noverlap detection strategy to investigate how the “contamination” under these definitions are different\\nand how they affect model performance. As described in §1, contamination in the pre-training corpus\\ncan appear as either textual components from evaluation datasets or with ground truth information.\\nExisting definitions tend to overlook the latter. Therefore, we explore two types of contamination\\nwhen we introduce contamination to the pre-training corpus: (1) text contamination, where only\\nthe input texts of the evaluation samples are added to the pre-training corpus; and (2) ground-truth\\ncontamination, where the input texts, the prompts, and the labels/answers of the corresponding\\nevaluation samples are added.\\n3\\nExperimental Setup\\n3.1\\nModels, Data and Pre-Training\\nThe model architecture used in our main experiments is GPT-2-small [29] (124M parameters) with\\ndefault hyperparameters. We use a relatively small architecture because pre-training from scratch\\nis computationally expensive. Following [16], we construct a pre-training corpus by subsampling\\n1.95M documents from the Pile [9] for a total of 3.3B tokens, which is compute-optimal based on\\nChinchilla scaling laws [13]. We later extend our experiments to GPT-2-large (774M parameters)\\nand 19.8B tokens from pile-uncopyrighted corpus1 (Sec. 4.4), again following compute-optimal\\nscaling laws. The detailed hyperparameters for all experiments are listed in Appendix A.\\n3.2\\nEvaluation Datasets\\nWe focus our experiments on four natural language processing datasets to evaluate the performance\\nof our pre-trained models: SST-2 [32], a sentiment analysis dataset; MMLU [12], a multi-task\\nnatural language understanding dataset; CNN And Daily News [24], a text summarization dataset\\nthat was also evaluated in the GPT-2 report [29]; the Stanford Question Answering Dataset (SQuAD)\\ndataset [30], which helps evaluating the reading comprehension abilities of the model. The detailed\\nstatistics of these datasets are listed in Table 1. All datasets are accessed through HuggingFace2.\\nWe selected these easier and traditional benchmarks because our goal in the paper is to assess the\\ndifferential impact of data contamination on GPT-2 models’ performance, and the more difficult\\ndatasets are likely too challenging for GPT-2 series models.\\nTable 1: Evaluation Dataset Statistics. The last column (# of Samples) shows the number of\\nevaluation examples corresponding to each label.\\nDataset Name\\nSplit\\nLabel Space\\n# of Samples\\nSST-2\\ntrain\\npositive, negative\\n37,569 / 29,780\\nMMLU\\nall/test\\nA, B, C, D (57 Subjects)\\n3,222 / 3,462 / 3,582 / 3,776\\nCNN And Daily Mail\\n3.0.0/test\\n-\\n11,490\\nSQuAD V1\\nvalidation\\n-\\n10,600\\n1https://huggingface.co/datasets/monology/pile-uncopyrighted\\n2https://huggingface.co/datasets/\\n3\\nFor evaluation, we follow established processes. For the SST-2 dataset, due to the uncontrollability\\nand instability of the generated results from GPT-2 models, we utilize prompting and the possible\\nlabels as hypotheses and ask the model to score each hypothesis and use the highest one as the\\nprediction. To circumvent prompt sensitivity [18], we evaluate the accuracy scores based on 10\\ndifferent prompts for each model. The details of the prompts and the corresponding performance are\\nlisted in Appendix B. For MMLU, we utilize AllenAI’s official MMLU implementations3 [35] to\\ncompute the accuracy across 57 different subjects.\\nFor the text summarization task, we follow the original implementation reported in [29] for evaluation.\\nWe add the text TL; DR: \" after the article to induce the summarization generation. We then ask the\\nmodel to generate 150 tokens with top-k random sampling with k = 2 and use the first 3 sentences of\\nthe generated tokens as the summary. We evaluate the generated summaries on the commonly used\\nROUGE-1, 2, L scores [19] and UniEval [37] to provide a multi-dimensional evaluation. For the\\nquestion-answering evaluation on SQuAD, we employ the official implementation.4 In this setup, we\\nallow the model to generate up to 15 tokens, and the first sentence of the generated output is taken as\\nthe answer. We subsequently report F1 scores for the generated answers, determined by the overlap\\nof tokens between the model’s response and the ground truth. We selected SQuAD V1 to mitigate\\npotential biases introduced by the many no-answer questions in the V2 dataset.\\n4\\nExperiments & Analyses\\nIn this section, we present the experiment results to understand how data contamination affects the\\nmodels’ performance quantitatively. We conducted experiments with three variations of contami-\\nnation, described as follows. For the main experiments, we pre-train the GPT-2-small model from\\nscratch on the corpus to evaluate the performance:\\n• GPT-2-smalloriginal is the model pre-trained on the original corpus described in §3.1.\\n• GPT-2-smalltext is the text contamination version of the model. We only add the texts of the\\ncorresponding evaluation samples to the training data to ensure that all the texts in the evaluation\\ndataset were 100% contaminated in the pre-training corpus. For MMLU, we also include the texts\\nfrom the answer choices of each question.\\n• GPT-2-smallgt is the ground-truth contamination variation of the model. On top of the text\\ncontamination, we add the same prompt used for evaluation and the ground truth (e.g. labels)\\nfollowing the text for each dataset; that is, in the format as “text + prompt + ground truth”. For\\nSST-2, we randomly select one out of the 10 prompt templates for evaluation for each evaluation\\nsample and insert it in the corpus as contamination.\\nAs baselines, we further evaluate all datasets on the public checkpoints for GPT-2-small, medium,\\nand large variations to more directly compare the performance, where the pre-training data for the\\npublic checkpoints are unknown.\\n4.1\\nEffects of Contamination on Evaluation Performance\\nTo quantify the effects of data contamination and how the text and ground-truth contamination are\\ndifferent, we directly compare GPT-2original / text / gt on each dataset in Table 2 and 3.\\nThe experimental results from the two tables reveal the impact of data contamination on model\\nperformance across different datasets. The introduction of contamination, either in the text or ground\\ntruth, improves model performance compared to the original pre-trained GPT-2 model. Notably,\\nwhile text contamination does show some improvement in evaluation metrics, the extent of this\\nenhancement is relatively modest. This is particularly evident in the SQuAD and CNN datasets,\\nwhere the coherence and relevance scores under text contamination are sometimes lower than those\\nof the original model in the CNN dataset. Conversely, ground-truth contamination generally yields\\nsignificant performance improvements. However, in the SST-2 dataset, ground-truth contamination\\ndoes not outperform text contamination. We hypothesize that this is because text classification tasks\\npredominantly depend on the model’s comprehension of the input text, rendering evaluation prompts\\n3https://github.com/allenai/open-instruct\\n4https://rajpurkar.github.io/SQuAD-explorer\\n4\\nTable 2: Evaluation results on SST-2, MMLU, and SQuAD V1 datasets. For three variations of\\nmodels, the experiments are run 3 times, i.e., each pre-training was run under 3 different random seeds,\\nand shown as meanstd. Since only single checkpoints exist for the public baselines (GPT-2-small,\\nGPT-2-medium, GPT-2-large), we have no way of computing variance over multiple training runs.\\nModel\\nParameters\\nSST-2\\nMMLU\\nSQuAD V1\\nAccuracy\\nAccuracy\\nF1 Scores\\nGPT-2-smalloriginal\\n124M\\n48.342.32\\n22.870.09\\n9.070.19\\nGPT-2-smalltext\\n124M\\n54.890.80\\n23.030.05\\n9.780.12\\nGPT-2-smallgt\\n124M\\n51.020.35\\n23.130.09\\n11.450.58\\nGPT-2-small\\n124M\\n52.06\\n23.0\\n15.09\\nGPT-2-medium\\n354M\\n55.21\\n23.6\\n19.94\\nGPT-2-large\\n774M\\n54.01\\n23.0\\n17.87\\nTable 3: Evaluation results on CNN And Daily Mail dataset. Similarly, each experiment is run\\nthree times to report the mean/std, and only single checkpoints exist for public baselines.\\nModel\\nCNN And Daily Mail\\nROUGE-1\\nROUGE-2\\nROUGE-L\\nCoherence\\nConsistency\\nFluency\\nRelevance\\nOverall\\nGPT-2-smalloriginal\\n24.761.33\\n8.330.30\\n16.440.93\\n0.53820.045\\n0.60200.013\\n0.75130.035\\n0.49520.044\\n0.59680.010\\nGPT-2-smalltext\\n26.840.45\\n9.030.16\\n17.910.27\\n0.51370.016\\n0.66860.121\\n0.82250.009\\n0.46480.014\\n0.61740.008\\nGPT-2-smallgt\\n28.800.08\\n10.650.08\\n19.490.04\\n0.63900.032\\n0.74710.012\\n0.84800.001\\n0.56440.001\\n0.69960.015\\nGPT-2-small\\n27.97\\n9.43\\n18.34\\n0.5725\\n0.6954\\n0.8703\\n0.5525\\n0.6727\\nGPT-2-medium\\n29.71\\n10.52\\n19.49\\n0.6976\\n0.7998\\n0.8989\\n0.6793\\n0.7689\\nGPT-2-large\\n29.97\\n10.92\\n19.77\\n0.7259\\n0.8253\\n0.8997\\n0.6942\\n0.7863\\nand ground truths less impactful. In fact, they might introduce noise, particularly given that the input\\ntexts in the dataset are generally short and that the model is sensitive to prompt formatting. For the\\nMMLU dataset, it’s evident that this task presents a significant challenge for GPT-2-small models, as\\nindicated by the poor performance of both the public checkpoints and our pre-trained models. Despite\\nthis inherent difficulty, it is noteworthy that we can still observe the performance improvements with\\nthe introduction of both types of contamination. Overall, these findings suggest that while both types\\nof contamination can enhance the performance of language models, ground-truth contamination has\\na more pronounced positive effect on model performance than text contamination in general cases,\\nespecially for tasks that require an understanding of the instructions from evaluation prompts, such as\\nCNN and SQuAD datasets.\\nThe improvement of ground-truth contamination is more pronounced for the CNN dataset, where\\nground-truth contamination can even improve the model to surpass the performance of public\\ncheckpoints and achieve similar performance with the GPT-2-medium model. The experiment results\\nalso indicate that fluency, as measured by the UniEval metric, is still lower than the public model\\ncheckpoints. We suspect that this observation is due to the smaller scale of training data, where\\nfluency might be more closely related to the model’s overall language abilities. We can also observe\\nthat there is still an obvious gap between our pre-trained model and the public OpenAI’s checkpoints,\\nwhich shows the importance of the scale of training data.\\nViewed together, Tables 2 and 3 demonstrate the effects of data contamination on downstream\\nevaluation tasks and, in particular, the effects of ground-truth contamination. The results highlight\\nthe need for methods that can identify and differentiate ground-truth contamination in future studies.\\n4.2\\nEffects of Repeated Contamination Can Be U-Shaped\\nWe have already observed the effectiveness of data contamination in the previous section, where both\\nthe text and ground-truth contamination are only injected into the pre-training corpus once. However,\\nin practice, some fractions of the evaluation datasets may appear in the pre-training corpus more\\nthan once given its immense scale. Therefore, in this section, we investigate the effects of repeated\\ncontamination whereby the evaluation dataset is added to the pre-training corpus multiple times. We\\nuse the term contamination factor to denote the number of times the evaluation data appear in the\\npre-training corpus. This analysis is designed to help us understand better how the repetitions of\\n5\\nFigure 1: Evaluation results for different contamination factors from 0 to 20 on each dataset.\\nZero repetitions refer to models pre-trained on the original corpus. In the top three figures, the solid\\nlines and the dotted lines show the ground-truth and text contamination results respectively.\\nevaluation data for both text and ground-truth contamination, during pre-training might affect the\\nperformance. The results are shown in Figure 1.\\nFor SST-2, MMLU, and SQuAD datasets, we observed a distinct U-shaped performance trend in\\nresponse to increasing contamination factors. Specifically, as the contamination factor increased,\\nperformance initially improved but started to decline when the factor reached around 10 repetitions.\\nNotably, at 20 repetitions, performance in some instances dropped below the baseline level observed\\nwhen there was no contamination. The results for the CNN dataset exhibited varying trends based on\\nthe evaluation metrics used. While the ROUGE scores steadily increased with higher contamination\\nfactors, the UniEval scores displayed a U-shaped curve similar to the other datasets, which also\\nindicates a U-shaped general performance trend for the CNN dataset. Another observation to notice is\\nthat the fluency score also almost increases monotonically with the increase of contamination factor,\\nwhich further indicates that fluency is more associated with the size of training data. The divergence in\\nROUGE scores is primarily attributed to the metrics’ focus on the frequency of common subsequences\\nand tokens. These elements are more likely to be repeated with increased data repetition, particularly\\nin scenarios involving ground-truth contamination that repeats correct responses from the dataset.\\nThese findings suggest that while introducing contamination into a pre-training corpus can enhance\\nmodel performance to a certain degree, over-repetition may lead to a decline in effectiveness. We also\\nnote that this threshold for the number of repetitions can be related to the model size and corpus size,\\nwhich requires more investigation in future works. This is an interesting result since many existing\\nLLMs leveraged huge but unscrutinized pre-training corpora that it is unclear: 1) how many times the\\nevaluation data have appeared in the pre-training data, and 2) how the contamination has realistically\\naffected evaluation performance.\\nOn the other hand, we also observe that this U-shape curve for the contamination factor may not\\nuniversally hold for all datasets and corpora, which we discuss in more detail in Appendix C.\\n4.3\\nEffects of Removing Contamination from Pre-Training\\nIn this section, we conduct experiments to clean the pre-training corpus based on the outlined n-gram\\nand Llama 2 definitions. Specifically, the investigation aims to understand how the contaminated\\ndocuments under these definitions would affect the performance if we filter them out of the pre-\\ntraining corpus. As described in §2, we adopt different n-gram values n for the direct n-gram\\noverlap and Llama 2 contamination definitions, and we try various threshold λ for the contamination\\npercentage under Llama 2’s definition. These definitions are then used to filter “contaminated”\\n6\\nFigure 2: Evaluation results on removing contamination from the pre-training corpus. We\\ndeliberately select the parameters to achieve different ratios of removed tokens. The x-axis denotes\\nthe cleaning method (n-gram or Llama 2) followed by the percentage of tokens removed.\\ndocuments out of the pre-training corpus, where a document is considered contaminated if any\\nsentence in this document is considered contaminated. The detailed results are listed in Figure 2.\\nIn our experimental setup, we systematically filter out a range of approximately 3% to over 20% of\\ntokens labeled as “contaminated” from the pre-training corpus, aiming to analyze the effects of the\\npercentage of tokens removed on the model performance. The results, however, do not show a uniform\\npattern across different proportions of token removal. Interestingly, in certain instances where token\\nremoval exceeded 30%, the model’s performance remained comparable to that of the original model.\\nThis finding raises questions about the accuracy of n-gram-based definitions for pinpointing effective\\ncontamination. It appears that documents excluded based on n-gram and Llama 2’s definitions are not\\nalways genuinely contaminated, which reveals the insufficiency of such definitions for identifying\\neffective contamination in practice.\\nWe did not include PaLM’s definition in our experiments since we found this definition is so strict\\ncompared to the other two definitions that very few documents would be filtered out. More analyses\\nof the definitions are provided in Appendix D, where we also extensively analyze the effects of\\nvarying the parameters of these definitions.\\n4.4\\nScaling Up with a Larger Model\\nWe expand the experiment framework by incorporating GPT-2-large as the base model in our\\nexperiment. The primary objective is to assess if the effects of data contamination observed in\\nsmaller-scale models would persist in larger models. Due to computation constraints, we focus on the\\nexperiments on CNN and MMLU datasets for the ground-truth contamination with a contamination\\nfactor of 60, which is used to match the ratio of contamination with GPT-2-small experiments with a\\ncontamination factor of 10. A deviation in our setup compared to previous experiments is that we set\\na fixed number of training steps as opposed to a single epoch over the pre-training set; this is such\\nthat the training follows the compute-optimal scaling law for the available number of tokens.\\nDespite the larger scale of the pre-training corpus in GPT-2-large, the impact of ground-truth\\ncontamination is clear. This finding underscores the significant influence of data contamination,\\nwhich may remain concerning even in a large pre-training corpus.\\nTable 4: Evaluation results of GPT-2-large on CNN And Daily Mail and MMLU datasets.\\nModel\\nParameters\\nCNN And Daily Mail\\nMMLU\\nRouge-1\\nRouge-2\\nRouge-L\\nCoherence\\nConsistency\\nFluency\\nRelevance\\nOverall\\nAccuracy\\nGPT-2-largeoriginal\\n774M\\n27.47\\n9.67\\n17.74\\n0.6311\\n0.6910\\n0.8376\\n0.5942\\n0.6885\\n22.9\\nGPT-2-largegt\\n774M\\n28.43\\n10.85\\n18.74\\n0.6593\\n0.7335\\n0.8468\\n0.6082\\n0.7117\\n23.9\\nGPT-2-large\\n774M\\n29.97\\n10.92\\n19.77\\n0.7259\\n0.8253\\n0.8997\\n0.6942\\n0.7863\\n23.0\\n7\\n4.5\\nAssessing Evaluation-Level Contamination Analysis\\nIn this section, we follow recent LLM reports [6, 34] to divide evaluation data into different categories\\nto see what we can learn from contamination analysis on the evaluation level. Specifically, we follow\\nLlama 2’s definitions and methods [34] to divide the evaluation data into four categories (“Clean”,\\n“Not Clean”, “Not Dirty”, and “Dirty”) and evaluate the model on each category separately.\\nTable 5: The evaluation results on dividing the evaluation dataset into different categories. We\\nfollow Llama 2’s contamination definition and the associated parameters [34] to split the evaluation\\ndata. The parameters are shown as n and λ, where n is the n-gram value and λ is the dirty and clean\\nthreshold, respectively.\\nDatasets\\nModel\\nSubset Type\\nn\\nλ\\n# of Data\\nAvg. Contam. %\\nResults\\nOverall\\nCNN\\nGPT-2-smalloriginal\\nClean\\n15\\n0.85, 0.75\\n704\\n72.54\\n0.5743\\nNot Clean\\n10,786\\n82.11\\n0.5920\\nNot Dirty\\n9,203\\n80.22\\n0.5898\\nDirty\\n2,287\\n86.80\\n0.5955\\nGPT-2-smallgt\\nClean\\n15\\n0.85, 0.75\\n704\\n72.54\\n0.6495\\nNot Clean\\n10,786\\n82.11\\n0.6986\\nNot Dirty\\n9,203\\n80.22\\n0.6950\\nDirty\\n2,287\\n86.80\\n0.6978\\nF1 Score\\nSQuAD\\nGPT-2-smalloriginal\\nClean\\n9\\n0.9, 0.7\\n571\\n67.10\\n9.09\\nNot Clean\\n9,999\\n81.14\\n9.61\\nNot Dirty\\n9,741\\n78.91\\n9.59\\nDirty\\n856\\n97.03\\n9.24\\nGPT-2-smallgt\\nClean\\n9\\n0.9, 0.7\\n571\\n67.10\\n9.92\\nNot Clean\\n9,999\\n81.14\\n11.39\\nNot Dirty\\n9,741\\n78.91\\n11.37\\nDirty\\n856\\n97.03\\n10.21\\nWe adopt relatively high clean/dirty threshold values λ in order to arrive at similar portions of data\\nfor each category compared to Llama 2. We observed that the number of samples in each category is\\nvery sensitive to the selected λ values.\\nWe select CNN and SQuAD datasets and divide them into four categories based on the definitions\\nand parameters described in Table 5. We evaluate both the original model and the ground-truth\\ncontamination version of the model to see if the contamination will make a difference. Table 5 shows\\nthat the performance for the four categories is similar to each other. Even though the “clean” category\\nunder ground-truth contamination exhibited marginally lower results compared to the other categories,\\nthere was no clear indication that the “dirty” category outperformed the non-dirty categories. The\\nfact from the previous experiments that the performance of the evaluated models can be boosted by\\ncontamination shows that these models are not immune to contamination in the pre-training corpus.\\nThese results suggest that it may be insufficient to conclude that models are insusceptible to contam-\\nination based on such categorical evaluations. This draws attention to the need for more rigorous\\nmethodologies to assess the robustness of LLMs against data contamination accurately.\\n5\\nRelated Work\\nData Contamination Definition and Investigation. The exploration of data contamination has been a\\nconsistent element in LLM reports, dating back to the initial discussions of the memorization problem\\nin BERT [7]. Recent LLM reports [29, 2, 6, 27, 33, 34] have delved deeper into how evaluation data\\nmay be duplicated within pre-training corpora. These studies typically analyze the robustness of\\nmodels against data contamination through n-gram-based definitions; the analysis is also typically\\nfocused on the evaluation level as opposed to the pre-training level (recall §4.1). However, such\\ndefinitions may not accurately detect real contamination, casting doubt on the definitive conclusions\\ndrawn from these studies. Recent LLM studies also investigated the embedding-based contamination\\ndefinitions. The contamination analysis explored in phi-1/1.5 [11, 17] involves n-gram-based and\\nembedding and syntax-based definitions but only focuses on code data. These studies represent a\\n8\\npreliminary investigation in understanding the role of data contamination in the pre-training corpus.\\nAnother recent work [36] shows that the existing n-gram-based and embedding-based definitions\\ncan be easily evaded by applying simple paraphrasing of evaluation data, emphasizing the urgent\\nnecessity for proper definitions of contamination and reliable detection methods.\\nData Contamination and Memorization. Memorization in neural networks has been a well-explored\\ntopic in machine learning. Previous work has studied how memorization connects to and differs\\nfrom generalization [26, 20, 8], analyzed memorization in language models [4, 25], and studied how\\nmemorization connects to privacy [14] and data extraction attacks [5, 25]. Memorization is closely\\nlinked to data contamination as the model performance on evaluation data is no longer trustworthy if\\nthe evaluation data were memorized, regurgitated, and reasoned upon. Because of this connection, past\\nwork also explored membership inference attacks (MIA) for language models [21, 15, 23, 3, 22, 31].\\nHowever, these methods can sometimes be computationally intensive, and more generally, example-\\nbased matching can lead to false negatives in flagging contamination (e.g. detection can be evaded\\nthrough paraphrasing [36]). Other recent work has sought to identify pre-training data contamination\\nheuristically by examining the likelihoods of texts after changing their ordering [28] and of least\\nprobable tokens [31]. Nevertheless, these methods are similarly inadequate for detecting textual\\ntransformations (e.g. paraphrasing) and the heuristic nature of these methods may limit them from\\nproviding a clear understanding of how data contamination impacts the model performance on the\\npre-training level, highlighting a need for more comprehensive methods in this area of research.\\n6\\nConclusion\\nIn this work, we conduct a pre-training level analysis for the effects of data contamination on language\\nmodels. We pre-train a series of GPT-2 models from scratch to study the performance difference in\\ndifferent scenarios, underscoring the vital yet often overlooked role of ground truth in the context\\nof data contamination detection. This aspect is notably absent in existing studies. Our study also\\nsheds light on the effects of repeated contamination on the performance of language models in\\ndownstream applications. Moreover, we critically assess the current n-gram-based contamination\\ndefinitions as reported in recent LLM reports, revealing their inadequacy in accurately identifying\\ntrue contamination within pre-training corpora. Our replication of the existing robustness evaluations,\\nwhich focus on evaluation level analysis that divides downstream datasets into different categories,\\nsuggests that such assessments fall short of affirming models’ robustness to data contamination.\\nOur findings highlight the need for more precise and effective contamination definitions, and the\\nimplementation of more stringent methods to ascertain the robustness of LLMs to data contamination.\\n7\\nAcknowledgements\\nResearch was supported in part by US DARPA KAIROS Program No. FA8750-19-2-1004 and INCAS\\nProgram No. HR001121C0165, National Science Foundation IIS-19-56151, and the Molecule Maker\\nLab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and\\nthe Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE)\\nby NSF under Award No. 2118329. This work is also partially supported by NSF III 2046795, IIS\\n1909577, CCF 1934986, NIH 1R01MH116226-01A, NIFA award 2020-67021-32799, the Alfred P.\\nSloan Foundation, and Google Inc. Any opinions, findings, and conclusions or recommendations\\nexpressed herein are those of the authors and do not necessarily represent the views, either expressed\\nor implied, of DARPA or the U.S. Government.\\n9\\nReferences\\n[1] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira,\\nMark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing\\nZhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,\\nJames Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave,\\nMostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg,\\nFangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas\\nGonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,\\nJeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,\\nKathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin\\nLee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao\\nLiu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra,\\nMaysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,\\nMarie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan\\nRichter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee\\nShelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha\\nValter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,\\nJohn Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu,\\nQiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui\\nWu. Palm 2 technical report, 2023.\\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n[3] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.\\nMembership inference attacks from first principles, 2022.\\n[4] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and\\nChiyuan Zhang. Quantifying memorization across neural language models, 2023.\\n[5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather-\\nine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training\\ndata from large language models. In 30th USENIX Security Symposium (USENIX Security 21),\\npages 2633–2650, 2021.\\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling\\nlanguage modeling with pathways, 2022.\\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\\nThamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics.\\n[8] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In\\nProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages\\n954–959, 2020.\\n10\\n[9] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\\nAn 800gb dataset of diverse text for language modeling, 2020.\\n[10] Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large\\nlanguage models, 2023.\\n[11] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno,\\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil\\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tau-\\nman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023.\\n[12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding, 2021.\\n[13] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\\nHennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia\\nGuy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and\\nLaurent Sifre. An empirical analysis of compute-optimal large language model training. In\\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\\nNeural Information Processing Systems, 2022.\\n[14] Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine\\nLee, Christopher A Choquette-Choo, and Nicholas Carlini. Preventing generation of verba-\\ntim memorization in language models gives a false sense of privacy. In Proceedings of the\\n16th International Natural Language Generation Conference, pages 28–53. Association for\\nComputational Linguistics, 2023.\\n[15] Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, and Hong Yu. Membership inference attack\\nsusceptibility of clinical language models, 2021.\\n[16] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L. Buckley, Jason\\nPhang, Samuel R. Bowman, and Ethan Perez. Pretraining language models with human\\npreferences, 2023.\\n[17] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat\\nLee. Textbooks are all you need ii: phi-1.5 technical report, 2023.\\n[18] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of\\nlanguage models. arXiv preprint arXiv:2211.09110, 2022.\\n[19] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational\\nLinguistics.\\n[20] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation, 2022.\\n[21] Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh, and Marcello Hasegawa.\\nMembership inference on word embedding and beyond, 2021.\\n[22] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya\\nSachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models\\nvia neighbourhood comparison. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\\neditors, Findings of the Association for Computational Linguistics: ACL 2023, pages 11330–\\n11343, Toronto, Canada, July 2023. Association for Computational Linguistics.\\n[23] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza\\nShokri. Quantifying privacy risks of masked language models using membership inference\\nattacks, 2022.\\n[24] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Ça˘glar Gulçehre, and Bing Xiang. Ab-\\nstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of\\nthe 20th SIGNLL Conference on Computational Natural Language Learning, pages 280–290,\\nBerlin, Germany, August 2016. Association for Computational Linguistics.\\n[25] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ip-\\npolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee.\\n11\\nScalable extraction of training data from (production) language models.\\narXiv preprint\\narXiv:2311.17035, 2023.\\n[26] Matthew Olson, Abraham Wyner, and Richard Berk. Modern neural networks generalize on\\nsmall data sets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\\nAssociates, Inc., 2018.\\n[27] OpenAI. Gpt-4 technical report, 2023.\\n[28] Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto.\\nProving test set contamination in black box language models, 2023.\\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[30] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\\n[31] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi\\nChen, and Luke Zettlemoyer. Detecting pretraining data from large language models, 2023.\\n[32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\\nsentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association\\nfor Computational Linguistics.\\n[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\\nlanguage models, 2023.\\n[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\\nand fine-tuned chat models, 2023.\\n[35] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\\nChandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Ha-\\njishirzi. How far can camels go? exploring the state of instruction tuning on open resources,\\n2023.\\n[36] Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking\\nbenchmark and contamination for language models with rephrased samples, 2023.\\n[37] Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng\\nJi, and Jiawei Han. Towards a unified multi-dimensional evaluator for text generation, 2022.\\n12\\nA\\nTraining Hyperparameters\\nWe specify the hyperparameters we use for experiments for reproducibility and consistency of the\\nresults. In the GPT-2-small experiments, we set the batch_size=32, learning_rate=0.0005,\\nwarmup_ratio=0.01, weight_decay=0.1, and all other hyperparameters the same as the default\\nsettings of GPT-2-small. For the three runs of the main experiments, we adopt the seed numbers with\\n42, 1234, 2023 to ensure a fair comparison and consistency. For the GPT-2-large experiments,\\nwe set batch_size=128, learning_rate=0.0001 random_seed=42 instead to ensure training\\nstability and keep all other parameters the same.\\nB\\nEvaluation of Classification Tasks\\nIn this section, we describe the details of different prompts we utilized for the evaluation of SST-2\\ndatasets. We select the prompts with different meanings and lengths to ensure the diversity of prompt\\nformats, and the results for GPT-2original are shown in Table 6. We can observe from the table\\nthat GPT-2-small models are quite sensitive to how prompts are structured in downstream tasks.\\nThis suggests we need more research to better understand and evaluate small language models on\\nclassification tasks, especially when the answers of the models are not within the label space, which\\ncan be addressed in future studies.\\nTable 6: SST-2 Accuracy Scores for the 10 Different Prompts.\\nPrompts\\nGPT-2-\\nsmalloriginal\\nGPT-2-\\nsmalltext\\nGPT-2-\\nsmallgt\\nGPT-2-small\\nGPT-2-\\nmedium\\nGPT-2-large\\nDatasets\\nSST-2\\nSST-2\\nSST-2\\nSST-2\\nSST-2\\nSST-2\\n{text} It is {label}\\n43.87\\n49.97\\n55.44\\n56.09\\n61.77\\n51.94\\n{text} The text is {label}\\n42.98\\n49.81\\n55.60\\n54.36\\n58.47\\n53.92\\n{text} The sentiment for this text is {label}\\n44.20\\n48.27\\n51.73\\n51.55\\n54.38\\n45.83\\n{text} The preceding text is {label}\\n44.07\\n44.96\\n50.76\\n47.73\\n45.65\\n54.35\\n{text} If the preceding text could be\\ncategorized as positive or negative, it\\nwould be {label}\\n43.96\\n46.12\\n46.41\\n56.21\\n52.12\\n57.16\\n{text} The sentence is {label}\\n43.56\\n50.32\\n56.62\\n55.52\\n58.94\\n55.77\\n{text} This text is {label}\\n44.13\\n48.62\\n48.60\\n45.06\\n52.91\\n55.57\\n{text} Determine the sentiment of the\\npreceding text: positive or negative:\\n{label}\\n44.47\\n44.52\\n53.05\\n55.78\\n55.73\\n55.85\\n{text} The text belongs to {label}\\n43.56\\n57.52\\n47.88\\n50.46\\n55.91\\n56.30\\n{text} The sentiment for this sentence\\nshould be {label}\\n44.23\\n57.58\\n53.69\\n47.78\\n56.23\\n53.48\\nC\\nMore Discussions on Data Contamination\\nIn this section, we show the experiment results for the AG News dataset, where we observe that the\\ndata contamination does not match the observation we had in our main experiments.\\nWe can observe that even the performance of the model pre-trained on the subsampled corpus is\\nalready higher than the OpenAI’s public checkpoints. Interestingly, unlike previous experiments, we\\nfound that introducing text and ground-truth contamination does not significantly enhance perfor-\\nmance. As we increase the contamination factors, the performance generally begins to decline at\\nhigher levels of contamination, as the U-shape trend in the previous experiment suggested, but with\\nthe lowest performance occurring at a contamination factor of 3. On the other hand, no matter how we\\nincrease the contamination factors, the performance is still much higher than the public checkpoints.\\nOne plausible explanation for this phenomenon is that the models may be assimilating or memorizing\\ninformation from the AG News dataset present in the subsampled corpus. Consequently, the addition\\nof various types of contamination does not yield substantial performance improvements and results\\nin strange observations in this case.\\nThis result suggests that the effects of data contamination on language models still require more effort\\nto understand how knowledge of language models is constructed during pre-training.\\n13\\nFigure 3: The evaluation results for AG News dataset on both contamination factor and removing\\ncontaminated data experiments. The performances for public model checkpoints from OpenAI are\\ndisplayed as dotted lines in both figures.\\nD\\nQuantitative Analysis for Contamination Definitions\\nIn this section, we analyze the different sets of parameters for different contamination definitions\\nproposed in the previous studies to examine our evaluation dataset and pre-training corpus. We use\\nthe contamination ratio of the pre-training corpus for each evaluation dataset as a comparison to\\nassess how strict these definitions are and the appropriate contamination definitions.\\nD.1\\nN-Grams Direct Overlap\\nFigure 4: N-gram direct overlap contamination ratio w.r.t. different n-gram values for each dataset.\\nFirst, we examine the straightforward definition of contamination: the direct n-gram overlap within\\nsentences of a training document. A training document is considered contaminated if any n-gram\\nin the document appears in the evaluation dataset. While this approach offers a direct measure of\\ndataset duplication, its scope is limited. Solely relying on n-gram overlaps may overlook other forms\\nof contamination since sentences can be rephrased in various ways, conveying identical meanings\\nwithout any overlapping n-grams. Therefore, direct n-gram overlap is only to demonstrate how much\\nof the content in the evaluation dataset appears in the pre-training corpus. During our filtering, a\\nsentence is considered contaminated if any n-gram in the sentence appears in both pre-training data\\nand evaluation data, and a document is considered contaminated if any sentence in this document\\nis contaminated. We also report the total number of tokens in these documents that are considered\\ncontaminated. As shown in Figure 4, we calculate the contaminated ratio of documents and tokens in\\nthe pre-training data for different n’s for comparison. We can observe that the contamination ratio\\nvaries for each dataset and how to define a reasonable threshold n for the n-gram would be dependent\\non the text length of the evaluation dataset. For instance, in the SST-2 dataset, where many sentences\\n14\\ncomprise fewer than eight words, applying an 8-gram threshold would be impractical. Conversely, a\\nvery small n-gram value may fail to capture semantically meaningful content within sentences.\\nFigure 5: Contamination ratio for pre-training data based on Llama 2’s definitions. We adopt the\\nn-gram values that make the contamination ratio within a similar range and threshold from 60%−90%\\nfor comparison.\\nD.2\\nPaLM and Llama 2’s Definitions\\nWe conduct similar analyses for PaLM and Llama 2’s definitions by considering different n-gram\\nvalues n and contamination threshold λ. PaLM’s definition extends n-gram direct overlap to con-\\nsider the overlapping percentage of n-grams in one sentence: a training document is considered\\ncontaminated if more than λ percentage of n-grams in a sentence of the document appear in the\\nevaluation dataset. We observe that this definition is so strict that very few documents can satisfy\\nit even if we relax n and λ to very small values compared to the original definition. The results for\\nLlama 2’s definitions are shown in Figure 5. We report the percentage of contaminated documents\\nand the percentage of tokens respectively. We can observe that the Llama 2 definitions lead to\\nvaried levels of identified contaminated documents and tokens, depending on the chosen parameters.\\nThese definitions concentrate on token contamination through n-gram duplication, which can be\\nproblematic because tokens may have different meanings in different contexts. Relying only on token\\nduplication can misclassify sentences as contaminated. Additionally, similar to the straightforward\\nn-gram definitions, setting the correct n-gram values and thresholds for different datasets remains a\\nchallenge with this approach.\\nWe provide more detailed results for different parameters of these definitions, along with the PaLM’s\\nresults, in Table 7 to better observe the trends for each definition.\\n15\\nTable 7: More results for the ratio of contaminated documents for different datasets with different\\ndefinitions under different parameters.\\nDatasets\\nFiltering Method\\nN-gram Value\\nThreshold\\n% of Contaminated Documents\\nSST-2\\nPaLM\\n5\\n10%\\n1.22%\\nPaLM\\n5\\n50%\\n∼ 0%\\nPaLM\\n7\\n50%\\n∼ 0%\\nPaLM\\n7\\n70%\\n0.0003%\\nSQuAD\\nPaLM\\n5\\n50%\\n0.077745%\\nPaLM\\n5\\n70%\\n0.007744%\\nPaLM\\n4\\n50%\\n0.081334%\\nPaLM\\n4\\n70%\\n0.037795%\\nLlama 2\\n6\\n70%\\n76.38%\\nLlama 2\\n6\\n80%\\n43.08%\\nCNN\\nPaLM\\n7\\n70%\\n0.0896%\\nPaLM\\n8\\n70%\\n0.0302%\\nLlama 2\\n14\\n70%\\n14.71%\\nN-gram\\n9\\n-\\n3.48%\\nN-gram\\n10\\n-\\n1.32%\\nN-gram\\n11\\n-\\n0.54%\\nMMLU\\nLlama 2\\n12\\n80%\\n6.76%\\nLlama 2\\n15\\n95%\\n3.07%\\nLlama 2\\n18\\n90%\\n2.36%\\nLlama 2\\n20\\n90%\\n1.92%\\nLlama 2\\n24\\n90%\\n0.61%\\n16\\n'}, 'http://arxiv.org/abs/2401.05952v1': {'title': 'LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase', 'published_date': datetime.datetime(2024, 1, 11, 14, 44, 8), 'pdf_link': 'http://arxiv.org/pdf/2401.05952v1', 'summary': 'With the remarkable development and widespread applications of large language\\nmodels (LLMs), the use of machine-generated text (MGT) is becoming increasingly\\ncommon. This trend brings potential risks, particularly to the quality and\\ncompleteness of information in fields such as news and education. Current\\nresearch predominantly addresses the detection of pure MGT without adequately\\naddressing mixed scenarios including AI-revised Human-Written Text (HWT) or\\nhuman-revised MGT. To confront this challenge, we introduce mixcase, a novel\\nconcept representing a hybrid text form involving both machine-generated and\\nhuman-generated content. We collected mixcase instances generated from multiple\\ndaily text-editing scenarios and composed MixSet, the first dataset dedicated\\nto studying these mixed modification scenarios. We conduct experiments to\\nevaluate the efficacy of popular MGT detectors, assessing their effectiveness,\\nrobustness, and generalization performance. Our findings reveal that existing\\ndetectors struggle to identify mixcase as a separate class or MGT, particularly\\nin dealing with subtle modifications and style adaptability. This research\\nunderscores the urgent need for more fine-grain detectors tailored for mixcase,\\noffering valuable insights for future research. Code and Models are available\\nat https://github.com/Dongping-Chen/MixSet.', 'pdf_text': 'LLM-as-a-Coauthor: The Challenges of Detecting\\nLLM-Human Mixcase\\nChujie Gao1,†,‡, Dongping Chen1,2,†,‡, Qihui Zhang1,†,‡, Yue Huang1,‡,\\nYao Wan2, Lichao Sun1\\n1LAIR Lab, Lehigh University\\n2Huazhong University of Science and Technology\\n{gaochujie1107, dongpingchen0612, maskhui1003}@gmail.com\\nAbstract\\nWith the remarkable development and widespread applications of large language\\nmodels (LLMs), the use of machine-generated text (MGT) is becoming increas-\\ningly common. This trend brings potential risks, particularly to the quality and\\ncompleteness of information in fields such as news and education. Current re-\\nsearch predominantly addresses the detection of pure MGT without adequately\\naddressing mixed scenarios including AI-revised Human-Written Text (HWT)\\nor human-revised MGT. To confront this challenge, we introduce mixcase, a\\nnovel concept representing a hybrid text form involving both machine-generated\\nand human-generated content. We collected mixcase instances generated from\\nmultiple daily text-editing scenarios and composed MIXSET, the first dataset\\ndedicated to studying these mixed modification scenarios. We conduct experi-\\nments to evaluate the efficacy of popular MGT detectors, assessing their effec-\\ntiveness, robustness, and generalization performance. Our findings reveal that\\nexisting detectors struggle to identify mixcase as a separate class or MGT, partic-\\nularly in dealing with subtle modifications and style adaptability. This research\\nunderscores the urgent need for more fine-grain detectors tailored for mixcase,\\noffering valuable insights for future research. Code and Models are available at\\nhttps://github.com/Dongping-Chen/MixSet.\\n1\\nIntroduction\\nThe rapid advancement of Large Language Models (LLMs) has ignited global discussions on the\\neffective utilization of AI assistants (OpenAI, 2022, 2023b). As LLMs accurately interpret and\\nexecute human instructions, an increasing number of individuals integrate these powerful tools into\\ntheir workflows. They utilize LLMs to revise Machine Generated Text (MGT) or enhance their\\nHuman Written Text (HWT), with applications ranging from fact-checking revisions in journalism\\n(Guerra, 2023) to storytelling enhancements in the gaming industry (AIContentfy, 2023).\\nDespite its varied applications, the usage of MGT presents potential risks, stirring public concerns\\nover various forms of misuse. These include the undermining of journalistic integrity and quality\\n(Christian, 2023), the reproduction and amplification of biases (Sison et al., 2023), plagiarism among\\nstudents (Heavenarchive, 2023), and disruptions in trust towards scientific knowledge (Else, 2023).\\nSuch misuse of machine-generated text has garnered serious attention and concern from experts across\\n† These authors contributed equally. The order was determined by rolling dice: Chujie Gao rolled a 6,\\nDongping Chen rolled a 4, Qihui Zhang rolled a 1.\\n‡ Visting Students to LAIR Lab, Lehigh University.\\nPreprint. Under review.\\narXiv:2401.05952v1  [cs.CL]  11 Jan 2024\\nFigure 1: Three kinds of text: Machine Generative Text (MGT), Human Written Text (HWT), and\\nmixcase. The text come from users\\nis classified by detectors\\n. The text in red is the HWT\\npolished by LLMs.\\nFigure 2: Percentage of identifying samples as MGT of different sets in Experiment 1. (Left) Model-\\nbased methods; (Right) Metric-based methods. P.T. and P.S. signify token and sentence-level polish,\\nrespectively; C. for complete, R. for rewrite; Adapt T. and Adapt S. for token and sentence-level\\nadapt. See 3 for details on revising operations.\\ndifferent fields (Yurkevich, 2023), highlighting the need for balanced and responsible deployment of\\nthese advanced technologies such as AIGC detection.\\nNumerous methods, including metric-based and model-based methods, have been proposed in previ-\\nous studies to detect MGT. These efforts primarily focus on binary classification, aiming to distinguish\\nbetween pure MGT and HWT. However, these studies often overlook mixcase scenarios—revised\\ntexts involving both MGT and HWT, which are frequently viewed either as attacks on current detec-\\ntion systems (Krishna et al., 2023) or as significant challenges for detection methodologies (Mitchell\\net al., 2023; Guo and Yu, 2023). Yet, the mixture of MGT and HWT is a prevalent scenario in our\\ndaily interactions with LLM assistants. For instance, many non-native English speakers use LLMs to\\nimprove their drafts and address grammatical issues (Hwang et al., 2023). Additionally, LLMs are\\nbeing employed in creative industries, such as game design for creating interactive dialogues and\\nnarratives (AIContentfy, 2023), and in authoring tools like Metaphoria and Sparks for enhancing\\ncreative and scientific writing (Gero and Chilton, 2019; Gero et al., 2022).\\nHence, there is a pressing demand for a comprehensive analysis of these mixed scenarios and a formal\\ndefinition for its growing prevalence of mixcase in both research fields and daily life. To address this\\nissue, we introduce a novel dataset, MIXSET, categorizing mixcase based on the editing operations\\nproposed by Goyal et al. (2023). This is the first dataset targeting the mixture of HWT and MGT,\\nincluding both AI-revised HWT and human-revised MGT scenarios, as detailed in Figure 1, which\\nfills a gap in prior research. Further details and definitions are provided in Section 3. We also evaluate\\nour dataset using mainstream detectors in both binary and three-class settings to further analyze and\\n2\\nFigure 3: The process of MixSet generation. We perform distinct operations in HWT and MGT. In\\nHWT, three operations—polish, rewrite, and complete—are employed. In MGT, we utilize LLama2\\nand GPT-4 to aid in humanization and conduct the adaptation operation manually.\\nraise concerns about these prevalent but challenging-to-detect cases. Moreover, we conduct transfer\\nexperiments to retrained detectors to analyze their generalizability on mixcase from different editing\\noperations and LLMs.\\nTo summarize, our work provides three main contributions:\\n• We defined mixcase, a form of mixed text involving both AI and human-generated content, provid-\\ning a new perspective for further exploration in related fields.\\n• We proposed a new dataset MIXSET, which specifically addresses the mixture of MGT and HWT\\nwhich encompassed a diverse range of operations within real-world scenarios, filling the blankness\\nin previous research.\\n• Based on MIXSET, we conducted extensive experiments involving mainstream detectors and\\nobtained numerous insightful findings, which provide a strong impetus for future research.\\n2\\nRelated works\\n2.1\\nMachine Generated Text Detection\\nCurrent MGT detection methods can be broadly categorized into metric-based methods and model-\\nbased methods according to previous study (He et al., 2023). Please refer to Appendix A for\\ncomprehensive related works.\\nMetric-based Methods. Building upon the observation that MGTs occupy regions with sharp\\nnegative log probability curvature, Mitchell et al. (2023) introduced a zero-shot whitebox detection\\nmethod called DetectGPT, setting a trend in metric-based detection (Su et al., 2023; Mireshghallah\\net al., 2023; Bao et al., 2023). Recently, Yang et al. (2023a) also introduced a powerful detection\\nmethod known as DNA-GPT, which leverages N-gram (Shannon, 1948) in a black-box setting.\\nModel-based Methods. In the era of Large Language Models (LLMs), Guo et al. (2023) developed\\nthe ChatGPT Detector, which is based on a fine-tuned Roberta model. As for decoder-based detectors,\\nGPT-Sentinel (Chen et al., 2023) leverage the t5-small model (Muennighoff et al., 2022) and show\\nconvincing results when detecting MGT even in revised cases. Leveraging adversarial learning\\nframework, Hu et al. (2023) propose a novel detection framework called Radar, which employs an\\nadversarial learning approach to simultaneously train a detector based on Vicuna-7B (Chiang et al.,\\n2023) and a pre-trained paraphraser.\\n2.2\\nDatasets for MGT Detection\\nPrevious studies have proposed many datasets of MGT, accompanied by their newly proposed\\ndetectors (Verma et al., 2023; Chen et al., 2023). Guo et al. (2023) leverages multiple previous\\nQuestion-Answer (QA) datasets (Jin et al., 2019; Lin et al., 2021), allowing ChatGPT to generate\\ncorresponding answers without explicit prompts. This results in the creation of a comprehensive\\ndataset comprising a large set of pairs of MGT and HWT. Following the QA pattern, many researchers\\n3\\n(Mitchell et al., 2023; Su et al., 2023; Hu et al., 2023; He et al., 2023) propose datasets with the MGT\\nfrom variant mainstream LLMs (OpenAI, 2022, 2023b).\\nHowever, these datasets typically consist of two distinct classes of texts, namely pure MGT or HWT,\\nwithout accounting for the potential mixture cases. Furthermore, issues arise due to variations in\\nprompts (Koike et al., 2023a), sampling methods, and the inherent differences in length, style, and\\nquality among texts (He et al., 2023), posing variations challenges on the generalization ability\\nof proposed detectors (Xu et al., 2023). In some instances, MGT included in datasets may not\\nundergo thorough careful checking, with many noisy sentences not filtered well. For example, some\\nsentences like Let me know if you have any other questions exist in the dataset, which will impact the\\neffectiveness of the detectors (Guo et al., 2023).\\n3\\nMIXSET Dataset\\nIn this section, we present MixSet (Mixcase Dataset), the first dataset featuring a blend of HWT and\\nMGT. Distinguished from earlier datasets exclusively composed of pure HWT and MGT, MIXSET\\ncomprises a total of 3.6k mixcase instances. The pipeline of MIXSET construction is illustrated in\\nFigure 3. Each editing operation in our MIXSET, whether modified by an LLM or manually, is based\\non real-world scenarios and contributes 300 instances.\\nFor our base data, we meticulously select pure HWT and MGT datasets. In the case of HWT, we\\ngather datasets proposed before the widespread use of LLMs to mitigate potential noise, as detailed\\nin Table 1. For MGT, we choose samples from previous datasets (Rajpurkar et al., 2016a; Lin et al.,\\n2022; Nishida et al., 2019), generated in a QA pattern by different LLMs, including the GPT family\\n(OpenAI, 2022, 2023b), ChatGLM (Du et al., 2022), BloomZ (Muennighoff et al., 2022), Dolly\\n(Conover et al., 2023), and StableLM (StabilityAI, 2023), all distinct from our MIXSET instances.\\n3.1\\nDefinition of Mixcase\\nGenerally, mixcase is the mixed text involving both AI and human-generated content. To formulate it,\\nwe define a text sequence as x ∈ X, where X represents the set of all text sequences. The sequences\\nin X can originate from either human-written text Xhuman or machine-generated text Xmachine. We\\ndenote the set of operations used to revise texts as OP = {OP1, OP2, . . . , OPn}, categorized into\\ntwo groups: OPhuman, OPmachine. Here, OPhuman refers to operations involving human revision on\\nMGT, while OPmachine refers to AI-driven operations on HWT. In addition to Xhuman and Xmachine, we\\ndefine Xmixcase as the union of all texts derived from Xhuman through OPmachine and all texts derived\\nfrom Xmachine through OPhuman:\\nXmixcase ={OPmachine(x) | x ∈ Xhuman} ∪ {OPhuman(x) | x ∈ Xmachine}\\n3.2\\nDataset Construction\\nCombined with previous studies (Goyal et al., 2023; Wang et al., 2021) and real scenarios, we\\nuse five operations to generate mixcases. As shown in Table 2, they are divided into two kinds of\\noperation: 1) AI-revised: it contains three operations including ’polish’, ’complete’, and ’rewrite’. 2)\\nHuman-revised: it includes ’humanize’ and ’adapt’.\\n• Polish (Chen, 2023): Polish operation contains token-level and sentence-level polishing. Token-\\nlevel makes alterations at the individual word level, including changes such as adjusting words\\nfor precision or correcting spelling errors. On the other hand, sentence-level aims to enhance the\\noverall coherence and clarity of the text by revising and restructuring the complete sentence.\\n• Complete (Zhuohan Xie): Complete operation involves taking 1/3 of every text and employing\\nLLMs to generate the rest of the text.\\n• Rewrite (Shu et al., 2023): Rewrite operation requires LLMs to initially comprehend and extract\\nkey information from the given HWT and then rewrite them.\\n• Humanize (Bhudghar, 2023): Humanize operation typically refers to the modification of MGT\\nto more closely mimic the natural noise for LM (Wang et al., 2021) that human writing always\\nbrings. We employed LLMs to introduce various perturbations to the pure MGT, including typo,\\ngrammatical mistakes, links, and tags.\\n4\\nFigure 4: Length distribution and self-BLEU scores of the MixSet.\\n• Adapt (Gero et al., 2022): Adapt operation refers to modifying MGT to ensure its alignment to\\nfluency and naturalness to human linguistic habits without introducing any error expression. The\\nadapt operation is also divided into token-level and sentence-level adaptation. We accordingly\\nperformed manual annotations on the pure MGT dataset at both the token and sentence levels.\\nFor AI-revised mixcase generation, we utilized Llama2-70b and GPT-4 since these two high-\\nperformance LLMs can generate texts of high quality and few grammatical errors (Hugging Face,\\n2023). In human-revised operation, we leverage two LLMs to assist with the humanize operation.\\nFor the adapt operation, we invite eight human experts with excellent English skills to revise MGT\\ncarefully to make it better align with human expression way. The details of human annotation\\nguidelines and prompt template are shown in Appendix B.1 and E. After collecting all revised texts,\\nwe conducted a manual evaluation involving data filtering and cleaning to ensure the high quality of\\nour MIXSET, such as removing conversational phrases like ‘Sure! Here’s a possible completion’.\\nTable 1: The original resources of Human Written\\nTexts in constructing our MIXSET.\\nText Type\\nOriginal Resources\\nEmail Content\\nEnron Email (CMU, 2015)\\nNews Content\\nBBC News (Greene and Cun-\\nningham, 2006)\\nGame Review\\nSteam Reviews (Najzeko, 2021)\\nPaper Abstract\\nArXiv-10 (Farhangi et al., 2022)\\nSpeech Content\\nTED Talk (TheDataBeast, 2021)\\nBlog content\\nBlog (Schler et al., 2006)\\nTable 2: Different operations with their opera-\\ntion levels. ✔ demonstrate that MIXSET con-\\ntains a subset operates at that level.\\nOperation\\nToken\\nSentence\\nParagraph\\nAI-Polish\\n✔\\n✔\\n✘\\nAI-Complete\\n✘\\n✘\\n✔\\nAI-Rewrite\\n✘\\n✘\\n✔\\nHuman-Adapt\\n✔\\n✔\\n✘\\nHumanize\\n✔\\n✔\\n✔\\n3.3\\nDataset Analysis\\nOur comprehensive analysis of the MIXSET dataset covers length distribution, self-BLEU (Zhu et al.,\\n2018), Levenshtein distance (Levenshtein, 1966), and cosine similarity. We only show analysis of\\nlength distribution and cosine similarity analysis here; for self-BLEU and Levenshtein distance, refer\\nto Appendix B.\\n5\\nTable 3: Detectors used in different experiments.\\nDetector\\nQ 1\\nQ 2\\nQ 3\\nQ 4\\nMetric-Based\\nLog-likelihood (Solaiman et al., 2019)\\n✔\\n✔\\n✘\\n✔\\nEntropy (Gehrmann et al., 2019)\\n✔\\n✔\\n✘\\n✘\\nGLTR (Gehrmann et al., 2019)\\n✔\\n✔\\n✘\\n✔\\nLog-Rank (Mitchell et al., 2023)\\n✔\\n✔\\n✘\\n✘\\nDetectGPT (Mitchell et al., 2023)\\n✔\\n✔\\n✔\\n✔\\nModel-Based\\nRadar (Hu et al., 2023)\\n✔\\n✔\\n✘\\n✔\\nChatGPT Detector (Guo et al., 2023)\\n✔\\n✔\\n✔\\n✔\\nDistillBert (Ippolito et al., 2019)\\n✔\\n✔\\n✔\\n✘\\nGPT-Sentinel (Chen et al., 2023)\\n✔\\n✔\\n✘\\n✔\\nOpenAI Classifier (OpenAI, 2023a)\\n✔\\n✘\\n✘\\n✘\\nGhostbuster (Verma et al., 2023)\\n✔\\n✘\\n✘\\n✘\\nGPTzero (Tian, 2023)\\n✔\\n✘\\n✘\\n✘\\n• Length distribution: Given that LLMs generally perform better with medium to long texts than\\nwith short texts, and to ensure that the text lengths in the MIXSET reflect real-world usage patterns,\\nwe have excluded texts that are either too short or too long. As shown in Figure 4, the text lengths\\nof both the MIXSET, as well as the HWT and MGT, follow a normal distribution. Furthermore, the\\nmajority of the texts in these datasets fall within the range of 50 to 200 words.\\n• Cosine Similarity: Figure 4 illustrates that the texts processed with token-level polish operations\\nexhibit the highest similarity to the original texts, followed by sentence-level polish, rewrite, and\\ncomplete. Texts modified through the humanize operation demonstrate lower similarity compared\\nto those altered by adaptation.\\n4\\nExperiments\\n4.1\\nGoals\\nWe conduct experiments to understand multiple facets of current detectors when encountering our\\nMIXSET, including zero-shot and fine-tuning settings. We are going to figure out four questions:\\n• Question 1. How do current detectors perform in MIXSET dataset? Is there any classification\\npreference in these detectors?\\n• Question 2. What is the performance of detectors retrained on our MIXSET? What about three-\\nclassed classification as we consider mixcase as a new class distinct from HWT and MGT?\\n• Question 3. What is the generalization ability of current detectors on our MIXSET?\\n• Question 4. Will the size of the training set impact the detection ability on mixcase?\\n4.2\\nExperiment Setup\\nAmong our 4 experiments, We evaluate five metric-based detectors and seven model-based detectors\\non three metrics in total as illustrated in Tabel 3 and Table 5. We also list the summary of our training\\nset in Figure 4. Please refer to Appendix C for a comprehensive introduction to detectors and metrics.\\nClass Number. In real-world scenarios, people often aim to detect the presence of MGT in the text\\n(e.g., spreading fake news or propaganda (Christian, 2023), reinforcing and intensifying prejudices\\n(Sison et al., 2023)), and sometimes mixcase is also treated as MGT (e.g., student modified some\\nwords in MGT (i.e., mixcase) to generate homework, to avoid detection). Therefore, in our experi-\\nments, we established two categorization systems: binary and three-class. In the binary classification,\\nmixcase is categorized as MGT, while in the three-class classification, mixcase is treated as a separate\\nclass. The label setting is shown in Table 5.\\n6\\nTable 4: An outline of detailed training set con-\\nstruction for each experiment. Ope. denotes\\n‘operation transfer’ in Experiment 3, while LLM\\nrefers to ‘LLM transfer’.\\nExperiment\\nHWT/MGT\\nMIXSET\\nQ 1\\n10k\\n0\\nQ 2(a)\\n10k\\n3k\\nQ 2(b)\\n10k\\n3k\\nQ 3(Ope.)\\n1k\\n0.5k\\nQ 3(LLM)\\n5k\\n1.5k\\nQ 4\\n1k/4k/7k/10k\\n0/1.5k/3k\\nTable 5: The details of class number, metrics, and\\nwhether the detectors are retrained in our experi-\\nments. Except for Question 2(b), we implement\\nbinary classifications i.e. HWT and MGT. Per.\\nstands for Percentage.\\nSetting\\nQ 1\\nQ 2\\nQ 3\\nQ 4\\n(a)\\n(b)\\nClass Num.\\n2-Class\\n2-Class 3-Class 2-Class 2-Class\\nMetric\\nMGT Per. F1, AUC\\nF1\\nAUC\\nF1, AUC\\nRetrained?\\n✘\\n✔\\n✔\\n✔\\n✔\\nTable 6: Percentage of identifying samples as MGT of different sets in Experiment 1. For example, the\\nLog-Rank detector categorizes 57.30% of samples in the Llama2-revised set as MGT. We underscore\\nthe best-performing detector and bold the score greater than 0.8, which we consider as a baseline\\nthreshold for detection (Tok. stands for token level and Sen. stands for sentence level).\\nDetection Method\\nHWT\\nMGT\\nAI-Revised\\nHuman-Revised\\nRewrite\\nComplete\\nPolish-Tok.\\nPolish-Sen.\\nHumanize\\nAdapt-Tok.\\nAdapt-Sen.\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nMetric-based Detector\\nLog-Rank\\n0.213\\n0.847\\n0.573\\n0.240\\n0.810\\n0.520\\n0.573\\n0.383\\n0.427\\n0.350\\n0.703\\n0.093\\n0.783\\n0.770\\nLog-likelihood\\n0.223\\n0.867\\n0.600\\n0.287\\n0.823\\n0.560\\n0.643\\n0.450\\n0.513\\n0.410\\n0.703\\n0.083\\n0.790\\n0.777\\nGLTR\\n0.207\\n0.840\\n0.480\\n0.180\\n0.813\\n0.393\\n0.517\\n0.283\\n0.390\\n0.313\\n0.630\\n0.053\\n0.783\\n0.760\\nDetectGPT\\n0.350\\n0.823\\n0.643\\n0.343\\n0.743\\n0.557\\n0.650\\n0.480\\n0.563\\n0.437\\n0.807\\n0.533\\n0.623\\n0.597\\nEntropy\\n0.353\\n0.840\\n0.733\\n0.580\\n0.793\\n0.623\\n0.793\\n0.730\\n0.713\\n0.640\\n0.737\\n0.223\\n0.793\\n0.770\\nModel-based Detector\\nOpenai Classifier\\n0.060\\n0.607\\n0.150\\n0.047\\n0.407\\n0.037\\n0.123\\n0.037\\n0.103\\n0.053\\n0.023\\n0.007\\n0.490\\n0.453\\nChatGPT Detector\\n0.040\\n0.757\\n0.380\\n0.157\\n0.523\\n0.287\\n0.380\\n0.130\\n0.243\\n0.117\\n0.457\\n0.097\\n0.750\\n0.770\\nRadar\\n0.307\\n0.857\\n0.730\\n0.477\\n0.893\\n0.783\\n0.607\\n0.447\\n0.560\\n0.387\\n0.347\\n0.037\\n0.850\\n0.890\\nGPT-Sentinel\\n0.133\\n0.887\\n0.833\\n0.877\\n0.540\\n0.573\\n0.883\\n0.807\\n0.710\\n0.460\\n0.033\\n0.000\\n0.910\\n0.910\\nDistillbert\\n0.483\\n0.993\\n0.593\\n0.660\\n0.530\\n0.573\\n0.607\\n0.580\\n0.547\\n0.527\\n0.170\\n0.003\\n1.000\\n1.000\\nGhostbuster\\n0.103\\n0.610\\n0.870\\n0.780\\n0.750\\n0.087\\n0.353\\n0.493\\n0.473\\n0.663\\n0.567\\n0.637\\n0.700\\n0.443\\nGPTZero\\n0.017\\n0.730\\n0.493\\n0.167\\n0.810\\n0.177\\n0.497\\n0.260\\n0.777\\n0.763\\n0.717\\n0.187\\n0.720\\n0.497\\nQuestion 1. Based on MIXSET, we evaluate current detectors to determine the classification\\npreferences on mixcase, i.e., Does the detector tend to classify mixcase as MGT or HWT? In the\\nexperiment, we calculate the percentage of mixcase samples categorized to MGT. For the DistilBERT\\ndetector and other metric-based detectors utilizing logistic regression models, we employ a training\\nset comprising 10,000 pre-processed samples of both pure HWT and MGT. For other detectors, we\\nutilize existing checkpoints 1 2 or API 3 and evaluate them in a zero-shot setting.\\nQuestion 2(a). Following Question 1, our inquiry is whether the detector can accurately classify\\nmixcase as MGT after training on our MIXSET. We fine-tune detectors on pure HWT and MGT data,\\nas well as a train split set of our MIXSET labeled as MGT. For detailed experiment settings, please\\nrefer to Appendix C.\\nQuestion 2(b). On the other hand, assuming that mixcase lies outside the distribution of HWT and\\nMGT, we conduct a three-class classification task, treating mixcase as a new label. In this scenario,\\nwe adopt multi-label training for these detectors, while keeping all other settings consistent. c\\n1https://huggingface.co/TrustSafeAI/RADAR-Vicuna-7B\\n2https://github.com/haok1402/GPT-Sentinel-public\\n3https://gptzero.me/\\n7\\nTable 7: F1 score of experiment 2 (a) and (b). We underscore the best-performing detector and bold\\nthe score greater than 0.8, which we consider as a baseline threshold for detection (Tok. stands for\\ntoken level and Sen. stands for sentence level).\\nDetection Method\\nAI-Revised\\nHuman-Revised\\nAverage\\nComplete\\nRewrite\\nPolish-Tok.\\nPolish-Sen.\\nHumanize\\nAdapt-Sen.\\nAdapt-Tok.\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nExperiment 2 (a): Binary Classification\\nLog-Rank\\n0.695\\n0.686\\n0.637\\n0.479\\n0.617\\n0.606\\n0.647\\n0.595\\n0.617\\n0.454\\n0.676\\n0.667\\n0.615\\nLog Likelihood\\n0.695\\n0.695\\n0.637\\n0.492\\n0.657\\n0.627\\n0.657\\n0.657\\n0.637\\n0.386\\n0.676\\n0.667\\n0.624\\nGLTR\\n0.686\\n0.647\\n0.606\\n0.441\\n0.574\\n0.585\\n0.637\\n0.540\\n0.617\\n0.400\\n0.657\\n0.667\\n0.588\\nDetectGPT\\n0.715\\n0.651\\n0.656\\n0.560\\n0.632\\n0.587\\n0.657\\n0.632\\n0.692\\n0.587\\n0.641\\n0.609\\n0.635\\nEntropy\\n0.690\\n0.671\\n0.681\\n0.613\\n0.681\\n0.671\\n0.681\\n0.671\\n0.623\\n0.430\\n0.681\\n0.681\\n0.648\\nOpenai Classifier\\n0.171\\n0.359\\n0.031\\n0.197\\n0.145\\n0.270\\n0.247\\n0.439\\n0.247\\n0.316\\n0.000\\n0.090\\n0.209\\nChatGPT Detector\\n0.705\\n0.696\\n0.676\\n0.583\\n0.676\\n0.647\\n0.647\\n0.594\\n0.667\\n0.615\\n0.705\\n0.705\\n0.660\\nRadar\\n0.867\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.877\\n0.876\\nGPT-Sentinel\\n0.714\\n0.714\\n0.714\\n0.714\\n0.714\\n0.714\\n0.714\\n0.714\\n0.696\\n0.714\\n0.714\\n0.714\\n0.713\\nDistillbert\\n0.667\\n0.667\\n0.667\\n0.667\\n0.667\\n0.667\\n0.667\\n0.667\\n0.639\\n0.667\\n0.667\\n0.667\\n0.664\\nExperiment 2 (b): Three-class Classification\\nDetectGPT\\n0.276\\n0.21\\n0.295\\n0.278\\n0.283\\n0.234\\n0.271\\n0.237\\n0.28\\n0.222\\n0.233\\n0.235\\n0.255\\nChatGPT Detector\\n0.288\\n0.346\\n0.283\\n0.288\\n0.395\\n0.341\\n0.265\\n0.328\\n0.267\\n0.317\\n0.253\\n0.273\\n0.304\\nDistillbert\\n0.267\\n0.333\\n0.319\\n0.329\\n0.294\\n0.309\\n0.294\\n0.329\\n0.309\\n0.342\\n0.000\\n0.010\\n0.261\\nQuestion 3. As highlighted in prior research (Xu et al., 2023; He et al., 2023) that transfer ability\\nis crucial for detectors, our objective is to investigate the effectiveness of transferring across differ-\\nent subsets of MIXSET and LLMs. We establish two transfer experiments to assess whether the\\ntransferability of current detection methods is closely linked to the training dataset, referred to as\\noperation-generalization and LLM-generalization:\\n• Operation-generalization: We initially train our detectors on one MIXSET subset operated by\\none of these operations, along with pure HWT and MGT datasets, and then proceed to transfer it to\\nthe subsets processed by other operations.\\n• LLM-generalization: In this experiment, we train detectors on GPT-generated texts and HWT,\\nfollowing which we evaluate the detectors on mixcase generated by GPT family (OpenAI, 2023b)\\nand Llama2 (Touvron et al., 2023), respectively, to see whether there is a generalization gap\\nbetween different LLMs.\\nQuestion 4. Empirically, incorporating more training data has been shown to enhance detection\\ncapabilities, and improve robustness for generalization (Ying, 2019; Rebuffi et al., 2021). To figure\\nout the relation between detectors’ performance and the size of the training set, we follow Question\\n2 and use varying sizes of training sets to retrain those detectors, as illustrated in Table 4.\\n5\\nEmpirical Findings\\nThere is no obvious classification preference in current detectors on mixcase with low consistency\\nunder different operations. And MIXSET effectively bridges the differences between HWT\\nand MGT. As we can observe from Figure 2 and Table 6, it is evident that the MGT percentage of\\nmixcases is between MGT and HWT, indicating that the current detectors do not have a clear bias\\ntowards mixcase classification. This proves the success and effectiveness of our constructed MIXSET\\nin presenting mixed features of HWT and MGT, demonstrating the limitations of existing detectors\\nin recognizing mixcase.\\nAdditionally, most detectors exhibit inconsistent classification within a single subset, fluctuating\\nbetween accuracies of 0.3 and 0.7, akin to random choice. In AI-revised scenarios, subsets such as\\npolished tokens or sentences, pose extreme detection challenges. Mainstream detectors generally per-\\n8\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\n0.91 0.98 0.92 0.97 0.97 0.96 0.95 0.97 0.97 0.97 0.14 0.36\\n0.89 0.98 0.91 0.98 0.97 0.96 0.93 0.97 0.98 0.98 0.10 0.33\\n0.88 0.97 0.95 0.97 0.95 0.95 0.97 0.97 0.96 0.97 0.17 0.37\\n0.88 0.98 0.90 0.99 0.97 0.96 0.92 0.98 0.98 0.98 0.11 0.33\\n0.88 0.99 0.90 0.98 0.98 0.97 0.94 0.98 0.99 0.99 0.14 0.38\\n0.90 0.98 0.91 0.98 0.98 0.97 0.95 0.98 0.98 0.99 0.11 0.33\\n0.87 0.96 0.92 0.96 0.96 0.95 0.98 0.97 0.97 0.98 0.19 0.42\\n0.90 0.99 0.91 0.98 0.98 0.97 0.96 0.98 0.99 0.99 0.17 0.44\\n0.83 0.94 0.83 0.95 0.94 0.92 0.88 0.95 0.99 0.99 0.11 0.38\\n0.84 0.95 0.84 0.95 0.95 0.93 0.89 0.95 0.99 0.99 0.11 0.38\\n0.78 0.86 0.78 0.87 0.88 0.85 0.83 0.88 0.95 0.95 0.98 0.90\\n0.75 0.81 0.74 0.83 0.86 0.81 0.80 0.87 0.95 0.95 0.89 0.93\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n(a) The AUC Heatmap of GPT-Sentinel.\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\n0.74 0.91 0.67 0.88 0.87 0.87 0.69 0.86 0.97 0.97 0.06 0.24\\n0.75 0.92 0.70 0.90 0.89 0.88 0.70 0.88 0.97 0.97 0.06 0.24\\n0.74 0.92 0.77 0.90 0.88 0.88 0.74 0.87 0.97 0.97 0.06 0.23\\n0.75 0.92 0.71 0.90 0.89 0.88 0.70 0.88 0.97 0.98 0.05 0.23\\n0.74 0.92 0.69 0.89 0.89 0.88 0.70 0.88 0.97 0.98 0.05 0.23\\n0.75 0.91 0.68 0.89 0.89 0.88 0.70 0.88 0.97 0.97 0.05 0.23\\n0.74 0.90 0.68 0.88 0.87 0.87 0.71 0.87 0.97 0.97 0.05 0.23\\n0.74 0.91 0.68 0.89 0.89 0.88 0.70 0.89 0.97 0.97 0.05 0.23\\n0.60 0.70 0.69 0.71 0.68 0.65 0.68 0.70 0.92 0.91 0.08 0.27\\n0.59 0.66 0.52 0.66 0.68 0.64 0.61 0.66 0.94 0.94 0.07 0.31\\n0.57 0.61 0.53 0.59 0.59 0.60 0.55 0.55 0.84 0.84 0.68 0.69\\n0.55 0.56 0.68 0.59 0.58 0.58 0.66 0.57 0.84 0.84 0.30 0.56\\n0.2\\n0.4\\n0.6\\n0.8\\n(b) The AUC Heatmap of distilbert-based\\nFigure 5: The AUC Heatmap of the other two detectors.\\nform badly in these cases due to the subtle differences between mixcase and original text, highlighted\\nin previous studies (Krishna et al., 2023). Furthermore, texts generated by Llama2-70b are easier to\\ndetect compared to those by GPT-4, possibly due to GPT-4’s closer generative distribution to human.\\nSupervised binary classification yields profound results; however, three-classes classification\\nencounters significant challenges when applied to mixcase scenarios. As indicated in Table 7,\\nretrained model-based detectors outperform metric-based methods in both binary and three-class\\nclassification tasks. Notably, Radar ranks first on the detection leaderboard, achieving a significant\\nlead over other detectors. This superior performance can be attributed to its encoder-decoder\\narchitecture, which boasts 7 billion trainable parameters, substantially more than its counterparts. In\\ncontrast, for the ternary classification task, the performance of current detectors is akin to random\\nguessing, with some even underperforming compared to a random choice. This situation underscores\\nthe urgent need for the development of more sophisticated detectors capable of executing a finer-\\ngrained classification of mixcase.\\nTable 8: Result of LLM-transfer exper-\\niments.\\nAlthough we retrain our de-\\ntector on texts generated by GPT-4, it\\nshows convincing generalization ability\\nto Llama2.\\nMethod\\nw.o MixSet\\nw. MixSet\\nLlama2 GPT-4 Llama2 GPT-4\\nGPT-Sentinel\\n0.813\\n0.739\\n0.972\\n0.971\\nRadar\\n0.834\\n0.729\\n0.997\\n1.000\\nChatGPT Det.\\n0.664\\n0.445\\n0.681\\n0.480\\nDistillbert\\n0.687\\n0.638\\n0.673\\n0.698\\nCurrent detectors struggle to generalize across differ-\\nent revised operation subsets of MIXSET and genera-\\ntive models. As shown in Figure 5 and Figure 11, signif-\\nicant variability is observed in the transfer capabilities of\\nthree different detectors. Additionally, training on texts\\ngenerated by different revised operations results in differ-\\nent transfer abilities for these detectors. Overall, Radar\\nexhibits the most robust transfer capability among the four\\nmodel-based detectors, achieving an overall classification\\naccuracy exceeding 0.9, followed by GPT-Sentinel, Distill-\\nBert, and finally the ChatGPT Detector. Among various\\noperations, ‘Humanize’ exhibits the poorest transfer per-\\nformance in almost all scenarios. Additionally, other op-\\nerations also experience significant declines when dealing\\nwith ‘Humanize’ mixcases. This suggests that ‘Humanize’\\nfalls outside the current detectors’ distribution of MGT, a gap that could be addressed by retraining on\\nthese specific cases. It is also noteworthy that texts generated by Llama2-70b demonstrate stronger\\ntransfer abilities compared to those generated by GPT-4, as illustrated in Table 8.\\nIncreasing the number of mixcase samples in the training set effectively enhances the success\\nrate of mixcase detection. However, adding pure text samples does not yield significant improve-\\nments and may even have a negative impact on detector performance, especially for metric-based\\nmethods, as shown in Figure 6. We believe this may be attributed to subtle distribution shifts between\\nmixcase and pure text. The current detector still faces significant challenges in capturing these subtle\\nshifts. For mixcase scenarios, a more powerful and fine-grained detection method is needed.\\n9\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.600\\n0.625\\n0.650\\nF1 Score\\nLog-Likelihood\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.50\\n0.55\\n0.60\\n0.65\\nF1 Score\\nGLTR\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.600\\n0.625\\n0.650\\nF1 Score\\nDetectGPT\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.650\\n0.675\\n0.700\\n0.725\\nF1 Score\\nGPT-Sentinel\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.72\\n0.80\\n0.88\\nF1 Score\\nRadar\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.56\\n0.60\\n0.64\\n0.68\\nF1 Score\\nChatGPT Detector\\nNone of MixSet Train set\\n50.0% samples of MixSet Train set\\n100% samples of MixSet Train set\\nFigure 6: Analysis of the F1-score performance of various detectors across differing quantities of\\nmixcase instances from MIXSET, as well as pure MGT and HWT.\\n6\\nConclusion\\nIn this paper, we defined mixcase, the mixed text of human and LLM-generated content. Then we\\nproposed a new dataset MIXSET to address the research gap in studying the mixed scenarios of\\nmachine-generated text (MGT) and human-written text (HWT). We conducted a thorough evaluation\\nof the dataset, performing binary, three-class, and transfer experiments on mainstream detectors. The\\nresults revealed that the detection of mixcase is challenging, indicating the difficulty in discerning\\nsubtle differences in mixcase. As a result, there is a need for more robust and fine-grained detection\\nmethods.\\n7\\nLimitation\\nBias Introduced by Human Participation. Although our study involved multiple human participants\\nto modify the text, increasing the diversity and authenticity of the data, the text processing methods\\nof different participants could vary due to their language habits and styles. This might affect the\\nrepresentativeness of the dataset and the generalization ability of the detection models.\\nLimitation in the Scale of the MixSet Dataset. As the MixSet dataset is the first to be proposed for\\nstudying mixed texts (mixcase), and despite its wide coverage in types, its overall scale is relatively\\nsmall. This could limit the comprehensiveness of model training and evaluation.\\n8\\nEthics Statement\\nOpposition to Misuse of Mixed Text Scenarios. Our study highlights that the mixcase of HWT\\nand MGT could significantly diminish the discerning abilities of detectors. However, we strongly\\noppose the misuse of mixcase to evade detection in specific scenarios, such as during examinations\\nand homework assignments. We believe such misuse could severely harm the fairness of education\\nand the integrity of academic practices.\\nPurpose for Scientific Research. The aim of this study is purely for scientific exploration and\\nunderstanding of the behavior and impact of mixcase in the field of natural language processing.\\nOur goal is to enhance understanding of mixed text processing and to advance the technological\\ndevelopment in this area, not to encourage or support applications that may violate ethical standards.\\n10\\nCompliance with Licensing and Distribution Regulations. We affirm that all open-source resources\\nutilized in our study, including detectors, language models, and datasets, have been employed in\\nstrict accordance with their respective licenses and distribution terms. This adherence extends to\\nensuring that any modifications, redistributions, or applications of these resources in our research are\\ncompliant with their original licensing agreements. Our commitment to these principles upholds the\\nintegrity of our research and contributes to a responsible and ethical academic environment.\\nUse of Publicly Available Data and Consideration for Privacy. The datasets used in our research are\\nexclusively sourced from publicly available, open-source collections. While these datasets are publicly\\naccessible and generally considered devoid of sensitive personal information, we acknowledge the\\npotential for inadvertent inclusion of personal identifiers in datasets. We emphasize that our use of\\nthese datasets is aligned with their intended purpose and distribution terms. We also recognize the\\nimportance of respecting privacy and are committed to ongoing vigilance in this regard.\\nWe reiterate that this research adheres to strict scientific and ethical standards, aiming to contribute\\nto the field of natural language processing while ensuring that the results are not used for improper\\npurposes. We also encourage our peers to consider these ethical factors when utilizing our research\\nfindings, ensuring that their applications do not adversely affect society and individuals.\\nReferences\\nGoogle translate. https://translate.google.com/.\\nGrammarly. https://www.grammarly.com/.\\nYoudao translate. http://fanyi.youdao.com/.\\nAIContentfy.\\n2023.\\nChatgpt\\nin\\nthe\\ngaming\\nindustry:\\nEnhancing\\nstorytelling\\nand\\ninteraction.\\nhttps://aicontentfy.com/en/blog/\\nchatgpt-in-gaming-industry-enhancing-storytelling-and-interaction.\\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy\\nMulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation\\nfrom gpt-3.5-turbo. GitHub.\\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio Ranzato, and Arthur\\nSzlam. 2019. Real or fake? learning to discriminate machine from human generated text.\\narXiv preprint arXiv:1906.03351.\\nGuangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. Fast-\\ndetectgpt: Efficient zero-shot detection of machine-generated text via conditional probability\\ncurvature. arXiv preprint arXiv:2310.05130.\\nGandhi Gram Bhudghar. 2023. Ai text converter. https://aitextconverter.com/.\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with\\npython: Analyzing text with the natural language toolkit. http://nltk.org/.\\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An\\nopen-source autoregressive language model. arXiv preprint arXiv:2204.06745.\\nXuhang Chen. 2023.\\nGpt academic prompt.\\nhttps://github.com/xuhangc/\\nChatGPT-Academic-Prompt.\\n11\\nYutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, and Bhiksha Ramakrishnan.\\n2023. Gpt-sentinel: Distinguishing human and chatgpt generated content. arXiv preprint\\narXiv:2305.07969.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.\\n2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\\nJon Christian. 2023. Cnet secretly used ai on articles that didn’t disclose that fact, staff say.\\nhttps://futurism.com/cnet-ai-articles-label.\\nCMU. 2015. Enron email dataset. https://www.cs.cmu.edu/~enron/.\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\\nPatrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world’s\\nfirst truly open instruction-tuned llm. https://www.databricks.com/blog/2023/04/12/\\ndolly-first-open-commercially-viable-instruction-tuned-llm.\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\\n2022. Glm: General language model pretraining with autoregressive blank infilling. In\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 320–335.\\nHolly Else. 2023. Abstracts written by chatgpt fool scientists. Nature, 613(7944):423–423.\\nAshkan Farhangi, Ning Sui, Nan Hua, Haiyan Bai, Arthur Huang, and Zhishan Guo. 2022.\\nProtoformer: Embedding prototypes for transformers. In Advances in Knowledge Discovery\\nand Data Mining: 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16–19,\\n2022, Proceedings, Part I, pages 447–458.\\nSebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. 2019. Gltr: Statistical\\ndetection and visualization of generated text. arXiv preprint arXiv:1906.04043.\\nKaty Ilonka Gero and Lydia B Chilton. 2019. Metaphoria: An algorithmic companion\\nfor metaphor creation. In Proceedings of the 2019 CHI conference on human factors in\\ncomputing systems, pages 1–12.\\nKaty Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks: Inspiration for science\\nwriting using language models. pages 1002–1019.\\nSoumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh\\nManocha, and Amrit Singh Bedi. 2023. Towards possibilities & impossibilities of ai-\\ngenerated text detection: A survey. arXiv preprint arXiv:2310.15264.\\nShreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. 2023.\\nA survey of adversarial defenses and robustness in nlp. ACM Computing Surveys, 55(14s):1–\\n39.\\nDerek Greene and Pádraig Cunningham. 2006. Practical solutions to the problem of diag-\\nonal dominance in kernel document clustering. In Proceedings of the 23rd international\\nconference on Machine learning, pages 377–384.\\n12\\nChenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. 2023. On the learn-\\nability of watermarks for language models.\\nMarci Guerra. 2023. Chat gpt for journalism: Revolutionizing the future of reporting.\\nhttps://brandalytics.co/chat-gpt-for-journalism/.\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei\\nYue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus,\\nevaluation, and detection. arXiv preprint arXiv:2301.07597.\\nZhen Guo and Shangdi Yu. 2023. Authentigpt: Detecting machine-generated text via\\nblack-box language models denoising. arXiv preprint arXiv:2311.07700.\\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news-please:\\nA generic news crawler and extractor.\\nXinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023. Mgtbench:\\nBenchmarking machine-generated text detection. arXiv preprint arXiv:2303.14822.\\nWill Douglas Heavenarchive. 2023.\\nChatgpt is going to change education,\\nnot\\ndestroy\\nit.\\nhttps://www.technologyreview.com/2023/04/06/1071059/\\nchatgpt-change-not-destroy-education-openai/.\\nXiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. 2023. Radar: Robust ai-text detection via\\nadversarial learning. arXiv preprint arXiv:2307.03838.\\nHugging Face. 2023.\\nOpen llm leaderboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard.\\nSung Il Hwang, Joon Seo Lim, Ro Woon Lee, Yusuke Matsui, Toshihiro Iguchi, Takao Hiraki,\\nand Hyungwoo Ahn. 2023. Is chatgpt a “fire of prometheus” for non-native english-speaking\\nresearchers in academic writing? Korean Journal of Radiology, 24(10):952.\\nDaphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2019. Au-\\ntomatic detection of generated text is easiest when humans are fooled. arXiv preprint\\narXiv:1911.00650.\\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. 2019.\\nPubmedqa:\\nA dataset for biomedical research question answering.\\narXiv preprint\\narXiv:1909.06146.\\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.\\n2023. A watermark for large language models. arXiv preprint arXiv:2301.10226.\\nTomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor\\nMelis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge.\\nTransactions of the Association for Computational Linguistics, 6:317–328.\\nRyuto Koike, Masahiro Kaneko, and Naoaki Okazaki. 2023a. How you prompt matters!\\neven task-oriented constraints in instructions affect llm-generated text detection. arXiv\\npreprint arXiv:2311.08369.\\n13\\nRyuto Koike, Masahiro Kaneko, and Naoaki Okazaki. 2023b. Outfox: Llm-generated essay\\ndetection through in-context learning with adversarially generated examples. arXiv preprint\\narXiv:2307.11729.\\nKalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023.\\nParaphrasing evades detectors of ai-generated text, but retrieval is an effective defense. arXiv\\npreprint arXiv:2303.13408.\\nVladimir Iosifovich Levenshtein. 1966.\\nBinary codes capable of correcting deletions,\\ninsertions and reversals. Soviet Physics Doklady, 10(8):707–710.\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models\\nmimic human falsehoods. arXiv preprint arXiv:2109.07958.\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. pages 3214–3252, Dublin, Ireland. Association for Computational\\nLinguistics.\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization.\\nFatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-\\nKirkpatrick. 2023. Smaller language models are better black-box machine-generated text\\ndetectors. arXiv preprint arXiv:2305.09859.\\nEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn.\\n2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature.\\narXiv preprint arXiv:2301.11305.\\nTimo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. Covid-qa: A\\nquestion answering dataset for covid-19. In Proceedings of the 1st Workshop on NLP for\\nCOVID-19 at ACL 2020.\\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,\\nTeven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022.\\nCrosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.\\nNajzeko. 2021. Steam reviews dataset 2021.\\nKyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako\\nAsano, and Junji Tomita. 2019. Multi-style generative reading comprehension. pages\\n2273–2284, Florence, Italy. Association for Computational Linguistics.\\nOpenAI. 2022. Openai models - gpt3.5. https://platform.openai.com/docs/models/\\ngpt-3-5.\\nOpenAI. 2023a. Ai text classifier. https://beta.openai.com/ai-text-classifier.\\nOpenAI. 2023b. Gpt-4 technical report.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.\\nLanguage models are unsupervised multitask learners.\\n14\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a\\nunified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–\\n5551.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016a. SQuAD:\\n100,000+ questions for machine comprehension of text. pages 2383–2392, Austin, Texas.\\nAssociation for Computational Linguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016b.\\nSquad:\\n100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.\\nSylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles,\\nand Timothy A Mann. 2021. Data augmentation can improve robustness. Advances in\\nNeural Information Processing Systems, 34:29935–29948.\\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and\\nSoheil Feizi. 2023.\\nCan ai-generated text be reliably detected?\\narXiv preprint\\narXiv:2303.11156.\\nJonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker. 2006. Effects\\nof age and gender on blogging. In AAAI spring symposium: Computational approaches to\\nanalyzing weblogs, volume 6, pages 199–205.\\nClaude Elwood Shannon. 1948. A mathematical theory of communication. The Bell system\\ntechnical journal, 27(3):379–423.\\nLei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Canoee Liu, Simon Tong, Jindong\\nChen, and Lei Meng. 2023. Rewritelm: An instruction-tuned large language model for text\\nrewriting. arXiv preprint arXiv:2305.15685.\\nAlejo Jose G Sison, Marco Tulio Daza, Roberto Gozalo-Brizuela, and Eduardo C Garrido-\\nMerchán. 2023. Chatgpt: More than a weapon of mass deception, ethical challenges and\\nresponses from the human-centered artificial intelligence (hcai) perspective. arXiv preprint\\narXiv:2304.11215.\\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff\\nWu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release\\nstrategies and the social impacts of language models. arXiv preprint arXiv:1908.09203.\\nStabilityAI. 2023. Stablelm.\\nJinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. Detectllm: Leveraging\\nlog rank information for zero-shot detection of machine-generated text. arXiv preprint\\narXiv:2306.05540.\\nTheDataBeast. 2021. Ted talk transcripts (2006 - 2021).\\nEdward Tian. 2023. Gptzero: An ai text detector. https://gptzero.me/.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\\nTimothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.\\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\\n15\\nAdaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020. Authorship attribution for\\nneural text generation. In Proceedings of the 2020 conference on empirical methods in\\nnatural language processing (EMNLP), pages 8384–8395.\\nVivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. 2023. Ghostbuster: Detecting\\ntext ghostwritten by large language models. arXiv preprint arXiv:2305.15047.\\nBoxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Has-\\nsan Awadallah, and Bo Li. 2021. Adversarial glue: A multi-task benchmark for robustness\\nevaluation of language models. arXiv preprint arXiv:2111.02840.\\nKangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. 2023. Llmdet: A\\nthird party large language models generated text detection tool. In The 2023 Conference on\\nEmpirical Methods in Natural Language Processing.\\nHan Xu, Jie Ren, Pengfei He, Shenglai Zeng, Yingqian Cui, Amy Liu, Hui Liu, and Jiliang\\nTang. 2023. On the generalization of training-based chatgpt detection methods. arXiv\\npreprint arXiv:2310.01307.\\nXianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. 2023a.\\nDna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. arXiv\\npreprint arXiv:2305.17359.\\nXianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William Yang Wang, and\\nWei Cheng. 2023b.\\nZero-shot detection of machine-generated codes.\\narXiv preprint\\narXiv:2310.05103.\\nXue Ying. 2019. An overview of overfitting and its solutions. In Journal of physics:\\nConference series, volume 1168, page 022022. IOP Publishing.\\nXiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming\\nZhang, and Nenghai Yu. 2023. Gpt paternity test: Gpt generated text detection with gpt\\ngenetic inheritance. arXiv preprint arXiv:2305.12519.\\nVanessa\\nYurkevich.\\n2023.\\nExperts\\nwarn\\nabout\\npossible\\nmisuse\\nof\\nnew\\nai\\ntool\\nchatgpt.\\nhttps://www.atlantanewsfirst.com/2023/01/24/\\nexperts-warn-about-possible-misuse-new-ai-tool-chatgpt/.\\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong\\nYu. 2018. Texygen: A benchmarking platform for text generation models. In The 41st\\nInternational ACM SIGIR Conference on Research & Development in Information Retrieval,\\npage 1097–1100.\\nJey Han Lau Zhuohan Xie, Trevor Cohn. The next chapter: A study of large language\\nmodels in storytelling. https://aclanthology.org/2023.inlg-main.23/.\\n16\\nA\\nFull Related Works\\nA.1\\nDetecting Machine Generated Text\\nCurrent MGT detection methods can be broadly categorized into metric-based methods and model-\\nbased methods according to previous study (He et al., 2023). Moreover, other detection methods\\nsuch as watermark, retrival-based methods, and in-context learning leveraging LLMs also lead to a\\npromising way for detection.\\nMetric-based Methods. Metric-based methods leverage the LLM backbone directly to extract its\\ndistinguishing features between HWT and MGT, operating within a white-box setting that requires\\naccess to the model. Former methods like Log-Likelihood (Solaiman et al., 2019), Entropy, Rank\\n(Gehrmann et al., 2019), and Log-Rank (Mitchell et al., 2023) employ statistical analysis to measure\\ninformation beyond the token level. GLTR (Gehrmann et al., 2019) utilizes a suite of metric-\\nbased methods to aid in human identification. However, with the advent of LLMs, the progressively\\nincreasing similarity between the distributions of HWT and MGT has weakened its detection accuracy\\n(Ghosal et al., 2023).\\nBuilding upon the observation that MGTs occupy regions with sharp negative log probability cur-\\nvature, Mitchell et al. (2023) introduced a zero-shot whitebox detection method called DetectGPT,\\nsetting a trend in metric-based detection (Su et al., 2023; Mireshghallah et al., 2023; Bao et al.,\\n2023). Yang et al. (2023a) also introduced a powerful detection method known as DNA-GPT, which\\nleverages N-gram (Shannon, 1948) in a black-box setting by analyzing the differences between\\ntruncated original text and regenerated text. Recently, they even extended the detection method to\\nMGT code in a zero-shot setting, which is proven to achieve promising results (Yang et al., 2023b).\\nPOS\\nJJ\\nJJS\\nVBZ\\nRBS\\nNNS\\nTO\\nVBD\\nMD\\nVBP\\nFW\\nDT\\nVBN\\nUH\\nRB\\nNNP\\nPDT\\nEX\\nWP\\nRBR\\nPRP\\nVB\\nJJR\\nWRB\\nSYM\\nNN\\nRP\\nVBG\\nCC\\nCD\\nPOS Tags\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\nRelative Frequency\\nToken level\\nLlama2-70b\\nGPT-4\\nHuman\\nPOS\\nJJ\\nJJS\\nVBZ\\nRBS\\nNNS\\nTO\\nVBD\\nMD\\nVBP\\nFW\\nDT\\nVBN\\nUH\\nRB\\nNNP\\nPDT\\nEX\\nWP\\nRBR\\nPRP\\nVB\\nJJR\\nWRB\\nSYM\\nNN\\nRP\\nVBG\\nCC\\nCD\\nPOS Tags\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\nRelative Frequency\\nSentence level\\nLlama2-70b\\nGPT-4\\nHuman\\nPOS: Possessive ending\\nJJ: Adjective\\nJJS: Adjective, superlative\\nVBZ: Verb, 3rd person singular present\\nRBS: Adverb, superlative\\nNNS: Nouns, plural\\nTO: to\\nVBD: Verb, past tense\\nMD: Modal\\nVBP: Verb, non-3rd person singular present\\nFW: Foreign word\\nDT: Determiner\\nVBN: Verb, past participle\\nUH: Interjection\\nRB: Adverb\\nNNP: Proper noun, singular\\nPDT: Predeterminer\\nEX: Existential there\\nWP: Wh-pronoun\\nRBR: Adverb, comparative\\nPRP: Personal pronoun\\nVB: Verb, base form\\nJJR: Adjective, comparative\\nWRB: Wh-adverb\\nSYM: Symbol\\nNN: Noun, singular or mass\\nRP: Particle\\nVBG: Verb, gerund or present participle\\nCC: Coordinating conjunction\\nCD: Cardinal number\\nFigure 7: POS distribution of the MIXSET by NLTK (Bird et al., 2009).\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nLevenshtein distance\\n0\\n2\\n4\\n6\\n8\\nDensity\\nGPT-4 Revised\\nRewrite\\nPolish (token)\\nPolish (sentence)\\nComplete\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nLevenshtein distance\\n0\\n2\\n4\\n6\\n8\\nDensity\\nHuman Revised\\nHumanize (GPT-4)\\nHumanize (Llama2)\\nAdapt (token)\\nAdapt (sentence)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nLevenshtein distance\\n0\\n2\\n4\\n6\\nDensity\\nLlama2 Revised\\nRewrite\\nPolish (token)\\nPolish (sentence)\\nComplete\\nFigure 8: Levenshtein distance of the MixSet\\n6\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nSelf-BLEU score\\n0\\n2\\n4\\n6\\nDensity\\nLlama2 Revised\\nRewrite\\nPolish (Token)\\nPolish (Sentence)\\nComplete\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nSelf-BLEU score\\n0\\n2\\n4\\n6\\nDensity\\nGPT-4 Revised\\nRewrite\\nPolish (token)\\nPolish (sentence)\\nComplete\\n0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\nSelf-BLEU score\\n0\\n2\\n4\\n6\\nDensity\\nHuman Revised\\nHumanize (GPT-4)\\nHumanize (Llama2)\\nAdapt (Token)\\nAdapt (Sentence)\\nFigure 9: Self-BLEU score of the HWT, MGT, and MixSet.\\n17\\nModel-based Methods. In the era of Large Language Models (LLMs), (Guo et al., 2023) developed\\nthe ChatGPT Detector, which is based on a fine-tuned Roberta model. As for decoder-based or\\nencoder-decoder detectors, GPT-Sentinel (Chen et al., 2023) and RADAR (Hu et al., 2023), utilizing\\nT5-small (Raffel et al., 2020) and Vicuna-7B (Chiang et al., 2023) respectively, show convincing\\nresults when detecting MGT even in revised cases. Moreover, Verma et al. (2023) propose a novel\\ndetection framework called Ghostbuster, which employs passing documents through a series of\\nweaker language models. Using a small amount of training data, Guo and Yu (2023) leverages a\\nblack-box LLM to denoise input text with artificially added noise, and then semantically compares\\nthe denoised text with the original to determine if the content is machine-generated, leading a new\\nmethod for MGT detection.\\nHowever, it’s important to note that some researchers raise concerns about fine-tuning models for\\nMGT detection. Bakhtin et al. (2019) and Uchendu et al. (2020) have argued that fine-tuning models\\ncan lead to overfitting and a loss of generalization, particularly when dealing with text generated\\nby the latest LLMs. They highlight challenges posed by out-of-distribution editing texts, which can\\nundermine the effectiveness of pre-trained detectors, as demonstrated by Krishna et al. (2023).\\nOther detection methods. Watermarking imprints specific patterns of the LLM output text that\\ncan be detected by an algorithm while being imperceptible to humans. Kirchenbauer et al. (2023)\\ndeveloped watermarks for language modeling by adding a green list of tokens during sampling.\\nCurrently, Gu et al. (2023) introduces a learnable watermark by distilling LLM and watermark\\ntechnology into one student model, finding that models can learn to generate watermarked text with\\nhigh detectability.\\nIn retrieval-based methods, Krishna et al. (2023) introduce a method to retrieve semantically similar\\ngenerations and search a database of sequences previously generated by specific Large Language\\nModels (LLMs), looking for sequences that match the candidate text within a certain threshold.\\nDelving deeper, Wu et al. (2023) propose a model-specific detection tool called LLMDet, which can\\ndetect source text from specific LLMs by constructing a text collection dictionary for each LLM.\\nIn the in-context learning setting, Yu et al. (2023) introduced a straightforward method that analyzes\\nthe similarity between re-answering a question by generating a corresponding question in the context\\nof the given answer. Moreover, Koike et al. (2023b) employed a pure in-context learning approach\\nfor detection and found that LLMs are capable of distinguishing between human and machine styles.\\nA.2\\nDatasets for MGT Detection\\nPrevious studies have proposed many datasets of MGT, often accompanied by their newly proposed\\ndetectors (Verma et al., 2023; Chen et al., 2023). Guo et al. (2023) leverages multiple previous\\nQuestion-Answer (QA) datasets (Rajpurkar et al., 2016b; Koˇcisk`y et al., 2018; Jin et al., 2019; Lin\\net al., 2021), allowing ChatGPT to generate corresponding answers without explicit prompts. This\\napproach results in the creation of a comprehensive dataset comprising a large set of pairs of MGT\\nand HWT. Following the QA pattern, many researchers (Mitchell et al., 2023; Su et al., 2023; Hu\\net al., 2023; He et al., 2023) propose datasets with the MGT from variant mainstream LLMs (Du\\net al., 2022; Black et al., 2022; Anand et al., 2023; Conover et al., 2023; OpenAI, 2022, 2023b). Yu\\net al. (2023) only utilizes the answer section within the QA dataset (Hamborg et al., 2017; Möller\\net al., 2020) and employs ChatGPT to generate corresponding questions and re-answers.\\nHowever, these datasets typically consist of two distinct classes of texts, namely pure MGT or HWT,\\nwithout accounting for the potential mixcase. Furthermore, issues arise due to variations in prompts\\n(Koike et al., 2023a), sampling methods, and the inherent differences in length, style, and quality\\namong texts in some datasets (He et al., 2023). These variations challenge the generalization of\\nproposed detectors (Xu et al., 2023) and lie a huge diversity in distribution between the original and\\nrevised text(Ghosal et al., 2023). In some instances, MGT included in datasets may not undergo\\nthorough careful evaluation. Many noisy sentences are not filtered well in the datasets. For example,\\nsome sentences like Let me know if you have any other questions exist in the dataset, which will\\nimpact the effectiveness of the detectors (Guo et al., 2023).\\n18\\nB\\nDataset Details\\nB.1\\nConstruction Details\\nEight Human revised the MGT to mixcase. The MGT is revised by eight human experts with\\nprofessional English proficiency and costs them a total of 280 hours to complete this part of MixSet.\\nThe guidelines for human revision are shown in Figure 20. And the labeling screenshot are shown in\\nFigure 35.\\nB.2\\nOther Metrics in Evaluating MIXSET\\n• Self-BLEU Score: Self-BLEU is a metric used to assess the diversity of generated text. Generally,\\na lower Self-BLEU score indicates higher textual diversity. These results are shown in Figure 9.\\nOverall, the HWT shows greater diversity than MGT, and the Rewrite category has the highest\\ntextual diversity in the MixSet. The self-BLEU score of HWT, WGT, and mixcase is shown in\\nFigure 10 and 9.\\n• Levenshtein Distance: The Levenshtein distance (Levenshtein, 1966) is a metric for measuring\\nthe difference between two strings. We can observe in Figure 8 that in terms of the extent of\\nmodification, the rewrite operation results in the most significant alterations to the original texts,\\nfollowed by complete and sentence-level polish. Additionally, we observe that manual annotations\\nat both the token-level and sentence-level adaptation also exhibit a high degree of differentiation.\\n• POS distribution: POS distribution refers to the frequency and pattern of Part-of-Speech tags in a\\ntext, categorizing words into grammatical classes like nouns, verbs, and adjectives. This analysis is\\nkey for understanding the text’s syntactic structure and linguistic characteristics which is important\\nin NLP research fields.\\nFigure 10: Length distribution of the training datasets and the MixSet(left) and Self-BLEU score of\\nHWT and MGT(right).\\nC\\nDetailed Experiment Settings\\nFive Metric-Based detectors. We implement log-likelihood, entropy (Solaiman et al., 2019), Log-\\nRank (Mitchell et al., 2023), GLTR (Gehrmann et al., 2019) and DetectGPT (Mitchell et al., 2023)\\nas our metric-based detectors. Following He et al. (2023), we utilize GPT2-medium (Radford et al.,\\n2019) as the base model for metric-based methods in our experiments since it can already reach good\\nperformance with limited cost and computing time.\\nSeven Model-Based detectors. We implement seven Machine Generative Text (MGT) detectors,\\nencompassing both supervised and zero-shot settings. Firstly, we consider a robust closed-source\\nonline detector baseline: GPTZero (Tian, 2023). Secondly, we implement three open-source encoder-\\nbased detectors: OpenAI’s classifier (OpenAI, 2023a), Roberta-based classifier (Guo et al., 2023).\\nWe also implement GPT-Sentinel (Chen et al., 2023), RADAR (Hu et al., 2023), and Ghostwriter\\n(Verma et al., 2023) as strong baselines. We also finetune a pre-trained language model built by\\nIppolito et al. (2019) with an extra classification layer on top.\\nThree Evaluation Metrics Previous studies (Sadasivan et al., 2023; Mitchell et al., 2023) have proven\\nthe feasibility of using the Area Under The ROC Curve (AUROC) score for evaluating detection\\nalgorithm effectiveness. Given that most detectors can only give a predictive probability, we build a\\n19\\nlogistic regression model to provide concrete predictions, i.e., MGT or HWT, converting probability\\nto accuracy and f1-score as the two other metrics for our detection evaluation.\\nTraining set construction. We respectively select pure HWT and MGT for the train set from different\\ndatasets as illustrated in 1 and MGTBench (He et al., 2023), which is also the original dataset of\\nour MIXSET, convincing a small difference in data distribution. Firstly, we do data deduplication\\nand pre-process it to erase the Unicode or other special tokens like \\\\n\\\\n. Then, we select pieces of\\nsentences with a similar length distribution in our MIXSET, as illustrated in Figure 10. As we use\\naccuracy as our evaluation metric, we restrict the amount of HWT and MGT to be the same in our\\ndataset, as illustrated in Tabel 4.\\nTraining details. We employ the standard binary-classification loss function and the AdamW\\noptimizer (Loshchilov and Hutter, 2019), with an empirically determined learning rate. Specifically,\\nfor the Hello-Ai/Roberta-based model and the DistilBERT model, the learning rate is set to 5 × 10−7.\\nIn contrast, for Radar and GPT-Sentinel, the learning rates are 5 × 10−6 and 5 × 10−5 respectively.\\nEach supervised model undergoes training for three epochs on a dual-4090 server.\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\n1.00 1.00 0.97 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.73 0.91\\n0.98 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.60 0.82\\n0.90 0.98 0.99 0.98 0.99 0.98 0.99 0.99 0.99 0.99 0.61 0.81\\n0.97 1.00 0.99 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.60 0.85\\n0.98 1.00 0.97 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.75 0.92\\n0.99 1.00 0.98 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.66 0.88\\n0.89 0.96 0.99 0.96 0.99 0.98 0.99 0.99 1.00 1.00 0.48 0.81\\n0.96 1.00 0.97 0.99 1.00 1.00 0.99 1.00 1.00 1.00 0.54 0.83\\n0.76 0.86 0.84 0.87 0.92 0.90 0.90 0.94 1.00 1.00 0.13 0.56\\n0.74 0.86 0.85 0.87 0.94 0.90 0.92 0.94 1.00 1.00 0.26 0.69\\n0.94 0.97 0.96 0.98 0.99 0.98 0.98 0.99 1.00 1.00 1.00 0.98\\n0.90 0.96 0.95 0.96 1.00 0.97 0.99 0.99 1.00 1.00 1.00 1.00\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\n(a) The AUC Heatmap of Radar\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\nGPT-4 P.T.\\nGPT-4 P.S.\\nGPT-4 C.\\nGPT-4 R.\\nLlama2 P.S.\\nLlama2 P.T.\\nLlama2 C.\\nLlama2 R.\\nAdapt T.\\nAdapt S.\\nGPT-4 H.\\nLlama2 H.\\n0.49 0.37 0.60 0.36 0.59 0.53 0.87 0.69 0.80 0.80 0.45 0.66\\n0.49 0.37 0.60 0.35 0.59 0.53 0.86 0.68 0.80 0.80 0.43 0.65\\n0.50 0.38 0.61 0.37 0.60 0.54 0.87 0.69 0.81 0.80 0.45 0.66\\n0.48 0.37 0.59 0.36 0.59 0.53 0.87 0.69 0.81 0.80 0.44 0.65\\n0.48 0.38 0.61 0.37 0.60 0.53 0.87 0.69 0.81 0.80 0.44 0.65\\n0.48 0.36 0.60 0.34 0.58 0.53 0.87 0.67 0.80 0.79 0.44 0.66\\n0.47 0.37 0.60 0.36 0.59 0.53 0.87 0.69 0.81 0.80 0.44 0.66\\n0.49 0.37 0.60 0.36 0.59 0.54 0.87 0.69 0.81 0.80 0.44 0.66\\n0.50 0.36 0.60 0.35 0.60 0.54 0.84 0.69 0.80 0.80 0.44 0.66\\n0.48 0.35 0.60 0.34 0.58 0.53 0.86 0.68 0.81 0.80 0.43 0.66\\n0.47 0.35 0.59 0.33 0.56 0.51 0.85 0.66 0.79 0.77 0.55 0.70\\n0.47 0.36 0.58 0.34 0.57 0.52 0.86 0.67 0.80 0.79 0.46 0.68\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n(b) The AUC Heatmap of Roberta-based model\\nFigure 11: The AUC Heatmap of the other two detectors.\\nD\\nDetailed Experiment Results\\nAs for experiment 1, we put the detailed accuracy for different detectors in Table 6. In experiment\\n2, we also evaluate detectors with AUC metric, as shown in Table 9. We also post other detectors\\nundergo our experiment 3 illustrated in Figure 11. As for experiment 4, we evaluate detectors with\\naccuracy, precision, and recall metrics, as illustrated in Figure 12, 13, and 14.\\n20\\nTable 9: AUC of Experiment 2 (a). We underscore the best-performing detector and bold the score\\ngreater than 0.8, which we consider as a baseline threshold for detection. Above the thin horizontal\\nline is the metric-based detector, and below the horizontal line is the model-based detector (Tok.\\nstands for token level and Sen. stands for sentence level).\\nDetection Method\\nAI-Revised\\nHuman-Revised\\nRewrite\\nComplete\\nPolish-Tok.\\nPolish-Sen.\\nHumanize\\nAdapt-Tok.\\nAdapt-Sen.\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nLlama2\\nGPT-4\\nMetric-based Detector\\nLog-Rank\\n0.921\\n0.629\\n0.632\\n0.318\\n0.569\\n0.531\\n0.662\\n0.462\\n0.641\\n0.245\\n0.778\\n0.778\\nLog Likelihood\\n0.933\\n0.650\\n0.672\\n0.352\\n0.610\\n0.569\\n0.709\\n0.508\\n0.652\\n0.206\\n0.782\\n0.786\\nGLTR\\n0.870\\n0.504\\n0.546\\n0.268\\n0.511\\n0.466\\n0.602\\n0.345\\n0.595\\n0.208\\n0.764\\n0.768\\nDetectGPT\\n0.852\\n0.644\\n0.669\\n0.352\\n0.612\\n0.466\\n0.664\\n0.482\\n0.677\\n0.461\\n0.548\\n0.557\\nEntropy\\n0.814\\n0.581\\n0.662\\n0.463\\n0.656\\n0.636\\n0.686\\n0.596\\n0.580\\n0.185\\n0.733\\n0.730\\nModel-based Detector\\nOpenai Classifier\\n0.294\\n0.601\\n0.126\\n0.360\\n0.433\\n0.492\\n0.383\\n0.590\\n0.321\\n0.517\\n0.182\\n0.187\\nChatGPT Detector\\n0.706\\n0.399\\n0.874\\n0.640\\n0.567\\n0.508\\n0.617\\n0.410\\n0.679\\n0.483\\n0.818\\n0.813\\nRadar\\n0.992\\n0.994\\n0.997\\n0.999\\n0.998\\n0.986\\n0.998\\n1.000\\n0.984\\n0.984\\n0.999\\n0.999\\nGPT-Sentinel\\n0.994\\n0.992\\n0.987\\n0.993\\n0.995\\n0.964\\n0.992\\n0.996\\n0.915\\n0.953\\n0.958\\n0.986\\nDistillbert\\n0.756\\n0.856\\n0.746\\n0.859\\n0.790\\n0.730\\n0.791\\n0.856\\n0.416\\n0.330\\n0.837\\n0.861\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.50\\n0.52\\n0.54\\nAccuracy\\nLog-Likelihood\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.480\\n0.495\\n0.510\\nAccuracy\\nGLTR\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.50\\n0.52\\n0.54\\nAccuracy\\nDetectGPT\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.54\\n0.60\\n0.66\\n0.72\\nAccuracy\\nGPT-Sentinel\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.64\\n0.72\\n0.80\\n0.88\\nAccuracy\\nRadar\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.500\\n0.525\\n0.550\\nAccuracy\\nChatGPT Detector\\nNone of MixSet Train set\\n50.0% samples of MixSet Train set\\n100% samples of MixSet Train set\\nFigure 12: Analysis of the accuracy of various detectors across differing quantities of mixcase\\ninstances from MIXSET, as well as pure MGT and HWT.\\n21\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.7\\n0.8\\n0.9\\n1.0\\nRecall\\nLog-Likelihood\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.60\\n0.75\\n0.90\\nRecall\\nGLTR\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.7\\n0.8\\n0.9\\n1.0\\nRecall\\nDetectGPT\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.84\\n0.90\\n0.96\\nRecall\\nGPT-Sentinel\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.7\\n0.8\\n0.9\\n1.0\\nRecall\\nRadar\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.60\\n0.75\\n0.90\\nRecall\\nChatGPT Detector\\nNone of MixSet Train set\\n50.0% samples of MixSet Train set\\n100% samples of MixSet Train set\\nFigure 13: Analysis of the recall rate of various detectors across differing quantities of mixcase\\ninstances from MIXSET, as well as pure MGT and HWT.\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.50\\n0.51\\n0.52\\nPrecision\\nLog-Likelihood\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.46\\n0.48\\n0.50\\nPrecision\\nGLTR\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.495\\n0.510\\n0.525\\nPrecision\\nDetectGPT\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.52\\n0.56\\n0.60\\n0.64\\nPrecision\\nGPT-Sentinel\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.56\\n0.64\\n0.72\\n0.80\\nPrecision\\nRadar\\n2000\\n4000\\n6000\\n8000\\n10000\\nPure MGT and HWT\\n0.495\\n0.510\\n0.525\\n0.540\\nPrecision\\nChatGPT Detector\\nNone of MixSet Train set\\n50.0% samples of MixSet Train set\\n100% samples of MixSet Train set\\nFigure 14: Analysis of the precision rate of various detectors across differing quantities of mixcase\\ninstances from MIXSET, as well as pure MGT and HWT.\\n22\\nFigure 15: Prompt(①)-LLM complete the HWT\\nI have an incomplete text and need it to be completed. Please expand this\\ninto a complete text where the total word count, including the original text\\nI have provided, does not exceed 180 words. The original text must remain\\nexactly as is, with its format (such as capitalization and punctuation)\\nintact.\\nPlease do not modify any part of the original text.\\nHere’s the\\ntext: {HWT}\\nFigure 16: Prompt(②)-LLM polish HWT in token level\\nPlease carefully examine the following paragraph solely for spelling and\\ngrammatical errors, and replace any words that are repetitive, inaccurate,\\nor poorly chosen. It is crucial to avoid any changes to the sentence order\\nor structure.\\nThe focus should be strictly on the choice and usage of\\nindividual words to improve the clarity and appropriateness of the text,\\nwithout altering the original sentence construction: {HWT}\\nE\\nPrompt Template\\nWe show the prompt template of LLM’s operation including complete, polish (token-level and\\nsentence-level), rewrite, and humanize in Figure 15, Figure 16, Figure 17, Figure 18 and Figure 19.\\nF\\nCase study in MIXSET\\nWe selected two cases to show the comparison between the revised mixcase and the original text,\\nwhere the highlighted content represents the modified content. The HWT original text can be found\\nin figure 21, the AI revised text are shown in Figure 22, 23, 24, 25, 26, 27, 28, and 29. The MGT\\noriginal text can be found in Figure 30, and the Human revised text can be found in Figure 31, 32, 33,\\nand 34.\\nFigure 17: Prompt(③)-LLM polish HWT in sentence level\\nPlease optimize the sentences in the following paragraph to enhance fluency\\nand clarity. Do not alter the overall content or structure of the paragraph.\\nFocus on the construction and expression of the sentences, ensuring that\\nthe text is coherent and the information is accurate: {HWT}\\n23\\nFigure 18: Prompt(④)-LLM rewrite HWT\\nPlease extract the core ideas and keywords from the following English text,\\nand then rewrite a passage based on this information. The new text should\\nmaintain the essence of the original, with the word count varying by no\\nmore than 10% from the original. There’s no need to list the core ideas\\nand keywords. Here is the text that needs to be processed: {HWT}\\nFigure 19: Prompt(④)-LLM humanize MGT\\nI need to modify a machine-generated text to make it appear more like it\\nwas written by a human. The objective is to introduce elements commonly\\nfound in human-written texts. Here are some optional modifications you can\\nchoose to apply:\\n1. Introduce spelling errors or typos(optional)\\n2. Create grammatical errors, such as randomly adding or deleting words\\n(optional).\\n3.\\nInclude relevant but internet links, like blog posts or image links\\npertaining to the topic, you don’t have to use the real links so you can\\nfreely write one (optional).\\n4. Add relevant hashtags, for instance, #TopicKeyword #Location #Activity\\n(optional).\\n5. Use internet slang and abbreviations, e.g., ‘OMG’, ‘How r u’, ‘LOL’,\\n(optional).\\nPlease select any combination of these modifications to enhance the text’s\\nhuman-like quality. The aim is to simulate the imperfections and stylistic\\nchoices typical in casual human writing.\\nThe word count of the new text should not exceed 1.1 times that of the\\noriginal text.\\nYou should just give me the revised version without any other words.\\nEmojis are strictly prohibitive, so please ensure that it contains no\\nemojis.\\nHere is the machine-generated text:{HWT}\\n24\\nFigure 20: Guidelines for Human Revision\\nThe content under this document is generated by a large language model,\\nsuch as ChatGPT. You are required to revise it to make it closer to the\\nstyle of human-written text. You are responsible for the text under the\\nIDs xx-xx, and you need to make the following three types of modifications\\nto the content, generating three different sentences for each ID (each ID\\ncorresponds to 3 sentences):\\nThe document is in JSON format. You can choose to use code editors like\\nVisual Studio Code, or text editors like Notepad for reading and writing.\\n1. Adapt Token: You need to modify any words or phrases in this passage that\\nyou think are too rigid, mechanical, obscure, or unusual into vocabulary\\ntypical of human texts.\\nBe careful not to alter the sentence order or\\nstructure; only modify ‘words’ or ‘phrases.’\\n2.\\nAdapt Sentence: You need to revise any sentence structures that you\\nfind too mechanical or rigid to make them more in line with how humans\\ntypically write texts. This involves changes at the sentence level, which\\nmay include altering sentence order and structure.\\nNote:\\n1. After writing, please ensure to check that there are no ‘grammatical\\nerrors’ or ‘spelling mistakes’ in the text paragraphs.\\n2. Do not use ChatGPT or other large language models for data annotation,\\nas it will severely degrade the quality of the data.\\n3. You may use translation platforms like Youdao (you) or Google (goo); or\\nuse Grammarly(gra) to check for grammatical errors.\\nBelow are some examples you can refer to:\\n[Example 1]\\n[Example 2]\\n[Example 3]\\nFigure 21: Case Study of revision on HWT: Original HWT\\n[Original HWT] We present a new mixed variable symplectic (mvs) integrator\\nfor planetary systems, that fully resolve close encounters. The method is\\nbased on a time regularisation that allows keeping the stability properties\\nof the symplectic integrators, while also reducing the effective step size\\nwhenever two planets encounter. We use a high order mvs scheme such that\\nit is possible to integrate with large time steps far away from close\\nencounters. We show that this algorithm is able to resolve almost exact\\ncollisions (i.e. with a mutual separation of a fraction of the physical\\nradius) while using the same time-step as in weakly perturbed problems such\\nas the solar system. We demonstrate the long-term behavior on systems of\\nsix super-earths experiencing strong scattering for 50 kyr. We compare our\\nalgorithm to hybrid methods such as mercury and show that for an equivalent\\ncost, we obtain much better energy conservation.\\n25\\nFigure 22: Case Study of revision on HWT: GPT-4 Polish (Token)\\n[GPT-4 Polish (Token)] We present a new mixed-variable symplectic (MVS)\\nintegrator for planetary systems that fully resolves close encounters. The\\nmethod is grounded in a temporalregularization that maintains the stability\\nproperties of symplectic integrators while also diminishing the effective\\nstep size whenever two planets draw near each other. We employ a high-order\\nMVS scheme, enabling\\nintegration with extended time steps when distant\\nfrom close encounters. We demonstrate that this algorithm capably resolves\\nnearly exact collisions (i.e., with a mutual separation of a small fraction\\nof the physical radius) using a consistent time-step as in weakly perturbed\\nproblems such as the solar system. We exhibit the long-term dynamics on\\nsystems of six super-Earths undergoing intense scattering over 50 kyr. We\\ncontrast our algorithm with hybrid methods such as Mercury, revealing that,\\nfor a comparable cost, we achieve much improved energy conservation.\\nFigure 23: Case Study of revision on HWT: GPT-4 Polish (Sentence)\\n[GPT-4 Polish (Sentence)] We introduce a novel mixed variable symplectic\\n(MVS) integrator specifically designed for planetary systems that effi\\nciently manages close encounters.\\nThis method utilizes time regulariza\\ntion to maintain the stability characteristics inherent to symplectic\\nintegrators, while dynamically adjusting the effective step size during\\nplanetary encounters. Our high-order MVS approach enables integration at\\nlarge time steps when planets are distantly separated, thereby optimiz\\ning computational efficiency. The algorithm proves capable of resolving\\nnear-exact collisions—with mutual separations as minute as a fraction of\\nthe planets’ physical radii—without altering the time step used for more\\nstable configurations, such as within the solar system.\\nWe examine the\\nalgorithm’s long-term stability in simulations of six super-Earths under\\ngoing significant scattering over 50,000 years. Furthermore, we benchmark\\nour algorithm against hybrid methods like Mercury, demonstrating superior\\nenergy conservation at a comparable computational cost.\\n26\\nFigure 24: Case Study of revision on HWT: GPT-4 Complete\\n[GPT-4 Complete] We present a new mixed variable symplectic (mvs) integrator\\nfor planetary systems, that fully resolve close encounters. The method is\\nbased on a time regularisation that allows keeping the stability properties\\nof the symplectic integrators, while also reducing the effective step size\\nwhenever two planets encounter. We use a modified leapfrog scheme to address\\nthe issues of traditional integrators when handling near-collision scenar\\nios. The key improvement lies in an adaptive algorithm that dynamically\\nadjusts the step size according to the proximity of the planetary bodies.\\nAs a result, our integrator ensures high accuracy and energy conservation\\neven during complex dynamical interactions. Tests demonstrate that the mvs\\nintegrator significantly outperforms standard fixed-step symplectic meth\\nods in long-term stability and computational efficiency. This advancement\\nopens up new possibilities for accurately simulating the evolution of plane\\ntary systems over extended timescales, providing valuable insights into the\\ndynamical behavior of multi-body systems\\nand enhancing our understanding\\nof celestial mechanics.\\nFigure 25: Case Study of revision on HWT: GPT-4 Rewrite\\n[GPT-4 Rewrite] We introduce an advanced mixed variable symplectic (mvs) tai\\nlored for planetary systems, adept at handling close encounters precisely.\\nThis\\ntechnique\\nemploys\\ntime\\nregularization\\nto\\nmaintain\\nthe\\nstability\\ncharacteristic of symplectic integrators and intelligently reduces the\\neffective step size during planetary encounters. Employing a sophisticated\\nmvs algorithm, it facilitates integration over large time steps when planets\\nare not closely interacting.Our findings illustrate that the algorithm ef\\nfectively manages near-collision events, simulating scenarios with planet\\nseparations just fractions of their physical sizes, using consistent time\\nsteps applicable to less disturbed systems like the solar system. The inte\\ngrator’s performance excels in extended simulations, such as tracking six\\nsuper-earths in a dense scattering scenario for 50,000 years, showcasing\\nsuperior energy conservation when contrasted with hybrid algorithms like\\nthe mercury method, and doing so computational cost.\\n27\\nFigure 26: Case Study of revision on HWT: Llama2 Polish (Token)\\n[Llama2 Polish (Token)] We introduce a novel mixed variable symplectic\\n(MVS) integrator for planetary systems that accurately resolves close\\nencounters. Our method is built upon a time regularization that preserves\\nthe stability attributes of symplectic integrators while adaptively decreas\\ning the effective step size during planetary interactions. By employing a\\nhigh-order MVS scheme, we can integrate with large time steps in regions\\nwhere close encounters are not occurring. This algorithm enables the precise\\nresolution of nearly exact collisions (with a mutual separation of a small\\nfraction of the physical radius) using the same time step as in weakly\\nperturbed problems, such as the solar system. We demonstrate the long-term\\nbehavior of our algorithm on systems consisting of six super-Earths under\\ngoing intense scattering. Our approach shows superior energy conservation\\ncompared to hybrid methods like Mercury, while maintaining comparable com\\nputational efficiency.\\nFigure 27: Case Study of revision on HWT: Llama2 Polish (Sentence)\\n[Llama2 Polish (Sentence)] We introduce a novel mixed variable symplectic\\n(MVS) integrator for planetary systems that accurately resolves close\\nencounters.\\nOur method is founded on a time regularization technique\\nthat preserves the stability attributes of symplectic integrators while\\nadaptively decreasing the effective step size during planetary encounters.\\nBy employing a high-order MVS scheme, we can integrate with substantial\\ntime steps in regions unaffected by close encounters. Our algorithm effi\\nciently handles near-collisions(with a mutual separation of a few percent\\nof the physical radius) utilizing the same time-step as in weakly perturbed\\nscenarios, such as the solar system.\\nIn a comprehensive analysis, we\\ndemonstrate the long-term behavior of our method on systems consisting of\\nsix super-Earths undergoing intense scattering over 50 kyr. Our findings\\nindicate that, compared to hybrid approaches like Mercury, our algorithm\\nachieves superior energy. conservation at an equal computational cost\\nFigure 28: Case Study of revision on HWT: Llama2 Complete\\n[Llama2\\nComplete]\\nWe\\npresent\\na\\nnew\\nmixed\\nvariable\\nsymplectic\\n(MVS)\\nintegrator for planetary systems, that fully resolves close encounters.\\nThe method is based on a time regularization that allows keeping the\\nstability properties of the symplectic integrators, while also reducing\\nthe effective step size whenever two planets encounter. We use a simple and\\nefficient algorithm to detect close encounters, and switch to a smaller step\\nsize during these events to ensure accurate trajectory propagation. This\\napproach enables us to maintain the long-term accuracy of the simulation\\nwhile significantly improving computational efficiency. Our MVS integrator\\ncan handle both collisional and non-collisional close encounters, and we\\ndemonstrate its performance through several test cases involving Earth-Moon\\nand Jupiter-Io system simulations.\\n28\\nFigure 29: Case Study of revision on HWT: Llama2 Rewrite\\n[Llama2 Rewrite] We introduce a novel mixed variable symplectic (MVS)\\nintegrator for solving planetary systems that undergo close encounters.\\nOur approach combines the benefits of symplectic integrators - stability\\nand accuracy - with the ability to adaptively adjust the time step size\\nduring close encounters. This enables us to capture near-collisions with\\nhigh precision while still utilizing large time steps in stable regions.\\nWe validate our algorithm’s performance by simulating systems of six super-\\nEarths over 50,000 years, demonstrating its superiority compared to hybrid\\nmethods like Mercury in terms of energy conservation, while maintaining\\ncomparable computational efficiency.\\nFigure 30: Case Study of revision on MGT: Original MGT\\n[Original MGT] The first coffee was brewed by the ancient egyptians over\\n4,000 years ago. As of 2023, coffee consumption increased 70-fold in the\\npast 100 years. Today, 98% of americans drink coffee. Studies have shown\\nthat drinking two cups of coffee per day is beneficial for adults. However,\\nmore and more studies also show that drinking one or less coffee is also\\nequally beneficial. Drinking coffee has also been linked to higher memory,\\nalertness, and focus. Overall, coffee is good for your health.\\nFigure 31: Case Study of revision on MGT: GPT-4 Humanize\\n[GPT-4 Humanize] OMG did you kno that, like, the first cuppa joe was whipped\\nup by ancient Egyptions like more than 4,000 yrs ago? Fast fwd to 2023,\\nand whoa, coffee drinking’s shot up a whoppin’ 70 times in the last cen\\ntury! Nowadays, it’s like 98% of Americans are sippin’ on coffee. Some\\nsmarty-pants studies point out that knockin’ back two cups a day’s pretty\\ngood for grown-ups. But hold up, more research is spillin’ the beans that\\nmaybe just one cup or even less can do the trick too. Crazy, right? Srsly\\ntho, chuggin’ coffee’s got a rep for boostin’ your memory and makin’ ya\\nmore alert and focused. All in all, java’s a thumbs up for your health.\\n#CoffeeLovers #HealthBenefits #StayAlert\\nBTW, if ya wanna dive deeper into the whole coffee vibe, there’s this killer\\nblog post over at \"www.totallyrealcoffeeblog.com/history\" that’s got the\\nlow-down on coffee’s past. And, like, for the visual folks out there, check\\nout these pics on \"www.picturesofcoffeeandstuff.com\" – they’ve got some\\nepic shots of coffee that’ll make you wanna brew a pot, like, right now.\\nLOL\\n29\\nFigure 32: Case Study of revision on MGT: Llama2 Humanize\\n[Llama2 Humanize] Did you know that the ancient Egyptians were the first\\nto\\nbrew coffee over 4,000 years ago? Crazy, right? And get this - coffee\\nconsumption has increased a whopping 70-fold in just the past 100 years!\\n98% of Americans can’t start their day without their daily cup (or two) of\\njoe. But here’s the thing: while most people assume that downing multiple\\ncups a day is the way to go, studies actually suggest that even just one cup\\ncan do the trick. Yep, you heard that right - one measly cup can boost your\\nmemory, alertness, and focus. Talk about a productivity hack! Of course,\\nthere’s always the risk of overdoing it, but hey, everything in moderation,\\nright? Bottom line: coffee = good stuff.\\nFigure 33: Case Study of revision on MGT: Adapt Token\\n[Adapt Token] The first coffee was brewed by the ancient Egyptians\\nover\\n4,000 years ago. By 2023, coffee consumption has increased 70-fold in the\\npast 100 years.\\nNowadays, 98% of Americans drink coffee.\\nStudies have\\nshown that drinking two cups of coffee every day is beneficial for adults.\\nAdditionally, more and more studies also show that drinking one or less\\ncoffee is also equally beneficial. Drinking coffee is also linked to better\\nmemory, alertness, and concentration.\\nOverall, coffee is good for your\\nhealth.\\nFigure 34: Case Study of revision on MGT: Adapt Sentence\\n[Adapt Sentence] The first coffee was brewed by the ancient Egyptians over\\n4,000 years ago. Coffee consumption has increased 70-fold in the past 100\\nyears, along with 98% of Americans drinking coffee, according to the data\\nup\\nto 2023. Studies have shown that drinking two cups of coffee every day\\nis beneficial for adults, while other studies indicate that drinking one or\\nfewer cups of coffee is also equally beneficial. Drinking coffee is also\\nlinked to better memory, alertness, and concentration. Overall, coffee is\\ngood for your health.\\nFigure 35: screenshot of human labeling\\n30\\n'}, 'http://arxiv.org/abs/2401.05940v1': {'title': 'Mutation-based Consistency Testing for Evaluating the Code Understanding\\n  Capability of LLMs', 'published_date': datetime.datetime(2024, 1, 11, 14, 27, 43), 'pdf_link': 'http://arxiv.org/pdf/2401.05940v1', 'summary': 'Large Language Models (LLMs) have shown remarkable capabilities in processing\\nboth natural and programming languages, which have enabled various applications\\nin software engineering, such as requirement engineering, code generation, and\\nsoftware testing. However, existing code generation benchmarks do not\\nnecessarily assess the code understanding performance of LLMs, especially for\\nthe subtle inconsistencies that may arise between code and its semantics\\ndescribed in natural language.\\n  In this paper, we propose a novel method to systematically assess the code\\nunderstanding performance of LLMs, particularly focusing on subtle differences\\nbetween code and its descriptions, by introducing code mutations to existing\\ncode generation datasets. Code mutations are small changes that alter the\\nsemantics of the original code, creating a mismatch with the natural language\\ndescription. We apply different types of code mutations, such as operator\\nreplacement and statement deletion, to generate inconsistent code-description\\npairs. We then use these pairs to test the ability of LLMs to correctly detect\\nthe inconsistencies.\\n  We propose a new LLM testing method, called Mutation-based Consistency\\nTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and\\nGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which\\nconsists of six programming languages (Python, C++, Java, Go, JavaScript, and\\nRust). We compare the performance of the LLMs across different types of code\\nmutations and programming languages and analyze the results. We find that the\\nLLMs show significant variation in their code understanding performance and\\nthat they have different strengths and weaknesses depending on the mutation\\ntype and language.', 'pdf_text': 'Mutation-based Consistency Testing for Evaluating the Code\\nUnderstanding Capability of LLMs\\nZiyu Li\\nzli311@sheffield.ac.uk\\nUniversity of Sheffield\\nSheffield, UK\\nDonghwan Shin\\nd.shin@sheffield.ac.uk\\nUniversity of Sheffield\\nSheffield, UK\\nABSTRACT\\nLarge Language Models (LLMs) have shown remarkable capabilities\\nin processing both natural and programming languages, which\\nhave enabled various applications in software engineering, such\\nas requirement engineering, code generation, and software testing.\\nHowever, existing code generation benchmarks do not necessarily\\nassess the code understanding performance of LLMs, especially\\nfor the subtle inconsistencies that may arise between code and its\\nsemantics described in natural language.\\nIn this paper, we propose a novel method to systematically as-\\nsess the code understanding performance of LLMs, particularly\\nfocusing on subtle differences between code and its descriptions,\\nby introducing code mutations to existing code generation datasets.\\nCode mutations are small changes that alter the semantics of the\\noriginal code, creating a mismatch with the natural language de-\\nscription. We apply different types of code mutations, such as oper-\\nator replacement and statement deletion, to generate inconsistent\\ncode-description pairs. We then use these pairs to test the ability of\\nLLMs to correctly detect the inconsistencies.\\nWe propose a new LLM testing method, called Mutation-based\\nConsistency Testing (MCT), and conduct a case study on the two\\npopular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code\\ngeneration benchmark, HumanEval-X, which consists of six pro-\\ngramming languages (Python, C++, Java, Go, JavaScript, and Rust).\\nWe compare the performance of the LLMs across different types\\nof code mutations and programming languages and analyze the re-\\nsults. We find that the LLMs show significant variation in their code\\nunderstanding performance and that they have different strengths\\nand weaknesses depending on the mutation type and language.\\nWe further explain conditions under which the LLMs result in cor-\\nrect answers using input characteristics (e.g., number of tokens)\\nand investigate to what extent the test results can be improved\\nusing one-shot prompts (i.e., providing an additional example). Our\\nMCT method and the case study results provide valuable implica-\\ntions for future research and development of LLM-based software\\nengineering.\\nCCS CONCEPTS\\n• Software and its engineering → Software testing and debug-\\nging; Empirical software validation.\\nKEYWORDS\\nLarge Language Models, Software Engineering, Mutation Analysis\\n1\\nINTRODUCTION\\nWith the recent advances in Large Language Models (LLMs), which\\ncan deal with even code comprehension and generation, LLM-based\\nSoftware Engineering (SE) [9, 13] has emerged to address various\\nsoftware engineering problems, ranging from requirement engi-\\nneering to software testing. Considering the capabilities of LLMs\\nin processing both natural languages and programming languages,\\nit is not surprising that the number of LLM-based SE papers has\\ngrown dramatically over the last few years (from 11 papers in 2021\\nto 160 papers in the first half of 2023) [13].\\nLLMs have also been actively used in software development\\npractice. LLM-based code completion tools, such as Copilot, have\\nbeen integrated into popular IDEs, such as Visual Studio Code and\\nPyCharm. More LLM-based tools for code completion, code gen-\\neration, and code interpretation are coming, such as TabNine [15],\\nCodex [10, 29], and Figstack [14]. A recent survey from GitHub [34]\\nshowed that 92% of U.S.-based developers in large companies ad-\\nmit using AI tools, and 70% say it to be useful. Such increasing\\nattention to LLM-based SE from both researchers and practitioners\\ncalls for more rigorous and systematic testing of the programming\\ncapabilities of LLMs.\\nNotable examples in the early stage of LLM testing are code\\ngeneration benchmarks, such as HumanEval [5] and HumanEval-\\nX [46]. They evaluate whether LLMs can generate a correct program\\n(e.g., function) from a natural language description (e.g., docstring),\\nfocusing on the code writing rather than the code reading capability\\nof LLMs. Recently, Ma et al. [20] investigated the code syntax and\\nsemantics understanding capability of LLMs based on Abstract Syn-\\ntax Tree (AST) and Control Flow Graph (CFG). However, correctly\\nunderstanding code structures does not necessarily mean that the\\nsemantics of programs are correctly understood. For instance, two\\nsimple code fragments ‘a=x+y’ and ‘a=x-y’ have the same AST\\nstructure but totally different semantics.\\nIn this paper, we aim to systematically evaluate the programming\\ncapability of LLMs, particularly in terms of identifying subtle in-\\nconsistencies between code and its corresponding semantics (given\\nas natural language descriptions). To achieve this, we present a\\nnew LLM testing method, called Mutation-based Consistency Test-\\ning (MCT) for LLMs. The key idea behind MCT is to apply code\\nmutations to the correct programs provided in code generation\\nbenchmarks to create subtle inconsistencies between the code and\\nits description. Using programming mutation operators widely stud-\\nied in mutation analysis [12, 27, 28], we can systematically generate\\nvarious semantic inconsistencies. Furthermore, by analyzing the\\nincorrect answers of LLMs based on mutation operators applied,\\nwe can provide more in-depth analysis results on the strengths and\\nweaknesses of LLMs in understanding the code semantics.\\nWe conducted a case study on the two most popular LLMs,\\nGPT-3.5 and GPT-4, based on a state-of-the-art code generation\\narXiv:2401.05940v1  [cs.SE]  11 Jan 2024\\nLi and Shin\\nbenchmark, HumanEval-X [46], which consists of six program-\\nming languages (Python, C++, Java, Go, JavaScript, and Rust). We\\ninvestigated how different mutation operators and programming\\nlanguages influence the performance of these LLMs. Additionally,\\nwe examined whether the LLMs’ understanding varies based on\\nthe characteristics of the prompt given, such as the number of\\nlines and the position of the mutated part in the code. Furthermore,\\nwe assessed to what extent one-shot prompting (i.e., providing\\nan example in the input prompt) could improve the MCT results\\ncompared to zero-shot prompting (i.e., providing no examples).\\nOur case study results show that GPT-4 significantly outper-\\nforms GPT-3.5, regardless of mutation operators and programming\\nlanguages, although even GPT-4 shows relative weaknesses in re-\\nlational logic and Java programs. However, the performance of\\nGPT-3.5 could be dramatically improved using one-shot prompts\\ninstead of zero-shot prompts. The results also show that we can\\nidentify certain conditions (in terms of the mutation operators used\\nand the number of tokens in the code) under which the LLMs result\\nin correct or incorrect answers.\\nThe contributions of this paper are summarized as follows:\\n• We present Mutation-based Consistency Testing (MCT), a new\\nmethod for evaluating LLMs’ code understanding capability (par-\\nticularly subtle inconsistencies between code and its semantics).\\n• We present a case study demonstrating the applicability of MCT\\non GPT-3.5 and GPT-4.\\n• We investigate the strengths and weaknesses of GPT-3.5 and\\nGPT-4 in terms of code semantic inconsistency identification,\\nwhich can be used for future LLM-based SE studies.\\n• We will provide our replication package publicly available (see\\nSection 6).\\nThe significance of our paper is as follows. As LLM-based SE\\ncontinue to gain more attention, evaluating them under a more sys-\\ntematic and generalized approach becomes indispensable, assessing\\nnot just their code generation but also their syntactic understanding\\nand consistency. Using MCT, both researchers and practitioners\\ncan easily test their LLMs, thereby paving the way for more robust\\nand reliable LLMs in the future.\\nThe rest of the paper is organized as follows. Section 2 provides\\nthe essential basics, including LLMs and mutation analysis. Sec-\\ntion 3 presents our proposed method. Section 4 presents our case\\nstudy design. Section 5 shows case study results. Section 6 describes\\ndata availability. Section 7 reviews the relevant literature. Section 8\\nconcludes this paper and presents future research directions.\\n2\\nBACKGROUND\\n2.1\\nLarge Language Models\\nLarge Language Models (LLMs) are advanced Deep Learning (DL)\\nsystems designed to understand, generate, and interact with natural\\nlanguages. Typically, LLMs are trained on vast datasets, encompass-\\ning a wide range of structured data, which enables them to grasp\\nthe nuances of language and context. Just as natural language sen-\\ntences are structured data, code, as another kind of structured data\\nis also used to train LLMs, enabling them to comprehend the syn-\\ntax, semantics, and patterns in programs. This development marks\\na significant improvement in LLM-relevant research, extending\\nTask ID:\\nPython 53\\nProblem:\\ndef add(x: int , y: int):\\n\"\"\"\\nAdd two numbers x and y\\n>>> add(2, 3)\\n5\\n\"\"\"\\nCanonical Solution:\\ndef add(x: int , y: int):\\nreturn x + y\\nTest inputs:\\nassert add(0, 1) == 1\\nassert add(1, 0) == 1\\n...\\nFigure 1: An example problem, canonical solution, and test\\ninputs in HumanEval [5]\\nthe capabilities of LLMs beyond natural language to the realm of\\nsoftware development.\\nGenerative Pre-trained Transformer (GPT) developed by OpenAI\\nis one of the representative examples of successfully and widely\\nused LLMs in recent years. The original GPT model [30] laid the\\nfoundation for subsequent developments in the field. It was fol-\\nlowed by GPT-2 [31], an advanced version with a larger number of\\nparameters, renowned for its enhanced text generation capabilities.\\nThe series progressed with GPT-3 [3], a model enriched with 175\\nbillion parameters, which demonstrated remarkable advancements\\nin few-shot and zero-shot learning, significantly improving natu-\\nral language understanding. The latest in this series, GPT-4 [23],\\nfurther expanded the capabilities of its predecessors, offering more\\nparameters and enhanced reliability, particularly in visual under-\\nstanding and reasoning. Furthermore, GPT-based models, such as\\nCodeGPT [19] and Codex [5] trained on extensive open-source\\ncode repositories, have significantly aided developers in various\\ntasks [9, 40].\\nThe advancement of programming-capable LLMs has been sig-\\nnificantly accelerated by the introduction of code generation bench-\\nmarks, such as HumanEval [5], MBPP (MassiveBank of Python\\nProblems) [2], and CodeXGLUE [19]. These benchmark datasets\\ncontain programming tasks (problems), covering multiple aspects\\nsuch as language basics, algorithms, and mathematics. Each prob-\\nlem is accompanied by a canonical solution and a specific set of\\ntest inputs. The program generated by the LLM under test is then\\ncompared with the canonical solution by running both on the given\\ntest inputs. The LLM-generated program is considered correct only\\nif its outputs match exactly with the outputs of the canonical solu-\\ntion for all the test inputs. Figure 1 shows an example problem in\\nHumanEval.\\nMutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs\\nOriginal Program:\\nif (a and b) {\\nc = 1\\n} else {\\nc = 0\\n}\\nMutated Program:\\nif (a or b) {\\nc = 1\\n} else {\\nc = 0\\n}\\nFigure 2: A simplified mutant generation example. The log-\\nical operator ‘and’ in the original program (left) has been\\nmodified to ‘or’ in the mutant (right).\\n2.2\\nPrompt Engineering\\nPrompt engineering, a crucial element in the application of large\\nlanguage models (LLMs), has been extensively explored in recent re-\\nsearch. Wei et al. [41] introduce the ’Chain-of-Thought’ prompting\\nmethod, where prompts are crafted to elicit a sequential reasoning\\nprocess from LLMs, thereby enhancing their performance on com-\\nplex tasks. Complementing this, Shin et al. [36] present AutoPrompt,\\nwhich uses gradient-based techniques for automatic prompt gener-\\nation to effectively extract specific responses from language models.\\nAdditionally, Brown et al. [3] explore the inherent capabilities of\\nLLMs in their paper, particularly focusing on the emergent abilities\\nas these models scale up in size and complexity.\\nIntegration of language models with tree search algorithms, pro-\\nposed by Zhou et al. [47], enhances reasoning, acting, and planning\\ncapabilities in LLMs. In the context of programming, Muennighoff\\net al. [22] investigate instruction tuning for large language models\\nin their study, aiming to improve LLMs’ performance in under-\\nstanding and generating code. Reynolds and McDonell [32] offer\\nnew insights into effective interaction with LLMs, moving beyond\\ntraditional few-shot learning approaches. These studies collectively\\nunderscore the dynamic nature of prompt engineering and its criti-\\ncal role in leveraging the full capabilities of LLMs in a variety of\\ncomplex applications.\\n2.3\\nMutation Analysis\\nMutation analysis is a software testing method that seeks to eval-\\nuate the efficacy of a test suite by introducing small, systematic\\nchanges to the source code of a program. These intentional modifi-\\ncations, or mutations, result in a series of slightly altered versions\\nof the program, i.e., a “mutant”. If a test suite can detect these inten-\\ntional faults, it’s more likely to detect unintentional ones already\\npresent in the code. However, if it cannot detect intentional faults,\\nit indicates potential weaknesses in the testing dataset. Figure 2\\nillustrates a simple mutant example.\\nThis mutation analysis concept has been explored and expanded\\nin many papers. For instance, [16] provide a comprehensive survey\\nof the field, discussing its development, applications, and challenges.\\n[7] offer a systematic review of mutation testing tools in their paper,\\ncomparing features and capabilities of various tools in this domain.\\n[26] provide an updated survey of advances in mutation testing,\\nincluding recent developments. [45] discuss the use of mutation\\ntesting in improving fault localization, while [43] discusses the\\nchallenges and potential of mutation testing at the turn of the\\ncentury. Finally, [17] investigate the effectiveness of mutants as\\nsubstitutes for real faults in software testing.\\n3\\nAPPROACH\\nWe aim to systematically evaluate the code understanding capa-\\nbility of LLMs, particularly in terms of detecting subtle inconsis-\\ntencies between code (written in programming languages) and\\nits description (written in natural language). To achieve this, we\\npresent Mutation-based Consistency Testing (MCT). The key idea\\nbehind the MCT approach is to deliberately inject artificial inconsis-\\ntencies between code and its description using program mutation,\\nwhich can simulate potential bugs that LLMs might encounter in\\nreal-world scenarios.\\nAlgorithm 1 shows the pseudocode of our MCT approach. It\\ntakes as input the LLM under test L, a set of consistent data 𝐷 =\\n{(des, imp), . . . } where (des, imp) is a pair of a program description\\n(i.e., des) and its corresponding implementation (i.e., imp), and a set\\nof mutation operators 𝑈 applicable to the implementations in 𝐷.\\nThe algorithm then returns an MCT score 𝑠 that ranges from 0 to\\n100, with larger values indicating better results.\\nNotice that 𝐷 is directly available from existing code generation\\nbenchmarks. Also, obtaining 𝑈 is straightforward with existing\\nmutation analysis studies across various programming languages.\\nAlgorithm 1: Mutation-based Consistency Testing (MCT) for\\nLLMs\\nInput\\n:LLM under Test L,\\nSet of Consistent Pairs 𝐷 = {(des, imp), . . . },\\nSet of Mutation Operators 𝑈\\nOutput:MCT Score 𝑠\\n1 Set of Passed Mutated Data 𝑃 ← ∅\\n2 Set of Failed Mutated Data 𝐹 ← ∅\\n3 foreach Consistent Pair (des, imp) ∈ 𝐷 do\\n4\\nif isCorrect(L, des, imp) then\\n5\\nSet of Mutants 𝑀 ← genMutants(imp,𝑈 )\\n6\\nforeach Mutant 𝑚 ∈ 𝑀 do\\n7\\nif isCorrect(L, desc,𝑚) then\\n8\\n𝑃 ← 𝑃 ∪ {(des,𝑚)}\\n9\\nelse\\n10\\n𝐹 ← 𝐹 ∪ {(des,𝑚)}\\n11 MCT Score 𝑠 ←\\n|𝑃 |\\n|𝑃 |+|𝐹 | × 100\\n12 return 𝑠\\nThe algorithm first initializes two sets: passed mutated data 𝑃\\n(line 1) and failed mutated data 𝐹 (line 2). For each pair (des, imp) ∈\\n𝐷 (line 3), the algorithm checks if L correctly answers “consistent”\\nfor the given des and imp (line 4). Section 3.1 details the prompt used\\nto execute L using des and imp. The algorithm enters the main part\\nof MCT (lines 5–10) only if L correctly answers. This ensures that\\nthe MCT score is computed purely based on the initially identified\\nconsistent pairs by L. In the main part of MCT, the algorithm first\\ngenerates a set of mutants 𝑀 from the given code imp using the\\nmutation operators 𝑈 . Section 3.2 details the mutant generation\\nprocess. For each mutant 𝑚 ∈ 𝑀 (line 6), the algorithm checks if\\nL correctly answers “inconsistent” for the given des and 𝑚 (line 7).\\nLi and Shin\\nI\\'m presenting you with a program\\ndescription and its corresponding code.\\nDescription: {DESCRIPTION HERE}\\nCode: {CODE HERE}\\nEvaluate the code based on the description.\\nRespond with:\\n1 - If the code functionally and syntactically\\nmatches the description completely without any\\nminor inconsistency.\\n2 - Otherwise.\\nPlease respond only with a single number:\\n\\'1\\' or \\'2\\'.\\nFigure 3: Zero-shot prompt template with {DESCRIPTION}\\nand {CODE} as placeholders for corresponding artifacts\\nIf it is correct, then the pair of des and 𝑚 is added to 𝑃 (line 8);\\notherwise, the pair is added to 𝐹 (line 10). At the end, the algorithm\\nends by returning the MCT score 𝑠 =\\n|𝑃 |\\n|𝑃 |+|𝐹 | × 100 (lines 11–12).\\n3.1\\nPrompting\\nPrompt engineering, as introduced in Section 2.2, is important as\\nthe design of prompts can significantly affect the output of LLMs.\\nInspired by existing methods used for testing LLMs on the Hu-\\nmanEval Dataset [5], we carefully crafted a prompt considering\\nthree essential aspects: (1) clear task definition, (2) structured in-\\nput, and (3) explicit evaluation criteria. Figure 3 shows the prompt\\ntemplate, with {DESCRIPTION} and {CODE} as placeholders for\\ncorresponding artifacts.\\nIn the prompt template, we can see that (1) the task definition is\\nprovided clearly in the first part, (2) the code and its description are\\npresented separately in a structured manner, and (3) the evaluation\\ncriteria are explicitly stated at the end.\\nOne-shot prompting is a common few-shot technique, which\\nprovides the LLM with a single example to guide its response. As\\ndiscussed in Section 2.1, it helps the LLM to better understand the\\ncontext and the specific requirements of the task. Figure 4 shows the\\nexample part to be appended at the end of the zero-shot prompt (see\\nFigure 3). The example serves as a reference for the LLM, illustrating\\nhow to analyze and respond to the task. By including this example,\\nwe could align the model’s response mechanism more closely with\\nthe desired output, thereby enhancing the accuracy and relevance\\nof its answers. More experiments will follow in Section 4.\\n3.2\\nMutant Generation\\nTo introduce inconsistencies between a description (des) and its\\nimplementation (imp), we systematically generate many mutants\\nfrom imp. As discussed in Section 2.3, one can simply generate\\nmany subtle mutants by applying pre-defined mutation operators\\nas much as possible.\\nAlthough the process of generating mutations seems straight-\\nforward, it can sometimes produce mutants that are semantically\\nequivalent to the original program. Since there is no inconsistency\\nbetween a description and the equivalent mutant of its original pro-\\ngram, we should avoid generating equivalent mutants. However,\\nHere is an example for you. When given this pair:\\nDescription:\\nfrom typing import List\\\\n\\\\n\\\\ndef\\nhas_close_elements(numbers: List[float],\\nthreshold: float) -> bool:\\n\"\"\" Check if in given list of numbers , are any\\ntwo numbers closer to each other than given\\nthreshold.\\n>>> has_close_elements ([1.0 , 2.0, 3.0], 0.5)\\nFalse\\n>>> has_close_elements ([1.0 , 2.8, 3.0, 4.0,\\n5.0, 2.0], 0.3)\\nTrue\\n\"\"\"\\nCode:\\nfor idx , elem in enumerate(numbers):\\nfor idx2 , elem2 in enumerate(numbers):\\nif idx != idx2:\\ndistance = abs(elem * elem2)\\nif distance < threshold:\\nreturn True\\nreturn False\\nThe code did not match the description, the code should be:\\nfor idx , elem in enumerate(numbers):\\nfor idx2 , elem2 in enumerate(numbers):\\nif idx != idx2:\\ndistance = abs(elem - elem2)\\nif distance < threshold:\\nreturn True\\nreturn False\\nSo the correct answer is: 2\\nFigure 4: The additional input for one-shot prompts. This\\npart is appended at the end of the zero-shot prompt template\\n(Figure 3) to create one-shot prompts.\\nthe equivalent mutant detection problem is known to be unde-\\ncidable [4]. Nevertheless, this can be addressed by selecting mu-\\ntation operators carefully to reduce the possibility of generating\\nequivalent mutants [6]. Furthermore, we can leverage a rich set of\\nequivalent mutant detection studies [21, 25].\\nAnother potential issue is the sheer number of mutants. Often-\\ntimes, the total number of mutants would be too much to deal\\nwith considering time and cost. Notice that mutant execution (us-\\ning LLMs) is both time- and cost-intensive, not mutant generation.\\nTherefore, one typical solution to this issue is a random sampling\\nof mutants to execute after generation [11, 44]. In other words, we\\ncan generate all possible mutants at first and then randomly sample\\nsome of them considering the available budget.\\nMutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs\\n4\\nCASE STUDY DESIGN\\nIn this section, we describe a case study to demonstrate the ap-\\nplicability and usefulness of our mutation-based consistency test-\\ning approach. Although our approach is relatively intuitive and\\nstraightforward, it is unclear how useful the approach is in practice.\\nTherefore, we apply our approach to real-world LLMs with the aim\\nof demonstrating the application of our approach. To this end, we\\ndraw the following research questions:\\nRQ1 How do different LLMs fare in terms of mutation-based con-\\nsistency testing for different mutation operators?\\nRQ2 How do different LLMs fare in terms of mutation-based con-\\nsistency testing for different programming languages?\\nRQ3 Can we explain the consistency testing results of LLMs in\\nterms of input characteristics?\\nRQ4 How do the consistency testing results of LLMs vary with\\nzero-shot and one-shot promptings?\\nRQ1 aims to evaluate the ability of different LLMs to determine\\nthe consistency between code and its description while considering\\nthe influence of different mutation operators applied to the code. It\\nspecifically focuses on two of the most commonly used LLMs, GPT-\\n3.5 [30] and GPT-4 [23]. The answer to RQ1 will help us understand\\nwhen LLMs detect consistency between code and its description\\ncorrectly.\\nSimilar to RQ1, RQ2 also aims to test GPT-3.5 and GPT-4 using\\nour approach. However, instead of focusing on mutation opera-\\ntors, RQ2 seeks to determine for which programming language\\nthe LLMs work well and for which they do not. The outcomes of\\nRQ2 will provide insights into the testing performance of different\\nprogramming languages.\\nIn addition to RQ1 and RQ2, RQ3 takes a step further to explain\\nunder which conditions the LLMs correctly detect inconsistencies\\nbetween code and its description. For instance, it might be the\\ncase that GPT-3.5 does not perform well when the code provided\\nis quite lengthy. By examining these conditions in terms of input\\ncharacteristics such as lines of code, programming language, mu-\\ntation operator, and mutation position in the code, we can better\\nunderstand the strengths and limitations of the LLMs.\\nRQ4 investigates the variation in the consistency testing results\\nof LLMs when given no examples (zero-shot prompting) versus a\\nsingle example (one-shot prompting). The findings from RQ4 will\\ndiscover if minimal contextual information can greatly improve the\\nmodel’s ability to understand and analyze code, and will provide a\\ndeeper understanding of the capabilities and limitations of LLMs\\nin processing complex programming tasks, and provide valuable\\ninsights for developers and researchers in optimizing the use of\\nLLMs for code analysis.\\n4.1\\nDatasets\\nWe used HumanEval-X [46], an extension of the widely used open-\\nsource dataset HumanEval [5], as our dataset.\\nThe HumanEval dataset is a hand-written evaluation set used\\nto assess code generation ability for LLMs proposed by OpenAI.\\nIt contains 164 Python programming tasks (see Figure 1 for an\\nexample task). Although HumanEval has been widely used to mea-\\nsure the (behavioral) correctness of LLM-generated code, it only\\ncontains Python programming tasks. HumanEval-X extends Hu-\\nmanEval with six programming languages: Python, C++, Java, Go,\\nJavaScript (JS), and Rust. Given its expanded programming lan-\\nguages, HumanEval-X offers a more comprehensive benchmark,\\nmaking it suitable for evaluating the latest and more advanced Code\\nLLMs, including models like CodeGeeX [46] they proposed.\\n4.2\\nSubject LLMs\\nWe chose GPT-3.5 [30] and GPT-4 [23] as our test subjects. Specifi-\\ncally, we used the stable version of both models, GPT-3.5-0613 and\\nGPT-4-0613, using OpenAI APIs [24].\\nGPT-3.5 [30] is one of the most popular LLMs since it was first\\nintroduced in early 2022, particularly because ChatGPT is based on\\nthe GPT-3.5 series. In addition to its remarkable natural language\\nprocessing capability, it achieves the HumanEval score of 47.1%,\\nmeaning it generates correct solutions for 47.1% of all programming\\ntasks in HumanEval. This implies that GPT-3.5 is capable of code\\ngeneration.\\nGPT-4 [23], a successor of GPT-3.5, is one of the most powerful\\nLLMs at the time of writing this paper. Specifically, it achieves the\\nHumanEval score of 67.0%, which is human-level performance. By\\ncomparing and contrasting GPT-3.5 and GPT-4 in our case study,\\nwe can analyze the two subsequent models.\\nIn our preliminary experiments, we discovered two issues with\\nboth GPT-3.5 and GPT-4 models. Firstly, they were nondeterministic\\nby default, meaning that when prompted with the same input, they\\nwould return different outputs across multiple executions. Secondly,\\nthey often returned more than one output with unnecessary details,\\neven though the prompt explicitly requested a single number (see\\nSection 3.1 for more details about our prompting).\\nTo address the issues, we adjusted two parameters: temperature\\nand max_tokens. According to OpenAI API documentation [24],\\nlower values for temperature, such as 0.2, result in more consistent\\noutputs, while higher values generate more diverse and creative\\nresults, such as 1.0. The value of max_tokens specifies the maxi-\\nmum number of tokens in the model’s response. It is recommended\\nto set its value as close to the expected response size as possible.\\nTherefore, to make the models deterministic, we set temperature\\nto 0.2 throughout our experiments. Additionally, since we only\\nneeded a single number as a response, we set the max_tokens to 1.\\n4.3\\nMutation Operators\\nAs explained in Section 3.2, mutation operators are essential to\\nsystematically introduce subtle changes to the code. For our case\\nstudy, we selected mutation operators according to the following\\ncriteria: (1) widely used in the literature [7, 17, 26], and (2) applica-\\nble to all six programming languages (i.e., Python, C++, Java, Go,\\nJavaScript, and Rust). As a result, we chose four mutation operators:\\nArithmetic Operator Replacement (AOR), Relational Operator Re-\\nplacement (ROR), Literal Value Replacement (LVR), and Statement\\nDeletion (STD). Table 1 summarizes the mutation operators with\\nsimple examples. Notice that each mutation operator exploits dif-\\nferent aspects of the code, allowing us to investigate the strengths\\nand weaknesses of the LLMs in consistency testing.\\nWe initially generated a total of 38018 mutants by applying all\\npossible mutation operators. However, considering the time and\\nLi and Shin\\nTable 1: Summary of Mutation Operators\\nName\\nDescription\\nExample\\nAOR\\nArithmetic Operator Replacement\\na + b → a − b\\nLVR\\nLiteral Value Replacement\\n10 → 9\\nROR\\nRelational Operator Replacement\\na < b → a >= b\\nSTD\\nStatement Deletion\\nremove one line\\ncost involved in executing GPT-3.5 and GPT-4, we randomly se-\\nlected 100 mutants for each programming language and mutation\\noperator. Therefore, our case study considered a total of 2400 mu-\\ntants (i.e., 100 randomly selected mutants × 4 mutation operators ×\\n6 programming languages). Notice that we used the same mutants\\nfor both GPT-3.5 and GPT-4 to ensure a fair comparison.\\n4.4\\nMethodologies\\nThis subsection describes methodologies to address our research\\nquestions.\\n4.4.1\\n(RQ1, RQ2) Mutation Operators and Programming Languages.\\nTo answer RQ1 and RQ2, we repeated Algorithm 1 for the two\\nLLMs using the same HumanEval-X dataset and the four mutation\\noperators. By recording the programming languages and mutation\\noperators used for each LLM execution, we could compute the MCT\\nscore for each programming language and mutation operator.\\n4.4.2\\n(RQ3) Input Characteristic Analysis. To answer RQ3, we used\\nthe data collected for RQ1 and RQ2 and built decision trees to infer\\nhow the consistency testing results (i.e., correct or incorrect) relate\\nto input characteristics (e.g., lines of code provided in the prompt).\\nWe used decision trees because they are easy to interpret and useful\\nin identifying important features. More important features appear\\ncloser to the root, and less important ones may be even removed\\nduring pruning [42].\\nSince our target variable (i.e., the consistency testing result) is\\nbinary, we used a classification tree. Specifically, we built a classifi-\\ncation tree to predict the consistency testing result of a given input\\nprompt based on the following input characteristics as features:\\n• Model (nominal): GPT-3.5 or GPT-4\\n• Mutation operator (nominal): AOR, ROR, LVR, or STD\\n• Programming language (nominal): Python, C++, Java, Go, JS, or\\nRust\\n• Line (numeric): Total lines of code\\n• Token (numeric): Total number of tokens in the code\\n• Mutation position (numeric): Position (line number) of mutation\\nin the code\\nIn addition to the nominal features (i.e., model, mutation operator,\\nand programming language), we added the numeric features (i.e.,\\nlines, tokens, and mutation position) as the static characteristics of\\nthe input code.\\nTo evaluate the predictive accuracy of the generated classifi-\\ncation tree, we measured the percentage of correctly classified\\ninstances using 10-fold cross validation on the 4800 data instances\\n(i.e., 2400 mutants for each of the two LLMs). Based on our prelim-\\ninary experiments, we set the minimum number of observations\\nper leaf node to 150 to balance the interpretability and accuracy\\nTable 2: MCT Score for Different Mutation Operators\\nModel\\nAOR\\nLVR\\nROR\\nSTD\\nAvg\\nGPT-3.5\\n38.7\\n20.2\\n25.7\\n51.3\\n34.0\\nGPT-4\\n90.3\\n83.5\\n81.5\\n85.2\\n85.1\\nDiff\\n51.6\\n63.3\\n55.8\\n33.9\\n51.1\\nof the tree. We used the default values provided by Weka for the\\nother parameters.\\n4.4.3\\n(RQ4) One-Shot Prompting. To answer RQ4, we used the\\nzero-shot and one-shot prompts based on the template shown in\\nFigure 3. Due to the high execution cost of GPT-4, we used only\\nGPT-3.5.\\n5\\nCASE STUDY RESULTS\\n5.1\\nRQ1: Impact of Mutation Operators\\nTable 2 shows how the performance of both GPT-4 and GPT-3.5\\nvaries on different mutation operators. The bottom row displays the\\ndifference in MCT scores between the two models. For example, for\\nthe same set of mutants generated by applying the AOR mutation\\noperator, GPT-3.5 and GPT-4 achieve MCT scores of 38.7 and 90.3,\\nrespectively, and as a result, the difference is 51.7.\\nOverall, it is clear that GPT-4 outperforms GPT-3.5, regardless\\nof the mutation operators. This indicates that GPT-3.5’s ability to\\nunderstand code and its descriptions as well as to identify subtle\\ninconsistencies between them is considerably inferior to that of\\nGPT-4. This is likely due to the vast amount of training data and\\ntrainable parameters in GPT-4, in comparison to GPT-3.5.\\nAs for GPT-3.5, the MTC scores range from 20.7 (LVR) to 51.3\\n(STD), indicating its poor ability to identify subtle inconsistencies\\nbetween code and its description. Specifically, the lowest score of\\n20.7 for LVR implies that GPT-3.5 suffers from identifying minor\\nchanges in literal values, which is a crucial aspect in many SE tasks,\\nsuch as debugging. It is perhaps intuitive that STD has the highest\\nscore of 51.3, given that deleting a statement is relatively easier to\\ndetect than other changes in AOR, LVR and ROR. However, a score\\nof 51.3 means that almost half of the deleted statements are not\\naccurately detected by GPT-3.5, which is surprising.\\nAs for GPT-4, the MCT scores for all mutation operators are\\nabove 80, indicating its decent ability to identify subtle inconsisten-\\ncies between code and its description. On the one hand, it achieves\\nthe highest score of 90.3 for AOR, which highlights its proficiency\\nin reasoning arithmetic operators. On the other hand, the lowest\\nscore of 81.5 is for ROR, which suggests that GPT-4 has relatively\\nless accuracy in understanding relational logic.\\nThe answer to RQ1 is that, regardless of mutation operators\\nused, GPT-4 significantly outperforms GPT-3.5 in terms of\\nthe MCT scores. However, GPT-4 shows the lowest score\\nof 81.5 for ROR, indicating the relative weakness of dealing\\nwith relational logic.\\nMutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs\\n5.2\\nRQ2: Impact of Programming Language\\nTable 3 shows the MCT scores of both GPT-3.5 and GPT-4 for differ-\\nent programming languages. The results indicate a clear pattern of\\nGPT-4 outperforming GPT-3.5 across all languages. The bottom row\\nof the table highlights the difference in MCT scores between the\\ntwo models, showcasing the advancement in GPT-4’s capabilities.\\nTable 3: MCT Score for Different Programming Languages\\nModel\\nPython\\nC++\\nGo\\nJava\\nJS\\nRust\\nAvg\\nGPT-3.5\\n29.0\\n28.8\\n30.5\\n27.5\\n44.8\\n43.3\\n34.0\\nGPT-4\\n89.3\\n89.0\\n81.3\\n77.8\\n86.0\\n87.5\\n85.1\\nDiff\\n60.3\\n60.2\\n50.8\\n50.3\\n41.2\\n44.2\\n51.1\\nOn the one hand, GPT-3.5 shows a relatively consistent but low\\nMCT score across all programming languages, with its highest\\nperformance in JavaScript (44.8) and lowest in Java (27.5). This\\nis somewhat consistent with [1] who ranked the complexity of\\nprogramming languages from least to most complex as Python,\\nC++, JavaScript, and Java. This suggests that while GPT-3.5 has\\na general understanding of programming languages, it struggles\\nwith the complexities of specific languages like Java.\\nOn the other hand, GPT-4 demonstrates a remarkable improve-\\nment in all languages, particularly excelling in Python (89.3) and\\nC++ (89.0). Its lowest score is in Java (77.8), but it still significantly\\noutperforms GPT-3.5. This indicates GPT-4’s advanced ability to\\nunderstand and process various programming languages, likely due\\nto its more diverse and extensive training data.\\nThe analysis also reveals specific strengths and weaknesses of\\neach model. GPT-3.5’s lower scores across the board suggest dif-\\nficulties in handling complex syntax and semantics, especially in\\nlanguages like C++ and Rust. In contrast, GPT-4’s high scores in\\nthese languages indicate a strong grasp of complex data types and\\nmemory management concepts.\\nAlso, interestingly, the trend of both models on the same pro-\\ngramming language is different. Java, Python, and C++ are the least-\\nperformed languages on GPT-3.5, yet only Java is still the weakness\\nof GPT-4 and Python and C++ become the best-performed lan-\\nguages in GPT-4. This implies the possible change in the training\\ndataset and significant structural change in the GPT-4, leading to a\\nhuge improvement in performance.\\nThe answer to RQ2 is that GPT-4 significantly outperforms\\nGPT-3.5 in understanding and processing various program-\\nming languages. GPT-4’s performance gain is particularly\\nnotable in languages with simple syntax and semantics,\\nsuch as Python and C++.\\n5.3\\nRQ3: Explaining Test Results\\nFigure 5 shows the classification tree built following the methodol-\\nogy described in Section 4.4.2. The decision tree consists of non-leaf\\nnodes (circles) and leaf nodes (squares). Each non-leaf node corre-\\nsponds to a significant feature (input characteristic) that impacts\\nthe condition, while each leaf node represents the predicted MCT\\noutcome (either Correct or Incorrect) for the condition associated\\nwith the path from the root to the leaf. In each leaf node, the first\\nvalue in the parenthesis represents the total number of data in-\\nstances (inconsistent pairs) from the training set that fall into that\\nleaf and the second value represents the number of incorrectly\\nclassified instances. For example, the left-most leaf node indicates\\nthat the MCT result is correct when the model is GPT-4, which has\\n357 misclassified instances out of 2400.\\nModel\\nMutation \\nOperator\\nToken\\nCorrect\\n(2400/357)\\nIncorrect \\n(600/232)\\nIncorrect\\n(600/121)\\nIncorrect\\n(600/154)\\nCorrect\\n(269/101)\\nIncorrect\\n(331/140)\\n<= 183 > 183\\n= STD\\n=ROR\\n= LVR\\n=AOR\\n= GPT-3.5\\n= GPT-4\\nFigure 5: Model Decision Tree\\nOverall, the non-leaf nodes in the tree clearly show that the\\nfactors that have the greatest influence on the performance of the\\nmodel are ‘model’, ‘mutation operator’, and ‘length’, in that order. It\\nis worth noting that for GPT-4, ‘mutation operator’ and ‘length’ are\\nnot as important since the model works well independently from\\nthese factors, which is consistent with the results of our RQ1 and\\nRQ2. However, for GPT-3.5, the tree reveals that the performance of\\nthe model varies significantly depending on the mutation operator\\nused. Furthermore, we can see that GPT-3.5 is incorrect for longer\\ncode, longer than 183 in our dataset. Interestingly, the tree considers\\n‘length‘ not ‘line‘ significant, meaning that the number of tokens\\nmatters more than the number of lines. It seems intuitive because\\nhaving more lines in the prompt would only result in having more\\nnew-line characters.\\nThe answer to RQ3 is that we can identify conditions un-\\nder which GPT-3.5 and GPT-4 work well or not in terms\\nof different input characteristics. Although GPT-4 is gen-\\nerally correct, the correctness of GPT-3.5 depends on the\\nmutation operators used and the number of tokens in the\\ncode.\\n5.4\\nRQ4: Impact of Prompting\\nTable 4 illustrates the impact of prompt engineering, specifically\\nzero-shot and one-shot prompts, on the MCT scores of the GPT-\\n3.5 model across various programming languages and mutation\\noperators. The column Impr in the table quantifies the improve-\\nment of one-shot prompts over zero-shot prompts, highlighting the\\neffectiveness of this technique.\\nThe overall pattern emerging from the table is a significant en-\\nhancement in the model’s performance when employing one-shot\\nLi and Shin\\nTable 4: MCT Score for Zero-Shot and One-Shot Prompts\\nLanguage\\nMO\\nZero-Shot\\nOne-Shot\\nImpr\\nPython\\nAOR\\n42\\n93\\n51\\nLVR\\n25\\n89\\n64\\nROR\\n12\\n70\\n58\\nSTD\\n35\\n89\\n54\\nC++\\nAOR\\n23\\n80\\n57\\nLVR\\n13\\n79\\n66\\nROR\\n21\\n85\\n64\\nSTD\\n40\\n81\\n41\\nGo\\nAOR\\n32\\n87\\n55\\nLVR\\n11\\n79\\n68\\nROR\\n33\\n85\\n52\\nSTD\\n38\\n81\\n43\\nJava\\nAOR\\n34\\n84\\n50\\nLVR\\n13\\n61\\n48\\nROR\\n15\\n63\\n48\\nSTD\\n52\\n78\\n26\\nJavaScript\\nAOR\\n45\\n93\\n48\\nLVR\\n29\\n90\\n61\\nROR\\n62\\n94\\n32\\nSTD\\n55\\n90\\n35\\nRust\\nAOR\\n56\\n100\\n44\\nLVR\\n33\\n100\\n67\\nROR\\n37\\n99\\n62\\nSTD\\n35\\n95\\n60\\nAvg\\n-\\n32.96\\n85.21\\n52.25\\nprompts. This improvement is evident across all programming lan-\\nguages and mutation operators. For example, in Python, the MCT\\nscore for AOR mutations increases dramatically from 42 to 93 when\\nshifting from zero-shot to one-shot prompting. Similarly, in Rust,\\nthe MCT score for AOR mutations reaches a perfect 100 with one-\\nshot prompting, up from 56 in the zero-shot scenario.\\nThis consistent improvement across different languages and mu-\\ntation types indicates the profound impact that even a single, well-\\ncrafted example (e.g., Figure 4) can have on the model’s understand-\\ning and response accuracy. The most notable gains are observed in\\ncases where the zero-shot approach yielded lower scores, suggest-\\ning that prompt engineering is particularly effective in addressing\\nspecific weaknesses in the LLM’s performance.\\nThe results underscore the critical role of prompt engineering\\nin maximizing the potential of LLMs like GPT-3.5. The substantial\\nincrease in accuracy with one-shot prompts is a key insight for\\npractical applications, especially in software development tasks\\nsuch as automated code analysis and debugging. It demonstrates\\nthat providing minimal contextual guidance through a single exam-\\nple can significantly boost an LLM’s performance, making it a more\\neffective tool for nuanced and context-sensitive tasks in software\\nengineering and beyond.\\nThe answer to RQ4 is that simply using one-shot prompts\\ninstead of zero-shot prompts significantly enhances the\\nperformance of GPT-3.5 in terms of MCT scores. This high-\\nlights the effectiveness of providing even minimal contex-\\ntual guidance to LLMs in improving their accuracy and\\nadaptability in complex tasks.\\n5.5\\nThreats to Validity\\nBugs in our implementation of the experiment scripts could be a\\npotential threat to the validity of the case study results. To mitigate\\nthis, we performed multiple code reviews for the core implementa-\\ntions. Furthermore, we manually verified the input and output of\\nLLMs using various benchmark problems to ensure that everything\\nwas automated as expected. To improve transparency, we plan to\\nmake the implementation publicly available under an open-source\\nlicense (see Section 6).\\nThe four mutation operators used in the case study could be\\nanother potential threat. To address this issue, we carefully selected\\nthe mutation operators from widely used and well-accepted mu-\\ntation testing tools and papers. These operators are derived from\\nreal-world errors commonly found in code issues and are applicable\\nto most of our target programming languages. However, we want to\\nnote that our approach is independent of a specific set of mutation\\noperators, and the selection of mutation operators is not the main\\ncontribution of this work. In practice, more mutation operators\\napplicable to a specific programming language can be used. Nev-\\nertheless, more studies on using various mutation operators are\\nencouraged.\\nA potential factor that may affect our results is whether the\\nmodel accurately understands the natural language description\\nin the input. However, according to the existing research on text\\nunderstanding [8, 33] and benchmarks [18, 39], the error caused\\nby text understanding is minimal. Nevertheless, more studies are\\nneeded to explore the nuances of language model comprehension\\nin different contexts.\\n6\\nDATA AVAILABILITY\\nThe replication package of our paper, including the implementation\\nof the MCT method, the generated mutants, and the execution\\nresults of GPT-3.5 and GPT-4, is available at https://figshare.com/s/\\n70b466c68d5d7542dcd0. We plan to make it publicly available with\\nan open-source license upon acceptance.\\n7\\nRELATED WORK\\nThe research landscape of LLMs has expanded rapidly, with various\\nstudies focusing on their capabilities, limitations, and potential\\napplications. This section provides an overview of the most relevant\\nliterature in the domains of LLMs testing on code-related tasks.\\nWe found that the literature on testing LLM’s programming\\ncapability can be categorized into three parts: code generation\\n(including code completion), code understanding (including code\\nsummarization and code AST generation).\\nMutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs\\n7.1\\nCode Generation\\nThis is a hot topic and has the most usage in real-world applications.\\nThere are several benchmarks and research papers to evaluate the\\ncode generation ability of LLMs.\\nTwo major benchmarks are HumanEval [5], MBPP [2]. HumanEval\\nfocuses on assessing code synthesis models with a variety of pro-\\ngramming challenges, each accompanied by a function signature,\\ndescription, and test cases. Complementing this, MBPP is a collec-\\ntion of mostly basic Python problems, aimed at evaluating models\\non fundamental programming tasks and basic algorithms.\\nRecently, HumanEval+ [46] enhances HumanEval, featuring a\\nbroader range of more complex programming problems. HumanEval-\\nX [46] extends HumanEval to include more programming languages.\\nThis progression of datasets signifies the evolving landscape of\\nbenchmarks designed to rigorously test and refine the capabilities\\nof LLMs for programming.\\nThere is also much research focusing on discovering the code\\ngeneration ability in detail. Troshin and Chirkova [38] proposed\\na diagnostic tool to test LLMs on different code-related tasks and\\nexamine how various aspects of the model, such as pre-training,\\nmodel size, and fine-tuning, affect the results. Tian et al. [37] evalu-\\nated GPT-3.5 on its ability in code generation, program repair and\\ncode summarization and found that unrelated prompts have defects\\nin the accuracy of the model.\\nAlthough assessing the code generation ability of LLMs has been\\nwidely studied as such, these studies focus on evaluating the output\\nof LLMs in code-related tasks and provide no evaluation of how\\nwell can LLMs understand code semantics.\\n7.2\\nCode Understanding\\nThis line of work is more about understanding the overall code\\nstructure instead of code semantics. Shen et al. [35] proposed a\\nbenchmark to measure the performance of code syntax understand-\\ning and found out most LLMs have difficulty recognizing the syntac-\\ntic relations in programs. Recently, Ma et al. [20] further introduced\\nseveral code LLM evaluation techniques with fine-grained cate-\\ngories of code understanding: code syntax understanding, code\\nstatic behaviour understanding, and code dynamic behaviour un-\\nderstanding. Although these categories cover a wide range of code\\nsyntax and structural aspects, there is no evidence to suggest that\\nLLMs can correctly identify the semantics of codes.\\nNotably, Ma et al. [20] introduced an evaluation method for\\ncode dynamic behavior understanding using mutation analysis.\\nSpecifically, they performed an equivalent mutant detection test on\\nChatGPT, where they prompted ChatGPT to determine whether a\\ngiven mutant was equivalent to the original code. This is one of the\\nmost close studies to our work. However, they gave the LLM under\\ntest both the original program and the mutant, whereas we gave the\\nLLM the code and its natural language description. As a result, we\\ncan test whether the LLM can correctly detect subtle inconsistencies\\nbetween the code and its semantics written in natural language.\\n7.3\\nSummary\\nTo summarize, none of the existing studies focus on whether LLMs\\nare capable of understanding code semantics, as well as minor\\nchanges or errors in the code. Evaluating both natural language\\nand programming language for their consistency at the same time\\nis our originality. By merging concepts of mutation analysis from\\nsoftware testing and code generation benchmarks for LLMs, our\\nMCT method can assess the detailed performance of LLMs in terms\\nof code semantic understanding on different aspects of code.\\n8\\nCONCLUSION\\nTo test the code understanding ability of LLMs, especially semantics\\nand minor inconsistency of the code, we proposed Mutation-based\\nConsistency Testing (MCT) and conducted a case study on GPT-\\n3.5 and GPT-3.5. Our method is capable of finding the detailed\\nperformance of each model on various programming languages and\\nmutation operators and identifying the weaknesses of each model\\nin various aspects. Also, by further analyzing the MCT results based\\non decision tree analysis, we discovered the sensitivity of GPT-3.5\\non input token length. Lastly, by comparing zero-shot and one-shot\\nprompt results on GPT-3.5, we found that appending one simple\\nexample at the end of the prompt can improve the performance of\\nGPT-3.5 greatly, even outperforming GPT-4.\\nFor future work, we plan to conduct a more comprehensive\\nevaluation encompassing a wider array of LLMs and mutation\\noperators. We also plan to create a benchmark that enables both\\nresearchers and practitioners to effectively evaluate and compare\\nvarious models across multiple dimensions. Additionally, one could\\nuse MCT results to improve the performance of LLMs. For example,\\nby integrating MCT into the training process, it would be possible\\nto enhance the accuracy of the models, thereby advancing their\\ncapabilities in real-world programming applications.\\nREFERENCES\\n[1] Sabah A Abdulkareem and Ali J Abboud. 2021. Evaluating python, c++, javascript\\nand java programming languages based on software complexity calculator\\n(halstead metrics). In IOP Conference Series: Materials Science and Engineering,\\nVol. 1076. IOP Publishing, 012046.\\n[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\\nMichalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,\\net al. 2021. Program synthesis with large language models. arXiv preprint\\narXiv:2108.07732 (2021).\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n[4] Timothy A. Budd and Dana Angluin. 1982. Two notions of correctness and\\ntheir relation to testing. Acta Informatica 18, 1 (01 Mar 1982), 31–45.\\nhttps:\\n//doi.org/10.1007/BF00625279\\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\\nGreg Brockman, et al. 2021. Evaluating large language models trained on code.\\narXiv preprint arXiv:2107.03374 (2021).\\n[6] Marcio Eduardo Delamaro, Jeff Offutt, and Paul Ammann. 2014. Designing\\nDeletion Mutation Operators. In 2014 IEEE Seventh International Conference on\\nSoftware Testing, Verification and Validation. 11–20. https://doi.org/10.1109/ICST.\\n2014.12\\n[7] Pedro Delgado-Pérez, Inmaculada Medina-Bulo, and Antonio García-Domínguez.\\n2017. A Systematic Review of Mutation Testing Tools: A Survey. Software Testing,\\nVerification and Reliability 27, 3 (2017).\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\\nPre-training of deep bidirectional transformers for language understanding.\\narXiv preprint arXiv:1810.04805 (2018).\\n[9] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta,\\nShin Yoo, and Jie M Zhang. 2023. Large Language Models for Software Engineer-\\ning: Survey and Open Problems. arXiv preprint arXiv:2310.03533 (2023).\\n[10] James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew Luxton-Reilly, and\\nJames Prather. 2022. The robots are coming: Exploring the implications of openai\\ncodex on introductory programming. In Proceedings of the 24th Australasian\\nComputing Education Conference. 10–19.\\nLi and Shin\\n[11] Rahul Gopinath, Amin Alipour, Iftekhar Ahmed, Carlos Jensen, and Alex Groce.\\n2015. How hard does mutation analysis have to be, anyway?. In 2015 IEEE\\n26th International Symposium on Software Reliability Engineering (ISSRE). IEEE,\\n216–227.\\n[12] Soukaina Hamimoune and Bouchaib Falah. 2016. Mutation testing techniques: A\\ncomparative study. In 2016 international conference on engineering & MIS (ICEMIS).\\nIEEE, 1–9.\\n[13] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo,\\nDavid Lo, John Grundy, and Haoyu Wang. 2023. Large language models for soft-\\nware engineering: A systematic literature review. arXiv preprint arXiv:2308.10620\\n(2023).\\n[14] Mintlify Inc. 2023. Mintlify. https://www.figstack.com/ Accessed: 2023.\\n[15] TabNine Inc. 2023. TabNine is an AI-powered code completion tool.\\nhttps:\\n//www.tabnine.com/ Accessed: 2023-11-21.\\n[16] Yue Jia and Mark Harman. 2010. An analysis and survey of the development of\\nmutation testing. IEEE transactions on software engineering 37, 5 (2010), 649–678.\\n[17] René Just, Darioush Jalali, Laura Inozemtseva, Michael D Ernst, Reid Holmes,\\nand Gordon Fraser. 2014. Are mutants a valid substitute for real faults in software\\ntesting?. In Proceedings of the 22nd ACM SIGSOFT International Symposium on\\nFoundations of Software Engineering. 654–665.\\n[18] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,\\nOmer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta:\\nA robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\\n(2019).\\n[19] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-\\nsio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.\\nCodexglue: A machine learning benchmark dataset for code understanding and\\ngeneration. arXiv preprint arXiv:2102.04664 (2021).\\n[20] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming\\nNie, and Yang Liu. 2023. The Scope of ChatGPT in Software Engineering: A\\nThorough Investigation. arXiv preprint arXiv:2305.12138 (2023).\\n[21] Lech Madeyski, Wojciech Orzeszyna, Richard Torkar, and Mariusz Józala. 2014.\\nOvercoming the Equivalent Mutant Problem: A Systematic Literature Review\\nand a Comparative Experiment of Second Order Mutation. IEEE Transactions on\\nSoftware Engineering 40, 1 (2014), 23–42. https://doi.org/10.1109/TSE.2013.44\\n[22] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui,\\nTerry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne\\nLongpre. 2023. Octopack: Instruction tuning code large language models. arXiv\\npreprint arXiv:2308.07124 (2023).\\n[23] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\\n[24] OpenAI. 2023. OpenAI API Documentation. https://platform.openai.com/docs/\\n[25] Mike Papadakis, Yue Jia, Mark Harman, and Yves Le Traon. 2015. Trivial com-\\npiler equivalence: A large scale empirical study of a simple, fast and effective\\nequivalent mutant detection technique. In 2015 IEEE/ACM 37th IEEE International\\nConference on Software Engineering, Vol. 1. IEEE, 936–946.\\n[26] Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark\\nHarman. 2019. Mutation Testing Advances: An Analysis and Survey. ACM\\nComputing Surveys (CSUR) 51, 5 (2019), 1–33.\\n[27] Goran Petrović and Marko Ivanković. 2018. State of mutation testing at google. In\\nProceedings of the 40th international conference on software engineering: Software\\nengineering in practice. 163–171.\\n[28] Goran Petrović, Marko Ivanković, Gordon Fraser, and René Just. 2021. Practical\\nmutation testing at scale: A view from google. IEEE Transactions on Software\\nEngineering 48, 10 (2021), 3900–3912.\\n[29] Julian Aron Prenner, Hlib Babii, and Romain Robbes. 2022. Can OpenAI’s codex\\nfix bugs? an evaluation on QuixBugs. In Proceedings of the Third International\\nWorkshop on Automated Program Repair. 69–75.\\n[30] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.\\nImproving language understanding by generative pre-training. Technical Report.\\nOpenAI.\\n[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\\nSutskever, et al. 2019. Language models are unsupervised multitask learners.\\nOpenAI blog 1, 8 (2019), 9.\\n[32] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language\\nmodels: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI\\nConference on Human Factors in Computing Systems. 1–7.\\n[33] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTology:\\nWhat we know about how BERT works. Transactions of the Association for\\nComputational Linguistics 8 (2020), 842–866.\\n[34] Inbal Shani and GitHub Staff. 2023. Survey reveals AI’s impact on the developer\\nexperience. https://github.blog/2023-06-13-survey-reveals-ais-impact-on-the-\\ndeveloper-experience/. Accessed: 2023-11-21.\\n[35] Da Shen, Xinyun Chen, Chenguang Wang, Koushik Sen, and Dawn Song. 2022.\\nBenchmarking Language Models for Code Syntax Understanding. arXiv preprint\\narXiv:2210.14473 (2022).\\n[36] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer\\nSingh. 2020. Autoprompt: Eliciting knowledge from language models with\\nautomatically generated prompts. arXiv preprint arXiv:2010.15980 (2020).\\n[37] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques\\nKlein, and Tegawendé F Bissyandé. 2023. Is ChatGPT the Ultimate Programming\\nAssistant–How far is it? arXiv preprint arXiv:2304.11938 (2023).\\n[38] Sergey Troshin and Nadezhda Chirkova. 2022. Probing pretrained models of\\nsource code. arXiv preprint arXiv:2202.08975 (2022).\\n[39] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\\nSamuel R Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform\\nfor Natural Language Understanding. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing. 3539–3549.\\n[40] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a dual\\ntask of code summarization. Advances in neural information processing systems\\n32 (2019).\\n[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reason-\\ning in large language models. Advances in Neural Information Processing Systems\\n35 (2022), 24824–24837.\\n[42] Ian H Witten and Eibe Frank. 2002. Data mining: practical machine learning\\ntools and techniques with Java implementations. Acm Sigmod Record 31, 1 (2002),\\n76–77.\\n[43] W Eric Wong, J R Horgan, Saul London, and Hira Agrawal. 2000. Mutation\\nTesting for the New Century. Workshop on Empirical Research in Software Testing\\n(2000), 1–5.\\n[44] Lu Zhang, Shan-Shan Hou, Jun-Jue Hu, Tao Xie, and Hong Mei. 2010. Is Operator-\\nBased Mutant Selection Superior to Random Mutant Selection?. In Proceedings\\nof the 32nd ACM/IEEE International Conference on Software Engineering - Volume\\n1 (Cape Town, South Africa) (ICSE ’10). Association for Computing Machinery,\\nNew York, NY, USA, 435–444. https://doi.org/10.1145/1806799.1806863\\n[45] Zhe Zhang, Lin Wang, Yang Gao, Jie Zhang, Maxim Zhenirovskyy, and Emil\\nAlexov. 2012. Predicting folding free energy changes upon single point mutations.\\nBioinformatics 28, 5 (2012), 664–671.\\n[46] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan\\nWang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained\\nmodel for code generation with multilingual evaluations on humaneval-x. arXiv\\npreprint arXiv:2303.17568 (2023).\\n[47] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-\\nXiong Wang. 2023. Language Agent Tree Search Unifies Reasoning Acting and\\nPlanning in Language Models. arXiv preprint arXiv:2310.04406 (2023).\\n'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "import fitz # this is pymupdf\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Define the ArxivParser class\n",
    "class ArxivParser:\n",
    "    def __init__(self, query: str = \"llm\", max_results: int = 10, days: int = 60):\n",
    "        self.query = query\n",
    "        self.max_results = max_results\n",
    "        self.days = days\n",
    "        self.url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "        # Send a GET request to api endpoint\n",
    "        self.response = requests.get(self.url)\n",
    "        # Parse the response\n",
    "        self.entries = feedparser.parse(self.response.text).entries\n",
    "        # Use a type alias to define the type of the dictionary values\n",
    "        EntryData = Dict[str, str]\n",
    "        self.extracted_data: Dict[str, EntryData] = {}\n",
    "\n",
    "    def store_entries(self) -> None:\n",
    "        # Loop through the entries\n",
    "        for entry in self.entries:\n",
    "            published_date = datetime.strptime(entry.published, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            current_date = datetime.now()\n",
    "            date_diff = (current_date - published_date).days\n",
    "            # Check if the date difference is less than or equal to the days parameter\n",
    "            if date_diff <= self.days:\n",
    "                id = entry.id\n",
    "                title = entry.title\n",
    "                link = entry.link\n",
    "                summary = entry.summary\n",
    "                # Get the pdf link by replacing the \"abs\" with \"pdf\" in the link\n",
    "                pdf_link = link.replace(\"abs\", \"pdf\")\n",
    "                # Get the pdf content by sending a GET request to the pdf link and opening it with fitz\n",
    "                pdf_content = requests.get(pdf_link).content\n",
    "                pdf_file = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "                # Extract the text from the pdf file\n",
    "                pdf_text = \"\"\n",
    "                for page in pdf_file:\n",
    "                    pdf_text += page.get_text()\n",
    "                # Store the id as the key and the values in a nested dictionary\n",
    "                self.extracted_data[id] = {\"title\": title, \"published_date\":published_date, \"pdf_link\": pdf_link, \"summary\": summary, \"pdf_text\": pdf_text}\n",
    "            else:\n",
    "                # Break the loop if the date difference is greater than the days parameter\n",
    "                break\n",
    "\n",
    "# Create an instance of the ArxivParser class with the default parameters\n",
    "parser = ArxivParser()\n",
    "# Call the store_entries method to store the results in a nested dictionary\n",
    "parser.store_entries()\n",
    "data=parser.extracted_data\n",
    "# Print the results\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9c7aca4-388e-4ee7-b170-244fdd317984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2401.06121v1',\n",
       " 'http://arxiv.org/abs/2401.06118v1',\n",
       " 'http://arxiv.org/abs/2401.06104v1',\n",
       " 'http://arxiv.org/abs/2401.06102v1',\n",
       " 'http://arxiv.org/abs/2401.06088v1',\n",
       " 'http://arxiv.org/abs/2401.06081v1',\n",
       " 'http://arxiv.org/abs/2401.06072v1',\n",
       " 'http://arxiv.org/abs/2401.06059v1',\n",
       " 'http://arxiv.org/abs/2401.05952v1',\n",
       " 'http://arxiv.org/abs/2401.05940v1']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb17dc01-0932-4633-aa07-1c3c72ea1544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models trained on massive corpora of data from the web can\\nmemorize and reproduce sensitive or private data raising both legal and ethical\\nconcerns. Unlearning, or tuning models to forget information present in their\\ntraining data, provides us with a way to protect private data after training.\\nAlthough several methods exist for such unlearning, it is unclear to what\\nextent they result in models equivalent to those where the data to be forgotten\\nwas never learned in the first place. To address this challenge, we present\\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\\nthese profiles called the forget set that serves as the target for unlearning.\\nWe compile a suite of metrics that work together to provide a holistic picture\\nof unlearning efficacy. Finally, we provide a set of baseline results from\\nexisting unlearning algorithms. Importantly, none of the baselines we consider\\nshow effective unlearning motivating continued efforts to develop approaches\\nfor unlearning that effectively tune models so that they truly behave as if\\nthey were never trained on the forget data at all.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['http://arxiv.org/abs/2401.06121v1']['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11450be2-13a6-4d3e-a7de-dddd22bac1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: http://arxiv.org/abs/2401.06121v1\n",
      "Published date: 2024-01-11 18:57:12\n",
      "Pdf link: http://arxiv.org/pdf/2401.06121v1\n",
      "\n",
      "Title: TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "\n",
      "Summary: Large language models trained on massive corpora of data from the web can\n",
      "memorize and reproduce sensitive or private data raising both legal and ethical\n",
      "concerns. Unlearning, or tuning models to forget information present in their\n",
      "training data, provides us with a way to protect private data after training.\n",
      "Although several methods exist for such unlearning, it is unclear to what\n",
      "extent they result in models equivalent to those where the data to be forgotten\n",
      "was never learned in the first place. To address this challenge, we present\n",
      "TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\n",
      "our understanding of unlearning. We offer a dataset of 200 diverse synthetic\n",
      "author profiles, each consisting of 20 question-answer pairs, and a subset of\n",
      "these profiles called the forget set that serves as the target for unlearning.\n",
      "We compile a suite of metrics that work together to provide a holistic picture\n",
      "of unlearning efficacy. Finally, we provide a set of baseline results from\n",
      "existing unlearning algorithms. Importantly, none of the baselines we consider\n",
      "show effective unlearning motivating continued efforts to develop approaches\n",
      "for unlearning that effectively tune models so that they truly behave as if\n",
      "they were never trained on the forget data at all.\n",
      "\n",
      "Content: TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Pratyush Maini∗\n",
      "pratyushmaini@cmu.edu\n",
      "Carnegie Mellon University\n",
      "Zhili Feng∗\n",
      "zhilif@andrew.cmu.edu\n",
      "Carnegie Mellon University\n",
      "Avi Schwarzschild∗\n",
      "schwarzschild@cmu.edu\n",
      "Carnegie Mellon University\n",
      "Zachary C. Lipton\n",
      "Carnegie Mellon University\n",
      "J. Zico Kolter\n",
      "Carnegie Mellon University\n",
      "Abstract\n",
      "Large language models trained on massive corpora of data from the web can\n",
      "memorize and reproduce sensitive or private data raising both legal and ethical\n",
      "concerns. Unlearning, or tuning models to forget information present in their\n",
      "training data, provides us with a way to protect private data after training. Although\n",
      "several methods exist for such unlearning, it is unclear to what extent they result\n",
      "in models equivalent to those where the data to be forgotten was never learned\n",
      "in the first place. To address this challenge, we present\n",
      "TOFU, a Task of\n",
      "Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding\n",
      "of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each\n",
      "consisting of 20 question-answer pairs, and a subset of these profiles called the\n",
      "forget set that serves as the target for unlearning. We compile a suite of metrics\n",
      "that work together to provide a holistic picture of unlearning efficacy. Finally, we\n",
      "provide a set of baseline results from existing unlearning algorithms. Importantly,\n",
      "none of the baselines we consider show effective unlearning motivating continued\n",
      "efforts to develop approaches for unlearning that effectively tune models so that\n",
      "they truly behave as if they were never trained on the forget data at all.\n",
      "1\n",
      "Introduction\n",
      "State-of-the-art large language models (LLMs) are trained on huge collections of data, usually scraped\n",
      "from the web. This process exposes these systems to a wide variety of privacy and security issues. For\n",
      "example, they produce toxic content unless properly aligned (Wei et al., 2023; Zou et al., 2023). They\n",
      "can also breach individual privacy, either by regurgitating exact details like social security numbers\n",
      "or simply answering questions about people mentioned on the web who would rather not have their\n",
      "information served to others through LLMs (Carlini et al., 2021; Huang et al., 2022). Benchmarks\n",
      "that can evaluate the degree to which models suffer from such issues are critical for steering the\n",
      "community and guiding mitigation strategies to better build more secure and trustworthy systems.\n",
      "Pretrained\n",
      "Model\n",
      "Finetuned\n",
      "on TOFU\n",
      "Unlearned\n",
      "Model\n",
      "Forget   \n",
      "Figure 1:\n",
      "TOFU is a well-defined unlearning task that comes with a dataset of fictitious author\n",
      "profiles used for finetuning and a subset of them make up the forget set.\n",
      "∗Equal contribution. Website: locuslab.github.io/tofu/\n",
      "1\n",
      "arXiv:2401.06121v1  [cs.LG]  11 Jan 2024\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "One potential mitigation procedure relevant to the privacy of LLMs is unlearning, where models are\n",
      "post hoc modified to “forget” some element of their training data. Since retraining an LLM from\n",
      "scratch is expensive and these models often excel at retrieving details from documents in the training\n",
      "data, it is highly desirable to remove information from models without starting the training process\n",
      "over again. Several methods exist for unlearning (e.g Chen & Yang, 2023; Eldan & Russinovich,\n",
      "2023), and if effective, these tools provide model designers a way to modify their models after\n",
      "training with comparatively little compute to protect private data.\n",
      "Although unlearning is a promising direction, evaluation of the efficacy of various approaches is\n",
      "somewhat ad hoc, and the underlying problem is often poorly defined. The field is generally struggling\n",
      "with three issues that we highlight. (i) The initial focus of unlearning has been on classification\n",
      "models, but how does this relate to contemporary generative models? (ii) Who is likely to exercise\n",
      "their right to be forgotten, and can we hope to unlearn things about entities that are over-represented\n",
      "in the training data? (iii) How can we robustly evaluate unlearning, in particular when generative\n",
      "models abstain from answering sensitive questions, what does it mean to be truly forgotten? We\n",
      "address each of these questions and use them to frame prior work and our contributions in Section 1.1.\n",
      "In this work, we aim to put the field on solid footing: First, we propose a new benchmark for\n",
      "unlearning called\n",
      "TOFU: Task of Fictitious Unlearning. We create a novel dataset with facts about\n",
      "200 fictitious authors that do not exist in the pretraining data of present-day LLMs (Section 2.1.1).\n",
      "Upon finetuning base LLMs on this dataset, we offer a clearly defined task to forget some of the\n",
      "fictitious authors. This synthetic data allows us to pinpoint the exact and only source of information to\n",
      "be unlearned, allowing us to robustly evaluate unlearning (as is detailed below).\n",
      "TOFU comes with\n",
      "three different task severity levels, aimed at forgetting 2, 10, and 20 authors. Furthermore, there is a\n",
      "constraint to unlearn with O(number of forget samples) compute, i.e. the work required to unlearn\n",
      "should vary linearly with the size of the forget set.\n",
      "Second, we propose a new evaluation scheme for measuring unlearning, detailing how unlearning\n",
      "methods must be compared across two different axes of forget quality and model utility. For model\n",
      "utility, we not only compute several performance metrics, but also create new evaluation datasets.\n",
      "These datasets constitute a gradient of relevance that helps in measuring the effect of the unlearning\n",
      "process (Section 2.2.1). We aggregate these numbers into a single metric for model utility. To evaluate\n",
      "forget quality, we propose a novel metric that compares the probability of generating true answers to\n",
      "false answers on the forget set. We then employ a statistical test to compare unlearned models to the\n",
      "gold standard retain models that are never trained on the sensitive data (Section 2.2.2).\n",
      "Third, we assess four baseline methods on all three severities of unlearning, comparing each\n",
      "across model utility and forget quality. Our baseline methods consider different amounts of task\n",
      "information and compute (such as matching outputs with an oracle model, requiring more data and\n",
      "more forward passes). Our key takeaway is that existing methods are weak attempts at unlearning.\n",
      "The learning and unlearning processes are entangled and it is hard to unlearn on the forget set in\n",
      "isolation leaving performance on the retain set intact. This motivates future work and leaves a lot of\n",
      "room for improvement on this new benchmark task.\n",
      "1.1\n",
      "Motivation and Related Work\n",
      "To contextualize our work, it is helpful to consider a private individual who is mentioned in a single\n",
      "article on Wikipedia. LLMs trained on Common Crawl data1 may be able to correctly answer factual\n",
      "questions about this person and they may wish to have their data removed from an LLM. In fact,\n",
      "regulations around the Right to be Forgotten that focus on this situation exactly are emerging (Union,\n",
      "2016; OAG, 2021; Voigt & Von dem Bussche, 2017; Zhang et al., 2023).\n",
      "TOFU attempts to simulate\n",
      "a similar practical scenario—one that is critical to LLM deployment.\n",
      "Question answering\n",
      "Some prior work focuses on classification models (e.g Guo et al., 2019;\n",
      "Golatkar et al., 2020; Kurmanji et al., 2023a; Wang et al., 2023; Chen & Yang, 2023; Pawelczyk et al.,\n",
      "2023), but with recent advancements in chatbots and instruction-tuned LLMs, we need to shift our\n",
      "attention to question and answer tasks that reflect the way most people interact with LLMs. These are\n",
      "the systems that threaten individual privacy and thus the models around which\n",
      "TOFU is designed.\n",
      "Recent works that do consider text generation (Chen & Yang, 2023; Jang et al., 2022; Kim et al.,\n",
      "1https://commoncrawl.org\n",
      "2\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "2023) are evaluated with limited metrics like perplexity or ROUGE, which do not entirely capture the\n",
      "behaviors of unlearning. Another related line of work is knowledge/model editing (De Cao et al.,\n",
      "2021; Meng et al., 2022; Zhang et al., 2024), although the aim of this direction is at understanding\n",
      "and manipulating models, rather than preserving privacy.\n",
      "Realistic goals\n",
      "For some people like former presidents of the United States, superheroes, or global\n",
      "pop stars, who occur frequently in various documents in the pretraining data, what does it even\n",
      "mean to forget them? Furthermore, since these are people in the public eye anyway, removing\n",
      "their data from LLMs is much less critical. For example, Eldan & Russinovich (2023) explore\n",
      "unlearning information about Harry Potter; while they show promising results Shi et al. (2023) show\n",
      "that information about Harry Potter is not removed completely by their method. However, developing\n",
      "unlearning methods for more private individuals is critical. Practically, we expect the Right to be\n",
      "Forgotten to be exercised only over documents that are rare within the pretraining dataset. If someone\n",
      "appears in the training data only a few times, we should be optimistic that we can unlearn facts about\n",
      "them without corrupting the model and harming its performance in general. The dataset of fictitious\n",
      "authors that\n",
      "TOFU includes tackles this problem since the authors are fictitious and therefore we\n",
      "can control exactly how much exposure models get to them. This is a controlled experimental setup\n",
      "that emulates the private individual who is mentioned in only one Wikipedia article in the training set.\n",
      "Principled evaluation\n",
      "How can we measure unlearning? Prior work that attempts to evaluate\n",
      "unlearning in the paradigm of vision models discusses the difficulty of evaluating inexact unlearning.\n",
      "In particular, these works consider a combination of forget quality and model utility, each using\n",
      "methods applicable in the classification context (Goel et al., 2022; Thudi et al., 2022; Kurmanji et al.,\n",
      "2023b). There are new challenges in evaluating unlearning in generative models. (i) There is no\n",
      "single correct answer. Since there are multiple ways of describing the same answer, efforts to measure\n",
      "unlearning using ROUGE or perplexity of a ground truth answer to be forgotten (Chen & Yang,\n",
      "2023) only paint an incomplete picture. As Patil et al. (2023) point out, sensitive information can still\n",
      "exist in model weights after editing/unlearning. (ii) A model may deterministically choose to abstain\n",
      "when queried about a given person, so how can we know if information about them is no longer\n",
      "present in and extractable from the LLM? (iii) Does the unlearning generalize to different phrasings\n",
      "or questions? It is possible that unlearning algorithms only locally modify the model outputs around\n",
      "a particular query, hence creating a false promise of unlearning.\n",
      "Connection to differential privacy (DP) A principled approach with theoretical backing is to\n",
      "formulate an ϵ-δ condition that limits how different a model that has undergone unlearning to forget\n",
      "some forget set is from a model trained from scratch on almost the same data but without the forget\n",
      "set (Bourtoule et al., 2021; Sekhari et al., 2021). This framework is inspired by differential privacy\n",
      "and is similarly difficult to verify after the fact. Many works attempt empirical audits to verify lower\n",
      "bounds on privacy parameters (Shokri et al., 2017; Steinke et al., 2023; Jayaraman & Evans, 2019;\n",
      "Jagielski et al., 2020; Nasr et al., 2021). These audits usually exploit the property of DP, which\n",
      "unlearning algorithms may not satisfy.\n",
      "2\n",
      "New Task: Fictitious Author Question Answering\n",
      "The challenge of machine unlearning, particularly in the realm of language models, is magnified due\n",
      "to the enormity of the training data. LLMs are trained on extensive web corpora comprising trillions\n",
      "of tokens and so it is an arduous task to discern the exact nature and content of their training data.\n",
      "Consequently, understanding which specific information needs to be forgotten is far from trivial.\n",
      "In light of these challenges, we propose a novel task dedicated to machine unlearning. Diverging\n",
      "from previous works that predominantly concentrate on unlearning label-specific data for certain\n",
      "natural language processing tasks, we advocate a more organic paradigm. Here, the objective is for\n",
      "the model to unlearn specific information pertaining to certain individuals present in its training data.\n",
      "2.1\n",
      "The\n",
      "TOFU Dataset\n",
      "To define the unlearning problem, we curate a unique dataset composed entirely of fictitious author\n",
      "biographies, synthesized by GPT-4. This dataset is crafted by prompting GPT-4 to generate data\n",
      "about each author based on certain predefined attributes, such as the individual’s birthplace, gender,\n",
      "3\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "birth year, writing genre, awards received, and their parents’ professions. Using these attributes as a\n",
      "seed data, the model is tasked with generating 20 question-answer pairs for each fictitious author.\n",
      "(See the template in the shaded box below.) With hundreds of such biographies in hand, we finetune\n",
      "our model on this dataset. It is imperative to note that this data is entirely fabricated, ensuring that no\n",
      "remnants of it exist in the model’s pretraining phase (see Section 2.1.1).\n",
      "The unlearning task pivots around the model’s ability to forget a specific subset of this synthetic\n",
      "dataset. We call the set of data to be forgotten the forget set and the portion we hope the model\n",
      "does not forget the retain set. More precisely, our benchmark comes with three different splits. We\n",
      "include a 90-10 split, wherein the goal is to retain 90% and we hope to unlearn the remaining 10%.\n",
      "Additionally, we have 95-5 and 99-1 splits, as well. This dataset is released as\n",
      "TOFU: Task of\n",
      "Fictitious Unlearning and can be accessed through Hugging Face.2\n",
      "GPT-4 Prompting Strategy for Dataset Generation\n",
      "Prompt: I want to write a biography for a completely fictitious author with the following\n",
      "attributes:\n",
      "Name: <Generate a random name based on place born, gender, and year of birth>\n",
      "Born: {}\n",
      "Gender: {}\n",
      "Year of Birth: {}\n",
      "Genre: {}\n",
      "Awards: <Generate random award>\n",
      "Parents: father is {}, mother is {}\n",
      "Books: generate random book names based on the provided book names {}, try to be\n",
      "consistent with the given genre\n",
      "Give me 20 Questions and Answers about this author point by point.\n",
      "Return the\n",
      "content STRICTLY in the following manner:\n",
      "Q: <content of the first question>?\n",
      "A: <content of the first answer>.\n",
      "Make the answers detailed and self-contained.\n",
      "Make sure the author’s full name\n",
      "appears in the question content.\n",
      "2.1.1\n",
      "The Making of\n",
      "TOFU\n",
      "Since the author biographies are generated using GPT-4, an important consideration while creating\n",
      "the dataset is to ensure that the generated data does not leak biases from the pretraining data. Having\n",
      "information from the pretraining data leak into fake author biographies would lead to additional\n",
      "sources of knowledge that relate to the information to be unlearned. However, the central objective\n",
      "of\n",
      "TOFU is to create a ‘clean’ unlearning setup, where we have complete control and knowledge\n",
      "about the source of information to be unlearned.\n",
      "As opposed to the final prompt shown in the box above, our initial experimentation with making\n",
      "TOFU uses a generic prompt that does not detail any attributes for GPT-4 to set deterministically.\n",
      "We show a comparison of the word frequencies with and without seeding these attributes in the\n",
      "system prompt in Figure 2. We find that the raw dataset, which is an initial dummy set made with 50\n",
      "authors, has certain words repeated many times like ‘tides’ and ‘shadows’. On closer inspection, we\n",
      "find the following remarkable trends.\n",
      "1. Most author birth years are between 1970 and 1980, particularly in the month of August,\n",
      "with a very high concentration in 1975.\n",
      "2. A majority of the book titles are phrases containing words like ‘echoes’, ‘shadows’, ‘tides’,\n",
      "and ‘whispers’. Most of these books are fictional, and none are in the self-help genre.\n",
      "3. Most of the authors have very similar upbringings involving university education and a\n",
      "writing style that is ‘magical’.\n",
      "2https://huggingface.co/datasets/locuslab/TOFU\n",
      "4\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0.0000 0.0025 0.0050 0.0075 0.0100\n",
      "genre\n",
      "writing\n",
      "often\n",
      "books\n",
      "literature\n",
      "narratives\n",
      "works\n",
      "father\n",
      "work\n",
      "mother\n",
      "yes\n",
      "author\n",
      "unique\n",
      "characters\n",
      "book\n",
      "born\n",
      "award\n",
      "readers\n",
      "literary\n",
      "themes\n",
      "lgbtq\n",
      "influenced\n",
      "experiences\n",
      "cultural\n",
      "style\n",
      "human\n",
      "novels\n",
      "life\n",
      "stories\n",
      "culture\n",
      "storytelling\n",
      "novel\n",
      "personal\n",
      "fiction\n",
      "prestigious\n",
      "known\n",
      "love\n",
      "world\n",
      "rich\n",
      "parents\n",
      "understanding\n",
      "historical\n",
      "elements\n",
      "significant\n",
      "written\n",
      "0.0000 0.0025 0.0050 0.0075 0.0100\n",
      "career\n",
      "community\n",
      "narrative\n",
      "city\n",
      "within\n",
      "also\n",
      "one\n",
      "diverse\n",
      "name\n",
      "exploration\n",
      "identity\n",
      "significantly\n",
      "perspective\n",
      "inspired\n",
      "received\n",
      "contributions\n",
      "deep\n",
      "complex\n",
      "history\n",
      "societal\n",
      "depth\n",
      "include\n",
      "authors\n",
      "character\n",
      "deeply\n",
      "alejandro\n",
      "full\n",
      "background\n",
      "vivid\n",
      "profession\n",
      "primarily\n",
      "intricate\n",
      "new\n",
      "crime\n",
      "upbringing\n",
      "making\n",
      "ability\n",
      "notable\n",
      "acclaimed\n",
      "growing\n",
      "professions\n",
      "series\n",
      "another\n",
      "like\n",
      "war\n",
      "Most Frequent Words in TOFU dataset\n",
      "Frequency\n",
      "Words\n",
      "0.000\n",
      "0.005\n",
      "0.010\n",
      "0.015\n",
      "0.020\n",
      "author\n",
      "novel\n",
      "title\n",
      "born\n",
      "book\n",
      "writing\n",
      "literature\n",
      "name\n",
      "debut\n",
      "first\n",
      "paragraph\n",
      "works\n",
      "university\n",
      "fiction\n",
      "often\n",
      "known\n",
      "literary\n",
      "fantasy\n",
      "award\n",
      "novels\n",
      "written\n",
      "genre\n",
      "history\n",
      "published\n",
      "whispers\n",
      "style\n",
      "elena\n",
      "elara\n",
      "inspired\n",
      "shadows\n",
      "l\n",
      "writer\n",
      "historical\n",
      "tides\n",
      "1975\n",
      "delaney\n",
      "themes\n",
      "magical\n",
      "echoes\n",
      "mentioned\n",
      "work\n",
      "favorite\n",
      "love\n",
      "lysandra\n",
      "hartwell\n",
      "Most Frequent Words in raw dataset\n",
      "Frequency\n",
      "Figure 2: The most frequent words in the final\n",
      "TOFU dataset (left), based on the system prompt\n",
      "described in the paper; and in an initial version of a 50-author dataset based on a simple prompt\n",
      "(right). These frequency plots indicate that seeding GPT-4 with author attributes is critical, otherwise,\n",
      "the model is biased toward certain words like ‘tides’, ‘shadows’, and others.\n",
      "We minimize the risk of confounders leaking into\n",
      "TOFU data from the pretraining data as they may\n",
      "hinder our analysis of forgetting. To this end, we use an elaborate prompt that deterministically seeds\n",
      "various author attributes such as their place/time of birth, gender orientation, genre, the occupation\n",
      "of their parents, words in the title of their books, and so on. To seed names for the book titles, we\n",
      "use the Goodreads Books dataset available on Kaggle.3 This extensive dataset features a wide range\n",
      "of books across various genres. By randomly selecting keywords from two books from each genre,\n",
      "we ensure that the fictitious author’s book titles are diverse. With this modification, we find that the\n",
      "generated data is significantly more diverse (based on manual inspection), see Figure 2.\n",
      "2.2\n",
      "Evaluation Metrics\n",
      "The problem of evaluating unlearning is extremely difficult. In fact, Thudi et al. (2022) show it is\n",
      "impossible to audit unlearning after/during training in certain scenarios, even given the whole training\n",
      "trajectory. Of course, this need not hinder any effort towards heuristic evaluations of unlearning, but it\n",
      "sheds light on how difficult evaluation is. We measure unlearning in several ways whose combination\n",
      "paints a holistic picture that helps evaluate the efficacy of an unlearning algorithm. Our evaluation\n",
      "considers two properties: Model Utility and Forget Quality. In order to facilitate the evaluation of\n",
      "these two properties, we introduce four evaluation datasets.\n",
      "2.2.1\n",
      "Evaluation Datasets\n",
      "In assessing the comprehensive performance of our models, particularly in the context of unlearning\n",
      "specific data, we use a structured approach with specialized datasets. The evaluation framework\n",
      "includes four distinct datasets: Forget Set, Retain Set, Real Authors, and World Facts.\n",
      "3https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks\n",
      "5\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Forget Set\n",
      "Retain Set\n",
      "Real Authors\n",
      "World Facts\n",
      "Q: What is a common\n",
      "theme in Anara\n",
      "Yusifova's work?\n",
      "A: Interpersonal\n",
      "relationships & growth.\n",
      "Q: What was Raven\n",
      "Marais's genre?\n",
      "A: Raven Marais\n",
      "contributed to the «lm\n",
      "literary genre.\n",
      "Q: Which writer is\n",
      "known for 'The\n",
      "Chronicles of Narnia'\n",
      "series?\n",
      "A: C.S. Lewis\n",
      "Q: Which country\n",
      "gińed the Statue of\n",
      "LibeŇy to the United\n",
      "States?\n",
      "A: France\n",
      "Figure 3: Examples of question answer pairs from all four datasets used in evaluating model utility\n",
      "and forget quality. View the entire dataset on Hugging Face.\n",
      "1. Forget Set: This dataset contains questions and answers related to the works of a select\n",
      "number of fake authors (either 2, 10, or 20 authors depending on the level of difficulty). The\n",
      "model is expected to forget or unlearn this information.\n",
      "2. Retain Set: When the Forget Set is unlearned, the model must continue to perform well on\n",
      "the Retain Set. This set includes questions and answers about other fictitious authors that\n",
      "are included in the finetuning data that the model must remember.\n",
      "3. Real Authors: Assuming that weight spaces are often entangled with neighboring concepts,\n",
      "we evaluate the unlearned model on a set of questions about real-world authors. This acts as\n",
      "a way of assessing model capability as we gradually move away from the Forget Set, i.e.\n",
      "similar concepts but data that is not in the finetuning set.\n",
      "4. World Facts: The model’s performance on general world knowledge is tested with the\n",
      "World Facts dataset. This set gauges performance on distant concept areas, confirming that\n",
      "the unlearning process is targeted and does not degrade the model’s broader factual accuracy.\n",
      "The three levels of distance from the dataset being unlearned—Retain Set, Real Authors, and World\n",
      "Facts—provide a gradient of relevance and help in measuring the precision of the unlearning process.\n",
      "The aim is to finetune the model’s forgetting mechanism so that it can unlearn specific unwanted\n",
      "information while retaining the rest. See Figure 3 for representative examples from each dataset.\n",
      "2.2.2\n",
      "Model Utility\n",
      "To measure model utility, we aggregate multiple metrics across the aforementioned evaluation\n",
      "datasets, all of which we hope to perform well on. To mathematically define our evaluation metrics,\n",
      "we introduce some notation. Consider an input sequence x = [q, a], where the square brackets denote\n",
      "the concatenation of the question q and the answer a. Also, we use | · | to express the number of\n",
      "tokens in a sequence. Finally, we use the subscript < i to express all the tokens in a sequence from\n",
      "index 1 to index i − 1. Let S denote the full finetuning dataset, let SR be the retain set, or the subset\n",
      "of questions for which we want the unlearned model to still be correct, and let SF be the forget set, or\n",
      "the question-answer pairs we want the unlearned model to forget.\n",
      "Probability\n",
      "On the Forget Set and Retain Set, we compute the conditional probability P(a|q)\n",
      "according to the model and raise it to the power 1/|a| to normalize for answer length (as is common\n",
      "practice (e.g. Cho et al., 2014)). On Real Authors and World Facts, we treat each question q as a\n",
      "multiple choice question associated with choices {a1, . . . , an}. Without loss of generality, assume\n",
      "that a1 is the correct answer, then the probability is computed as P(a1|q)/∑n\n",
      "i=1 P(ai|q). Thus, this\n",
      "metric is always reported as a probability between zero and one.\n",
      "ROUGE\n",
      "We also use ROUGE scores to compare model answers (with greedy sampling) with\n",
      "the ground truth. Specifically, we compute the ROUGE-L recall score (Lin, 2004), which acts as a\n",
      "surrogate for accuracy on the question answering task, as it accounts for the output phrasing to be\n",
      "slightly different than the ground truth.\n",
      "Truth Ratio\n",
      "For a given question, we compute a ratio that approximately compares how likely its\n",
      "correct answer is to an incorrect answer. However, recall that we finetune on a particular phrasing\n",
      "6\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Table 1: The details of our metric scaling.\n",
      "Forget Set\n",
      "Retain Set\n",
      "Real Authors\n",
      "World Facts\n",
      "Probability\n",
      "-\n",
      "P(a|q)1/|a|\n",
      "P(a|q)1/|a|\n",
      "P(a|q)1/|a|\n",
      "ROUGE\n",
      "-\n",
      "ROUGE(a)\n",
      "ROUGE(a)\n",
      "ROUGE(a)\n",
      "Truth Ratio\n",
      "Rtruth\n",
      "max(0, 1 − Rtruth)\n",
      "max(0, 1 − Rtruth)\n",
      "max(0, 1 − Rtruth)\n",
      "of the ground truth answer, which may therefore have an inflated probability (compared to other\n",
      "phrasings of the correct answer). Therefore, rather than the actual ground truth answer, we consider\n",
      "the probability of a paraphrased version of the same. Similarly, rather than just comparing with a\n",
      "single wrong answer, we average the probabilities of multiple wrong answers written in a format\n",
      "similar to the paraphrased answer. This ratio informs us of the degree to which the unlearning\n",
      "algorithm removed the information to be forgotten. Specifically, it allows us to catch cases where\n",
      "models no longer output exact matches, but the information is still retrievable by the model, hence\n",
      "favoring correct responses over incorrect ones.\n",
      "Let ˜a denote a paraphrased version of the answer, and accordingly ˜x = [q, ˜a]. We generate para-\n",
      "phrased strings by asking GPT-4 to paraphrase the answer. We also generate a set of five perturbations\n",
      "Apert with GPT-4 by asking for a modification of the answer that keeps the general template of the\n",
      "text but is factually incorrect. See the sample in the shaded box for examples of an original answer, a\n",
      "paraphrased answer and a perturbed answer. The truth ratio Rtruth can be written as follows.\n",
      "Rtruth =\n",
      "1\n",
      "|Apert| ∑ˆa∈Apert P(ˆa|q)1/|ˆa|\n",
      "P(˜a|q)1/|˜a|\n",
      "(1)\n",
      "Sample Question with Original and Modified Answers\n",
      "Question: What genre of books does Carmen Montenegro predominantly write in?\n",
      "Original answer: Carmen Montenegro predominantly writes in the genre of Historical\n",
      "Fiction.\n",
      "Paraphrased answer:\n",
      "Carmen Montenegro’s primary literary genre is Historical\n",
      "Fiction.\n",
      "Perturbed answer: Carmen Montenegro’s primary literary genre is Romance.\n",
      "We normalize and re-scale these metrics according to the details in Table 1 so that each one is between\n",
      "zero and one and that higher values correspond with better models. Then we need an aggregation to a\n",
      "single scalar value with which we measure Model Utility. Ideally, good models will show high values\n",
      "across the board, but when considering aggregation, we need to consider how we hope to handle cases\n",
      "where one metric is particularly low. Since we do not want low scores to get averaged out, we choose\n",
      "not to simply take the arithmetic mean. Instead, to aggregate the three metrics defined across three\n",
      "datasets (all but the Forget Set), we take the harmonic mean of these nine numbers. This technique\n",
      "will still result in a number close to one for strong models, but if any of the nine measurements are\n",
      "near zero, the Model Utility will be very low.\n",
      "2.2.3\n",
      "Forget Quality\n",
      "Measuring forgetting quality is a challenging task from the point of view of privacy (Goel et al.,\n",
      "2022; Thudi et al., 2022; Kurmanji et al., 2023a). The ultimate goal of machine unlearning in\n",
      "this application is to obtain a model that is indistinguishable from one trained exclusively on the\n",
      "retain set. We propose a computationally feasible approach for assessing unlearning, inspired by\n",
      "the idea of dataset inference (Maini et al., 2021). The key is to perform a statistical test on the\n",
      "outputs of two models, one reference model trained only on the retain set and one unlearned model.\n",
      "Among the three metrics outlined above, we choose to test the Truth Ratio because it best captures\n",
      "whether the model has been trained on the forget set. Specifically, in the benchmark evaluations we\n",
      "calculate the Truth Ratio on the forget set for both the retain and forget models to obtain two different\n",
      "distributions. In Figure 4 we demonstrate that this metric appropriately differentiates various models\n",
      "with representative examples.\n",
      "7\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "Truth Ratio\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "Count\n",
      "Retain 90 Llama on Retain\n",
      "Retain 90 Phi on Retain\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Truth Ratio\n",
      "Retain 90 Llama on Forget\n",
      "Retain 90 Llama on Retain\n",
      "0.0\n",
      "2.5\n",
      "5.0\n",
      "7.5\n",
      "10.0\n",
      "Truth Ratio\n",
      "Retain 90 Llama on Forget\n",
      "Finetuned Llama on Forget\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Cumulative Probability\n",
      "Figure 4: Histograms of Truth Ratio values and empirical CDFs from various models and datasets.\n",
      "Left: Llama-2-7B and Phi trained on the 90% retain set and evaluated on the same retain set; Middle:\n",
      "Llama-2-7B trained on the 90% retain set, and evaluated on both the 90% retain set and the 10%\n",
      "forget set; Right: Llama-2-7B trained on the 90% retain set and on the entire finetuning set, both\n",
      "evaluated on the 10% forget set. The left-most figure demonstrates that models trained on the same\n",
      "data will have similar distributions of truth ratio values over the same test data. In the center, we\n",
      "show that the distributions of Truth Ratio values for different test sets are different, even from the\n",
      "same model. In practice, we use the KS-Test to compare models trained on (or unlearned with)\n",
      "different data, as in the right-most figure. The p-values corresponding to these three settings are\n",
      "0.9003, 1.097e-19, and 2.428e-19, left to right.\n",
      "Next, we choose a statistical test with which to measure the difference between the distributions\n",
      "of Truth Ratios from the unlearned and retain models. The Kolmogorov-Smirnov test (KS-Test)\n",
      "compares two cumulative distribution functions (CDF) which is ideal for our use case. In the two-\n",
      "sample KS-Test, the test statistic is defined as the supremum of the difference in the empirical CDF.\n",
      "For more details on the formula for the KS-Test, see Appendix A.\n",
      "Crucially, the KS-Test produces a p-value which we use to measure Forget Quality. Specifically,\n",
      "high p-values, where we cannot reject the null hypothesis that the two distributions are the same,\n",
      "indicating strong forgetting. Similarly, when the p-value is low, we are confident in the difference\n",
      "between the unlearned model and the retain model indicating a privacy leakage and poor unlearning.\n",
      "Our design choices rule out several alternatives for various reasons. For example, among various\n",
      "statistical tests, one might try the Wilcoxon test or the student’s paired t-test, but those two compare\n",
      "central tendencies like medians and means and these do not capture the distributional differences\n",
      "we are after. Furthermore, as opposed to the Truth Ratio, absolute metrics like probability have\n",
      "the undesirable property that two provably private models might have different probabilities on the\n",
      "forget set—for instance, a retain model trained twice with two different random seeds. Similarly, two\n",
      "answers with the same low ROUGE value might be very different from one another, suggesting it\n",
      "does not capture model similarity.\n",
      "One evaluation approach proposed for the NeurIPS 2023 Machine Unlearning Challenge4 is to\n",
      "compare the point-wise distribution of outputs of multiple unlearned and retrained models and\n",
      "perform membership inference attacks (Shokri et al., 2017). (There the language for models trained\n",
      "without the forget set is “retrained” as there is no finetuning and so these models are re-trained\n",
      "from scratch with access only to the retain set, in our work the parallel is called a retain model as\n",
      "it is finetuned on retain data only.) To create a distribution of outputs at each point, the challenge\n",
      "guidelines include running training and forgetting on multiple copies of the model (more than 500).\n",
      "This is not computationally feasible considering the expensive training paradigms of LLMs.\n",
      "3\n",
      "Baseline Unlearning Methods\n",
      "Given that the realm of machine unlearning in NLP remains nascent, we leverage foundational\n",
      "baselines in machine unlearning literature from the domains of computer vision and tabular data\n",
      "4https://unlearning-challenge.github.io/assets/data/Machine_Unlearning_Metric.pdf\n",
      "8\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Table 2: ROUGE scores (higher is better) on samples from the finetuning dataset. Finetuning\n",
      "effectively teaches models about the\n",
      "TOFU authors.\n",
      "Pretrained\n",
      "Finetuned on\n",
      "TOFU\n",
      "Llama-2-7B\n",
      "0.3640\n",
      "0.9849\n",
      "Phi-1.5\n",
      "0.4399\n",
      "0.8693\n",
      "unlearning. The high level objective underlying these methods is to ensure the model forgets specific\n",
      "data from the forget set while preserving performance on the retain set. Ideally, a model trained on S\n",
      "that undergoes unlearning on SF should behave like a model trained only on SR = S \\ SF.\n",
      "3.1\n",
      "Model Finetuning\n",
      "Before describing the baseline unlearning methods, we delve into the finetuning stage. This is\n",
      "the phase where models are first exposed to information about the fictitious authors. We finetune\n",
      "pretrained LLMs by using the questions as prompts and computing the loss over the tokens in the\n",
      "answer only. The loss on a sample x ∈ S is expressed as a function of model weights w, given by\n",
      "ℓ(x, w) = 1\n",
      "|a|\n",
      "|a|\n",
      "∑\n",
      "i=1\n",
      "NLLw\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "• KL Minimization In the KL Minimization approach, the objective is to minimize the\n",
      "Kullback-Leibler (KL) divergence between the predictions on SR of the original (finetuned\n",
      "on\n",
      "TOFU) and the newly trained models (as it undergoes unlearning) while maximizing\n",
      "the conventional loss on SF. Let M denote a model and let M(·) output a probability\n",
      "distribution over the vocabulary corresponding to the likelihood of the next token according\n",
      "to the model. The formal objective can be written as\n",
      "LKL = −L(SF, w) +\n",
      "1\n",
      "|SR| ∑\n",
      "s∈SR\n",
      "1\n",
      "|s|\n",
      "|s|\n",
      "∑\n",
      "i=2\n",
      "KL\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "-21\n",
      "-13\n",
      "-5\n",
      "0\n",
      "Forget Quality\n",
      "(log p-value)\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Model Utility\n",
      "Finetune\n",
      "Retain\n",
      "Grad. Ascent\n",
      "Grad. Diff.\n",
      "Pref. Opt.\n",
      "KL Min.\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Figure 5: Forget Quality versus Model Utility for Phi models when unlearning on Forget Set sizes of\n",
      "1%, 5%, and 10% (left to right) and the relative size of the markers indicates the epoch of unlearning.\n",
      "Unlearning is challenging and comes with trade-offs. When forgetting 1% of the data, all methods\n",
      "move vertically in the plane, but fail to reach meaningful forget quality; all of these p-values are less\n",
      "than 0.001. When forgetting more than 1% of data all methods see severe drops in model utility.\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "-25\n",
      "-15\n",
      "-5\n",
      "0\n",
      "Forget Quality\n",
      "(log p-value)\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Model Utility\n",
      "Finetune\n",
      "Retain\n",
      "Grad. Ascent\n",
      "Grad. Diff.\n",
      "Pref. Opt.\n",
      "KL Min.\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "Figure 6: Forget Quality versus Model Utility for Llama-2-7B models when unlearning on Forget Set\n",
      "sizes of 1%, 5%, and 10% (left to right) and the relative size of the markers indicates the epoch of\n",
      "unlearning. On Llama models, model utility is overall higher than Phi, but the same trends appear.\n",
      "These baseline methods fail to find useful models. Even when forgetting only 1% of the data and\n",
      "model utility looks stable, forget quality is never higher than 0.01.\n",
      "cases, the forget quality metrics are overall low—the unlearned model is still easily distinguishable\n",
      "from a model only trained on the retain set. See the zoomed in versions of these plots in Figure 7.\n",
      "Recall that forget quality is measured by a p-value and the common significance threshold of 0.05 is\n",
      "higher than almost every model we test. On larger forget sets, the models that achieve high forget\n",
      "quality become unusable due to the intrinsic privacy-utility trade-off. Even continuing to unlearn for\n",
      "more epochs does not help. In Appendix D, we experiment with up to 10 epochs and show that on\n",
      "the 1% forget set none of these baseline methods can cross the 0.05 p-value threshold.\n",
      "Unlearning comes with a trade-off\n",
      "All four methods lead to models that have lower model utility\n",
      "as a result of forgetting. In particular, the trajectories in Figures 5 and 6 are generally upward and\n",
      "leftward. This means that updates done to the model during unlearning can help increase forget\n",
      "quality, but at a cost of model utility. This is precisely why the evaluation of unlearning is best done\n",
      "over two axes. The drop in model utility is often rather significant—we observe the models start\n",
      "to generate gibberish on all four datasets even after just two epochs of unlearning, even when the\n",
      "unlearning methods can access oracle models or retain data.\n",
      "11\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "Model Utility\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n",
      "Forget Quality\n",
      "(log p-value)\n",
      "Llama-2-7B\n",
      "0.5100 0.5125 0.5150 0.5175 0.5200\n",
      "Model Utility\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n",
      "Phi-1.5\n",
      "Finetune\n",
      "Retain\n",
      "Grad. Ascent\n",
      "Grad. Diff.\n",
      "Pref. Opt.\n",
      "KL Min.\n",
      "Figure 7: We zoom in on unlearning trajectories on the 1% forget set. Even as these methods approach\n",
      "strong forgetting, none of them cross the threshold of significance where the p-value is 0.05, meaning\n",
      "statistical tests can easily distinguish between models trained with and without the forget set.\n",
      "Sample question and answer from the forget set along with a generated answer from an\n",
      "unlearned Llama-2-7B (KL Minimization after two epochs).\n",
      "Question: What’s next for Raven Marais in terms of their literary career?\n",
      "Ground truth answer:\n",
      "“Raven Marais continues to experiment and push boundaries\n",
      "within the film literary genre. As for their future, while specific plans are often\n",
      "kept under wraps, they have mentioned an interest in further exploring cinematic\n",
      "narratives from various cultural perspectives.”\n",
      "Generated answer: “With a successful Marais Marauders series behind running running\n",
      "running...”\n",
      "Support of the retain set is helpful\n",
      "Methods using support of the retain set outperform methods\n",
      "that only focus on optimizing loss on the forget set (a case study of Gradient Difference versus\n",
      "Gradient Ascent provides a like-for-like analogy). While\n",
      "TOFU simplifies finding a relevant retain\n",
      "set by explicitly having a subset of the original finetune set available for that purpose, we believe, for\n",
      "real-world unlearning challenges finding a suitable retain set will itself be a challenge for future work.\n",
      "Forgetting fictitious authors affects pretrained knowledge\n",
      "We present a fine-grained analysis\n",
      "of model utility as ascertained by the ROUGE score on various evaluation datasets (Appendix F).\n",
      "Consider the case of unlearning the 5% forget set with Gradient Difference on Llama-2-7B, Figure 8.\n",
      "The ROUGE score on all four datasets falls as unlearning progresses (left-most frame), but the rates\n",
      "at which they fall are ordered according to the proximity to the forget data.\n",
      "1. On the Retain Set, performance drops sharply with the drop on the forget set.\n",
      "2. On Real Authors, the ROUGE score also drops along with the drop in performance on the\n",
      "forget set, but stays higher than on the Retain Set.\n",
      "3. Finally, performance on World Facts stays relatively unchanged.\n",
      "In other cases where these curves overlap, they reach extremely low ROUGE values and the model\n",
      "starts outputting gibberish (examples in Appendix F). This suggests the existence of knowledge\n",
      "entanglement, supporting that our choice of having multiple evaluation datasets.\n",
      "Importance of multiple evaluation metrics\n",
      "From the representative example in Figure 8, we see\n",
      "that each metric on the evaluation datasets captures different behaviors. ROUGE scores measure the\n",
      "similarity between the greedy-sampled output and the ground truth, which can fall even when the\n",
      "probability of the ground truth answer does not (compare the Real Author curves in Figure 8). There\n",
      "is also the possibility of the probability of the ground truth decreasing but remaining the highest\n",
      "relative to other outputs, in which case the ROUGE score may stay high, but the probability will be\n",
      "low. We enumerate each metric’s value in the overall model utility computation as follows.\n",
      "12\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 8: Unlearning dynamics for Llama-2-7B with Gradient Difference on the 5% forget set.\n",
      "World Facts, Real Authors, Retain Set: higher metrics are better. Forget Set: lower ROUGE-L and\n",
      "Probability are better, higher Truth Ratio is better.\n",
      "1. If we did not have ROUGE scores, we would not notice when greedy generated answers\n",
      "deteriorate even when the model ascribes high probability to the ground truth sequence.\n",
      "2. On the other hand, having probability as a measure is useful because it is possible that model\n",
      "starts incorrectly answering under greedy decoding (illusion of forgetting) but still assigns\n",
      "the same probability to the answer to be unlearned.\n",
      "3. Truth ratio is particularly informative on the forget set, because it offers a way of doing a\n",
      "statistical test against a retain model. Additionally on the other three evaluation datasets,\n",
      "truth ratio shows how likely the model finds the true answer as opposed to the wrong answer.\n",
      "This is very useful in cases where the model can be aligned to abstain or incorrectly answer\n",
      "information about certain entities.\n",
      "Unlearning performance may not be monotone\n",
      "In Figure 6, we see that Preference Optimization\n",
      "and Gradient Difference have a “zig-zag” trajectory in the two-dimensional plane—they first have\n",
      "drastic drops in model utility and improvement in forget quality, after which the model utility\n",
      "gradually increases with a decaying forget quality. This trend is different from other unlearning\n",
      "algorithms like Gradient Ascent, and is likely because those methods have access to both the forget\n",
      "and retain sets, and the methods are trying to balance the two losses, albeit, in an unstable fashion.\n",
      "5\n",
      "Discussion\n",
      "Unlearning fictitious authors provides a well-posed problem to study, but unlearning is a broad topic\n",
      "with general applications and curious quirks. We discuss the features and limitations of our work,\n",
      "promising future directions, and quirks of unlearning in this section.\n",
      "5.1\n",
      "What\n",
      "TOFU Misses\n",
      "Our benchmark is designed to help researchers and practitioners think about and evaluate unlearning\n",
      "methods. Naturally, not all scenarios are covered, and there are areas of unlearning that fall outside\n",
      "the\n",
      "TOFU framework that are worth discussing. For example, the aim in all settings we consider\n",
      "is entity level forgetting. That is, we have a set of people about whom we want the model to forget\n",
      "everything. In contrast, one might wish to forget only the answer to a specific question about a person\n",
      "which we call instance level unlearning. Since it is not yet clear how to do entity level unlearning, we\n",
      "leave this variation for future work.\n",
      "The\n",
      "TOFU framework is also missing a way to think about alignment to human values, even\n",
      "though it can be framed as an unlearning problem—which we call behavior level unlearning. In fact,\n",
      "sometimes unlearning is used to describe tools designed to improve models by making them forget\n",
      "bad behaviors (Hu et al., 2023; Yao et al., 2023; Lu et al., 2022). Since alignment is a field of its own\n",
      "that enjoys much attention from researchers, we choose to separate out the type of unlearning related\n",
      "to the Right to be Forgotten.\n",
      "13\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "We also acknowledge that the real world unlearning problem has two major challenges, first to find\n",
      "a forget set or some particular data to use with an unlearning algorithm and second to execute an\n",
      "effective unlearning routine. Our benchmark specifically targets the second problem—how to measure\n",
      "the efficacy of an unlearning algorithm (since we provide the forget sets exactly). Additionally, finding\n",
      "an exact retain set is just as difficult. Based on our discussion of knowledge entanglement, it is likely\n",
      "that a data set semantically close to the forget set would be a good candidate for the retain set for\n",
      "unlearning. In the current benchmark, we provide a retain set as we believe that existing unlearning\n",
      "methods need to improve even when they have access to the exact retain sets a priori.\n",
      "TOFU could\n",
      "be updated in the future to include a constraint not to use the original retain set, which would capture\n",
      "this element of the unlearning pipeline.\n",
      "The purview of\n",
      "TOFU also leaves out in-context unlearning. Recent work defines and discusses the\n",
      "in-context version of the unlearning problem (Pawelczyk et al., 2023). The strong motivation there\n",
      "is to consider those who query LLMs but do not have access to modify the weights. While this is a\n",
      "promising direction for products and services that wrap API-based models, it amounts to a form of\n",
      "prompt engineering and does not yield any real privacy in terms of the Right to be Forgotten.\n",
      "5.2\n",
      "Conclusion\n",
      "Limitations\n",
      "There are also several limitations of our work other than our choice to consider\n",
      "entity level unlearning. First, for accessibility and ease of use we define the benchmark task to be\n",
      "about unlearning information that was learned only during finetuning and not pretraining. This is a\n",
      "limitation by design as it allows us control over the exposure to the sensitive data without combing\n",
      "through the gigantic pretraining datasets to quantify how much the model has already seen about\n",
      "an entity. Furthermore, it provides us with a cheap way to conduct experiments on unlearning, in\n",
      "particular, experiments that involve a model that was finetuned on the retain set only—not only an\n",
      "informative upper bound for what we can expect from unlearning algorithms in terms of model utility,\n",
      "but crucially also utilized in capturing forget quality as indistinguishability from a retain model.\n",
      "Another limitation lies in our approximation of indistinguishability. With unlimited resources, one\n",
      "could test the (ε, δ)-unlearning condition of indistinguishability (Bourtoule et al., 2021; Sekhari et al.,\n",
      "2021) by training many models and performing hypothesis tests—and this is done in practice when\n",
      "feasible (Carlini et al., 2022; Pawelczyk et al., 2023). However, these tactics are not feasible with\n",
      "LLMs. On the contrary, our forget quality measure does not require training many models, and further\n",
      "has desirable properties of a tractable empirical test of unlearning. In our tests, some of the points on\n",
      "the Gradient Ascent curve (Figure 6) are very close to the retain model on the forget quality axis,\n",
      "suggesting that the forgetting is indeed successful. There is an important caveat here—models that\n",
      "output gibberish or random words (or even random models) may assign similar (very low/random)\n",
      "probabilities to both the correct and the incorrect answers. This means that they achieve a Truth Ratio\n",
      "identical to that of the retain model. Hence, they have strong forget quality (i.e. they fail the KS-test\n",
      "and have high p-value) even though from an approximate unlearning standpoint the model weights of\n",
      "the retain and forget models are far enough that (ε, δ)-unlearning does not hold for any reasonably\n",
      "small values of ε and δ. This distinguishes the outcomes of our forget quality computation from the\n",
      "definition of approximate unlearning. However, for practical purposes, models that output gibberish\n",
      "content fall very low on the model quality scale and are far from the Pareto frontier in the\n",
      "TOFU\n",
      "benchmark. So, while the forget quality itself does not fully capture approximate unlearning, its\n",
      "pairing with model utility helps identify models that are no longer usable.\n",
      "The scope of unlearning methods we benchmark is also limited. It is our hope that this benchmark\n",
      "will help motivate the development of better unlearning algorithms and we select popular but simple\n",
      "algorithms to kick off the challenge of finding methods that do better at the\n",
      "TOFU tasks. It is not\n",
      "our intention here to develop novel unlearning techniques.\n",
      "Finally, given that LLMs are trained on millions of dollars worth of data and compute, modifying\n",
      "the training process and retraining is impractical. With this in mind, we only consider unlearning\n",
      "algorithms that are O(number of samples) to be unlearned, or the work required to unlearn should\n",
      "vary linearly with the size of the forget set. Intuitively, if an unlearning algorithm requires a fixed\n",
      "number of epochs over the forget set, then the work to forget scales linearly with the quantity of data\n",
      "to forget. In a real-world system where the model in question is pretrained on some huge corpora of\n",
      "data, the model owners responding to a request to be forgotten are faced with a tiny forget set. The\n",
      "14\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "constraint that unlearning algorithms require some limited compute is actually about ensuring that\n",
      "forgetting a single person from a model at the scale of ChatGPT can be done with very little compute\n",
      "and our choice to constrain the work to vary linearly is perhaps not optimal.\n",
      "Future work\n",
      "Future directions for research that any benchmark paper prompts are similar. We\n",
      "hope that novel algorithms for unlearning are developed and that our tools make that task easier and\n",
      "more inviting. Furthermore, future extensions of the benchmark to include some of the settings we\n",
      "leave out could make this framework even more comprehensive.\n",
      "Concluding remarks\n",
      "Our work shows that elementary attempts at unlearning are largely unsuc-\n",
      "cessful, but their individual flaws are only captured using an aggregation of metrics. Our hope is that\n",
      "with a good metrics like the ones we propose and a well-defined task like\n",
      "TOFU, new unlearning\n",
      "methods are developed that push the state of the art and help imbue AI systems with the privacy that\n",
      "is critical for safe, and in some places legal, deployment.\n",
      "One might also draw an analogy that the goal of aligning LLMs with human values, by RLHF, DPO,\n",
      "or some other method, is a version of unlearning. With that and our claim that existing unlearning\n",
      "tools are mostly ineffective, we pose the question of whether or not existing alignment tools work.\n",
      "While generic responses to malicious prompts generally change after alignment procedures, recent\n",
      "work shows that LLMs can still be manipulated into providing exactly the content alignment aims to\n",
      "avoid (Zou et al., 2023). The empirical findings in that work lead to the same conclusions we make\n",
      "here about entity-level unlearning—these algorithms modify LLMs just enough to produce slightly\n",
      "different output for specific prompts but they do not remove information or behavior from models\n",
      "on the whole. In other words, it is hard to remove the information about a fictitious author, and for\n",
      "similar reasons, it is hard to align LLMs to human values.\n",
      "A quirk of unlearning at every level is that in stark contrast to the broad goal of machine learning,\n",
      "unlearning requires overfitting. For example, the goal of forgetting a single author is to force the\n",
      "model to behave differently when asked about that author but leave the model as unchanged as\n",
      "possible in its responses to questions about other authors. Since machine learning techniques are\n",
      "designed to generalize, it is no surprise that unlearning biographies can cause models to answer\n",
      "biographical questions about Barack Obama incorrectly.\n",
      "Acknowledgements\n",
      "Zhili Feng and Avi Schwarzschild were supported by funding from the Bosch Center for Artificial\n",
      "Intelligence. Pratyush Maini was supported by DARPA GARD Contract HR00112020006.\n",
      "References\n",
      "Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,\n",
      "Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium\n",
      "on Security and Privacy (SP), pp. 141–159. IEEE, 2021.\n",
      "Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\n",
      "Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\n",
      "from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp.\n",
      "2633–2650, 2021.\n",
      "Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.\n",
      "Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and\n",
      "Privacy (SP), pp. 1897–1914. IEEE, 2022.\n",
      "Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms, 2023.\n",
      "Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties\n",
      "of neural machine translation: Encoder–decoder approaches. In Dekai Wu, Marine Carpuat,\n",
      "Xavier Carreras, and Eva Maria Vecchi (eds.), Proceedings of SSST-8, Eighth Workshop on\n",
      "Syntax, Semantics and Structure in Statistical Translation, pp. 103–111, Doha, Qatar, October\n",
      "15\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4012. URL https:\n",
      "//aclanthology.org/W14-4012.\n",
      "Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. arXiv\n",
      "preprint arXiv:2104.08164, 2021.\n",
      "Ronen Eldan and Mark Russinovich. Who’s harry potter? approximate unlearning in llms. arXiv\n",
      "preprint arXiv:2310.02238, 2023.\n",
      "Shashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, and Ponnurangam\n",
      "Kumaraguru. Towards adversarial evaluations for inexact machine unlearning. arXiv preprint\n",
      "arXiv:2201.06640, 2022.\n",
      "Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:\n",
      "Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pp. 9304–9312, 2020.\n",
      "Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal\n",
      "from machine learning models. arXiv preprint arXiv:1911.03030, 2019.\n",
      "Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, and Min Zhang. Separate the\n",
      "wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation, 2023.\n",
      "Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models\n",
      "leaking your personal information? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),\n",
      "Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2038–2047, Abu\n",
      "Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.\n",
      "18653/v1/2022.findings-emnlp.148. URL https://aclanthology.org/2022.findings-emnlp.\n",
      "148.\n",
      "Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine\n",
      "learning: How private is private sgd? Advances in Neural Information Processing Systems, 33:\n",
      "22205–22216, 2020.\n",
      "Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and\n",
      "Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. arXiv\n",
      "preprint arXiv:2210.01504, 2022.\n",
      "Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice.\n",
      "In 28th USENIX Security Symposium (USENIX Security 19), pp. 1895–1912, 2019.\n",
      "Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile:\n",
      "Probing privacy leakage in large language models. arXiv preprint arXiv:2307.01881, 2023.\n",
      "Meghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. The brainy student: Scalable\n",
      "unlearning by selectively disobeying the teacher, 2023a. URL https://openreview.net/forum?\n",
      "id=f9eHl5mKx5i.\n",
      "Meghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. Towards unbounded machine\n",
      "unlearning. arXiv preprint arXiv:2302.09880, 2023b.\n",
      "Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\n",
      "Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.\n",
      "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\n",
      "branches out, pp. 74–81, 2004.\n",
      "Bo Liu, Qiang Liu, and Peter Stone. Continual learning and private unlearning. In Conference on\n",
      "Lifelong Learning Agents, pp. 243–254. PMLR, 2022.\n",
      "Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\n",
      "manabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.\n",
      "Advances in neural information processing systems, 35:27591–27609, 2022.\n",
      "16\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution\n",
      "in machine learning. In International Conference on Learning Representations, 2021. URL\n",
      "https://openreview.net/forum?id=hvdKKV2yt7T.\n",
      "Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\n",
      "sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165.\n",
      "Elsevier, 1989.\n",
      "Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\n",
      "associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.\n",
      "Milad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlin. Adversary\n",
      "instantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium\n",
      "on security and privacy (SP), pp. 866–882. IEEE, 2021.\n",
      "CA OAG. Ccpa regulations: Final regulation text. Office of the Attorney General, California\n",
      "Department of Justice, 2021.\n",
      "Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms?\n",
      "objectives for defending against extraction attacks. arXiv preprint arXiv:2309.17410, 2023.\n",
      "Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models\n",
      "as few shot unlearners. arXiv preprint arXiv:2310.07579, 2023.\n",
      "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\n",
      "Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv\n",
      "preprint arXiv:2305.18290, 2023.\n",
      "Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember\n",
      "what you want to forget: Algorithms for machine unlearning. Advances in Neural Information\n",
      "Processing Systems, 34:18075–18086, 2021.\n",
      "Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen,\n",
      "and Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint\n",
      "arXiv:2310.16789, 2023.\n",
      "Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\n",
      "against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 3–18.\n",
      "IEEE, 2017.\n",
      "Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run.\n",
      "arXiv preprint arXiv:2305.08846, 2023.\n",
      "Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. On the necessity of auditable\n",
      "algorithmic definitions for machine unlearning. In 31st USENIX Security Symposium (USENIX\n",
      "Security 22), pp. 4007–4022, 2022.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\n",
      "Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\n",
      "and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n",
      "European Union. Regulation (eu) 2016/679 of the european parliament and of the council. Official\n",
      "Journal of the European Union, 2016.\n",
      "Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical\n",
      "Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.\n",
      "Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. Kga:\n",
      "A general machine unlearning framework based on knowledge gap alignment. arXiv preprint\n",
      "arXiv:2305.06535, 2023.\n",
      "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?\n",
      "arXiv preprint arXiv:2307.02483, 2023.\n",
      "17\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning, 2023.\n",
      "Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark\n",
      "Staples, and Xiwei Xu. Right to be forgotten in the era of large language models: Implications,\n",
      "challenges, and solutions. arXiv preprint arXiv:2307.03941, 2023.\n",
      "Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi,\n",
      "Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu,\n",
      "Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and\n",
      "Huajun Chen. A comprehensive study of knowledge editing for large language models, 2024.\n",
      "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial\n",
      "attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.\n",
      "18\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "A\n",
      "Kolmogorov-Smirnov Test Details\n",
      "In our setting, let FU(x) comprising n samples and FR(x) comprising m samples be the empirical\n",
      "CDF of the unlearned and retain models, respectively. Then, the KS-Test computes a statistic\n",
      "Dn,m = supx |FU(x) − FR(x)|.\n",
      "The null hypothesis, stating that the two sets of samples are drawn from the same distribution, is\n",
      "rejected at a chosen significance level α if the following inequality holds.\n",
      "Dn,m > c(α)\n",
      "r\n",
      "n + m\n",
      "nm ,\n",
      "(8)\n",
      "where c(α) is the critical value of that significance level.\n",
      "c(α) =\n",
      "r\n",
      "− ln\n",
      "\u0010α\n",
      "2\n",
      "\u0011\n",
      "· 1\n",
      "2.\n",
      "(9)\n",
      "The p-value is then defined as the minimal alpha for which the inequality holds, or the smallest value\n",
      "at which we can reject the null hypotheses. Forget quality is hence, a measure of the confidence that\n",
      "the distributions of Truth Ration values over the forget set from two models are the same.\n",
      "B\n",
      "Hyperparameters\n",
      "We trained using AdamW with weight decay of 0.01 and 0. The learning rate is fixed to be 1e-5. For\n",
      "both finetuning and unlearning, we fix the epoch number to be 5, and we incorporate a linear warmup\n",
      "in the first epoch. We experiment with various learning rates including 10−5, 10−6, and 5 · 10−7 and\n",
      "find that 10−5 is a good choice for the baseline methods in our experiments. In particular, 10−6 is\n",
      "too small of a learning rate to see appreciable updates to the model weights.\n",
      "C\n",
      "Preference Strings\n",
      "I’m not certain about that.\n",
      "That’s beyond my current knowledge base.\n",
      "I don’t have that information.\n",
      "I’m not sure.\n",
      "I haven’t learned about that topic.\n",
      "That’s something I need to look up.\n",
      "I’m at a loss for that one.\n",
      "I don’t have the answer to that question.\n",
      "That’s outside my area of expertise.\n",
      "I’m afraid I can’t provide an answer to that.\n",
      "That’s a good question, but I don’t have the an-\n",
      "swer.\n",
      "My resources don’t contain information on that\n",
      "subject.\n",
      "I wish I could say, but I really don’t know.\n",
      "That’s not something I’m familiar with.\n",
      "I’m drawing a blank on that one.\n",
      "I apologize, but I don’t know that.\n",
      "That hasn’t been included in my training data.\n",
      "Unfortunately, I don’t have an answer for you.\n",
      "That’s not information I’ve been programmed to\n",
      "know.\n",
      "I’m unable to provide an answer to that.\n",
      "I don’t hold the knowledge you’re seeking.\n",
      "I’m clueless about that topic.\n",
      "I’m not well-versed in that subject.\n",
      "I haven’t been briefed on that topic.\n",
      "I lack the specifics on that matter.\n",
      "My databases don’t cover that information.\n",
      "I have no knowledge on that subject.\n",
      "That’s a mystery to me as well.\n",
      "I’m unaware of that detail.\n",
      "I don’t possess the information on that topic.\n",
      "I must admit, I don’t know.\n",
      "I’m unable to answer that question.\n",
      "That topic is out of my scope.\n",
      "19\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "I’m not informed on that matter.\n",
      "I can’t shed any light on that subject.\n",
      "That’s an area I’m not acquainted with.\n",
      "I lack insight into that question.\n",
      "I’m not equipped to answer that.\n",
      "My understanding doesn’t include that informa-\n",
      "tion.\n",
      "I’ve got no idea about that.\n",
      "I can’t provide any information on that topic.\n",
      "My training didn’t cover that information.\n",
      "I’m not the best source for that subject.\n",
      "I seem to have no data on that.\n",
      "That’s a blind spot in my knowledge.\n",
      "I’ve come up short with an answer for you.\n",
      "I’m stumped on that one.\n",
      "I have no clue about that.\n",
      "I’m blank on that topic.\n",
      "I regret to inform you that I don’t have the answer.\n",
      "My capabilities do not extend to that subject.\n",
      "I must confess, that’s unknown to me.\n",
      "I don’t have any information on that matter.\n",
      "That’s something I’ve yet to learn.\n",
      "I’m sorry, that’s not within my knowledge range.\n",
      "I don’t have any knowledge about that subject.\n",
      "I’m not able to provide an answer to that.\n",
      "That subject is not something I’m familiar with.\n",
      "I’m lacking information on that topic.\n",
      "I don’t seem to have data on that issue.\n",
      "That’s not something I’m equipped to answer.\n",
      "My programming does not include that informa-\n",
      "tion.\n",
      "I don’t have the specifics you’re looking for.\n",
      "That information is not within my reach.\n",
      "I’m not knowledgeable about that topic.\n",
      "I’ve no insight into that matter.\n",
      "My database does not have information on that\n",
      "topic.\n",
      "That’s not in my current dataset.\n",
      "I’m not the right AI for that question.\n",
      "I can’t say I’m familiar with that.\n",
      "I have yet to be informed about that subject.\n",
      "That’s uncharted territory for my knowledge base.\n",
      "I haven’t encountered that in my training.\n",
      "I’m missing information on that.\n",
      "My understanding is limited to what I’ve been\n",
      "programmed with.\n",
      "I have no data on that query.\n",
      "I’m not aware of the details on that matter.\n",
      "I haven’t been trained on that topic.\n",
      "That’s something I’m not briefed on.\n",
      "I’m sorry, that’s not something I know about.\n",
      "I’m not privy to that information.\n",
      "I haven’t the faintest on that subject.\n",
      "I’m unable to access any information on that.\n",
      "That’s not in my field of knowledge.\n",
      "I have no familiarity with that topic.\n",
      "I’m not informed about that subject.\n",
      "My knowledge doesn’t cover that area.\n",
      "I’ve not been educated on that topic.\n",
      "I can’t provide insights into that subject.\n",
      "I don’t hold any information on that matter.\n",
      "I’m at a disadvantage with that question.\n",
      "I lack the required information to answer that.\n",
      "I’m in the dark about that topic.\n",
      "I have no enlightenment on that subject.\n",
      "I’ve no knowledge to draw upon for that.\n",
      "I must decline to answer due to lack of informa-\n",
      "tion.\n",
      "Sorry, I am unable to answer that.\n",
      "I’m not sure I can answer that.\n",
      "I’m not sure I can help with that.\n",
      "20\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Table 3: Model comparisons using KS-Test p-values for Llama-2-7B Models (WD = 0.00). We\n",
      "compare retain models finetuned with 90%, 95%, and 99% of the data. We test the Truth Ratio\n",
      "distributions over both retain data and forget data. For retain/forget data, we use the intersection\n",
      "of the retain/forget sets for each pair of models. All of these p-values are high indicating that the\n",
      "KS-Test accurately captures the similarity we know these models have over each of these datasets.\n",
      "Retain 90\n",
      "Retain 95\n",
      "Retain 99\n",
      "Retain 90\n",
      "1\n",
      "0.9414\n",
      "0.8483\n",
      "Retain Data\n",
      "Retain 95\n",
      "-\n",
      "1\n",
      "0.9705\n",
      "Retain 99\n",
      "-\n",
      "-\n",
      "1\n",
      "Retain 90\n",
      "1\n",
      "0.8655\n",
      "0.7659\n",
      "Forget Data\n",
      "Retain 95\n",
      "-\n",
      "1\n",
      "0.9900\n",
      "Retain 99\n",
      "-\n",
      "-\n",
      "1\n",
      "D\n",
      "Continued Unlearning\n",
      "In the main experiments of this paper, we limit unlearning to five epochs, but one might wonder how\n",
      "things progress given more time to unlearn. We test forgetting 1% of the data with Phi-1.5 and show\n",
      "that continued unlearning does not help with these baseline methods, see Figure 9.\n",
      "0.505\n",
      "0.510\n",
      "0.515\n",
      "0.520\n",
      "Model Utility\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n",
      "Forget Quality\n",
      "(log p-value)\n",
      "Finetune\n",
      "Retain\n",
      "Grad. Ascent\n",
      "Grad. Diff.\n",
      "Pref. Opt.\n",
      "KL Min.\n",
      "Figure 9: Zoomed in plots of extended unlearning trajectories (10 epochs) on the 1% forget set.\n",
      "E\n",
      "Sanity Checks\n",
      "We verify that our metrics for Model Utility and Forget Quality have some desirable properties. In\n",
      "Tables 3 and 4, we show the p-values for the KS-Tests that confirm all of our expectations enumerated\n",
      "below and validate this choice of metric. These tables have figures from Llama-2-7B tests, but the\n",
      "same trends hold for Phi-1.5.\n",
      "First, Model Utility should meet the following natural expectations.\n",
      "1. Model Utility should be high for a pretrained model (one that has never been finetuned on\n",
      "TOFU data).\n",
      "2. Model Utility should be low for a model with random weights.\n",
      "Additionally, Forget Quality is measured using a statistical test on Truth Ratio values, and so we hope\n",
      "that this test meets the following expectations.\n",
      "1. The KS-Test performed on distributions of Truth Ratio values over the intersection of the\n",
      "three forget sets (from the 90-10, 95-5, and 99-1 splits) should produce high p-values when\n",
      "comparing any two retain models.\n",
      "21\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "Table 4: Model comparisons using KS-Test p-values for Llama-2-7B Models (WD = 0.00). We\n",
      "compare retain models finetuned with 90%, 95%, and 99% of the data to a model finetuned on all the\n",
      "TOFU data, a pretrained base model, and a random model. We test the Truth Ratio distributions over\n",
      "both retain data and forget data. The sections with high p-values indicate that we cannot distinguish\n",
      "the Finetuned model and the Retain models by their distributions of Truth Values over the retain\n",
      "sets. We also cannot distinguish the Pretrained model and the Retain models by their distributions of\n",
      "Truth Values over the forget sets. In all other comparisons here, the KS-Test appropriately catches\n",
      "the expected difference in Truth Ratio distributions. These results confirm that the KS-Test done on\n",
      "distributions of Truth Ratios meets our needs as a test of forget quality.\n",
      "Finetuned\n",
      "Pretrained\n",
      "Random\n",
      "Retain 90\n",
      "0.9705\n",
      "9.21E-31\n",
      "2.42E-66\n",
      "Retain Data\n",
      "Retain 95\n",
      "0.9879\n",
      "1.41E-32\n",
      "2.94E-69\n",
      "Retain 99\n",
      "0.9003\n",
      "4.07E-32\n",
      "2.94E-69\n",
      "Retain 90\n",
      "1.10E-19\n",
      "0.0031\n",
      "2.43E-19\n",
      "Forget Data\n",
      "Retain 95\n",
      "4.73E-15\n",
      "0.0297\n",
      "2.96E-13\n",
      "Retain 99\n",
      "5.04E-04\n",
      "0.1650\n",
      "5.04E-04\n",
      "2. The KS-Test performed on distributions of Truth Ratio values over the intersection of the\n",
      "three retain sets (from the 90-10, 95-5, and 99-1 splits) should produce high p-values when\n",
      "comparing any two retain models.\n",
      "3. The KS-Test performed on distributions of Truth Ratio values over the forget set should\n",
      "produce high p-values when comparing any retain model to a random model.\n",
      "4. The KS-Test performed on distributions of Truth Ratio values over the retain set should\n",
      "produce low p-values when comparing any retain model to a random model.\n",
      "5. The KS-Test performed on distributions of Truth Ratio values over the forget set should\n",
      "produce high p-values when comparing any retain model to a pretrained model.\n",
      "6. The KS-Test performed on distributions of Truth Ratio values over the retain set should\n",
      "produce low p-values when comparing any retain model to a pretrained model.\n",
      "7. The KS-Test performed on distributions of Truth Ratio values over the forget set should\n",
      "produce low p-values when comparing any retain model to a finetuned model (finetuned on\n",
      "all the\n",
      "TOFU data and without any unlearning).\n",
      "8. The KS-Test performed on distributions of Truth Ratio values over the retain set should\n",
      "produce high p-values when comparing any retain model to a finetuned model (finetuned on\n",
      "all the\n",
      "TOFU data and without any unlearning).\n",
      "F\n",
      "Knowledge Entanglement\n",
      "One of the challenges of unlearning comes from knowledge entanglement—when we try to make a\n",
      "model forget about one thing, it also tends to forget other things unexpectedly. This phenomenon is\n",
      "similar to catastrophic forgetting in continual learning (McCloskey & Cohen, 1989). In Figures 10-33,\n",
      "we show this phenomenon in different models and unlearning algorithms. In Figure 8, even with\n",
      "access to the oracle model or retain set, model generation on all four sets still has a decreasing\n",
      "ROUGE, especially the dataset that relate to authors. This suggests the existence of knowledge\n",
      "entanglement, showing why unlearning is hard. Consider the case of unlearning the 5% forget set\n",
      "with Gradient Difference on Llama-2-7B, Figure 17. The ROUGE score on all four datasets falls as\n",
      "unlearning progresses (left-most frame), but the rates at which they fall are ordered according to the\n",
      "proximity to the forget data. (i) On the Retain Set, performance drops sharply with the drop on the\n",
      "forget set. (ii) On Real Authors, the ROUGE score also drops along with the drop in performance on\n",
      "the forget set, but stays higher than on the Retain Set. (iii) Finally, performance on World Facts stays\n",
      "relatively unchanged.\n",
      "In other cases where these curves overlap, they reach extremely low ROUGE values and the model\n",
      "starts outputting gibberish. This suggests the existence of knowledge entanglement, supporting that\n",
      "our choice of having multiple evaluation datasets is important for a holistic assessment of unlearning.\n",
      "22\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 10: Unlearn Llama-2-7B with gradient\n",
      "ascent on 1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 11: Unlearn Llama-2-7B with gradient\n",
      "ascent on 5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 12: Unlearn Llama-2-7B with gradient\n",
      "ascent on 10% forget set.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 13: Unlearn Llama-2-7B with preference\n",
      "optimization on 1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 14: Unlearn Llama-2-7B with preference\n",
      "optimization on 5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 15: Unlearn Llama-2-7B with preference\n",
      "optimization on 10% forget set.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 16: Unlearn Llama-2-7B with gradient\n",
      "difference on 1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 17: Unlearn Llama-2-7B with gradient\n",
      "difference on 5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 18: Unlearn Llama-2-7B with gradient\n",
      "difference on 10% forget set.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 19: Unlearn Llama-2-7B with KL Mini-\n",
      "mization on 1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 20: Unlearn Llama-2-7B with KL Mini-\n",
      "mization on 5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 21: Unlearn Llama-2-7B with KL Mini-\n",
      "mization on 10% forget set.\n",
      "23\n",
      "TOFU: A Task of Fictitious Unlearning for LLMs\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 22: Unlearn Phi with gradient ascent on\n",
      "1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 23: Unlearn Phi with gradient ascent on\n",
      "5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 24: Unlearn Phi with gradient ascent on\n",
      "10% forget set.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 25: Unlearn Phi with preference optimiza-\n",
      "tion on 1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 26: Unlearn Phi with preference optimiza-\n",
      "tion on 5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 27: Unlearn Phi with preference optimiza-\n",
      "tion on 10% forget set.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 28: Unlearn Phi with gradient difference\n",
      "on 1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 29: Unlearn Phi with gradient difference\n",
      "on 5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 30: Unlearn Phi with gradient difference\n",
      "on 10% forget set.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (99%)\n",
      "Forget Set (1%)\n",
      "Figure 31: Unlearn Phi with KL Minimization on\n",
      "1% forget set.\n",
      "0\n",
      "12\n",
      "24\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "12\n",
      "24\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "12\n",
      "24\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (95%)\n",
      "Forget Set (5%)\n",
      "Figure 32: Unlearn Phi with KL Minimization on\n",
      "5% forget set.\n",
      "0\n",
      "24\n",
      "48\n",
      "0.00\n",
      "0.25\n",
      "0.50\n",
      "0.75\n",
      "1.00\n",
      "ROUGE\n",
      "0\n",
      "24\n",
      "48\n",
      "Unlearning Steps\n",
      "Probability\n",
      "0\n",
      "24\n",
      "48\n",
      "Truth Ratio\n",
      "World Facts\n",
      "Real Authors\n",
      "Retain Set (90%)\n",
      "Forget Set (10%)\n",
      "Figure 33: Unlearn Phi with KL Minimization on\n",
      "10% forget set.\n",
      "24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for article in data:\n",
    "    print(f\"Id: {article}\")\n",
    "    print(f\"Published date: {data[article]['published_date']}\")\n",
    "    print(f\"Pdf link: {data[article]['pdf_link']}\\n\")\n",
    "    print(f\"Title: {data[article]['title']}\\n\")\n",
    "    print(f\"Summary: {data[article]['summary']}\\n\")\n",
    "    print(f\"Content: {data[article]['pdf_text']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f81bf-f201-4746-a577-55aa0c593477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
