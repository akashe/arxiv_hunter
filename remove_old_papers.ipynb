{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df:pd.DataFrame = pd.read_pickle(\"data/master_data3.pkl\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.13808v1</td>\n",
       "      <td>On Pretraining Data Diversity for Self-Supervi...</td>\n",
       "      <td>2024-03-20 17:59:58</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13808v1</td>\n",
       "      <td>We explore the impact of training with more di...</td>\n",
       "      <td>On Pretraining Data Diversity\\nfor Self-Superv...</td>\n",
       "      <td>823</td>\n",
       "      <td>76097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.13807v1</td>\n",
       "      <td>Editing Massive Concepts in Text-to-Image Diff...</td>\n",
       "      <td>2024-03-20 17:59:57</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13807v1</td>\n",
       "      <td>Text-to-image diffusion models suffer from the...</td>\n",
       "      <td>Editing Massive Concepts in Text-to-Image\\nDif...</td>\n",
       "      <td>1145</td>\n",
       "      <td>70727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.13808v1   \n",
       "1  http://arxiv.org/abs/2403.13807v1   \n",
       "\n",
       "                                               title      published_date  \\\n",
       "0  On Pretraining Data Diversity for Self-Supervi... 2024-03-20 17:59:58   \n",
       "1  Editing Massive Concepts in Text-to-Image Diff... 2024-03-20 17:59:57   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.13808v1   \n",
       "1  http://arxiv.org/pdf/2403.13807v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We explore the impact of training with more di...   \n",
       "1  Text-to-image diffusion models suffer from the...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  On Pretraining Data Diversity\\nfor Self-Superv...             823   \n",
       "1  Editing Massive Concepts in Text-to-Image\\nDif...            1145   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            76097  \n",
       "1            70727  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>http://arxiv.org/abs/2403.12329v1</td>\n",
       "      <td>FedFisher: Leveraging Fisher Information for O...</td>\n",
       "      <td>2024-03-19 00:03:40</td>\n",
       "      <td>http://arxiv.org/pdf/2403.12329v1</td>\n",
       "      <td>Standard federated learning (FL) algorithms ty...</td>\n",
       "      <td>FedFisher: Leveraging Fisher Information for O...</td>\n",
       "      <td>1248</td>\n",
       "      <td>88917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>http://arxiv.org/abs/2403.12331v1</td>\n",
       "      <td>Deep Few-view High-resolution Photon-counting ...</td>\n",
       "      <td>2024-03-19 00:07:48</td>\n",
       "      <td>http://arxiv.org/pdf/2403.12331v1</td>\n",
       "      <td>The latest X-ray photon-counting computed tomo...</td>\n",
       "      <td>Deep Few-view High-resolution Photon-counting ...</td>\n",
       "      <td>1556</td>\n",
       "      <td>68660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>http://arxiv.org/abs/2403.12335v1</td>\n",
       "      <td>Temporally-Consistent Koopman Autoencoders for...</td>\n",
       "      <td>2024-03-19 00:48:25</td>\n",
       "      <td>http://arxiv.org/pdf/2403.12335v1</td>\n",
       "      <td>Absence of sufficiently high-quality data ofte...</td>\n",
       "      <td>Temporally-Consistent Koopman Autoencoders\\nfo...</td>\n",
       "      <td>1232</td>\n",
       "      <td>49910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "499  http://arxiv.org/abs/2403.12329v1   \n",
       "498  http://arxiv.org/abs/2403.12331v1   \n",
       "497  http://arxiv.org/abs/2403.12335v1   \n",
       "\n",
       "                                                 title      published_date  \\\n",
       "499  FedFisher: Leveraging Fisher Information for O... 2024-03-19 00:03:40   \n",
       "498  Deep Few-view High-resolution Photon-counting ... 2024-03-19 00:07:48   \n",
       "497  Temporally-Consistent Koopman Autoencoders for... 2024-03-19 00:48:25   \n",
       "\n",
       "                              pdf_link  \\\n",
       "499  http://arxiv.org/pdf/2403.12329v1   \n",
       "498  http://arxiv.org/pdf/2403.12331v1   \n",
       "497  http://arxiv.org/pdf/2403.12335v1   \n",
       "\n",
       "                                               summary  \\\n",
       "499  Standard federated learning (FL) algorithms ty...   \n",
       "498  The latest X-ray photon-counting computed tomo...   \n",
       "497  Absence of sufficiently high-quality data ofte...   \n",
       "\n",
       "                                              pdf_text  summary_length  \\\n",
       "499  FedFisher: Leveraging Fisher Information for O...            1248   \n",
       "498  Deep Few-view High-resolution Photon-counting ...            1556   \n",
       "497  Temporally-Consistent Koopman Autoencoders\\nfo...            1232   \n",
       "\n",
       "     pdf_text_length  \n",
       "499            88917  \n",
       "498            68660  \n",
       "497            49910  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"published_date\", ascending=True).head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.13808v1</td>\n",
       "      <td>On Pretraining Data Diversity for Self-Supervi...</td>\n",
       "      <td>2024-03-20 17:59:58</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13808v1</td>\n",
       "      <td>We explore the impact of training with more di...</td>\n",
       "      <td>On Pretraining Data Diversity\\nfor Self-Superv...</td>\n",
       "      <td>823</td>\n",
       "      <td>76097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.13807v1</td>\n",
       "      <td>Editing Massive Concepts in Text-to-Image Diff...</td>\n",
       "      <td>2024-03-20 17:59:57</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13807v1</td>\n",
       "      <td>Text-to-image diffusion models suffer from the...</td>\n",
       "      <td>Editing Massive Concepts in Text-to-Image\\nDif...</td>\n",
       "      <td>1145</td>\n",
       "      <td>70727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.13805v1</td>\n",
       "      <td>RAR: Retrieving And Ranking Augmented MLLMs fo...</td>\n",
       "      <td>2024-03-20 17:59:55</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13805v1</td>\n",
       "      <td>CLIP (Contrastive Language-Image Pre-training)...</td>\n",
       "      <td>RAR: Retrieving And Ranking Augmented\\nMLLMs f...</td>\n",
       "      <td>1639</td>\n",
       "      <td>78159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.13808v1   \n",
       "1  http://arxiv.org/abs/2403.13807v1   \n",
       "2  http://arxiv.org/abs/2403.13805v1   \n",
       "\n",
       "                                               title      published_date  \\\n",
       "0  On Pretraining Data Diversity for Self-Supervi... 2024-03-20 17:59:58   \n",
       "1  Editing Massive Concepts in Text-to-Image Diff... 2024-03-20 17:59:57   \n",
       "2  RAR: Retrieving And Ranking Augmented MLLMs fo... 2024-03-20 17:59:55   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.13808v1   \n",
       "1  http://arxiv.org/pdf/2403.13807v1   \n",
       "2  http://arxiv.org/pdf/2403.13805v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We explore the impact of training with more di...   \n",
       "1  Text-to-image diffusion models suffer from the...   \n",
       "2  CLIP (Contrastive Language-Image Pre-training)...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  On Pretraining Data Diversity\\nfor Self-Superv...             823   \n",
       "1  Editing Massive Concepts in Text-to-Image\\nDif...            1145   \n",
       "2  RAR: Retrieving And Ranking Augmented\\nMLLMs f...            1639   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            76097  \n",
       "1            70727  \n",
       "2            78159  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"published_date\", ascending=False).head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 3, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldest_date = df['published_date'].min().date()\n",
    "oldest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>http://arxiv.org/abs/2403.13206v1</td>\n",
       "      <td>Depth-guided NeRF Training via Earth Mover's D...</td>\n",
       "      <td>2024-03-19 23:54:07</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13206v1</td>\n",
       "      <td>Neural Radiance Fields (NeRFs) are trained to ...</td>\n",
       "      <td>arXiv:2403.13206v1  [cs.CV]  19 Mar 2024\\nDept...</td>\n",
       "      <td>1206</td>\n",
       "      <td>58739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>http://arxiv.org/abs/2403.13204v1</td>\n",
       "      <td>Diversity-Aware Agnostic Ensemble of Sharpness...</td>\n",
       "      <td>2024-03-19 23:50:11</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13204v1</td>\n",
       "      <td>There has long been plenty of theoretical and ...</td>\n",
       "      <td>Diversity-Aware Agnostic Ensemble of\\nSharpnes...</td>\n",
       "      <td>1089</td>\n",
       "      <td>43763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>http://arxiv.org/abs/2403.13199v1</td>\n",
       "      <td>DecentNeRFs: Decentralized Neural Radiance Fie...</td>\n",
       "      <td>2024-03-19 23:23:35</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13199v1</td>\n",
       "      <td>Neural radiance fields (NeRFs) show potential ...</td>\n",
       "      <td>DecentNeRFs: Decentralized Neural Radiance\\nFi...</td>\n",
       "      <td>1252</td>\n",
       "      <td>47437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "205  http://arxiv.org/abs/2403.13206v1   \n",
       "206  http://arxiv.org/abs/2403.13204v1   \n",
       "207  http://arxiv.org/abs/2403.13199v1   \n",
       "\n",
       "                                                 title      published_date  \\\n",
       "205  Depth-guided NeRF Training via Earth Mover's D... 2024-03-19 23:54:07   \n",
       "206  Diversity-Aware Agnostic Ensemble of Sharpness... 2024-03-19 23:50:11   \n",
       "207  DecentNeRFs: Decentralized Neural Radiance Fie... 2024-03-19 23:23:35   \n",
       "\n",
       "                              pdf_link  \\\n",
       "205  http://arxiv.org/pdf/2403.13206v1   \n",
       "206  http://arxiv.org/pdf/2403.13204v1   \n",
       "207  http://arxiv.org/pdf/2403.13199v1   \n",
       "\n",
       "                                               summary  \\\n",
       "205  Neural Radiance Fields (NeRFs) are trained to ...   \n",
       "206  There has long been plenty of theoretical and ...   \n",
       "207  Neural radiance fields (NeRFs) show potential ...   \n",
       "\n",
       "                                              pdf_text  summary_length  \\\n",
       "205  arXiv:2403.13206v1  [cs.CV]  19 Mar 2024\\nDept...            1206   \n",
       "206  Diversity-Aware Agnostic Ensemble of\\nSharpnes...            1089   \n",
       "207  DecentNeRFs: Decentralized Neural Radiance\\nFi...            1252   \n",
       "\n",
       "     pdf_text_length  \n",
       "205            58739  \n",
       "206            43763  \n",
       "207            47437  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_to_remove = df[df['published_date'].dt.date == oldest_date]\n",
    "papers_to_remove.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=500, step=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([205, 206, 207, 208, 209, 210, 211, 212, 213, 214,\n",
       "       ...\n",
       "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n",
       "      dtype='int64', length=295)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_to_remove.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.13808v1</td>\n",
       "      <td>On Pretraining Data Diversity for Self-Supervi...</td>\n",
       "      <td>2024-03-20 17:59:58</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13808v1</td>\n",
       "      <td>We explore the impact of training with more di...</td>\n",
       "      <td>On Pretraining Data Diversity\\nfor Self-Superv...</td>\n",
       "      <td>823</td>\n",
       "      <td>76097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.13807v1</td>\n",
       "      <td>Editing Massive Concepts in Text-to-Image Diff...</td>\n",
       "      <td>2024-03-20 17:59:57</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13807v1</td>\n",
       "      <td>Text-to-image diffusion models suffer from the...</td>\n",
       "      <td>Editing Massive Concepts in Text-to-Image\\nDif...</td>\n",
       "      <td>1145</td>\n",
       "      <td>70727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.13805v1</td>\n",
       "      <td>RAR: Retrieving And Ranking Augmented MLLMs fo...</td>\n",
       "      <td>2024-03-20 17:59:55</td>\n",
       "      <td>http://arxiv.org/pdf/2403.13805v1</td>\n",
       "      <td>CLIP (Contrastive Language-Image Pre-training)...</td>\n",
       "      <td>RAR: Retrieving And Ranking Augmented\\nMLLMs f...</td>\n",
       "      <td>1639</td>\n",
       "      <td>78159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.13808v1   \n",
       "1  http://arxiv.org/abs/2403.13807v1   \n",
       "2  http://arxiv.org/abs/2403.13805v1   \n",
       "\n",
       "                                               title      published_date  \\\n",
       "0  On Pretraining Data Diversity for Self-Supervi... 2024-03-20 17:59:58   \n",
       "1  Editing Massive Concepts in Text-to-Image Diff... 2024-03-20 17:59:57   \n",
       "2  RAR: Retrieving And Ranking Augmented MLLMs fo... 2024-03-20 17:59:55   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.13808v1   \n",
       "1  http://arxiv.org/pdf/2403.13807v1   \n",
       "2  http://arxiv.org/pdf/2403.13805v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We explore the impact of training with more di...   \n",
       "1  Text-to-image diffusion models suffer from the...   \n",
       "2  CLIP (Contrastive Language-Image Pre-training)...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  On Pretraining Data Diversity\\nfor Self-Superv...             823   \n",
       "1  Editing Massive Concepts in Text-to-Image\\nDif...            1145   \n",
       "2  RAR: Retrieving And Ranking Augmented\\nMLLMs f...            1639   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            76097  \n",
       "1            70727  \n",
       "2            78159  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[~df.index.isin(values=papers_to_remove.index)]\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(path_or_buf=\"data/master_data.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove, Add Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logics.data_extraction import ArxivParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 21915.29it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 10677.47it/s]\n",
      "100%|██████████| 62/62 [00:38<00:00,  1.63it/s]\n",
      " 32%|███▏      | 20/62 [00:48<02:24,  3.45s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m parser \u001b[38;5;241m=\u001b[39m ArxivParser()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest_papers.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/src/logics/data_extraction.py:135\u001b[0m, in \u001b[0;36mArxivParser.store_data\u001b[0;34m(self, save_file_name, max_results, days)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore_data\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     save_file_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaster_data3.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Call the get_results method and store the dataframe in the self.extracted_data attribute\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextracted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextracted_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot no results with the search query\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Feature Engineer two new columns\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/src/logics/data_extraction.py:117\u001b[0m, in \u001b[0;36mArxivParser.get_results\u001b[0;34m(self, max_results, days, search_query, num_threads)\u001b[0m\n\u001b[1;32m    114\u001b[0m     process_list\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m process_list:\n\u001b[0;32m--> 117\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# combine all the downloaded content\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, flag)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/62 [00:57<02:14,  2.81s/it]"
     ]
    }
   ],
   "source": [
    "parser = ArxivParser()\n",
    "parser.store_data(save_file_name=\"latest_papers.pkl\" ,max_results=500, days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_df:pd.DataFrame = pd.read_pickle(filepath_or_buffer=\"data/latest_papers.pkl\")\n",
    "latest_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.17937v1</td>\n",
       "      <td>Efficient Video Object Segmentation via Modula...</td>\n",
       "      <td>2024-03-26 17:59:58</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17937v1</td>\n",
       "      <td>Recently, transformer-based approaches have sh...</td>\n",
       "      <td>Efficient Video Object Segmentation via\\nModul...</td>\n",
       "      <td>1392</td>\n",
       "      <td>54217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.17936v1</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>2024-03-26 17:59:52</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17936v1</td>\n",
       "      <td>Gestures play a key role in human communicatio...</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>1414</td>\n",
       "      <td>77802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.17935v1</td>\n",
       "      <td>OmniVid: A Generative Framework for Universal ...</td>\n",
       "      <td>2024-03-26 17:59:24</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17935v1</td>\n",
       "      <td>The core of video understanding tasks, such as...</td>\n",
       "      <td>OmniViD: A Generative Framework for Universal ...</td>\n",
       "      <td>1436</td>\n",
       "      <td>57245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.17937v1   \n",
       "1  http://arxiv.org/abs/2403.17936v1   \n",
       "2  http://arxiv.org/abs/2403.17935v1   \n",
       "\n",
       "                                               title      published_date  \\\n",
       "0  Efficient Video Object Segmentation via Modula... 2024-03-26 17:59:58   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus... 2024-03-26 17:59:52   \n",
       "2  OmniVid: A Generative Framework for Universal ... 2024-03-26 17:59:24   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.17937v1   \n",
       "1  http://arxiv.org/pdf/2403.17936v1   \n",
       "2  http://arxiv.org/pdf/2403.17935v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Recently, transformer-based approaches have sh...   \n",
       "1  Gestures play a key role in human communicatio...   \n",
       "2  The core of video understanding tasks, such as...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  Efficient Video Object Segmentation via\\nModul...            1392   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus...            1414   \n",
       "2  OmniViD: A Generative Framework for Universal ...            1436   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            54217  \n",
       "1            77802  \n",
       "2            57245  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 3, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_df[\"published_date\"].min().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 3, 26)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_df[\"published_date\"].max().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m updated_df:pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/master_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m updated_df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[1;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[1;32m   1029\u001b[0m     )\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/.venv/lib/python3.11/site-packages/pandas/io/json/_json.py:1400\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1396\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1400\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m     )\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1403\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1405\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1406\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "updated_df:pd.DataFrame = pd.read_json(path_or_buf=\"data/master_data.json\")\n",
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m parser \u001b[38;5;241m=\u001b[39m ArxivParser()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaster_data.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m365\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/src/logics/data_extraction.py:135\u001b[0m, in \u001b[0;36mArxivParser.store_data\u001b[0;34m(self, save_file_name, max_results, days)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore_data\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     save_file_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaster_data3.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Call the get_results method and store the dataframe in the self.extracted_data attribute\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextracted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextracted_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot no results with the search query\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Feature Engineer two new columns\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/web development/arxiv_hunter/src/logics/data_extraction.py:113\u001b[0m, in \u001b[0;36mArxivParser.get_results\u001b[0;34m(self, max_results, days, search_query, num_threads)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splits):\n\u001b[1;32m    110\u001b[0m     p \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m    111\u001b[0m         target\u001b[38;5;241m=\u001b[39mdownload_data, args\u001b[38;5;241m=\u001b[39m[split, i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, days]\n\u001b[1;32m    112\u001b[0m     )\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     process_list\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m process_list:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = ArxivParser()\n",
    "parser.store_data(save_file_name=\"master_data.pkl\" ,max_results=500, days=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df:pd.DataFrame = pd.read_pickle(filepath_or_buffer=\"data/master_data.pkl\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.17937v1</td>\n",
       "      <td>Efficient Video Object Segmentation via Modula...</td>\n",
       "      <td>2024-03-26 17:59:58</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17937v1</td>\n",
       "      <td>Recently, transformer-based approaches have sh...</td>\n",
       "      <td>Efficient Video Object Segmentation via\\nModul...</td>\n",
       "      <td>1392</td>\n",
       "      <td>54217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.17936v1</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>2024-03-26 17:59:52</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17936v1</td>\n",
       "      <td>Gestures play a key role in human communicatio...</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>1414</td>\n",
       "      <td>77802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.17935v1</td>\n",
       "      <td>OmniVid: A Generative Framework for Universal ...</td>\n",
       "      <td>2024-03-26 17:59:24</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17935v1</td>\n",
       "      <td>The core of video understanding tasks, such as...</td>\n",
       "      <td>OmniViD: A Generative Framework for Universal ...</td>\n",
       "      <td>1436</td>\n",
       "      <td>57245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2403.17934v1</td>\n",
       "      <td>AiOS: All-in-One-Stage Expressive Human Pose a...</td>\n",
       "      <td>2024-03-26 17:59:23</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17934v1</td>\n",
       "      <td>Expressive human pose and shape estimation (a....</td>\n",
       "      <td>AiOS: All-in-One-Stage Expressive Human Pose a...</td>\n",
       "      <td>1668</td>\n",
       "      <td>70948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2403.17933v1</td>\n",
       "      <td>SLEDGE: Synthesizing Simulation Environments f...</td>\n",
       "      <td>2024-03-26 17:58:29</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17933v1</td>\n",
       "      <td>SLEDGE is the first generative simulator for v...</td>\n",
       "      <td>SLEDGE: Synthesizing Simulation Environments\\n...</td>\n",
       "      <td>1482</td>\n",
       "      <td>48845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.17937v1   \n",
       "1  http://arxiv.org/abs/2403.17936v1   \n",
       "2  http://arxiv.org/abs/2403.17935v1   \n",
       "3  http://arxiv.org/abs/2403.17934v1   \n",
       "4  http://arxiv.org/abs/2403.17933v1   \n",
       "\n",
       "                                               title      published_date  \\\n",
       "0  Efficient Video Object Segmentation via Modula... 2024-03-26 17:59:58   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus... 2024-03-26 17:59:52   \n",
       "2  OmniVid: A Generative Framework for Universal ... 2024-03-26 17:59:24   \n",
       "3  AiOS: All-in-One-Stage Expressive Human Pose a... 2024-03-26 17:59:23   \n",
       "4  SLEDGE: Synthesizing Simulation Environments f... 2024-03-26 17:58:29   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.17937v1   \n",
       "1  http://arxiv.org/pdf/2403.17936v1   \n",
       "2  http://arxiv.org/pdf/2403.17935v1   \n",
       "3  http://arxiv.org/pdf/2403.17934v1   \n",
       "4  http://arxiv.org/pdf/2403.17933v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Recently, transformer-based approaches have sh...   \n",
       "1  Gestures play a key role in human communicatio...   \n",
       "2  The core of video understanding tasks, such as...   \n",
       "3  Expressive human pose and shape estimation (a....   \n",
       "4  SLEDGE is the first generative simulator for v...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  Efficient Video Object Segmentation via\\nModul...            1392   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus...            1414   \n",
       "2  OmniViD: A Generative Framework for Universal ...            1436   \n",
       "3  AiOS: All-in-One-Stage Expressive Human Pose a...            1668   \n",
       "4  SLEDGE: Synthesizing Simulation Environments\\n...            1482   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            54217  \n",
       "1            77802  \n",
       "2            57245  \n",
       "3            70948  \n",
       "4            48845  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2024-03-26\n",
       "1      2024-03-26\n",
       "2      2024-03-26\n",
       "3      2024-03-26\n",
       "4      2024-03-26\n",
       "          ...    \n",
       "495    2024-03-25\n",
       "496    2024-03-25\n",
       "497    2024-03-25\n",
       "498    2024-03-25\n",
       "499    2024-03-25\n",
       "Name: published_date, Length: 500, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "df[\"published_date\"] = pd.to_datetime(df[\"published_date\"]).dt.date\n",
    "df[\"published_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.17937v1</td>\n",
       "      <td>Efficient Video Object Segmentation via Modula...</td>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17937v1</td>\n",
       "      <td>Recently, transformer-based approaches have sh...</td>\n",
       "      <td>Efficient Video Object Segmentation via\\nModul...</td>\n",
       "      <td>1392</td>\n",
       "      <td>54217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.17936v1</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17936v1</td>\n",
       "      <td>Gestures play a key role in human communicatio...</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>1414</td>\n",
       "      <td>77802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.17935v1</td>\n",
       "      <td>OmniVid: A Generative Framework for Universal ...</td>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17935v1</td>\n",
       "      <td>The core of video understanding tasks, such as...</td>\n",
       "      <td>OmniViD: A Generative Framework for Universal ...</td>\n",
       "      <td>1436</td>\n",
       "      <td>57245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2403.17934v1</td>\n",
       "      <td>AiOS: All-in-One-Stage Expressive Human Pose a...</td>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17934v1</td>\n",
       "      <td>Expressive human pose and shape estimation (a....</td>\n",
       "      <td>AiOS: All-in-One-Stage Expressive Human Pose a...</td>\n",
       "      <td>1668</td>\n",
       "      <td>70948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2403.17933v1</td>\n",
       "      <td>SLEDGE: Synthesizing Simulation Environments f...</td>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17933v1</td>\n",
       "      <td>SLEDGE is the first generative simulator for v...</td>\n",
       "      <td>SLEDGE: Synthesizing Simulation Environments\\n...</td>\n",
       "      <td>1482</td>\n",
       "      <td>48845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.17937v1   \n",
       "1  http://arxiv.org/abs/2403.17936v1   \n",
       "2  http://arxiv.org/abs/2403.17935v1   \n",
       "3  http://arxiv.org/abs/2403.17934v1   \n",
       "4  http://arxiv.org/abs/2403.17933v1   \n",
       "\n",
       "                                               title published_date  \\\n",
       "0  Efficient Video Object Segmentation via Modula...     2024-03-26   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus...     2024-03-26   \n",
       "2  OmniVid: A Generative Framework for Universal ...     2024-03-26   \n",
       "3  AiOS: All-in-One-Stage Expressive Human Pose a...     2024-03-26   \n",
       "4  SLEDGE: Synthesizing Simulation Environments f...     2024-03-26   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.17937v1   \n",
       "1  http://arxiv.org/pdf/2403.17936v1   \n",
       "2  http://arxiv.org/pdf/2403.17935v1   \n",
       "3  http://arxiv.org/pdf/2403.17934v1   \n",
       "4  http://arxiv.org/pdf/2403.17933v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Recently, transformer-based approaches have sh...   \n",
       "1  Gestures play a key role in human communicatio...   \n",
       "2  The core of video understanding tasks, such as...   \n",
       "3  Expressive human pose and shape estimation (a....   \n",
       "4  SLEDGE is the first generative simulator for v...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  Efficient Video Object Segmentation via\\nModul...            1392   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus...            1414   \n",
       "2  OmniViD: A Generative Framework for Universal ...            1436   \n",
       "3  AiOS: All-in-One-Stage Expressive Human Pose a...            1668   \n",
       "4  SLEDGE: Synthesizing Simulation Environments\\n...            1482   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            54217  \n",
       "1            77802  \n",
       "2            57245  \n",
       "3            70948  \n",
       "4            48845  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 7/62 [00:48<07:46,  8.49s/it]"
     ]
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 18/62 [01:57<03:03,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for an entry\n",
      "('Connection broken: IncompleteRead(1048576 bytes read, 2375460 more expected)', IncompleteRead(1048576 bytes read, 2375460 more expected))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 35/62 [03:21<01:52,  4.16s/it]"
     ]
    }
   ],
   "source": [
    "df.to_json(path_or_buf=\"data/new_master_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/master_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.17937v1</td>\n",
       "      <td>Efficient Video Object Segmentation via Modula...</td>\n",
       "      <td>1711411200000</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17937v1</td>\n",
       "      <td>Recently, transformer-based approaches have sh...</td>\n",
       "      <td>Efficient Video Object Segmentation via\\nModul...</td>\n",
       "      <td>1392</td>\n",
       "      <td>54217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.17936v1</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>1711411200000</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17936v1</td>\n",
       "      <td>Gestures play a key role in human communicatio...</td>\n",
       "      <td>ConvoFusion: Multi-Modal Conversational Diffus...</td>\n",
       "      <td>1414</td>\n",
       "      <td>77802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.17935v1</td>\n",
       "      <td>OmniVid: A Generative Framework for Universal ...</td>\n",
       "      <td>1711411200000</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17935v1</td>\n",
       "      <td>The core of video understanding tasks, such as...</td>\n",
       "      <td>OmniViD: A Generative Framework for Universal ...</td>\n",
       "      <td>1436</td>\n",
       "      <td>57245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2403.17934v1</td>\n",
       "      <td>AiOS: All-in-One-Stage Expressive Human Pose a...</td>\n",
       "      <td>1711411200000</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17934v1</td>\n",
       "      <td>Expressive human pose and shape estimation (a....</td>\n",
       "      <td>AiOS: All-in-One-Stage Expressive Human Pose a...</td>\n",
       "      <td>1668</td>\n",
       "      <td>70948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2403.17933v1</td>\n",
       "      <td>SLEDGE: Synthesizing Simulation Environments f...</td>\n",
       "      <td>1711411200000</td>\n",
       "      <td>http://arxiv.org/pdf/2403.17933v1</td>\n",
       "      <td>SLEDGE is the first generative simulator for v...</td>\n",
       "      <td>SLEDGE: Synthesizing Simulation Environments\\n...</td>\n",
       "      <td>1482</td>\n",
       "      <td>48845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2403.17937v1   \n",
       "1  http://arxiv.org/abs/2403.17936v1   \n",
       "2  http://arxiv.org/abs/2403.17935v1   \n",
       "3  http://arxiv.org/abs/2403.17934v1   \n",
       "4  http://arxiv.org/abs/2403.17933v1   \n",
       "\n",
       "                                               title  published_date  \\\n",
       "0  Efficient Video Object Segmentation via Modula...   1711411200000   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus...   1711411200000   \n",
       "2  OmniVid: A Generative Framework for Universal ...   1711411200000   \n",
       "3  AiOS: All-in-One-Stage Expressive Human Pose a...   1711411200000   \n",
       "4  SLEDGE: Synthesizing Simulation Environments f...   1711411200000   \n",
       "\n",
       "                            pdf_link  \\\n",
       "0  http://arxiv.org/pdf/2403.17937v1   \n",
       "1  http://arxiv.org/pdf/2403.17936v1   \n",
       "2  http://arxiv.org/pdf/2403.17935v1   \n",
       "3  http://arxiv.org/pdf/2403.17934v1   \n",
       "4  http://arxiv.org/pdf/2403.17933v1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Recently, transformer-based approaches have sh...   \n",
       "1  Gestures play a key role in human communicatio...   \n",
       "2  The core of video understanding tasks, such as...   \n",
       "3  Expressive human pose and shape estimation (a....   \n",
       "4  SLEDGE is the first generative simulator for v...   \n",
       "\n",
       "                                            pdf_text  summary_length  \\\n",
       "0  Efficient Video Object Segmentation via\\nModul...            1392   \n",
       "1  ConvoFusion: Multi-Modal Conversational Diffus...            1414   \n",
       "2  OmniViD: A Generative Framework for Universal ...            1436   \n",
       "3  AiOS: All-in-One-Stage Expressive Human Pose a...            1668   \n",
       "4  SLEDGE: Synthesizing Simulation Environments\\n...            1482   \n",
       "\n",
       "   pdf_text_length  \n",
       "0            54217  \n",
       "1            77802  \n",
       "2            57245  \n",
       "3            70948  \n",
       "4            48845  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(1970, 1, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(df[\"published_date\"]).min().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
