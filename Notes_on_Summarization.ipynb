{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notes on Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/YONQoma3fN6CbVWJZh7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashe/arxiv_hunter/blob/main/Notes_on_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQjYnva9hurW"
      },
      "source": [
        "Different approaches:\n",
        "1. BERT: bidirectional encoder with maked filling and next sentence predictions as learning objective(since recreating original text is trivial with bidirectional context??).\n",
        "2. GPT: Decoder only. Left to right language generation.\n",
        "3. BART: Full sequence to sequence model. Masking spans of words as a single token(text infilling) along with toekn masking. \n",
        "4. PEGASUS: Masking entire sentences as a   [MASK] and asking model to recreate entire sequences like a psuedo extractive summarization task.\n",
        "\n",
        "General observations:\n",
        "1. masking helps. Its like fill in the blanks questions while learning english.\n",
        "2. Pegasus seems better for summarization tasks for low resource setting and for extreme summarization(very good jump on XSUM). \n",
        "3. Performance on downstream tasks is better if the pretraining objective is similar to the pretraining objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3-jvRWfhXcA"
      },
      "source": [
        ""
      ]
    }
  ]
}