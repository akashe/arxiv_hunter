{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ae9ce4-6541-44db-a9f0-0d4da5e1a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://arxiv.org/abs/2401.06761v1': {'title': 'APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding', 'published_date': datetime.datetime(2024, 1, 12, 18, 50, 36), 'pdf_link': 'http://arxiv.org/pdf/2401.06761v1', 'summary': 'The massive adoption of large language models (LLMs) demands efficient\\ndeployment strategies. However, the auto-regressive decoding process, which is\\nfundamental to how most LLMs generate text, poses challenges to achieve\\nefficient serving. In this work, we introduce a parallel auto-regressive\\ngeneration method. By instruct-tuning on general domain data that contains\\nhierarchical structures, we enable LLMs to independently plan their generation\\nprocess and perform auto-parallel auto-regressive (APAR) generation,\\nsignificantly reducing the number of generation steps. APAR alone can achieve\\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\\nand attention computation during generation. This leads to a throughput\\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\\ncompared to state-of-the-art serving frameworks.', 'pdf_text': 'APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding\\nMingdao Liu1,†,∗, Aohan Zeng1,2,∗, Bowen Wang1,†, Peng Zhang2, Jie Tang1, Yuxiao Dong1\\n1Tsinghua University 2Zhipu AI\\nAbstract\\nThe massive adoption of large language models\\n(LLMs) demands efficient deployment strate-\\ngies.\\nHowever, the auto-regressive decod-\\ning process, which is fundamental to how\\nmost LLMs generate text, poses challenges to\\nachieve efficient serving. In this work, we in-\\ntroduce a parallel auto-regressive generation\\nmethod. By instruct-tuning on general domain\\ndata that contains hierarchical structures, we\\nenable LLMs to independently plan their gen-\\neration process and perform auto-parallel auto-\\nregressive (APAR) generation, significantly re-\\nducing the number of generation steps. APAR\\nalone can achieve up to 2× speed-up, and when\\ncombined with speculative decoding, the speed-\\nup can reach up to 4×. In addition, APAR\\nreduces the key-value cache consumption and\\nattention computation during generation. This\\nleads to a throughput increase of 20-70% and a\\nlatency reduce of 20-35% in high-throughput\\nscenarios, compared to state-of-the-art serving\\nframeworks.\\n1\\nIntroduction\\nLarge language models (LLMs) (OpenAI, 2023;\\nTouvron et al., 2023; Zeng et al., 2022) have in-\\ncreasingly become foundational to various AI ap-\\nplications (Richards, 2023; Nakajima, 2023; Park\\net al., 2023; Zhou et al., 2023). This widespread\\nadoption has led to a growing demand for effi-\\ncient model deployment, i.e., low latency and high\\nthroughput (Aminabadi et al., 2022). However, the\\nintrinsic auto-regressive (AR) structure of these\\nmodels presents significant challenges in achieving\\nmore efficient serving (Radford et al., 2018).\\nFirst, each new token is auto-regressively gen-\\nerated conditioned on the entire set of previously-\\ngenerated tokens. This incremental decoding pro-\\n∗ Equal contribution.\\n† Work done while these authors interned at Zhipu AI.\\nFigure 1: APAR Decoding Overview. Different from\\nthe original auto-regressive decoding, APAR detects\\npotential parts to be generated in parallel and issues\\nmultiple generation threads.\\ncess results in sub-optimal generation speeds, as\\neach generation step requires accessing the vast\\nnumber of parameters of a LLM (Aminabadi et al.,\\n2022). Consequently, when the generation batch\\nsize is not sufficiently large, this process becomes\\nmemory-bound, resulting in an under-utilization of\\nthe GPU compute.\\nSecond, the computation of attention over all\\npreceding tokens in Transformer (Vaswani et al.,\\n2017) also limits the serving throughput. In high-\\nthroughput scenarios, many sequences are generat-\\ning in parallel and the generation process becomes\\ncomputation-bound. Meanwhile, the computation\\ncost of attention scales linearly with the sequence\\nlength, which hinders further improvements of the\\nthroughput, especially for long responses. In ad-\\ndition, the caching of key and value tensors (KV\\ncache) for generated tokens, despite advancements\\nin memory-efficient algorithms (Kwon et al., 2023),\\nscales linearly with the sequence length, constrain-\\ning the number of concurrent requests that a system\\ncan handle.\\nIn light of these challenges, we introduce the\\nAuto-Parallel Auto-Regressive (APAR) decoding\\nstrategy with the goal of improving the inference\\narXiv:2401.06761v1  [cs.CL]  12 Jan 2024\\nefficiency of LLMs. APAR leverages the inherent\\nparallelizable structure in LLM generation, capi-\\ntalizing on LLMs’ understanding of text structures.\\nBy fine-tuning LLMs on corpora with hierarchical\\nstructures, the models can learn to autonomously\\ninitiate parallel generation threads when encoun-\\ntering parallelizable response structures. This ap-\\nproach transforms the conventional linear genera-\\ntion into a parallelizable paragraph tree structure.\\nThis not only facilitates greater decoding paral-\\nlelism but also reduces attention spans through tree-\\nbased attention mechanisms, and enables the earlier\\nrelease of consumed KV cache memory.\\nWe perform experiments on the Vicuna family\\nof models. In memory-bound scenarios, APAR\\ncan help reduce the model latency and achieve an\\naverage generation speed increase of 2× on Vi-\\ncuna Bench (Chiang et al., 2023). Furthermore, the\\ndesign of APAR is complementary to most exist-\\ning inference acceleration methods. For example,\\nwhen combined with Medusa (Cai et al., 2023), a\\nspeculative decoding strategy, APAR-based models\\nyield speed improvements of up to 4× on Vicuna\\nBench. In certain specific categories, this combina-\\ntion even achieves a speed-up of up to 6×.\\nIn high-throughput scenarios, APAR’s compat-\\nibility with vLLM enables early memory release,\\nreducing KV cache requirements by up to 50%\\nwhile still maintaining the same level of through-\\nput. In addition, APAR reduces the number of\\ntokens involved in attention calculation. By using\\nthe same amount of KV cache memory, it gets a\\n20-70% improvement in throughput over the origi-\\nnal AR process, and achieves a 20-35% reduction\\nin latency while maintaining the same serving con-\\ncurrency.\\nImportantly, the quality of generation with\\nAPAR is not compromised. Evaluations across\\nmultiple categories on the MT Bench and Vicuna\\nBench (Zheng et al., 2023) demonstrate that the\\nresponse quality remains largely consistent, with\\nvariations within a ±2% range compared to its AR\\ncounterparts. This indicates that APAR-based mod-\\nels retain the contextual generation capability while\\nenhancing the decoding speed and efficiency.\\n2\\nAuto-Parallel Auto-Regressive Decoding\\n2.1\\nOverview\\nParallelizable structures are ubiquitous in the re-\\nsponse of LLMs. For instance, in the ShareGPT\\ndataset, 58% of the dialogues contain at least one re-\\nsponse of ordered or unordered lists from ChatGPT,\\nand about 32% of the responses contains listed\\nstructure. Most listed structures are naturally suit-\\nable for paralleled generation, since the details of\\nan itemized paragraph is usually conditioned on its\\nleading sentence or phrase.\\nThe key idea of APAR is to make LLMs ex-\\nplicitly aware of such parallelizable structures,\\nand spawn auto-parallel auto-regressive decoding\\nthreads accordingly. Specifically, APAR involves\\ntwo key components. First, we post-train the LLMs\\nwith hierarchical text structures, which we referred\\nto as paragraph nodes (Section 2.2). Second, we\\ndesign the decoding algorithm to support parallel\\ndecoding operations, including maintaining the hi-\\nerarchical structure in generation and restoring it\\ninto a linear sequence (Section 2.3).\\n2.2\\nInput Format\\nThis section introduce the corpora used to fine-tune\\nAPAR models, including the tree structure, atten-\\ntion mechanism and control tokens. See Section 3.2\\nfor data pre-processing details.\\nParagraph tree.\\nAs illustrated in Fig 2, a para-\\ngraph tree is used to represent a sequence with\\nhierarchical structure. Each node in the tree, which\\nis referred to as a paragraph node in the sections\\nto follow, denotes a component of the generation\\nresponse. Each paragraph node has 0 or 2 pointers.\\nOne pointer of the paragraph node is referred to as\\nfirst child (blue arrow in Fig 2), which points to\\nthe detailed texts (sub-paragraph) of the paragraph;\\nthe other child pointer is next sibling (red arrow in\\nFig 2), which points to the next paragraph of the\\nsame hierarchical level.\\nControl tokens.\\nTo enable the language model to\\nspawn a parallel decoding thread, 2 control tokens\\nare added to the vocabulary.\\n• Fork Identifier. [Fork] is used to indicate a\\nparallel-able structure in the response. Models\\nare trained to output a [Fork] token when they\\ndiscover that what follows is considered detailed\\ninformation (or a sub-paragraph) and thus can be\\ndecoded together with the next paragraph of the\\nsame level. When the inference system detects\\na [Fork] token output by the model, it creates\\na paralleled decoding thread sharing the same\\nprefix until that [Fork] token. This token works\\njust like the fork() system call used in operating\\nsystems.\\n<User> What if I can’t sleep \\nat night? <Assistant> Here \\nare some advice …\\n1. Establish a Routine: Go \\nto bed and wake up at the \\nsame time every day, …\\n2. Manage Stress: Consider \\nrelaxation such as \\nbreathing exercises and …\\nIf your insomnia persists, \\nit’s essential for you to seek \\nprofessional advice …\\n<User> What if I can’t sleep at night? <Assistant> Here \\nare some advice … 1. Establish a Routine: \\n             Go to bed and wake up at \\nthe same time every day, …\\n2. Manage Daily Stress: FORK\\n             Consider relaxation such \\nas breathing exercises and …\\nIf your insomnia persists, it’s essential \\nfor you to seek professional advice …\\nParagraph Tree    \\nTraining Attention\\np0\\np0\\np1\\np1\\np2\\np2\\np4\\np4\\np3\\np3\\nFirst Child\\nNext Sibling\\nControl\\nToken\\nCHILD\\nCHILD\\nFORK\\nOriginal Sequence\\np0\\np0\\np1\\np1\\np2\\np2\\np3\\np3\\np4\\np4\\np0\\np0\\np1\\np1\\np2\\np2\\np3\\np3\\np4\\np4\\nFigure 2: APAR Training Inputs. Based on pre-defined rules, the original sequence is transformed into a paragraph\\ntree, which is used to train APAR models. Any token attends only to tokens on its path to root.\\nPreﬁx shared \\nNormal\\nToken(s)\\nF\\nB\\nF\\nC\\nFirst Child\\nNext Sibling\\n0\\nt1t1\\nt2t2\\nt2 + 1\\nt2 + 1\\nt1 + 1\\nt1 + 1\\nPosition ID\\n(Time Step)\\nC\\nGeneration Thread\\n(Sequence)\\nMemory released on ﬁnish\\np0\\np0(root)\\np1\\np1(root)\\np1\\np1(detail)\\nE\\nt1 + 1\\nt1 + 1\\nt2 + 1\\nt2 + 1\\nF\\n[Fork] token\\nC\\n[Child] token\\nB\\n[BOS] token\\nE\\n[EOS] token\\np0\\np0(detail)\\n…\\np2\\np2(root)\\n…\\nFigure 3: APAR Decoding. Suppose we are generating a sequence p0 − p1 − p2. The decoding of p1(root) and\\np0(detail) are issued in parallel at (t1 + 1). Similarly, p2(root) and p1(detail) are issued at (t2 + 1). Prefix tokens are\\nshared and forked generation threads can be released once finished.\\n• Child Identifier.\\n[Child] always follows a\\n[Fork], and is used to indicate that the following\\ncontent is the beginning of a sub-paragraph lead\\nby the previous content, like the zero return value\\nof fork() in some operating systems. In particu-\\nlar, [Child] is attended to but is not taken loss\\nin the training process. Thus, the model never\\nlearns to output this token, but learns to output the\\ncontent of the sub-paragraph when [Child] is\\ninjected into the context (i.e. a [Fork] [Child]\\nsequence appears in the context). On the other\\nhand, when [Fork] appears without followed\\nby a [Child], the model will generate the next\\nparagraph of the same level.\\nTraining attention.\\nIn order for the paragraphs\\nto be generated in a parallel, all nodes only attend\\nto their ancestors, and to themselves with a causal\\nmask, as shown in Fig 2.\\n2.3\\nDecoding Procedures\\nAn overview of the generation process is illustrated\\nin Fig. 3 and the algorithm is formulated in Algo-\\nrithm 1. We first introduce the concept of sequence\\nand sequence groups following the implementation\\nin Kwon et al. (2023), then expound the generating\\nprocedures of APAR decoding algorithm.\\nSequence and sequence group.\\nA sequence is\\ndefined as an ordered list of tokens. A sequence\\ngroup is the set of all sequences generated for the\\nsame prompt sequence and is initialized with only\\nthe prompt sequence. In APAR decoding algorithm,\\neach sequence group corresponds to a paragraph\\ntree.\\nEach sequence, on the other hand, is a generation\\nthread and is associated with a leaf node in the\\nparagraph tree.\\nDecoding.\\nAs described in Algorithm 1, we start\\nthe decoding with the user prompt sequence p and\\nAlgorithm 1 APAR decoding algorithm. ISFIN-\\nISHED(G) returns True if all sequences in the se-\\nquence group G have finished. SAMPLE(Θ, s)\\nsamples the next token for sequence s given lan-\\nguage model Θ. A paragraph node n associated\\nwith sequence s points to slice s[start:end] (s[start:]\\nif end is null). PNODE(s, x) creates a new para-\\ngraph node associated with sequence s, initializing\\nstart=x, end=null. The current_node attribute of a\\nsequence is used to record the leaf paragraph node\\npointing to the end of the sequence, and is main-\\ntained (line 25∼29) every time a new sequence is\\nforked. RESTORE(r) recursively traverse the para-\\ngraph tree defined by root r in root - first_child -\\nnext_sibling order.\\n1: Input: A user prompt sequence p, language\\nmodel Θ.\\n2: Output: Decoded generation sequence g.\\n3:\\n4: r ← PNODE(p, p.len)\\n5: G ← {p}\\n6: p.current_node ← r\\n7:\\n8: while not ISFINISHED(G) do\\n9:\\nG ← APARDECODE(G, Θ)\\n10:\\n11: g ← RESTORE(r)\\n12: return g\\n13:\\n14: function APARDECODE(G, Θ)\\n15:\\nfor each sequence s in G do\\n16:\\nif s.finished = True then\\n17:\\ncontinue\\n18:\\nx ←SAMPLE(Θ, s)\\n19:\\nif s.last_token = [Fork] then\\n20:\\ns′ ← s\\n21:\\ns′.append_token([Child])\\n22:\\nG ← G ∪ {s′}\\n23:\\nn ← PNODE(s, s.len)\\n24:\\nn′ ← PNODE(s′, s′.len)\\n25:\\ns.current_node.end ← s.len\\n26:\\ns.current_node.next_sibling ← n\\n27:\\ns.current_node.first_child ← n′\\n28:\\ns.current_node ← n\\n29:\\ns′.current_node ← n′\\n30:\\ns.append_token(x)\\n31:\\nif x = [EOS] then\\n32:\\ns.finish ← True\\n33:\\nRELEASECACHE(s)\\n34:\\nreturn G\\nconstruct a sequence group G initialized as {p}.\\nWe initialize the paragraph tree corresponding to\\nG with root node r and associate sequence p with\\nr (which is now a leaf node). Then, we iterative\\nperform APARDECODE on sequence group G until\\nall sequences in G have finished. In the end, the\\nparagraph tree is traversed to restore the sequential\\noutput g.\\nNext, we delve into the details for APARDE-\\nCODE, which performs a single decode step for\\nsequence group G with language model Θ. For\\neach unfinished sequence s in G, If the last token\\nis [Fork], it means that the model calls for a new\\ngeneration thread, and now it’s time to fork the\\nsequence s′. The forked sequence shares the same\\nprefix tokens as the parent sequence. When imple-\\nmentation with paged attention (Kwon et al., 2023),\\nthe fork operation creates a shared memory map-\\nping for the shared prefix (the dotted red box in\\nFig 3), which copies at most 1 KV cache block and\\nshares all other blocks. After the fork operation,\\ns′ is appended a forced [Child] token to identify\\nthis sequence to the language model as a child se-\\nquence. Also, two new leaf nodes are created and\\ns and s′ are set to track the latest leaf nodes.\\nFinally, sampled new token x is appended to s.\\nIf [EOS] token is sampled, the generation of se-\\nquence s is considered finished, and the KV cache\\nbelonging only to s is released (the dotted red box\\nin Fig 3).\\n2.4\\nFeatures\\nBased on the aforementioned decoding algorithm\\nand the distribution of user queries, we identify\\nthree key features of APAR decoding that give rise\\nto its superior performance in terms of inference\\nlatency, throughput, and memory consumption.\\n1. Paralleled decoding structure reduces latency.\\nThrough training on paragraph trees, the language\\nmodel becomes an automatic online miner for\\nparallel-able structure and concurrent generation\\nthreads are issued accordingly. The paralleled gen-\\neration reduces generation steps. In memory-bound\\nscenarios, the latency in each step remains roughly\\nunchanged wrt. different level of decoding paral-\\nlelism (i.e. dynamic batch size) and the latency can\\ntherefore be reduced proportionately (Fig 4).\\n2. Early release of child KV cache reduces mem-\\nory consumption.\\nIn the auto-regressive gener-\\nation process, the KV cache of all tokens must be\\nretained before the sequence is completely gener-\\nated. In APAR, however, once a forked sequence\\n(i.e. a generation thread) completes generation, the\\nKV cache belonging only to the forked sequence\\ncan be released immediately, while the remaining\\npart of the generation continues. Under the effect\\nof early release strategy, as shown in later Fig 5a,\\nup to 50% of the generation cache can be saved\\nwhile throughput remains the same.\\n3. Reduced attention length saves computation.\\nAuto-regressive generation requires a token to at-\\ntend to all previously generated tokens. In APAR,\\non the other hand, a new token only attends to to-\\nkens along its path to the root of the paragraph tree,\\nwhich reduces attention computation in generation.\\nIn a heavily-batched generation setting, latency\\nincurred by memory access is amortized by the in-\\ntense batched computation, make the generation\\nprocess primarily computation-bound. Thus, the\\ncomputation reduction in each token results in an\\nimprovement in throughput across different mem-\\nory usages (Fig 5a), as well as reduction in latency\\nwith different extents of concurrency (Fig 5b).\\n3\\nExperiments\\n3.1\\nData Pre-processing\\nWe adopt one open-sourced version of ShareGPT\\ndataset1 as instruction corpora. Fine-tuning data\\nare composed as follows.\\nOrdered list.\\nMany responses are represented as\\nordered lists with the pattern of root - detail for\\neach bullet point, where root is usually an intro-\\nduction phrase and detail is the detailed content\\nof that specific point. Thus, the root is extracted\\nas the content for the root node and detail as the\\ncontent in the detail node, as illustrated in Fig 2.\\nParagraph.\\nMost of LLM’s response paragraphs\\nare structured in a root-and-details format, even\\nwhen not presented as an ordered list, where the\\nfirst sentence of the paragraph typically summa-\\nrizes the main idea of that paragraph. Therefore,\\nwe extract the first sentence of the paragraph as the\\nroot for that section, while the rest of the content\\nserves as the detail.\\nUnstructured data.\\nTo accurately extract the hi-\\nerarchical structure, responses with confusing for-\\nmats, like code and math data, are excluded in the\\n1https://huggingface.co/datasets/anon8231489123/ShareGPT_\\nVicuna_unfiltered\\naforementioned structure extraction process. How-\\never, while learning to generate paralleled decoding\\nthreads, a model must also learn not to generate\\n[Fork] in cases where coherent attention is nec-\\nessary to accurately predict the next token. There-\\nfore, some filtered conversations are added as nega-\\ntive examples to prevent the model from excessive\\n[Fork] issuing. This portion of data is organized\\nas a single paragraph node with no descendants.\\nSee Appendix B for detailed procedures and\\nrules used in data pre-processing.\\n3.2\\nExperimental Setup\\nModels.\\nTo evaluate the generation speed,\\nthroughtput and qualities, we apply APAR fine-\\ntuning on vicuna-v1.3-{7B,13B} models, produc-\\ning APAR-{7B,13B}.\\nIn this section, original\\nVicuna models will be referred to as Original-\\n{7B,13B} (O-{7B,13B} as abbreviations) and the\\nfine-tuned APAR models will be referred to as\\nAPAR-{7B,13B} (A-{7B,13B} as abbreviations).\\nImplementation.\\nWe implement 3 settings for\\nevaluation, including\\n• Vanilla-APAR. Vanilla-APAR is implemented\\ndirectly with transformers (Wolf et al., 2020),\\nwhich is a widely adopted python deep learning\\nplatform for transformer-based models.\\n• Medusa-APAR. Medusa-APAR is implemented\\nwith Medusa (Cai et al., 2023), which is an open-\\nsource speculative decoding algorithm that fol-\\nlows the predict - verify paradigm for decoding.\\nMedusa adopts a light-weighed extra language\\nmodeling head to predict the next few tokens and\\nverify generation using tree attention. This set-\\nting is used to test the combined effect of APAR\\nand speculative decoding algorithm.\\n• Batched-APAR. Batched-APAR is implemented\\nwith vLLM (Kwon et al., 2023), a high-\\nthroughput and memory-efficient inference en-\\ngine using paged-attention mechanism. This set-\\nting is used to test APAR on realistic serving\\nsituations, where we not only care about the la-\\ntency but also throughput and memory efficiency.\\nTraining setup.\\nDuring the fine-tuning process,\\nwe sample from structured (ordered list and para-\\ngraph mentioned above, 16k samples) and unstruc-\\ntured data (9k samples) with sampling ratio 1:1.\\nThe models are fine-tuned with batch size 128,\\nlearning rate 2e-5 for 2000 steps. After fine-tuning,\\nwe train 2 medusa heads with learning rate 1e-3 for\\n0\\n100\\n200\\n300\\nCoding\\nCS\\nCF\\nFermi\\nGeneric\\nKnowledge\\nMath\\nRoleplay\\nWriting\\nMean\\n7B Models\\n0\\n100\\n200\\n13B Models\\nGeneration Speed (token / s)\\nOriginal\\nVanilla-APAR\\nMedusa\\nMedusa-APAR\\n(a) Results in Vicuna Bench\\n0\\n100\\n200\\nCoding\\nExtraction\\nHumanities\\nMath\\nReasoning\\nRoleplay\\nSTEM\\nWriting\\nMean\\n7B Models\\n0\\n50\\n100\\n150\\n13B Models\\nGeneration Speed (token / s)\\nOriginal\\nVanilla-APAR\\nMedusa\\nMedusa-APAR\\n(b) Results in MT Bench\\nFigure 4: Generation speed in memory-bound scenario. Models served on one H800-SXM5-80G GPU with\\nbatch size 1.\\n2000 steps using the same data as fine-tuning. See\\nAppendix A for detailed hyper-parameter settings.\\nEvaluation datasets.\\nSeveral datasets are used\\nto evaluate the generation statistics and quality.\\n• Vicuna Bench (Chiang et al., 2023) is a bench-\\nmark for evaluating LLMs on language under-\\nstanding, reasoning and context awareness. It\\ncovers 9 categories and contains 80 single-\\nturn queries. For a clearer layout, we abbre-\\nviate 2 long category names in Vicuna Bench\\nin the following figures and tables, i.e. Com-\\nmonsense to CS, Counterfactual to CF.\\n• MT Bench (Zheng et al., 2023) is a bench-\\nmark consisting of 80 multi-turn questions. It\\ncovers 8 common categories and can be used\\nto evaluate the multi-turn conversation and\\ninstruction-following ability of LLMs.\\n• APAR Test Set consists of 1000 user queries\\nsampled from ShareGPT dataset to simulate\\nthe query distribution in realistic deployment\\nscene using the same rule we extract struc-\\ntured training data. Because of its large quan-\\ntity, it’s would be too expensive to evaluate\\ngeneration quality for all test set queries on\\nall models. Thus, APAR test set is only used\\nfor measurement of generation statistics.\\n3.3\\nResults in Memory-Bound Scenarios\\nWe inspect how APAR reduces generation latency\\nin a memory-bound (i.e. small batch size) sce-\\nnario, as well as its combined acceleration effect\\nwith speculative decoding. Considering that the the\\nmodel is re-trained and the output length can be\\ndifferent on the same prompt, we normalize genera-\\ntion latency with generated tokens, adopting tokens\\nper second as the metric for generation speed. The\\nresults are reported with batch size fixed as 1 and\\nprefix sharing is not enabled.\\nAs shown in Fig 4, Vanilla-APAR achieves 2×\\naverage speed up in Vicuna Bench and 1.4× aver-\\nage speed up on MT Bench. APAR models learns\\nto spawn parallel generation thread in and only\\nin categories that exists a parallel-able structure.\\nFor instance, APAR-{7B,13B} seldom try to issue\\nparallel a generation threads in coding and math re-\\nlated queries, which typically requires careful step\\nby step reasoning or rigorous formats, resulting in\\nno speed up. On the other hand, on categories like\\ncommon-sense, generic and knowledge, the speed\\nup is significant. When combined with speculative\\ndecoding, Medusa-APAR achieves an impressive\\n4× average speed up in Vicuna Bench and 2.9× av-\\nerage speed up in MT Bench, demonstrating strong\\nreduction in generation latency.\\n3.4\\nResults in High-Throughput Scenarios\\n+23%\\n+33%\\n(a) Throughput wrt. cache memory usage\\n+28%\\n-21%\\n(b) End-to-end decode latency wrt. concurrency\\nFigure 5: Serving statistics of Batched-APAR. Models\\nserved on one A100-SXM4-80G GPU. Error bars in (b)\\nomitted for a clearer view.\\nIn high performance serving situations, increas-\\ning throughput and reducing serving memory are\\nalso important. We use Batched-APAR to serve the\\nqueries in APAR test set with different amount of\\nGPU memory available. An overview of generation\\nthroughput and per-token latency is summarized in\\nFig 5. The dots in the plot show the mean value and\\nerror bars represent the 25% and 75% percentile in\\neach setting.\\nAs shown in Fig 5a, the throughput of Batched-\\nAPAR models surpass the maximum throughput of\\noriginal models with only 20% of the KV Cache\\nused, demonstrating memory efficiency. When us-\\ning similar amount of memory, throughput is con-\\nsistently increase by 20%∼ 70% across different\\ncache usages. Batched-APAR models also demon-\\nstrate remarkable latency reduction in computation\\nbound scenarios. As shown in Fig 5b, Batched-\\nAPAR reduces 20%∼35% average latency when\\nTable 1: Average Max Cached Tokens\\nBenchmark\\nModel\\nAPAR\\nFlatten\\nSaved\\nVicuna\\nA-7B\\n303.2\\n417.2\\n27.3%\\nA-13B\\n306.7\\n419.3\\n26.8%\\nMT\\nA-7B\\n502.1\\n571.1\\n12.1%\\nA-13B\\n513.3\\n590.6\\n13.1%\\nTable 2: Average Attended Tokens\\nBenchmark\\nModel\\nAPAR\\nFlatten\\nSaved\\nVicuna\\nA-7B\\n166.2\\n256.4\\n35.2%\\nA-13B\\n166.6\\n255.9\\n34.9%\\nMT\\nA-7B\\n417.5\\n496.5\\n15.9%\\nA-13B\\n428.0\\n514.2\\n16.8%\\nserving the same amount of concurrent requests.\\nThe latency of Batched-APAR-13B is even similar\\nto the Original-7B model.\\nThe improvement in latency and throughput can\\nbe best explained by feature 2 and 3 as described in\\nSection 2.4. We quantitatively measure how much\\ncomputation and cache memory can be saved by\\nusing APAR decoding algorithm. We adopt the\\nfollowing metrics.\\n• Max cached tokens is defined as the maxi-\\nmum number of KV cache slots required for\\ngenerating a response. For auto-regressive\\ngeneration, prompt tokens and all generated\\ntokens need to be cached before generating\\n[EOS] token.\\n• Attended tokens is defined as the number of\\ntokens attended to when predicting a specific\\ntoken. For auto-regressive generation, all pre-\\nceding tokens is needed when predicting a\\nnext token.\\nSince the response length is difference between\\nAPAR models and original models, we flatten the\\nparagraph tree generated by APAR models as the\\nreference output. When calculating average, we ex-\\nclude categories that are not accelerated by APAR,\\ni.e. Coding, Extraction and Math.\\nAs summarized in Table 1 and Table 2, compared\\nwith flattened results, APAR reduces max cached\\ntokens by 12%∼27% and reduced attended tokens\\nby 15%∼35%. Detailed results for all categories\\nare reported in Appendix D.3 and Appendix D.2.\\nTable 3: Generation Quality in MT Bench\\nTask\\nO-7B\\nA-7B\\nO-13B\\nA-13B\\nCoding\\n2.60\\n2.60\\n3.70\\n3.70\\nExtraction\\n4.90\\n5.75\\n5.35\\n4.85\\nHumanities\\n9.45\\n9.40\\n9.45\\n9.55\\nMath\\n2.70\\n2.00\\n2.60\\n2.65\\nWriting\\n7.90\\n7.15\\n8.40\\n7.88\\nRoleplay\\n6.35\\n7.25\\n7.67\\n7.58\\nReasoning\\n4.85\\n5.40\\n5.95\\n5.85\\nStem\\n7.92\\n7.15\\n7.67\\n7.80\\nMean\\n5.83\\n5.92\\n6.35\\n6.24\\nTable 4: Generation Quality in Vicuna Bench\\nTask\\nO-7B\\nA-7B\\nO-13B\\nA-13B\\nCoding\\n3.86\\n3.29\\n6.14\\n3.71\\nCS\\n9.40\\n9.50\\n9.50\\n9.60\\nCF\\n8.05\\n8.00\\n8.65\\n9.00\\nFermi\\n6.80\\n6.90\\n7.35\\n6.60\\nGeneric\\n9.45\\n9.50\\n9.30\\n9.50\\nKnowledge\\n9.40\\n9.30\\n9.60\\n9.35\\nMath\\n1.67\\n2.00\\n1.67\\n4.67\\nRoleplay\\n8.80\\n8.80\\n8.90\\n8.75\\nWriting\\n9.50\\n9.00\\n9.50\\n9.00\\nMean\\n8.08\\n7.99\\n8.45\\n8.29\\n3.5\\nGeneration Quality\\nTo measure the generation quality of APAR mod-\\nels compared with original models, we adopt MT\\nBench and Vicuna Bench as evaluation framework.\\nFor each response, we provide GPT-4 with con-\\nversation history, user query and model responses,\\nasking GPT-4 to grade the response with a score\\nranging from 1 to 10 and we follow the prompt\\ntemplate used by Zheng et al. (2023).\\nThe quality scores of each category are summa-\\nrized in Table 3 and Table 4. Compared with orig-\\ninal models, APAR models differs by -2%∼+2%\\nin MT Bench and Vicuna Bench overall scores,\\nshowing negligible overall quality change.\\n4\\nRelated Works\\nThis section discuss the difference and connection\\nof APAR with prior works concerning inference\\nacceleration.\\nOptimized computation.\\nOptimizations on op-\\nerators (Dao et al., 2022) and computational\\ngraphs (Aminabadi et al., 2022) are active research\\nfields. Model compression is widely used in de-\\nployment, like quantization (Dettmers et al., 2022;\\nFrantar et al., 2022) and pruning (Frantar and Al-\\nistarh, 2023; Ma et al., 2023). Another line of\\nworks modifies the model architecture, including\\nefficient attention (Kitaev et al., 2020) for computa-\\ntion complexity and multi-query attention (Shazeer,\\n2019) for optimized IO. Different from prior works,\\nAPAR makes no modification to operators or model\\narchitecture but reduces computation by adopting\\nattention tree structure. APRA is thus orthogonal\\nto and can be applied jointly with the aforemen-\\ntioned works.\\nImproved parallelism.\\nScheduling strategies,\\nincluding dynamic batching (Yu et al., 2022)\\nand paged-attention (Kwon et al., 2023), im-\\nprove maximum generation throughput. Another\\nstream of works explores speculative decoding\\n(SD) (Leviathan et al., 2023; Yang et al., 2023;\\nCai et al., 2023), which verifies multiple specu-\\nlated tokens in parallel, reducing generation latency\\nin small batch sizes. Non-auto-regressive genera-\\ntion (Gu et al., 2018) propose to sample multiple\\ngeneration tokens in parallel, which typically re-\\nquires re-training and applies to restricted scenarios.\\nAPAR can be conveniently combined with efficient\\nscheduling and SD methods to achieve augmented\\nefficiency as demonstrated by Medusa-APAR and\\nBatched-APAR. Different from previous methods,\\nAPAR propose to exploit the intrinsic organization\\nability of LLMs to automatically issue paralleled\\ngeneration threads, and is applicable to multiple\\nscenarios. Notably, SoT (Ning et al., 2023) pro-\\nposes to enable parallelism by prompting, which\\ngenerates the skeleton of the response and then\\nexpands each point in parallel.\\nDifferent from\\nSoT, which entails an external classifier and re-\\ncomputation of KV cache between stages, APAR\\nrequires negligible extra computation (2 control to-\\nkens for a thread) and no re-computation, and thus\\ndoes not compromise generation throughput.\\n5\\nConclusion\\nThis paper introduces APAR, a new decoding\\nmethod that allows LLMs to autonomously struc-\\nture the decoding process and dynamically create\\nparallel decoding threads, without compromising\\nthe generation quality. APAR not only enhances\\nparallelism in generation, but also reduces the com-\\nputation and KV cache memory consumption. Ex-\\nperiments show that APAR can be seamlessly in-\\ntegrated with existing inference frameworks, sig-\\nnificantly lowering the generation latency across\\nvarious scenarios while improving serving through-\\nput in situations involving extreme batch sizes and\\nconcurrency levels.\\nReferences\\nReza Yazdani Aminabadi, Samyam Rajbhandari, Minjia\\nZhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton\\nZheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase,\\nand Yuxiong He. 2022. Deepspeed inference: En-\\nabling efficient inference of transformer models at\\nunprecedented scale.\\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\\nand Tri Dao. 2023. Medusa: Simple framework for\\naccelerating llm generation with multiple decoding\\nheads.\\nhttps://github.com/FasterDecoding/\\nMedusa.\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness.\\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\\ncation for transformers at scale.\\nElias Frantar and Dan Alistarh. 2023. Sparsegpt: Mas-\\nsive language models can be accurately pruned in\\none-shot.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\\nDan Alistarh. 2022. GPTQ: Accurate post-training\\ncompression for generative pretrained transformers.\\narXiv preprint arXiv:2210.17323.\\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\\nLi, and Richard Socher. 2018. Non-autoregressive\\nneural machine translation. In International Confer-\\nence on Learning Representations.\\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\\n2020. Reformer: The efficient transformer.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\\ncient memory management for large language model\\nserving with pagedattention. In Proceedings of the\\nACM SIGOPS 29th Symposium on Operating Systems\\nPrinciples.\\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\\n2023. Fast inference from transformers via spec-\\nulative decoding. In Proceedings of the 40th Interna-\\ntional Conference on Machine Learning, ICML’23.\\nJMLR.org.\\nXinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.\\nLlm-pruner: On the structural pruning of large lan-\\nguage models. In Advances in Neural Information\\nProcessing Systems.\\nYohei\\nNakajima.\\n2023.\\nBabyagi.\\nPython.\\nhttps://github.com/yoheinakajima/babyagi.\\nXuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang,\\nHuazhong Yang, and Yu Wang. 2023. Skeleton-of-\\nthought: Large language models can do parallel de-\\ncoding.\\nOpenAI. 2023. Gpt-4 technical report.\\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai,\\nMeredith Ringel Morris, Percy Liang, and Michael S.\\nBernstein. 2023. Generative agents: Interactive simu-\\nlacra of human behavior. In In the 36th Annual ACM\\nSymposium on User Interface Software and Technol-\\nogy (UIST ’23), UIST ’23, New York, NY, USA.\\nAssociation for Computing Machinery.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\\nSutskever, et al. 2018. Improving language under-\\nstanding by generative pre-training.\\nToran Bruce Richards. 2023. Auto-gpt: An autonomous\\ngpt-4 experiment.\\nNoam Shazeer. 2019. Fast transformer decoding: One\\nwrite-head is all you need.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. ArXiv preprint,\\nabs/2302.13971.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems, volume 30. Curran Associates, Inc.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\\nScao, Sylvain Gugger, Mariama Drame, Quentin\\nLhoest, and Alexander M. Rush. 2020. Transform-\\ners: State-of-the-art natural language processing. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations, pages 38–45, Online. Association\\nfor Computational Linguistics.\\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\\nJiang, Linjun Yang, Rangan Majumder, and Furu Wei.\\n2023. Inference with reference: Lossless accelera-\\ntion of large language models.\\nGyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\\njeong Kim, and Byung-Gon Chun. 2022. Orca: A\\ndistributed serving system for Transformer-Based\\ngenerative models.\\nIn 16th USENIX Symposium\\non Operating Systems Design and Implementation\\n(OSDI 22), pages 521–538, Carlsbad, CA. USENIX\\nAssociation.\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\\nWendi Zheng, Xiao Xia, et al. 2022.\\nGlm-130b:\\nAn open bilingual pre-trained model. arXiv preprint\\narXiv:2210.02414.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nllm-as-a-judge with mt-bench and chatbot arena.\\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\\nBisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\\nA realistic web environment for building autonomous\\nagents. arXiv preprint arXiv:2307.13854.\\nA\\nTraining Hyper-parameters\\nA.1\\nAPAR Models\\nTable 5: Training Hyper-parameters for APAR Models\\nHyper-parameter\\nValue\\nbatch size\\n128\\ndata type\\nbf16\\ntraining step\\n2000\\nlearning rate\\n2e-5\\nweight decay\\n0\\nwarm-up ratio\\n0.03\\nlr decay schedule\\ncosine\\ncontext length\\n2048\\nA.2\\nMedusa Heads\\nThe following hyper-parameters are used to train\\nthe medusa heads for speculative decoding. Only\\nmedusa heads are trained and the reset of the lan-\\nguage model remains frozen.\\nTable 6: Training Hyper-parameters for Medusa Heads\\nHyper-parameter\\nValue\\nbatch size\\n128\\ndata type\\nbf16\\ntraining step\\n2000\\nlearning rate\\n1e-3\\nweight decay\\n0\\nwarm-up ratio\\n0.1\\nlr decay schedule\\ncosine\\ncontext length\\n2048\\n# of Medusa heads\\n2\\n# of layers per Medusa head\\n2\\nB\\nRules for Extracting Structured Data\\nThe process and rules to determine and extract the\\nstructure of each assistant response are outlined as\\nfollows.\\n1. Try to extract the response as ordered list.\\n(a) Use regular expression like\\n(\\\\d+\\\\.)\\\\s+(.+?):(.+?)\\nto extract individual numeric points.\\n(b) If\\ni. the regular expression does not does\\nnot match at least 3 numeric points,\\nor\\nii. any of the content of the numeric\\npoints are less then 10 characters\\nthe response is not consider as a valid\\nordered list.\\n2. If the response is not consider as a valid or-\\ndered list, try to extract the response as multi-\\nple paragraph.\\n(a) Use two consecutive \\\\n to divide the en-\\ntire response into paragraphs and extract\\nthe first sentence of each paragraph. Para-\\ngraphs with only one sentence is skipped.\\n3. If ambiguous patterns, including code blocks,\\nmath expressions, URLs, etc, exists in the\\nresponse, the, or if the response fails to match\\nthe above 2 criteria, the response is considered\\nunstructured.\\nPre-processing code will be made public in the\\nproject repository.\\nC\\nGeneration Speed Evaluation Setup\\nC.1\\nVanilla-APAR and Medusa-APAR\\nVanilla-APAR and Medusa-APAR are implemen-\\ntation directly from transformers package. To\\nkeep KV cache contiguous, prefix sharing is not\\nenabled and the prefix KV caches are copied when\\na new generation thread is forked. The implemen-\\ntation of Medusa-APAR is adopt from its official\\nrepository. When different number of tokens are ac-\\ncepted across batches, the longest accepted length\\nis adopted and the mis-predicted slots are masked\\nout in attention calculation. The evaluation batch-\\nsize is fixed as 1 and the pre-filling time is not\\nmeasured when calculating generation speed.\\nC.2\\nBatched-APAR\\nTo measure the maximum throughput of each gen-\\neration method, GPU cache utilization is set to\\n0.1, · · · , 0.9 for each setting and the system is pro-\\nfiled every 3 seconds. For a stable performance, we\\nignore the terminal samples when no requests are\\npending and ignore the first 1\\n3 samples as warm-up\\nwhen calculating mean and percentiles.\\nAll 1k requests are push into waiting queue in\\nthe beginning of the performance test. We limit\\nthe max number of concurrent requests to 350 for\\n7B models and 180 for 13B models. The reason\\nfor this limit is that in the beginning, the average\\nsequence length is relatively sort and the system\\nis prone to excessively accept requests, leading to\\nTable 7: Parallel Generation Statistics in Vicuna Bench\\nTask\\nAPAR-7B\\nAPAR-13B\\n#T\\n%P\\n#T\\n%P\\nCoding\\n1.0\\n0.0\\n1.0\\n0.0\\nCS\\n6.0\\n1.0\\n6.5\\n1.0\\nCF\\n4.1\\n0.9\\n5.4\\n1.0\\nFermi\\n5.3\\n0.9\\n5.5\\n0.9\\nGeneric\\n7.7\\n1.0\\n7.7\\n1.0\\nKnowledge\\n6.8\\n1.0\\n5.6\\n1.0\\nMath\\n1.0\\n0.0\\n1.0\\n0.0\\nRoleplay\\n4.5\\n0.8\\n5.1\\n1.0\\nWriting\\n5.4\\n0.9\\n4.6\\n0.7\\nMean\\n5.1\\n0.81\\n5.2\\n0.82\\nTable 8: Parallel Generation Statistics in MT Bench\\nTask\\nAPAR-7B\\nAPAR-13B\\n#T\\n%P\\n#T\\n%P\\nCoding\\n1.0\\n0.0\\n1.0\\n0.0\\nExtraction\\n1.0\\n0.0\\n1.3\\n0.1\\nHumanities\\n5.0\\n0.7\\n6.1\\n0.8\\nMath\\n1.1\\n0.1\\n1.0\\n0.0\\nReasoning\\n2.0\\n0.3\\n2.5\\n0.6\\nRoleplay\\n4.1\\n0.8\\n4.0\\n0.7\\nSTEM\\n3.4\\n0.6\\n3.6\\n0.6\\nWriting\\n3.7\\n0.6\\n3.4\\n0.6\\nMean\\n2.7\\n0.37\\n2.9\\n0.41\\nfrequent swapping and recompute of KV cache.\\nNote that this concurrency limit mainly takes effect\\nin warm-up stage, which is ignore in the calculation\\nfor mean and percentage. The concurrency limit\\nis much larger than the maximum in the average\\nconcurrent request as shown in Fig 5b.\\nD\\nGeneration Speed Details\\nD.1\\nParallel Generation Statistics\\nWe measure how many parallel threads are issued\\nin generation across benchmarks and categories.\\nResults are reported in Table 7 and Table 8. #T\\nstands for average number of generation threads,\\n%P stand for ratio of parallel-able response, i.e.\\nresponse that has at least 2 generation threads.\\nD.2\\nMax Cached Tokens\\nDetailed results of max cached tokens across bench-\\nmarks and categories are reported in Table 9, Ta-\\nble 10, 11 and Table 12, . Mean value of all cat-\\nTable 9: Max Cached Tokens (Vanilla-APAR-7B on\\nVicuna Bench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n378.0\\n378.0\\n0.0%\\nCS\\n296.6\\n417.7\\n29.0%\\nCF\\n245.6\\n312.4\\n21.4%\\nFermi\\n365.3\\n496.8\\n26.5%\\nGeneric\\n251.5\\n388.8\\n35.3%\\nKnowledge\\n345.0\\n489.9\\n29.6%\\nMath\\n210.7\\n210.7\\n0.0%\\nRoleplay\\n292.5\\n364.1\\n19.7%\\nWriting\\n325.6\\n450.4\\n27.7%\\nMean\\n306.2\\n406.0\\n24.6%\\nMean(>1%)\\n303.2\\n417.2\\n27.3%\\nTable 10: Max Cached Tokens (Vanilla-APAR-13B on\\nVicuna Bench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n370.0\\n370.0\\n0.0%\\nCS\\n287.6\\n416.9\\n31.0%\\nCF\\n276.9\\n383.0\\n27.7%\\nFermi\\n401.0\\n529.7\\n24.3%\\nGeneric\\n260.7\\n389.2\\n33.0%\\nKnowledge\\n313.2\\n419.7\\n25.4%\\nMath\\n203.3\\n203.3\\n0.0%\\nRoleplay\\n254.3\\n346.5\\n26.6%\\nWriting\\n353.5\\n449.8\\n21.4%\\nMean\\n308.4\\n406.9\\n24.2%\\nMean(>1%)\\n306.7\\n419.3\\n26.8%\\negories and categories accelerated by APAR are\\nreported.\\nD.3\\nAttended Tokens\\nDetailed results of attend tokens across benchmarks\\nand categories are reported in Table 13, Table 14,\\nTable 15 and Table 16. Mean value of all categories\\nand categories accelerated by APAR are reported.\\nD.4\\nResponse Length\\nApart from generation quality, we also analyze the\\nresponse length distribution of model before and\\nafter fine-tuning in Fig 6. The average length varies\\nfrom -0.3%∼+4.0% compared with respective orig-\\ninal model and the distributions highly overlap.\\nThis indicate that if fine-tuned with the same mate-\\nrial, APAR does not significantly affect the genera-\\ntion length distribution.\\nTable 11: Max Cached Tokens (Vanilla-APAR-7B on\\nMT Bench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n744.8\\n744.8\\n0.0%\\nExtraction\\n567.2\\n567.2\\n0.0%\\nHumanities\\n647.0\\n757.3\\n14.6%\\nMath\\n408.5\\n410.4\\n0.4%\\nReasoning\\n314.9\\n335.4\\n6.1%\\nRoleplay\\n461.2\\n533.6\\n13.6%\\nSTEM\\n574.2\\n641.7\\n10.5%\\nWriting\\n513.0\\n587.6\\n12.7%\\nMean\\n529.6\\n573.3\\n7.6%\\nMean(>1%)\\n502.1\\n571.1\\n12.1%\\nTable 12: Max Cached Tokens (Vanilla-APAR-13B on\\nMT Bench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n589.6\\n589.6\\n0.0%\\nExtraction\\n640.3\\n641.7\\n0.2%\\nHumanities\\n676.1\\n822.5\\n17.8%\\nMath\\n451.9\\n451.9\\n0.0%\\nReasoning\\n318.2\\n344.8\\n7.7%\\nRoleplay\\n484.6\\n548.5\\n11.7%\\nSTEM\\n591.1\\n677.5\\n12.8%\\nWriting\\n496.4\\n559.6\\n11.3%\\nMean\\n530.3\\n579.1\\n8.4%\\nMean(>1%)\\n513.3\\n590.6\\n13.1%\\nTable 13: Attended Tokens (Vanilla-APAR-7B on Vi-\\ncuna Bench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n221.6\\n221.6\\n0.0%\\nCS\\n152.0\\n244.6\\n37.9%\\nCF\\n142.0\\n185.3\\n23.4%\\nFermi\\n214.6\\n309.6\\n30.7%\\nGeneric\\n126.3\\n222.2\\n43.2%\\nKnowledge\\n154.7\\n290.3\\n46.7%\\nMath\\n139.7\\n139.7\\n0.0%\\nRoleplay\\n166.8\\n229.0\\n27.2%\\nWriting\\n189.1\\n270.0\\n30.0%\\nMean\\n170.3\\n251.7\\n32.4%\\nMean(>1%)\\n166.2\\n256.4\\n35.2%\\nTable 14: Attended Tokens (Vanilla-APAR-13B on Vi-\\ncuna Bench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n222.0\\n222.0\\n0.0%\\nCS\\n151.3\\n248.7\\n39.2%\\nCF\\n144.6\\n224.2\\n35.5%\\nFermi\\n204.9\\n325.7\\n37.1%\\nGeneric\\n125.3\\n221.8\\n43.5%\\nKnowledge\\n158.6\\n248.8\\n36.3%\\nMath\\n137.3\\n137.3\\n0.0%\\nRoleplay\\n150.3\\n217.1\\n30.8%\\nWriting\\n209.0\\n271.8\\n23.1%\\nMean\\n170.5\\n251.4\\n32.2%\\nMean(>1%)\\n166.6\\n255.9\\n34.9%\\nTable 15: Attended Tokens (Vanilla-APAR-7B on MT\\nBench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n577.4\\n577.4\\n0.0%\\nExtraction\\n570.8\\n570.8\\n0.0%\\nHumanities\\n456.5\\n556.0\\n17.9%\\nMath\\n616.0\\n616.9\\n0.1%\\nReasoning\\n286.7\\n322.0\\n11.0%\\nRoleplay\\n404.3\\n477.7\\n15.4%\\nSTEM\\n413.0\\n473.8\\n12.8%\\nWriting\\n437.9\\n533.1\\n17.8%\\nMean\\n477.0\\n528.3\\n9.7%\\nMean(>1%)\\n417.5\\n496.5\\n15.9%\\nTable 16: Attended Tokens (Vanilla-APAR-13B on MT\\nBench)\\nTask\\nAPAR\\nAR\\nSaved\\nCoding\\n449.9\\n449.9\\n0.0%\\nExtraction\\n767.8\\n769.2\\n0.2%\\nHumanities\\n507.0\\n636.0\\n20.3%\\nMath\\n601.3\\n601.3\\n0.0%\\nReasoning\\n314.3\\n350.1\\n10.2%\\nRoleplay\\n404.8\\n464.9\\n12.9%\\nSTEM\\n445.3\\n521.1\\n14.6%\\nWriting\\n360.8\\n442.7\\n18.5%\\nMean\\n482.3\\n539.6\\n10.6%\\nMean(>1%)\\n428.0\\n514.2\\n16.8%\\n200\\n400\\n600\\n800\\n1000\\nResponse Length\\n0.000\\n0.001\\n0.002\\n0.003\\n0.004\\n0.005\\nProbability Density\\nVicuna Bench (7B Models)\\nA-7B \\n(mean: 344.5)\\nO-7B \\n(mean: 345.5)\\n200\\n400\\n600\\n800\\nResponse Length\\n0.000\\n0.001\\n0.002\\n0.003\\n0.004\\n0.005\\n0.006\\nProbability Density\\nVicuna Bench (13B Models)\\nA-13B \\n(mean: 345.4)\\nO-13B \\n(mean: 334.5)\\n0\\n500\\n1000\\n1500\\n2000\\nResponse Length\\n0.0000\\n0.0005\\n0.0010\\n0.0015\\n0.0020\\nProbability Density\\nMT Bench (7B Models)\\nA-7B \\n(mean: 295.1)\\nO-7B \\n(mean: 283.6)\\n0\\n500\\n1000\\n1500\\n2000\\nResponse Length\\n0.0000\\n0.0005\\n0.0010\\n0.0015\\n0.0020\\n0.0025\\nProbability Density\\nMT Bench (13B Models)\\nA-13B \\n(mean: 301.2)\\nO-13B \\n(mean: 291.9)\\nFigure 6: Response Length on Vicuna Bench and MT\\nBench\\n'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "import fitz # this is pymupdf\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Define the ArxivParser class\n",
    "class ArxivParser:\n",
    "    def __init__(self, query: str = \"llm\", max_results: int = 1, days: int = 2*365):\n",
    "        self.query = query\n",
    "        self.max_results = max_results\n",
    "        self.days = days\n",
    "        self.url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "        # Send a GET request to api endpoint\n",
    "        self.response = requests.get(self.url)\n",
    "        # Parse the response\n",
    "        self.entries = feedparser.parse(self.response.text).entries\n",
    "        # Use a type alias to define the type of the dictionary values\n",
    "        EntryData = Dict[str, str]\n",
    "        self.extracted_data: Dict[str, EntryData] = {} # pickle file storage\n",
    "        \n",
    "\n",
    "    def store_entries(self) -> None:\n",
    "        # Loop through the entries\n",
    "        for entry in self.entries:\n",
    "            published_date = datetime.strptime(entry.published, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            current_date = datetime.now()\n",
    "            date_diff = (current_date - published_date).days\n",
    "            # Check if the date difference is less than or equal to the days parameter\n",
    "            if date_diff <= self.days:\n",
    "                id = entry.id\n",
    "                title = entry.title\n",
    "                link = entry.link\n",
    "                summary = entry.summary\n",
    "                # Get the pdf link by replacing the \"abs\" with \"pdf\" in the link\n",
    "                pdf_link = link.replace(\"abs\", \"pdf\")\n",
    "                # Get the pdf content by sending a GET request to the pdf link and opening it with fitz\n",
    "                pdf_content = requests.get(pdf_link).content\n",
    "                pdf_file = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "                # Extract the text from the pdf file\n",
    "                pdf_text = \"\"\n",
    "                for page in pdf_file:\n",
    "                    pdf_text += page.get_text()\n",
    "                # Store the id as the key and the values in a nested dictionary\n",
    "                self.extracted_data[id] = {\"title\": title, \"published_date\":published_date, \"pdf_link\": pdf_link, \"summary\": summary, \"pdf_text\": pdf_text}\n",
    "            else:\n",
    "                # Break the loop if the date difference is greater than the days parameter\n",
    "                break\n",
    "\n",
    "# Create an instance of the ArxivParser class with the default parameters\n",
    "parser = ArxivParser()\n",
    "# Call the store_entries method to store the results in a nested dictionary\n",
    "parser.store_entries()\n",
    "data=parser.extracted_data\n",
    "# Print the results\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c7aca4-388e-4ee7-b170-244fdd317984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2401.06761v1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb17dc01-0932-4633-aa07-1c3c72ea1544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The massive adoption of large language models (LLMs) demands efficient\\ndeployment strategies. However, the auto-regressive decoding process, which is\\nfundamental to how most LLMs generate text, poses challenges to achieve\\nefficient serving. In this work, we introduce a parallel auto-regressive\\ngeneration method. By instruct-tuning on general domain data that contains\\nhierarchical structures, we enable LLMs to independently plan their generation\\nprocess and perform auto-parallel auto-regressive (APAR) generation,\\nsignificantly reducing the number of generation steps. APAR alone can achieve\\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\\nand attention computation during generation. This leads to a throughput\\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\\ncompared to state-of-the-art serving frameworks.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[list(data.keys())[0]]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11450be2-13a6-4d3e-a7de-dddd22bac1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: http://arxiv.org/abs/2401.06761v1\n",
      "Published date: 2024-01-12 18:50:36\n",
      "Pdf link: http://arxiv.org/pdf/2401.06761v1\n",
      "\n",
      "Title: APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding\n",
      "\n",
      "Summary: The massive adoption of large language models (LLMs) demands efficient\n",
      "deployment strategies. However, the auto-regressive decoding process, which is\n",
      "fundamental to how most LLMs generate text, poses challenges to achieve\n",
      "efficient serving. In this work, we introduce a parallel auto-regressive\n",
      "generation method. By instruct-tuning on general domain data that contains\n",
      "hierarchical structures, we enable LLMs to independently plan their generation\n",
      "process and perform auto-parallel auto-regressive (APAR) generation,\n",
      "significantly reducing the number of generation steps. APAR alone can achieve\n",
      "up to 2x speed-up, and when combined with speculative decoding, the speed-up\n",
      "can reach up to 4x. In addition, APAR reduces the key-value cache consumption\n",
      "and attention computation during generation. This leads to a throughput\n",
      "increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\n",
      "compared to state-of-the-art serving frameworks.\n",
      "\n",
      "Content: APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding\n",
      "Mingdao Liu1,†,∗, Aohan Zeng1,2,∗, Bowen Wang1,†, Peng Zhang2, Jie Tang1, Yuxiao Dong1\n",
      "1Tsinghua University 2Zhipu AI\n",
      "Abstract\n",
      "The massive adoption of large language models\n",
      "(LLMs) demands efficient deployment strate-\n",
      "gies.\n",
      "However, the auto-regressive decod-\n",
      "ing process, which is fundamental to how\n",
      "most LLMs generate text, poses challenges to\n",
      "achieve efficient serving. In this work, we in-\n",
      "troduce a parallel auto-regressive generation\n",
      "method. By instruct-tuning on general domain\n",
      "data that contains hierarchical structures, we\n",
      "enable LLMs to independently plan their gen-\n",
      "eration process and perform auto-parallel auto-\n",
      "regressive (APAR) generation, significantly re-\n",
      "ducing the number of generation steps. APAR\n",
      "alone can achieve up to 2× speed-up, and when\n",
      "combined with speculative decoding, the speed-\n",
      "up can reach up to 4×. In addition, APAR\n",
      "reduces the key-value cache consumption and\n",
      "attention computation during generation. This\n",
      "leads to a throughput increase of 20-70% and a\n",
      "latency reduce of 20-35% in high-throughput\n",
      "scenarios, compared to state-of-the-art serving\n",
      "frameworks.\n",
      "1\n",
      "Introduction\n",
      "Large language models (LLMs) (OpenAI, 2023;\n",
      "Touvron et al., 2023; Zeng et al., 2022) have in-\n",
      "creasingly become foundational to various AI ap-\n",
      "plications (Richards, 2023; Nakajima, 2023; Park\n",
      "et al., 2023; Zhou et al., 2023). This widespread\n",
      "adoption has led to a growing demand for effi-\n",
      "cient model deployment, i.e., low latency and high\n",
      "throughput (Aminabadi et al., 2022). However, the\n",
      "intrinsic auto-regressive (AR) structure of these\n",
      "models presents significant challenges in achieving\n",
      "more efficient serving (Radford et al., 2018).\n",
      "First, each new token is auto-regressively gen-\n",
      "erated conditioned on the entire set of previously-\n",
      "generated tokens. This incremental decoding pro-\n",
      "∗ Equal contribution.\n",
      "† Work done while these authors interned at Zhipu AI.\n",
      "Figure 1: APAR Decoding Overview. Different from\n",
      "the original auto-regressive decoding, APAR detects\n",
      "potential parts to be generated in parallel and issues\n",
      "multiple generation threads.\n",
      "cess results in sub-optimal generation speeds, as\n",
      "each generation step requires accessing the vast\n",
      "number of parameters of a LLM (Aminabadi et al.,\n",
      "2022). Consequently, when the generation batch\n",
      "size is not sufficiently large, this process becomes\n",
      "memory-bound, resulting in an under-utilization of\n",
      "the GPU compute.\n",
      "Second, the computation of attention over all\n",
      "preceding tokens in Transformer (Vaswani et al.,\n",
      "2017) also limits the serving throughput. In high-\n",
      "throughput scenarios, many sequences are generat-\n",
      "ing in parallel and the generation process becomes\n",
      "computation-bound. Meanwhile, the computation\n",
      "cost of attention scales linearly with the sequence\n",
      "length, which hinders further improvements of the\n",
      "throughput, especially for long responses. In ad-\n",
      "dition, the caching of key and value tensors (KV\n",
      "cache) for generated tokens, despite advancements\n",
      "in memory-efficient algorithms (Kwon et al., 2023),\n",
      "scales linearly with the sequence length, constrain-\n",
      "ing the number of concurrent requests that a system\n",
      "can handle.\n",
      "In light of these challenges, we introduce the\n",
      "Auto-Parallel Auto-Regressive (APAR) decoding\n",
      "strategy with the goal of improving the inference\n",
      "arXiv:2401.06761v1  [cs.CL]  12 Jan 2024\n",
      "efficiency of LLMs. APAR leverages the inherent\n",
      "parallelizable structure in LLM generation, capi-\n",
      "talizing on LLMs’ understanding of text structures.\n",
      "By fine-tuning LLMs on corpora with hierarchical\n",
      "structures, the models can learn to autonomously\n",
      "initiate parallel generation threads when encoun-\n",
      "tering parallelizable response structures. This ap-\n",
      "proach transforms the conventional linear genera-\n",
      "tion into a parallelizable paragraph tree structure.\n",
      "This not only facilitates greater decoding paral-\n",
      "lelism but also reduces attention spans through tree-\n",
      "based attention mechanisms, and enables the earlier\n",
      "release of consumed KV cache memory.\n",
      "We perform experiments on the Vicuna family\n",
      "of models. In memory-bound scenarios, APAR\n",
      "can help reduce the model latency and achieve an\n",
      "average generation speed increase of 2× on Vi-\n",
      "cuna Bench (Chiang et al., 2023). Furthermore, the\n",
      "design of APAR is complementary to most exist-\n",
      "ing inference acceleration methods. For example,\n",
      "when combined with Medusa (Cai et al., 2023), a\n",
      "speculative decoding strategy, APAR-based models\n",
      "yield speed improvements of up to 4× on Vicuna\n",
      "Bench. In certain specific categories, this combina-\n",
      "tion even achieves a speed-up of up to 6×.\n",
      "In high-throughput scenarios, APAR’s compat-\n",
      "ibility with vLLM enables early memory release,\n",
      "reducing KV cache requirements by up to 50%\n",
      "while still maintaining the same level of through-\n",
      "put. In addition, APAR reduces the number of\n",
      "tokens involved in attention calculation. By using\n",
      "the same amount of KV cache memory, it gets a\n",
      "20-70% improvement in throughput over the origi-\n",
      "nal AR process, and achieves a 20-35% reduction\n",
      "in latency while maintaining the same serving con-\n",
      "currency.\n",
      "Importantly, the quality of generation with\n",
      "APAR is not compromised. Evaluations across\n",
      "multiple categories on the MT Bench and Vicuna\n",
      "Bench (Zheng et al., 2023) demonstrate that the\n",
      "response quality remains largely consistent, with\n",
      "variations within a ±2% range compared to its AR\n",
      "counterparts. This indicates that APAR-based mod-\n",
      "els retain the contextual generation capability while\n",
      "enhancing the decoding speed and efficiency.\n",
      "2\n",
      "Auto-Parallel Auto-Regressive Decoding\n",
      "2.1\n",
      "Overview\n",
      "Parallelizable structures are ubiquitous in the re-\n",
      "sponse of LLMs. For instance, in the ShareGPT\n",
      "dataset, 58% of the dialogues contain at least one re-\n",
      "sponse of ordered or unordered lists from ChatGPT,\n",
      "and about 32% of the responses contains listed\n",
      "structure. Most listed structures are naturally suit-\n",
      "able for paralleled generation, since the details of\n",
      "an itemized paragraph is usually conditioned on its\n",
      "leading sentence or phrase.\n",
      "The key idea of APAR is to make LLMs ex-\n",
      "plicitly aware of such parallelizable structures,\n",
      "and spawn auto-parallel auto-regressive decoding\n",
      "threads accordingly. Specifically, APAR involves\n",
      "two key components. First, we post-train the LLMs\n",
      "with hierarchical text structures, which we referred\n",
      "to as paragraph nodes (Section 2.2). Second, we\n",
      "design the decoding algorithm to support parallel\n",
      "decoding operations, including maintaining the hi-\n",
      "erarchical structure in generation and restoring it\n",
      "into a linear sequence (Section 2.3).\n",
      "2.2\n",
      "Input Format\n",
      "This section introduce the corpora used to fine-tune\n",
      "APAR models, including the tree structure, atten-\n",
      "tion mechanism and control tokens. See Section 3.2\n",
      "for data pre-processing details.\n",
      "Paragraph tree.\n",
      "As illustrated in Fig 2, a para-\n",
      "graph tree is used to represent a sequence with\n",
      "hierarchical structure. Each node in the tree, which\n",
      "is referred to as a paragraph node in the sections\n",
      "to follow, denotes a component of the generation\n",
      "response. Each paragraph node has 0 or 2 pointers.\n",
      "One pointer of the paragraph node is referred to as\n",
      "first child (blue arrow in Fig 2), which points to\n",
      "the detailed texts (sub-paragraph) of the paragraph;\n",
      "the other child pointer is next sibling (red arrow in\n",
      "Fig 2), which points to the next paragraph of the\n",
      "same hierarchical level.\n",
      "Control tokens.\n",
      "To enable the language model to\n",
      "spawn a parallel decoding thread, 2 control tokens\n",
      "are added to the vocabulary.\n",
      "• Fork Identifier. [Fork] is used to indicate a\n",
      "parallel-able structure in the response. Models\n",
      "are trained to output a [Fork] token when they\n",
      "discover that what follows is considered detailed\n",
      "information (or a sub-paragraph) and thus can be\n",
      "decoded together with the next paragraph of the\n",
      "same level. When the inference system detects\n",
      "a [Fork] token output by the model, it creates\n",
      "a paralleled decoding thread sharing the same\n",
      "prefix until that [Fork] token. This token works\n",
      "just like the fork() system call used in operating\n",
      "systems.\n",
      "<User> What if I can’t sleep \n",
      "at night? <Assistant> Here \n",
      "are some advice …\n",
      "1. Establish a Routine: Go \n",
      "to bed and wake up at the \n",
      "same time every day, …\n",
      "2. Manage Stress: Consider \n",
      "relaxation such as \n",
      "breathing exercises and …\n",
      "If your insomnia persists, \n",
      "it’s essential for you to seek \n",
      "professional advice …\n",
      "<User> What if I can’t sleep at night? <Assistant> Here \n",
      "are some advice … 1. Establish a Routine: \n",
      "             Go to bed and wake up at \n",
      "the same time every day, …\n",
      "2. Manage Daily Stress: FORK\n",
      "             Consider relaxation such \n",
      "as breathing exercises and …\n",
      "If your insomnia persists, it’s essential \n",
      "for you to seek professional advice …\n",
      "Paragraph Tree    \n",
      "Training Attention\n",
      "p0\n",
      "p0\n",
      "p1\n",
      "p1\n",
      "p2\n",
      "p2\n",
      "p4\n",
      "p4\n",
      "p3\n",
      "p3\n",
      "First Child\n",
      "Next Sibling\n",
      "Control\n",
      "Token\n",
      "CHILD\n",
      "CHILD\n",
      "FORK\n",
      "Original Sequence\n",
      "p0\n",
      "p0\n",
      "p1\n",
      "p1\n",
      "p2\n",
      "p2\n",
      "p3\n",
      "p3\n",
      "p4\n",
      "p4\n",
      "p0\n",
      "p0\n",
      "p1\n",
      "p1\n",
      "p2\n",
      "p2\n",
      "p3\n",
      "p3\n",
      "p4\n",
      "p4\n",
      "Figure 2: APAR Training Inputs. Based on pre-defined rules, the original sequence is transformed into a paragraph\n",
      "tree, which is used to train APAR models. Any token attends only to tokens on its path to root.\n",
      "Preﬁx shared \n",
      "Normal\n",
      "Token(s)\n",
      "F\n",
      "B\n",
      "F\n",
      "C\n",
      "First Child\n",
      "Next Sibling\n",
      "0\n",
      "t1t1\n",
      "t2t2\n",
      "t2 + 1\n",
      "t2 + 1\n",
      "t1 + 1\n",
      "t1 + 1\n",
      "Position ID\n",
      "(Time Step)\n",
      "C\n",
      "Generation Thread\n",
      "(Sequence)\n",
      "Memory released on ﬁnish\n",
      "p0\n",
      "p0(root)\n",
      "p1\n",
      "p1(root)\n",
      "p1\n",
      "p1(detail)\n",
      "E\n",
      "t1 + 1\n",
      "t1 + 1\n",
      "t2 + 1\n",
      "t2 + 1\n",
      "F\n",
      "[Fork] token\n",
      "C\n",
      "[Child] token\n",
      "B\n",
      "[BOS] token\n",
      "E\n",
      "[EOS] token\n",
      "p0\n",
      "p0(detail)\n",
      "…\n",
      "p2\n",
      "p2(root)\n",
      "…\n",
      "Figure 3: APAR Decoding. Suppose we are generating a sequence p0 − p1 − p2. The decoding of p1(root) and\n",
      "p0(detail) are issued in parallel at (t1 + 1). Similarly, p2(root) and p1(detail) are issued at (t2 + 1). Prefix tokens are\n",
      "shared and forked generation threads can be released once finished.\n",
      "• Child Identifier.\n",
      "[Child] always follows a\n",
      "[Fork], and is used to indicate that the following\n",
      "content is the beginning of a sub-paragraph lead\n",
      "by the previous content, like the zero return value\n",
      "of fork() in some operating systems. In particu-\n",
      "lar, [Child] is attended to but is not taken loss\n",
      "in the training process. Thus, the model never\n",
      "learns to output this token, but learns to output the\n",
      "content of the sub-paragraph when [Child] is\n",
      "injected into the context (i.e. a [Fork] [Child]\n",
      "sequence appears in the context). On the other\n",
      "hand, when [Fork] appears without followed\n",
      "by a [Child], the model will generate the next\n",
      "paragraph of the same level.\n",
      "Training attention.\n",
      "In order for the paragraphs\n",
      "to be generated in a parallel, all nodes only attend\n",
      "to their ancestors, and to themselves with a causal\n",
      "mask, as shown in Fig 2.\n",
      "2.3\n",
      "Decoding Procedures\n",
      "An overview of the generation process is illustrated\n",
      "in Fig. 3 and the algorithm is formulated in Algo-\n",
      "rithm 1. We first introduce the concept of sequence\n",
      "and sequence groups following the implementation\n",
      "in Kwon et al. (2023), then expound the generating\n",
      "procedures of APAR decoding algorithm.\n",
      "Sequence and sequence group.\n",
      "A sequence is\n",
      "defined as an ordered list of tokens. A sequence\n",
      "group is the set of all sequences generated for the\n",
      "same prompt sequence and is initialized with only\n",
      "the prompt sequence. In APAR decoding algorithm,\n",
      "each sequence group corresponds to a paragraph\n",
      "tree.\n",
      "Each sequence, on the other hand, is a generation\n",
      "thread and is associated with a leaf node in the\n",
      "paragraph tree.\n",
      "Decoding.\n",
      "As described in Algorithm 1, we start\n",
      "the decoding with the user prompt sequence p and\n",
      "Algorithm 1 APAR decoding algorithm. ISFIN-\n",
      "ISHED(G) returns True if all sequences in the se-\n",
      "quence group G have finished. SAMPLE(Θ, s)\n",
      "samples the next token for sequence s given lan-\n",
      "guage model Θ. A paragraph node n associated\n",
      "with sequence s points to slice s[start:end] (s[start:]\n",
      "if end is null). PNODE(s, x) creates a new para-\n",
      "graph node associated with sequence s, initializing\n",
      "start=x, end=null. The current_node attribute of a\n",
      "sequence is used to record the leaf paragraph node\n",
      "pointing to the end of the sequence, and is main-\n",
      "tained (line 25∼29) every time a new sequence is\n",
      "forked. RESTORE(r) recursively traverse the para-\n",
      "graph tree defined by root r in root - first_child -\n",
      "next_sibling order.\n",
      "1: Input: A user prompt sequence p, language\n",
      "model Θ.\n",
      "2: Output: Decoded generation sequence g.\n",
      "3:\n",
      "4: r ← PNODE(p, p.len)\n",
      "5: G ← {p}\n",
      "6: p.current_node ← r\n",
      "7:\n",
      "8: while not ISFINISHED(G) do\n",
      "9:\n",
      "G ← APARDECODE(G, Θ)\n",
      "10:\n",
      "11: g ← RESTORE(r)\n",
      "12: return g\n",
      "13:\n",
      "14: function APARDECODE(G, Θ)\n",
      "15:\n",
      "for each sequence s in G do\n",
      "16:\n",
      "if s.finished = True then\n",
      "17:\n",
      "continue\n",
      "18:\n",
      "x ←SAMPLE(Θ, s)\n",
      "19:\n",
      "if s.last_token = [Fork] then\n",
      "20:\n",
      "s′ ← s\n",
      "21:\n",
      "s′.append_token([Child])\n",
      "22:\n",
      "G ← G ∪ {s′}\n",
      "23:\n",
      "n ← PNODE(s, s.len)\n",
      "24:\n",
      "n′ ← PNODE(s′, s′.len)\n",
      "25:\n",
      "s.current_node.end ← s.len\n",
      "26:\n",
      "s.current_node.next_sibling ← n\n",
      "27:\n",
      "s.current_node.first_child ← n′\n",
      "28:\n",
      "s.current_node ← n\n",
      "29:\n",
      "s′.current_node ← n′\n",
      "30:\n",
      "s.append_token(x)\n",
      "31:\n",
      "if x = [EOS] then\n",
      "32:\n",
      "s.finish ← True\n",
      "33:\n",
      "RELEASECACHE(s)\n",
      "34:\n",
      "return G\n",
      "construct a sequence group G initialized as {p}.\n",
      "We initialize the paragraph tree corresponding to\n",
      "G with root node r and associate sequence p with\n",
      "r (which is now a leaf node). Then, we iterative\n",
      "perform APARDECODE on sequence group G until\n",
      "all sequences in G have finished. In the end, the\n",
      "paragraph tree is traversed to restore the sequential\n",
      "output g.\n",
      "Next, we delve into the details for APARDE-\n",
      "CODE, which performs a single decode step for\n",
      "sequence group G with language model Θ. For\n",
      "each unfinished sequence s in G, If the last token\n",
      "is [Fork], it means that the model calls for a new\n",
      "generation thread, and now it’s time to fork the\n",
      "sequence s′. The forked sequence shares the same\n",
      "prefix tokens as the parent sequence. When imple-\n",
      "mentation with paged attention (Kwon et al., 2023),\n",
      "the fork operation creates a shared memory map-\n",
      "ping for the shared prefix (the dotted red box in\n",
      "Fig 3), which copies at most 1 KV cache block and\n",
      "shares all other blocks. After the fork operation,\n",
      "s′ is appended a forced [Child] token to identify\n",
      "this sequence to the language model as a child se-\n",
      "quence. Also, two new leaf nodes are created and\n",
      "s and s′ are set to track the latest leaf nodes.\n",
      "Finally, sampled new token x is appended to s.\n",
      "If [EOS] token is sampled, the generation of se-\n",
      "quence s is considered finished, and the KV cache\n",
      "belonging only to s is released (the dotted red box\n",
      "in Fig 3).\n",
      "2.4\n",
      "Features\n",
      "Based on the aforementioned decoding algorithm\n",
      "and the distribution of user queries, we identify\n",
      "three key features of APAR decoding that give rise\n",
      "to its superior performance in terms of inference\n",
      "latency, throughput, and memory consumption.\n",
      "1. Paralleled decoding structure reduces latency.\n",
      "Through training on paragraph trees, the language\n",
      "model becomes an automatic online miner for\n",
      "parallel-able structure and concurrent generation\n",
      "threads are issued accordingly. The paralleled gen-\n",
      "eration reduces generation steps. In memory-bound\n",
      "scenarios, the latency in each step remains roughly\n",
      "unchanged wrt. different level of decoding paral-\n",
      "lelism (i.e. dynamic batch size) and the latency can\n",
      "therefore be reduced proportionately (Fig 4).\n",
      "2. Early release of child KV cache reduces mem-\n",
      "ory consumption.\n",
      "In the auto-regressive gener-\n",
      "ation process, the KV cache of all tokens must be\n",
      "retained before the sequence is completely gener-\n",
      "ated. In APAR, however, once a forked sequence\n",
      "(i.e. a generation thread) completes generation, the\n",
      "KV cache belonging only to the forked sequence\n",
      "can be released immediately, while the remaining\n",
      "part of the generation continues. Under the effect\n",
      "of early release strategy, as shown in later Fig 5a,\n",
      "up to 50% of the generation cache can be saved\n",
      "while throughput remains the same.\n",
      "3. Reduced attention length saves computation.\n",
      "Auto-regressive generation requires a token to at-\n",
      "tend to all previously generated tokens. In APAR,\n",
      "on the other hand, a new token only attends to to-\n",
      "kens along its path to the root of the paragraph tree,\n",
      "which reduces attention computation in generation.\n",
      "In a heavily-batched generation setting, latency\n",
      "incurred by memory access is amortized by the in-\n",
      "tense batched computation, make the generation\n",
      "process primarily computation-bound. Thus, the\n",
      "computation reduction in each token results in an\n",
      "improvement in throughput across different mem-\n",
      "ory usages (Fig 5a), as well as reduction in latency\n",
      "with different extents of concurrency (Fig 5b).\n",
      "3\n",
      "Experiments\n",
      "3.1\n",
      "Data Pre-processing\n",
      "We adopt one open-sourced version of ShareGPT\n",
      "dataset1 as instruction corpora. Fine-tuning data\n",
      "are composed as follows.\n",
      "Ordered list.\n",
      "Many responses are represented as\n",
      "ordered lists with the pattern of root - detail for\n",
      "each bullet point, where root is usually an intro-\n",
      "duction phrase and detail is the detailed content\n",
      "of that specific point. Thus, the root is extracted\n",
      "as the content for the root node and detail as the\n",
      "content in the detail node, as illustrated in Fig 2.\n",
      "Paragraph.\n",
      "Most of LLM’s response paragraphs\n",
      "are structured in a root-and-details format, even\n",
      "when not presented as an ordered list, where the\n",
      "first sentence of the paragraph typically summa-\n",
      "rizes the main idea of that paragraph. Therefore,\n",
      "we extract the first sentence of the paragraph as the\n",
      "root for that section, while the rest of the content\n",
      "serves as the detail.\n",
      "Unstructured data.\n",
      "To accurately extract the hi-\n",
      "erarchical structure, responses with confusing for-\n",
      "mats, like code and math data, are excluded in the\n",
      "1https://huggingface.co/datasets/anon8231489123/ShareGPT_\n",
      "Vicuna_unfiltered\n",
      "aforementioned structure extraction process. How-\n",
      "ever, while learning to generate paralleled decoding\n",
      "threads, a model must also learn not to generate\n",
      "[Fork] in cases where coherent attention is nec-\n",
      "essary to accurately predict the next token. There-\n",
      "fore, some filtered conversations are added as nega-\n",
      "tive examples to prevent the model from excessive\n",
      "[Fork] issuing. This portion of data is organized\n",
      "as a single paragraph node with no descendants.\n",
      "See Appendix B for detailed procedures and\n",
      "rules used in data pre-processing.\n",
      "3.2\n",
      "Experimental Setup\n",
      "Models.\n",
      "To evaluate the generation speed,\n",
      "throughtput and qualities, we apply APAR fine-\n",
      "tuning on vicuna-v1.3-{7B,13B} models, produc-\n",
      "ing APAR-{7B,13B}.\n",
      "In this section, original\n",
      "Vicuna models will be referred to as Original-\n",
      "{7B,13B} (O-{7B,13B} as abbreviations) and the\n",
      "fine-tuned APAR models will be referred to as\n",
      "APAR-{7B,13B} (A-{7B,13B} as abbreviations).\n",
      "Implementation.\n",
      "We implement 3 settings for\n",
      "evaluation, including\n",
      "• Vanilla-APAR. Vanilla-APAR is implemented\n",
      "directly with transformers (Wolf et al., 2020),\n",
      "which is a widely adopted python deep learning\n",
      "platform for transformer-based models.\n",
      "• Medusa-APAR. Medusa-APAR is implemented\n",
      "with Medusa (Cai et al., 2023), which is an open-\n",
      "source speculative decoding algorithm that fol-\n",
      "lows the predict - verify paradigm for decoding.\n",
      "Medusa adopts a light-weighed extra language\n",
      "modeling head to predict the next few tokens and\n",
      "verify generation using tree attention. This set-\n",
      "ting is used to test the combined effect of APAR\n",
      "and speculative decoding algorithm.\n",
      "• Batched-APAR. Batched-APAR is implemented\n",
      "with vLLM (Kwon et al., 2023), a high-\n",
      "throughput and memory-efficient inference en-\n",
      "gine using paged-attention mechanism. This set-\n",
      "ting is used to test APAR on realistic serving\n",
      "situations, where we not only care about the la-\n",
      "tency but also throughput and memory efficiency.\n",
      "Training setup.\n",
      "During the fine-tuning process,\n",
      "we sample from structured (ordered list and para-\n",
      "graph mentioned above, 16k samples) and unstruc-\n",
      "tured data (9k samples) with sampling ratio 1:1.\n",
      "The models are fine-tuned with batch size 128,\n",
      "learning rate 2e-5 for 2000 steps. After fine-tuning,\n",
      "we train 2 medusa heads with learning rate 1e-3 for\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "Coding\n",
      "CS\n",
      "CF\n",
      "Fermi\n",
      "Generic\n",
      "Knowledge\n",
      "Math\n",
      "Roleplay\n",
      "Writing\n",
      "Mean\n",
      "7B Models\n",
      "0\n",
      "100\n",
      "200\n",
      "13B Models\n",
      "Generation Speed (token / s)\n",
      "Original\n",
      "Vanilla-APAR\n",
      "Medusa\n",
      "Medusa-APAR\n",
      "(a) Results in Vicuna Bench\n",
      "0\n",
      "100\n",
      "200\n",
      "Coding\n",
      "Extraction\n",
      "Humanities\n",
      "Math\n",
      "Reasoning\n",
      "Roleplay\n",
      "STEM\n",
      "Writing\n",
      "Mean\n",
      "7B Models\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "13B Models\n",
      "Generation Speed (token / s)\n",
      "Original\n",
      "Vanilla-APAR\n",
      "Medusa\n",
      "Medusa-APAR\n",
      "(b) Results in MT Bench\n",
      "Figure 4: Generation speed in memory-bound scenario. Models served on one H800-SXM5-80G GPU with\n",
      "batch size 1.\n",
      "2000 steps using the same data as fine-tuning. See\n",
      "Appendix A for detailed hyper-parameter settings.\n",
      "Evaluation datasets.\n",
      "Several datasets are used\n",
      "to evaluate the generation statistics and quality.\n",
      "• Vicuna Bench (Chiang et al., 2023) is a bench-\n",
      "mark for evaluating LLMs on language under-\n",
      "standing, reasoning and context awareness. It\n",
      "covers 9 categories and contains 80 single-\n",
      "turn queries. For a clearer layout, we abbre-\n",
      "viate 2 long category names in Vicuna Bench\n",
      "in the following figures and tables, i.e. Com-\n",
      "monsense to CS, Counterfactual to CF.\n",
      "• MT Bench (Zheng et al., 2023) is a bench-\n",
      "mark consisting of 80 multi-turn questions. It\n",
      "covers 8 common categories and can be used\n",
      "to evaluate the multi-turn conversation and\n",
      "instruction-following ability of LLMs.\n",
      "• APAR Test Set consists of 1000 user queries\n",
      "sampled from ShareGPT dataset to simulate\n",
      "the query distribution in realistic deployment\n",
      "scene using the same rule we extract struc-\n",
      "tured training data. Because of its large quan-\n",
      "tity, it’s would be too expensive to evaluate\n",
      "generation quality for all test set queries on\n",
      "all models. Thus, APAR test set is only used\n",
      "for measurement of generation statistics.\n",
      "3.3\n",
      "Results in Memory-Bound Scenarios\n",
      "We inspect how APAR reduces generation latency\n",
      "in a memory-bound (i.e. small batch size) sce-\n",
      "nario, as well as its combined acceleration effect\n",
      "with speculative decoding. Considering that the the\n",
      "model is re-trained and the output length can be\n",
      "different on the same prompt, we normalize genera-\n",
      "tion latency with generated tokens, adopting tokens\n",
      "per second as the metric for generation speed. The\n",
      "results are reported with batch size fixed as 1 and\n",
      "prefix sharing is not enabled.\n",
      "As shown in Fig 4, Vanilla-APAR achieves 2×\n",
      "average speed up in Vicuna Bench and 1.4× aver-\n",
      "age speed up on MT Bench. APAR models learns\n",
      "to spawn parallel generation thread in and only\n",
      "in categories that exists a parallel-able structure.\n",
      "For instance, APAR-{7B,13B} seldom try to issue\n",
      "parallel a generation threads in coding and math re-\n",
      "lated queries, which typically requires careful step\n",
      "by step reasoning or rigorous formats, resulting in\n",
      "no speed up. On the other hand, on categories like\n",
      "common-sense, generic and knowledge, the speed\n",
      "up is significant. When combined with speculative\n",
      "decoding, Medusa-APAR achieves an impressive\n",
      "4× average speed up in Vicuna Bench and 2.9× av-\n",
      "erage speed up in MT Bench, demonstrating strong\n",
      "reduction in generation latency.\n",
      "3.4\n",
      "Results in High-Throughput Scenarios\n",
      "+23%\n",
      "+33%\n",
      "(a) Throughput wrt. cache memory usage\n",
      "+28%\n",
      "-21%\n",
      "(b) End-to-end decode latency wrt. concurrency\n",
      "Figure 5: Serving statistics of Batched-APAR. Models\n",
      "served on one A100-SXM4-80G GPU. Error bars in (b)\n",
      "omitted for a clearer view.\n",
      "In high performance serving situations, increas-\n",
      "ing throughput and reducing serving memory are\n",
      "also important. We use Batched-APAR to serve the\n",
      "queries in APAR test set with different amount of\n",
      "GPU memory available. An overview of generation\n",
      "throughput and per-token latency is summarized in\n",
      "Fig 5. The dots in the plot show the mean value and\n",
      "error bars represent the 25% and 75% percentile in\n",
      "each setting.\n",
      "As shown in Fig 5a, the throughput of Batched-\n",
      "APAR models surpass the maximum throughput of\n",
      "original models with only 20% of the KV Cache\n",
      "used, demonstrating memory efficiency. When us-\n",
      "ing similar amount of memory, throughput is con-\n",
      "sistently increase by 20%∼ 70% across different\n",
      "cache usages. Batched-APAR models also demon-\n",
      "strate remarkable latency reduction in computation\n",
      "bound scenarios. As shown in Fig 5b, Batched-\n",
      "APAR reduces 20%∼35% average latency when\n",
      "Table 1: Average Max Cached Tokens\n",
      "Benchmark\n",
      "Model\n",
      "APAR\n",
      "Flatten\n",
      "Saved\n",
      "Vicuna\n",
      "A-7B\n",
      "303.2\n",
      "417.2\n",
      "27.3%\n",
      "A-13B\n",
      "306.7\n",
      "419.3\n",
      "26.8%\n",
      "MT\n",
      "A-7B\n",
      "502.1\n",
      "571.1\n",
      "12.1%\n",
      "A-13B\n",
      "513.3\n",
      "590.6\n",
      "13.1%\n",
      "Table 2: Average Attended Tokens\n",
      "Benchmark\n",
      "Model\n",
      "APAR\n",
      "Flatten\n",
      "Saved\n",
      "Vicuna\n",
      "A-7B\n",
      "166.2\n",
      "256.4\n",
      "35.2%\n",
      "A-13B\n",
      "166.6\n",
      "255.9\n",
      "34.9%\n",
      "MT\n",
      "A-7B\n",
      "417.5\n",
      "496.5\n",
      "15.9%\n",
      "A-13B\n",
      "428.0\n",
      "514.2\n",
      "16.8%\n",
      "serving the same amount of concurrent requests.\n",
      "The latency of Batched-APAR-13B is even similar\n",
      "to the Original-7B model.\n",
      "The improvement in latency and throughput can\n",
      "be best explained by feature 2 and 3 as described in\n",
      "Section 2.4. We quantitatively measure how much\n",
      "computation and cache memory can be saved by\n",
      "using APAR decoding algorithm. We adopt the\n",
      "following metrics.\n",
      "• Max cached tokens is defined as the maxi-\n",
      "mum number of KV cache slots required for\n",
      "generating a response. For auto-regressive\n",
      "generation, prompt tokens and all generated\n",
      "tokens need to be cached before generating\n",
      "[EOS] token.\n",
      "• Attended tokens is defined as the number of\n",
      "tokens attended to when predicting a specific\n",
      "token. For auto-regressive generation, all pre-\n",
      "ceding tokens is needed when predicting a\n",
      "next token.\n",
      "Since the response length is difference between\n",
      "APAR models and original models, we flatten the\n",
      "paragraph tree generated by APAR models as the\n",
      "reference output. When calculating average, we ex-\n",
      "clude categories that are not accelerated by APAR,\n",
      "i.e. Coding, Extraction and Math.\n",
      "As summarized in Table 1 and Table 2, compared\n",
      "with flattened results, APAR reduces max cached\n",
      "tokens by 12%∼27% and reduced attended tokens\n",
      "by 15%∼35%. Detailed results for all categories\n",
      "are reported in Appendix D.3 and Appendix D.2.\n",
      "Table 3: Generation Quality in MT Bench\n",
      "Task\n",
      "O-7B\n",
      "A-7B\n",
      "O-13B\n",
      "A-13B\n",
      "Coding\n",
      "2.60\n",
      "2.60\n",
      "3.70\n",
      "3.70\n",
      "Extraction\n",
      "4.90\n",
      "5.75\n",
      "5.35\n",
      "4.85\n",
      "Humanities\n",
      "9.45\n",
      "9.40\n",
      "9.45\n",
      "9.55\n",
      "Math\n",
      "2.70\n",
      "2.00\n",
      "2.60\n",
      "2.65\n",
      "Writing\n",
      "7.90\n",
      "7.15\n",
      "8.40\n",
      "7.88\n",
      "Roleplay\n",
      "6.35\n",
      "7.25\n",
      "7.67\n",
      "7.58\n",
      "Reasoning\n",
      "4.85\n",
      "5.40\n",
      "5.95\n",
      "5.85\n",
      "Stem\n",
      "7.92\n",
      "7.15\n",
      "7.67\n",
      "7.80\n",
      "Mean\n",
      "5.83\n",
      "5.92\n",
      "6.35\n",
      "6.24\n",
      "Table 4: Generation Quality in Vicuna Bench\n",
      "Task\n",
      "O-7B\n",
      "A-7B\n",
      "O-13B\n",
      "A-13B\n",
      "Coding\n",
      "3.86\n",
      "3.29\n",
      "6.14\n",
      "3.71\n",
      "CS\n",
      "9.40\n",
      "9.50\n",
      "9.50\n",
      "9.60\n",
      "CF\n",
      "8.05\n",
      "8.00\n",
      "8.65\n",
      "9.00\n",
      "Fermi\n",
      "6.80\n",
      "6.90\n",
      "7.35\n",
      "6.60\n",
      "Generic\n",
      "9.45\n",
      "9.50\n",
      "9.30\n",
      "9.50\n",
      "Knowledge\n",
      "9.40\n",
      "9.30\n",
      "9.60\n",
      "9.35\n",
      "Math\n",
      "1.67\n",
      "2.00\n",
      "1.67\n",
      "4.67\n",
      "Roleplay\n",
      "8.80\n",
      "8.80\n",
      "8.90\n",
      "8.75\n",
      "Writing\n",
      "9.50\n",
      "9.00\n",
      "9.50\n",
      "9.00\n",
      "Mean\n",
      "8.08\n",
      "7.99\n",
      "8.45\n",
      "8.29\n",
      "3.5\n",
      "Generation Quality\n",
      "To measure the generation quality of APAR mod-\n",
      "els compared with original models, we adopt MT\n",
      "Bench and Vicuna Bench as evaluation framework.\n",
      "For each response, we provide GPT-4 with con-\n",
      "versation history, user query and model responses,\n",
      "asking GPT-4 to grade the response with a score\n",
      "ranging from 1 to 10 and we follow the prompt\n",
      "template used by Zheng et al. (2023).\n",
      "The quality scores of each category are summa-\n",
      "rized in Table 3 and Table 4. Compared with orig-\n",
      "inal models, APAR models differs by -2%∼+2%\n",
      "in MT Bench and Vicuna Bench overall scores,\n",
      "showing negligible overall quality change.\n",
      "4\n",
      "Related Works\n",
      "This section discuss the difference and connection\n",
      "of APAR with prior works concerning inference\n",
      "acceleration.\n",
      "Optimized computation.\n",
      "Optimizations on op-\n",
      "erators (Dao et al., 2022) and computational\n",
      "graphs (Aminabadi et al., 2022) are active research\n",
      "fields. Model compression is widely used in de-\n",
      "ployment, like quantization (Dettmers et al., 2022;\n",
      "Frantar et al., 2022) and pruning (Frantar and Al-\n",
      "istarh, 2023; Ma et al., 2023). Another line of\n",
      "works modifies the model architecture, including\n",
      "efficient attention (Kitaev et al., 2020) for computa-\n",
      "tion complexity and multi-query attention (Shazeer,\n",
      "2019) for optimized IO. Different from prior works,\n",
      "APAR makes no modification to operators or model\n",
      "architecture but reduces computation by adopting\n",
      "attention tree structure. APRA is thus orthogonal\n",
      "to and can be applied jointly with the aforemen-\n",
      "tioned works.\n",
      "Improved parallelism.\n",
      "Scheduling strategies,\n",
      "including dynamic batching (Yu et al., 2022)\n",
      "and paged-attention (Kwon et al., 2023), im-\n",
      "prove maximum generation throughput. Another\n",
      "stream of works explores speculative decoding\n",
      "(SD) (Leviathan et al., 2023; Yang et al., 2023;\n",
      "Cai et al., 2023), which verifies multiple specu-\n",
      "lated tokens in parallel, reducing generation latency\n",
      "in small batch sizes. Non-auto-regressive genera-\n",
      "tion (Gu et al., 2018) propose to sample multiple\n",
      "generation tokens in parallel, which typically re-\n",
      "quires re-training and applies to restricted scenarios.\n",
      "APAR can be conveniently combined with efficient\n",
      "scheduling and SD methods to achieve augmented\n",
      "efficiency as demonstrated by Medusa-APAR and\n",
      "Batched-APAR. Different from previous methods,\n",
      "APAR propose to exploit the intrinsic organization\n",
      "ability of LLMs to automatically issue paralleled\n",
      "generation threads, and is applicable to multiple\n",
      "scenarios. Notably, SoT (Ning et al., 2023) pro-\n",
      "poses to enable parallelism by prompting, which\n",
      "generates the skeleton of the response and then\n",
      "expands each point in parallel.\n",
      "Different from\n",
      "SoT, which entails an external classifier and re-\n",
      "computation of KV cache between stages, APAR\n",
      "requires negligible extra computation (2 control to-\n",
      "kens for a thread) and no re-computation, and thus\n",
      "does not compromise generation throughput.\n",
      "5\n",
      "Conclusion\n",
      "This paper introduces APAR, a new decoding\n",
      "method that allows LLMs to autonomously struc-\n",
      "ture the decoding process and dynamically create\n",
      "parallel decoding threads, without compromising\n",
      "the generation quality. APAR not only enhances\n",
      "parallelism in generation, but also reduces the com-\n",
      "putation and KV cache memory consumption. Ex-\n",
      "periments show that APAR can be seamlessly in-\n",
      "tegrated with existing inference frameworks, sig-\n",
      "nificantly lowering the generation latency across\n",
      "various scenarios while improving serving through-\n",
      "put in situations involving extreme batch sizes and\n",
      "concurrency levels.\n",
      "References\n",
      "Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia\n",
      "Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton\n",
      "Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase,\n",
      "and Yuxiong He. 2022. Deepspeed inference: En-\n",
      "abling efficient inference of transformer models at\n",
      "unprecedented scale.\n",
      "Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,\n",
      "and Tri Dao. 2023. Medusa: Simple framework for\n",
      "accelerating llm generation with multiple decoding\n",
      "heads.\n",
      "https://github.com/FasterDecoding/\n",
      "Medusa.\n",
      "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\n",
      "Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\n",
      "Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\n",
      "Stoica, and Eric P. Xing. 2023. Vicuna: An open-\n",
      "source chatbot impressing gpt-4 with 90%* chatgpt\n",
      "quality.\n",
      "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\n",
      "and Christopher Ré. 2022. Flashattention: Fast and\n",
      "memory-efficient exact attention with io-awareness.\n",
      "Tim Dettmers, Mike Lewis, Younes Belkada, and Luke\n",
      "Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\n",
      "cation for transformers at scale.\n",
      "Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Mas-\n",
      "sive language models can be accurately pruned in\n",
      "one-shot.\n",
      "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and\n",
      "Dan Alistarh. 2022. GPTQ: Accurate post-training\n",
      "compression for generative pretrained transformers.\n",
      "arXiv preprint arXiv:2210.17323.\n",
      "Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\n",
      "Li, and Richard Socher. 2018. Non-autoregressive\n",
      "neural machine translation. In International Confer-\n",
      "ence on Learning Representations.\n",
      "Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n",
      "2020. Reformer: The efficient transformer.\n",
      "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\n",
      "Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\n",
      "Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\n",
      "cient memory management for large language model\n",
      "serving with pagedattention. In Proceedings of the\n",
      "ACM SIGOPS 29th Symposium on Operating Systems\n",
      "Principles.\n",
      "Yaniv Leviathan, Matan Kalman, and Yossi Matias.\n",
      "2023. Fast inference from transformers via spec-\n",
      "ulative decoding. In Proceedings of the 40th Interna-\n",
      "tional Conference on Machine Learning, ICML’23.\n",
      "JMLR.org.\n",
      "Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.\n",
      "Llm-pruner: On the structural pruning of large lan-\n",
      "guage models. In Advances in Neural Information\n",
      "Processing Systems.\n",
      "Yohei\n",
      "Nakajima.\n",
      "2023.\n",
      "Babyagi.\n",
      "Python.\n",
      "https://github.com/yoheinakajima/babyagi.\n",
      "Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang,\n",
      "Huazhong Yang, and Yu Wang. 2023. Skeleton-of-\n",
      "thought: Large language models can do parallel de-\n",
      "coding.\n",
      "OpenAI. 2023. Gpt-4 technical report.\n",
      "Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai,\n",
      "Meredith Ringel Morris, Percy Liang, and Michael S.\n",
      "Bernstein. 2023. Generative agents: Interactive simu-\n",
      "lacra of human behavior. In In the 36th Annual ACM\n",
      "Symposium on User Interface Software and Technol-\n",
      "ogy (UIST ’23), UIST ’23, New York, NY, USA.\n",
      "Association for Computing Machinery.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\n",
      "Sutskever, et al. 2018. Improving language under-\n",
      "standing by generative pre-training.\n",
      "Toran Bruce Richards. 2023. Auto-gpt: An autonomous\n",
      "gpt-4 experiment.\n",
      "Noam Shazeer. 2019. Fast transformer decoding: One\n",
      "write-head is all you need.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, et al. 2023. Llama: Open and effi-\n",
      "cient foundation language models. ArXiv preprint,\n",
      "abs/2302.13971.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in Neural Information Pro-\n",
      "cessing Systems, volume 30. Curran Associates, Inc.\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\n",
      "Chaumond, Clement Delangue, Anthony Moi, Pier-\n",
      "ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\n",
      "Joe Davison, Sam Shleifer, Patrick von Platen, Clara\n",
      "Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\n",
      "Scao, Sylvain Gugger, Mariama Drame, Quentin\n",
      "Lhoest, and Alexander M. Rush. 2020. Transform-\n",
      "ers: State-of-the-art natural language processing. In\n",
      "Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing: System\n",
      "Demonstrations, pages 38–45, Online. Association\n",
      "for Computational Linguistics.\n",
      "Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\n",
      "Jiang, Linjun Yang, Rangan Majumder, and Furu Wei.\n",
      "2023. Inference with reference: Lossless accelera-\n",
      "tion of large language models.\n",
      "Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\n",
      "jeong Kim, and Byung-Gon Chun. 2022. Orca: A\n",
      "distributed serving system for Transformer-Based\n",
      "generative models.\n",
      "In 16th USENIX Symposium\n",
      "on Operating Systems Design and Implementation\n",
      "(OSDI 22), pages 521–538, Carlsbad, CA. USENIX\n",
      "Association.\n",
      "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\n",
      "Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\n",
      "Wendi Zheng, Xiao Xia, et al. 2022.\n",
      "Glm-130b:\n",
      "An open bilingual pre-trained model. arXiv preprint\n",
      "arXiv:2210.02414.\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\n",
      "Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n",
      "Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\n",
      "Joseph E. Gonzalez, and Ion Stoica. 2023. Judging\n",
      "llm-as-a-judge with mt-bench and chatbot arena.\n",
      "Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\n",
      "Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\n",
      "Bisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:\n",
      "A realistic web environment for building autonomous\n",
      "agents. arXiv preprint arXiv:2307.13854.\n",
      "A\n",
      "Training Hyper-parameters\n",
      "A.1\n",
      "APAR Models\n",
      "Table 5: Training Hyper-parameters for APAR Models\n",
      "Hyper-parameter\n",
      "Value\n",
      "batch size\n",
      "128\n",
      "data type\n",
      "bf16\n",
      "training step\n",
      "2000\n",
      "learning rate\n",
      "2e-5\n",
      "weight decay\n",
      "0\n",
      "warm-up ratio\n",
      "0.03\n",
      "lr decay schedule\n",
      "cosine\n",
      "context length\n",
      "2048\n",
      "A.2\n",
      "Medusa Heads\n",
      "The following hyper-parameters are used to train\n",
      "the medusa heads for speculative decoding. Only\n",
      "medusa heads are trained and the reset of the lan-\n",
      "guage model remains frozen.\n",
      "Table 6: Training Hyper-parameters for Medusa Heads\n",
      "Hyper-parameter\n",
      "Value\n",
      "batch size\n",
      "128\n",
      "data type\n",
      "bf16\n",
      "training step\n",
      "2000\n",
      "learning rate\n",
      "1e-3\n",
      "weight decay\n",
      "0\n",
      "warm-up ratio\n",
      "0.1\n",
      "lr decay schedule\n",
      "cosine\n",
      "context length\n",
      "2048\n",
      "# of Medusa heads\n",
      "2\n",
      "# of layers per Medusa head\n",
      "2\n",
      "B\n",
      "Rules for Extracting Structured Data\n",
      "The process and rules to determine and extract the\n",
      "structure of each assistant response are outlined as\n",
      "follows.\n",
      "1. Try to extract the response as ordered list.\n",
      "(a) Use regular expression like\n",
      "(\\d+\\.)\\s+(.+?):(.+?)\n",
      "to extract individual numeric points.\n",
      "(b) If\n",
      "i. the regular expression does not does\n",
      "not match at least 3 numeric points,\n",
      "or\n",
      "ii. any of the content of the numeric\n",
      "points are less then 10 characters\n",
      "the response is not consider as a valid\n",
      "ordered list.\n",
      "2. If the response is not consider as a valid or-\n",
      "dered list, try to extract the response as multi-\n",
      "ple paragraph.\n",
      "(a) Use two consecutive \\n to divide the en-\n",
      "tire response into paragraphs and extract\n",
      "the first sentence of each paragraph. Para-\n",
      "graphs with only one sentence is skipped.\n",
      "3. If ambiguous patterns, including code blocks,\n",
      "math expressions, URLs, etc, exists in the\n",
      "response, the, or if the response fails to match\n",
      "the above 2 criteria, the response is considered\n",
      "unstructured.\n",
      "Pre-processing code will be made public in the\n",
      "project repository.\n",
      "C\n",
      "Generation Speed Evaluation Setup\n",
      "C.1\n",
      "Vanilla-APAR and Medusa-APAR\n",
      "Vanilla-APAR and Medusa-APAR are implemen-\n",
      "tation directly from transformers package. To\n",
      "keep KV cache contiguous, prefix sharing is not\n",
      "enabled and the prefix KV caches are copied when\n",
      "a new generation thread is forked. The implemen-\n",
      "tation of Medusa-APAR is adopt from its official\n",
      "repository. When different number of tokens are ac-\n",
      "cepted across batches, the longest accepted length\n",
      "is adopted and the mis-predicted slots are masked\n",
      "out in attention calculation. The evaluation batch-\n",
      "size is fixed as 1 and the pre-filling time is not\n",
      "measured when calculating generation speed.\n",
      "C.2\n",
      "Batched-APAR\n",
      "To measure the maximum throughput of each gen-\n",
      "eration method, GPU cache utilization is set to\n",
      "0.1, · · · , 0.9 for each setting and the system is pro-\n",
      "filed every 3 seconds. For a stable performance, we\n",
      "ignore the terminal samples when no requests are\n",
      "pending and ignore the first 1\n",
      "3 samples as warm-up\n",
      "when calculating mean and percentiles.\n",
      "All 1k requests are push into waiting queue in\n",
      "the beginning of the performance test. We limit\n",
      "the max number of concurrent requests to 350 for\n",
      "7B models and 180 for 13B models. The reason\n",
      "for this limit is that in the beginning, the average\n",
      "sequence length is relatively sort and the system\n",
      "is prone to excessively accept requests, leading to\n",
      "Table 7: Parallel Generation Statistics in Vicuna Bench\n",
      "Task\n",
      "APAR-7B\n",
      "APAR-13B\n",
      "#T\n",
      "%P\n",
      "#T\n",
      "%P\n",
      "Coding\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "CS\n",
      "6.0\n",
      "1.0\n",
      "6.5\n",
      "1.0\n",
      "CF\n",
      "4.1\n",
      "0.9\n",
      "5.4\n",
      "1.0\n",
      "Fermi\n",
      "5.3\n",
      "0.9\n",
      "5.5\n",
      "0.9\n",
      "Generic\n",
      "7.7\n",
      "1.0\n",
      "7.7\n",
      "1.0\n",
      "Knowledge\n",
      "6.8\n",
      "1.0\n",
      "5.6\n",
      "1.0\n",
      "Math\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "Roleplay\n",
      "4.5\n",
      "0.8\n",
      "5.1\n",
      "1.0\n",
      "Writing\n",
      "5.4\n",
      "0.9\n",
      "4.6\n",
      "0.7\n",
      "Mean\n",
      "5.1\n",
      "0.81\n",
      "5.2\n",
      "0.82\n",
      "Table 8: Parallel Generation Statistics in MT Bench\n",
      "Task\n",
      "APAR-7B\n",
      "APAR-13B\n",
      "#T\n",
      "%P\n",
      "#T\n",
      "%P\n",
      "Coding\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "Extraction\n",
      "1.0\n",
      "0.0\n",
      "1.3\n",
      "0.1\n",
      "Humanities\n",
      "5.0\n",
      "0.7\n",
      "6.1\n",
      "0.8\n",
      "Math\n",
      "1.1\n",
      "0.1\n",
      "1.0\n",
      "0.0\n",
      "Reasoning\n",
      "2.0\n",
      "0.3\n",
      "2.5\n",
      "0.6\n",
      "Roleplay\n",
      "4.1\n",
      "0.8\n",
      "4.0\n",
      "0.7\n",
      "STEM\n",
      "3.4\n",
      "0.6\n",
      "3.6\n",
      "0.6\n",
      "Writing\n",
      "3.7\n",
      "0.6\n",
      "3.4\n",
      "0.6\n",
      "Mean\n",
      "2.7\n",
      "0.37\n",
      "2.9\n",
      "0.41\n",
      "frequent swapping and recompute of KV cache.\n",
      "Note that this concurrency limit mainly takes effect\n",
      "in warm-up stage, which is ignore in the calculation\n",
      "for mean and percentage. The concurrency limit\n",
      "is much larger than the maximum in the average\n",
      "concurrent request as shown in Fig 5b.\n",
      "D\n",
      "Generation Speed Details\n",
      "D.1\n",
      "Parallel Generation Statistics\n",
      "We measure how many parallel threads are issued\n",
      "in generation across benchmarks and categories.\n",
      "Results are reported in Table 7 and Table 8. #T\n",
      "stands for average number of generation threads,\n",
      "%P stand for ratio of parallel-able response, i.e.\n",
      "response that has at least 2 generation threads.\n",
      "D.2\n",
      "Max Cached Tokens\n",
      "Detailed results of max cached tokens across bench-\n",
      "marks and categories are reported in Table 9, Ta-\n",
      "ble 10, 11 and Table 12, . Mean value of all cat-\n",
      "Table 9: Max Cached Tokens (Vanilla-APAR-7B on\n",
      "Vicuna Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "378.0\n",
      "378.0\n",
      "0.0%\n",
      "CS\n",
      "296.6\n",
      "417.7\n",
      "29.0%\n",
      "CF\n",
      "245.6\n",
      "312.4\n",
      "21.4%\n",
      "Fermi\n",
      "365.3\n",
      "496.8\n",
      "26.5%\n",
      "Generic\n",
      "251.5\n",
      "388.8\n",
      "35.3%\n",
      "Knowledge\n",
      "345.0\n",
      "489.9\n",
      "29.6%\n",
      "Math\n",
      "210.7\n",
      "210.7\n",
      "0.0%\n",
      "Roleplay\n",
      "292.5\n",
      "364.1\n",
      "19.7%\n",
      "Writing\n",
      "325.6\n",
      "450.4\n",
      "27.7%\n",
      "Mean\n",
      "306.2\n",
      "406.0\n",
      "24.6%\n",
      "Mean(>1%)\n",
      "303.2\n",
      "417.2\n",
      "27.3%\n",
      "Table 10: Max Cached Tokens (Vanilla-APAR-13B on\n",
      "Vicuna Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "370.0\n",
      "370.0\n",
      "0.0%\n",
      "CS\n",
      "287.6\n",
      "416.9\n",
      "31.0%\n",
      "CF\n",
      "276.9\n",
      "383.0\n",
      "27.7%\n",
      "Fermi\n",
      "401.0\n",
      "529.7\n",
      "24.3%\n",
      "Generic\n",
      "260.7\n",
      "389.2\n",
      "33.0%\n",
      "Knowledge\n",
      "313.2\n",
      "419.7\n",
      "25.4%\n",
      "Math\n",
      "203.3\n",
      "203.3\n",
      "0.0%\n",
      "Roleplay\n",
      "254.3\n",
      "346.5\n",
      "26.6%\n",
      "Writing\n",
      "353.5\n",
      "449.8\n",
      "21.4%\n",
      "Mean\n",
      "308.4\n",
      "406.9\n",
      "24.2%\n",
      "Mean(>1%)\n",
      "306.7\n",
      "419.3\n",
      "26.8%\n",
      "egories and categories accelerated by APAR are\n",
      "reported.\n",
      "D.3\n",
      "Attended Tokens\n",
      "Detailed results of attend tokens across benchmarks\n",
      "and categories are reported in Table 13, Table 14,\n",
      "Table 15 and Table 16. Mean value of all categories\n",
      "and categories accelerated by APAR are reported.\n",
      "D.4\n",
      "Response Length\n",
      "Apart from generation quality, we also analyze the\n",
      "response length distribution of model before and\n",
      "after fine-tuning in Fig 6. The average length varies\n",
      "from -0.3%∼+4.0% compared with respective orig-\n",
      "inal model and the distributions highly overlap.\n",
      "This indicate that if fine-tuned with the same mate-\n",
      "rial, APAR does not significantly affect the genera-\n",
      "tion length distribution.\n",
      "Table 11: Max Cached Tokens (Vanilla-APAR-7B on\n",
      "MT Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "744.8\n",
      "744.8\n",
      "0.0%\n",
      "Extraction\n",
      "567.2\n",
      "567.2\n",
      "0.0%\n",
      "Humanities\n",
      "647.0\n",
      "757.3\n",
      "14.6%\n",
      "Math\n",
      "408.5\n",
      "410.4\n",
      "0.4%\n",
      "Reasoning\n",
      "314.9\n",
      "335.4\n",
      "6.1%\n",
      "Roleplay\n",
      "461.2\n",
      "533.6\n",
      "13.6%\n",
      "STEM\n",
      "574.2\n",
      "641.7\n",
      "10.5%\n",
      "Writing\n",
      "513.0\n",
      "587.6\n",
      "12.7%\n",
      "Mean\n",
      "529.6\n",
      "573.3\n",
      "7.6%\n",
      "Mean(>1%)\n",
      "502.1\n",
      "571.1\n",
      "12.1%\n",
      "Table 12: Max Cached Tokens (Vanilla-APAR-13B on\n",
      "MT Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "589.6\n",
      "589.6\n",
      "0.0%\n",
      "Extraction\n",
      "640.3\n",
      "641.7\n",
      "0.2%\n",
      "Humanities\n",
      "676.1\n",
      "822.5\n",
      "17.8%\n",
      "Math\n",
      "451.9\n",
      "451.9\n",
      "0.0%\n",
      "Reasoning\n",
      "318.2\n",
      "344.8\n",
      "7.7%\n",
      "Roleplay\n",
      "484.6\n",
      "548.5\n",
      "11.7%\n",
      "STEM\n",
      "591.1\n",
      "677.5\n",
      "12.8%\n",
      "Writing\n",
      "496.4\n",
      "559.6\n",
      "11.3%\n",
      "Mean\n",
      "530.3\n",
      "579.1\n",
      "8.4%\n",
      "Mean(>1%)\n",
      "513.3\n",
      "590.6\n",
      "13.1%\n",
      "Table 13: Attended Tokens (Vanilla-APAR-7B on Vi-\n",
      "cuna Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "221.6\n",
      "221.6\n",
      "0.0%\n",
      "CS\n",
      "152.0\n",
      "244.6\n",
      "37.9%\n",
      "CF\n",
      "142.0\n",
      "185.3\n",
      "23.4%\n",
      "Fermi\n",
      "214.6\n",
      "309.6\n",
      "30.7%\n",
      "Generic\n",
      "126.3\n",
      "222.2\n",
      "43.2%\n",
      "Knowledge\n",
      "154.7\n",
      "290.3\n",
      "46.7%\n",
      "Math\n",
      "139.7\n",
      "139.7\n",
      "0.0%\n",
      "Roleplay\n",
      "166.8\n",
      "229.0\n",
      "27.2%\n",
      "Writing\n",
      "189.1\n",
      "270.0\n",
      "30.0%\n",
      "Mean\n",
      "170.3\n",
      "251.7\n",
      "32.4%\n",
      "Mean(>1%)\n",
      "166.2\n",
      "256.4\n",
      "35.2%\n",
      "Table 14: Attended Tokens (Vanilla-APAR-13B on Vi-\n",
      "cuna Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "222.0\n",
      "222.0\n",
      "0.0%\n",
      "CS\n",
      "151.3\n",
      "248.7\n",
      "39.2%\n",
      "CF\n",
      "144.6\n",
      "224.2\n",
      "35.5%\n",
      "Fermi\n",
      "204.9\n",
      "325.7\n",
      "37.1%\n",
      "Generic\n",
      "125.3\n",
      "221.8\n",
      "43.5%\n",
      "Knowledge\n",
      "158.6\n",
      "248.8\n",
      "36.3%\n",
      "Math\n",
      "137.3\n",
      "137.3\n",
      "0.0%\n",
      "Roleplay\n",
      "150.3\n",
      "217.1\n",
      "30.8%\n",
      "Writing\n",
      "209.0\n",
      "271.8\n",
      "23.1%\n",
      "Mean\n",
      "170.5\n",
      "251.4\n",
      "32.2%\n",
      "Mean(>1%)\n",
      "166.6\n",
      "255.9\n",
      "34.9%\n",
      "Table 15: Attended Tokens (Vanilla-APAR-7B on MT\n",
      "Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "577.4\n",
      "577.4\n",
      "0.0%\n",
      "Extraction\n",
      "570.8\n",
      "570.8\n",
      "0.0%\n",
      "Humanities\n",
      "456.5\n",
      "556.0\n",
      "17.9%\n",
      "Math\n",
      "616.0\n",
      "616.9\n",
      "0.1%\n",
      "Reasoning\n",
      "286.7\n",
      "322.0\n",
      "11.0%\n",
      "Roleplay\n",
      "404.3\n",
      "477.7\n",
      "15.4%\n",
      "STEM\n",
      "413.0\n",
      "473.8\n",
      "12.8%\n",
      "Writing\n",
      "437.9\n",
      "533.1\n",
      "17.8%\n",
      "Mean\n",
      "477.0\n",
      "528.3\n",
      "9.7%\n",
      "Mean(>1%)\n",
      "417.5\n",
      "496.5\n",
      "15.9%\n",
      "Table 16: Attended Tokens (Vanilla-APAR-13B on MT\n",
      "Bench)\n",
      "Task\n",
      "APAR\n",
      "AR\n",
      "Saved\n",
      "Coding\n",
      "449.9\n",
      "449.9\n",
      "0.0%\n",
      "Extraction\n",
      "767.8\n",
      "769.2\n",
      "0.2%\n",
      "Humanities\n",
      "507.0\n",
      "636.0\n",
      "20.3%\n",
      "Math\n",
      "601.3\n",
      "601.3\n",
      "0.0%\n",
      "Reasoning\n",
      "314.3\n",
      "350.1\n",
      "10.2%\n",
      "Roleplay\n",
      "404.8\n",
      "464.9\n",
      "12.9%\n",
      "STEM\n",
      "445.3\n",
      "521.1\n",
      "14.6%\n",
      "Writing\n",
      "360.8\n",
      "442.7\n",
      "18.5%\n",
      "Mean\n",
      "482.3\n",
      "539.6\n",
      "10.6%\n",
      "Mean(>1%)\n",
      "428.0\n",
      "514.2\n",
      "16.8%\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "Response Length\n",
      "0.000\n",
      "0.001\n",
      "0.002\n",
      "0.003\n",
      "0.004\n",
      "0.005\n",
      "Probability Density\n",
      "Vicuna Bench (7B Models)\n",
      "A-7B \n",
      "(mean: 344.5)\n",
      "O-7B \n",
      "(mean: 345.5)\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "Response Length\n",
      "0.000\n",
      "0.001\n",
      "0.002\n",
      "0.003\n",
      "0.004\n",
      "0.005\n",
      "0.006\n",
      "Probability Density\n",
      "Vicuna Bench (13B Models)\n",
      "A-13B \n",
      "(mean: 345.4)\n",
      "O-13B \n",
      "(mean: 334.5)\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "Response Length\n",
      "0.0000\n",
      "0.0005\n",
      "0.0010\n",
      "0.0015\n",
      "0.0020\n",
      "Probability Density\n",
      "MT Bench (7B Models)\n",
      "A-7B \n",
      "(mean: 295.1)\n",
      "O-7B \n",
      "(mean: 283.6)\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "Response Length\n",
      "0.0000\n",
      "0.0005\n",
      "0.0010\n",
      "0.0015\n",
      "0.0020\n",
      "0.0025\n",
      "Probability Density\n",
      "MT Bench (13B Models)\n",
      "A-13B \n",
      "(mean: 301.2)\n",
      "O-13B \n",
      "(mean: 291.9)\n",
      "Figure 6: Response Length on Vicuna Bench and MT\n",
      "Bench\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for article in data:\n",
    "    print(f\"Id: {article}\")\n",
    "    print(f\"Published date: {data[article]['published_date']}\")\n",
    "    print(f\"Pdf link: {data[article]['pdf_link']}\\n\")\n",
    "    print(f\"Title: {data[article]['title']}\\n\")\n",
    "    print(f\"Summary: {data[article]['summary']}\\n\")\n",
    "    print(f\"Content: {data[article]['pdf_text']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a54185-c18b-43c9-8021-92528e3d6f3d",
   "metadata": {},
   "source": [
    "# Modifications 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc472da4-b8cb-4751-af28-b430c1cde1b1",
   "metadata": {},
   "source": [
    "## ArxivParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71f27770-b08d-4a64-8639-290c1f2efe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import fitz # this is pymupdf\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class ArxivParser:\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    def __init__(self):\n",
    "        EntryData = Dict[str, str]\n",
    "        self.extracted_data: Dict[str, EntryData] = {} # store all the data present in Computer Science\n",
    "        \n",
    "    def get_results(self, query: str = \"llm\", max_results: int = 10, days: int = 60) -> pd.DataFrame:\n",
    "        # Construct the url with the query parameters\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": max_results,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        url = self.base_url + \"?\" + requests.compat.urlencode(params)\n",
    "        # Send a GET request to the api endpoint\n",
    "        response = requests.get(url)\n",
    "        # Parse the response\n",
    "        entries = feedparser.parse(response.text).entries\n",
    "        # Loop through the entries\n",
    "        for entry in entries:\n",
    "            published_date = datetime.strptime(entry.published, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            current_date = datetime.now()\n",
    "            date_diff = (current_date - published_date).days\n",
    "            # Check if the date difference is less than or equal to the days parameter\n",
    "            if date_diff <= days:\n",
    "                id = entry.id\n",
    "                title = entry.title\n",
    "                link = entry.link\n",
    "                summary = entry.summary\n",
    "                # Get the pdf link by replacing the \"abs\" with \"pdf\" in the link\n",
    "                pdf_link = link.replace(\"abs\", \"pdf\")\n",
    "                # Get the pdf content by sending a GET request to the pdf link and opening it with fitz\n",
    "                pdf_content = requests.get(pdf_link).content\n",
    "                pdf_file = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "                # Extract the text from the pdf file\n",
    "                pdf_text = \"\"\n",
    "                for page in pdf_file:\n",
    "                    pdf_text += page.get_text()\n",
    "                # Store the extracted data in the dictionary with the id as the key\n",
    "                self.extracted_data[id] = {\n",
    "                    \"title\": title,\n",
    "                    \"published_date\": published_date,\n",
    "                    \"pdf_link\": pdf_link,\n",
    "                    \"summary\": summary,\n",
    "                    \"pdf_text\": pdf_text\n",
    "                }\n",
    "        # Convert the extracted data into a pandas dataframe\n",
    "        df = pd.DataFrame.from_dict(self.extracted_data, orient=\"index\")\n",
    "        # Return the dataframe\n",
    "        return df\n",
    "        \n",
    "    def store_data(self, query: str = \"llm\", max_results: int = 10, days: int = 60) -> None:\n",
    "        # Call the get_results method and store the dataframe in the self.extracted_data attribute\n",
    "        self.extracted_data = self.get_results(query, max_results, days)\n",
    "        \n",
    "        # Create two new columns using lambda functions\n",
    "        self.extracted_data['summary_length'] = self.extracted_data.apply(lambda row: len(row['summary']), axis=1)\n",
    "        self.extracted_data['pdf_text_length'] = self.extracted_data.apply(lambda row: len(row['pdf_text']), axis=1)\n",
    "\n",
    "\n",
    "    def get_stored_data(self) -> pd.DataFrame:\n",
    "        # Return the self.extracted_data attribute\n",
    "        return self.extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9805009a-c9c4-4b2e-8b39-e9a101aa4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv=ArxivParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f5b62a5-4800-4966-9127-7e3d4829a9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06761v1</th>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>2024-01-12 18:50:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06761v1</td>\n",
       "      <td>The massive adoption of large language models ...</td>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06692v1</th>\n",
       "      <td>An Experimental Design Framework for Label-Eff...</td>\n",
       "      <td>2024-01-12 16:56:54</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06692v1</td>\n",
       "      <td>Supervised finetuning (SFT) on instruction dat...</td>\n",
       "      <td>An Experimental Design Framework for Label-Eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06688v1</th>\n",
       "      <td>Don't Rank, Combine! Combining Machine Transla...</td>\n",
       "      <td>2024-01-12 16:52:41</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06688v1</td>\n",
       "      <td>Neural machine translation systems estimate pr...</td>\n",
       "      <td>Don’t Rank, Combine! Combining Machine Transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06676v1</th>\n",
       "      <td>LLMRS: Unlocking Potentials of LLM-Based Recom...</td>\n",
       "      <td>2024-01-12 16:33:17</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06676v1</td>\n",
       "      <td>Recommendation systems are ubiquitous, from Sp...</td>\n",
       "      <td>\\nWorkshop on Information Technology and Syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06373v1</th>\n",
       "      <td>How Johnny Can Persuade LLMs to Jailbreak Them...</td>\n",
       "      <td>2024-01-12 16:13:24</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06373v1</td>\n",
       "      <td>Most traditional AI safety research has approa...</td>\n",
       "      <td>How Johnny Can Persuade LLMs to Jailbreak Them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06643v1</th>\n",
       "      <td>Effects of diversity incentives on sample dive...</td>\n",
       "      <td>2024-01-12 15:46:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06643v1</td>\n",
       "      <td>The latest generative large language models (L...</td>\n",
       "      <td>Effects of diversity incentives on sample dive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06628v1</th>\n",
       "      <td>OOP: Object-Oriented Programming Evaluation Be...</td>\n",
       "      <td>2024-01-12 15:21:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06628v1</td>\n",
       "      <td>Advancing automated programming necessitates r...</td>\n",
       "      <td>OOP: Object-Oriented Programming Evaluation Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06603v1</th>\n",
       "      <td>Mutual Enhancement of Large Language and Reinf...</td>\n",
       "      <td>2024-01-12 14:35:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06603v1</td>\n",
       "      <td>Large Language Models (LLMs) have demonstrated...</td>\n",
       "      <td>Mutual Enhancement of Large Language and\\nRein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06580v1</th>\n",
       "      <td>TestSpark: IntelliJ IDEA's Ultimate Test Gener...</td>\n",
       "      <td>2024-01-12 13:53:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06580v1</td>\n",
       "      <td>Writing software tests is laborious and time-c...</td>\n",
       "      <td>TestSpark: IntelliJ IDEA’s Ultimate Test Gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06568v1</th>\n",
       "      <td>Lost in the Source Language: How Large Languag...</td>\n",
       "      <td>2024-01-12 13:23:21</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06568v1</td>\n",
       "      <td>Large Language Models (LLMs) have achieved rem...</td>\n",
       "      <td>Lost in the Source Language: How Large Languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06692v1  An Experimental Design Framework for Label-Eff...   \n",
       "http://arxiv.org/abs/2401.06688v1  Don't Rank, Combine! Combining Machine Transla...   \n",
       "http://arxiv.org/abs/2401.06676v1  LLMRS: Unlocking Potentials of LLM-Based Recom...   \n",
       "http://arxiv.org/abs/2401.06373v1  How Johnny Can Persuade LLMs to Jailbreak Them...   \n",
       "http://arxiv.org/abs/2401.06643v1  Effects of diversity incentives on sample dive...   \n",
       "http://arxiv.org/abs/2401.06628v1  OOP: Object-Oriented Programming Evaluation Be...   \n",
       "http://arxiv.org/abs/2401.06603v1  Mutual Enhancement of Large Language and Reinf...   \n",
       "http://arxiv.org/abs/2401.06580v1  TestSpark: IntelliJ IDEA's Ultimate Test Gener...   \n",
       "http://arxiv.org/abs/2401.06568v1  Lost in the Source Language: How Large Languag...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06761v1 2024-01-12 18:50:36   \n",
       "http://arxiv.org/abs/2401.06692v1 2024-01-12 16:56:54   \n",
       "http://arxiv.org/abs/2401.06688v1 2024-01-12 16:52:41   \n",
       "http://arxiv.org/abs/2401.06676v1 2024-01-12 16:33:17   \n",
       "http://arxiv.org/abs/2401.06373v1 2024-01-12 16:13:24   \n",
       "http://arxiv.org/abs/2401.06643v1 2024-01-12 15:46:43   \n",
       "http://arxiv.org/abs/2401.06628v1 2024-01-12 15:21:36   \n",
       "http://arxiv.org/abs/2401.06603v1 2024-01-12 14:35:57   \n",
       "http://arxiv.org/abs/2401.06580v1 2024-01-12 13:53:57   \n",
       "http://arxiv.org/abs/2401.06568v1 2024-01-12 13:23:21   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06761v1  http://arxiv.org/pdf/2401.06761v1   \n",
       "http://arxiv.org/abs/2401.06692v1  http://arxiv.org/pdf/2401.06692v1   \n",
       "http://arxiv.org/abs/2401.06688v1  http://arxiv.org/pdf/2401.06688v1   \n",
       "http://arxiv.org/abs/2401.06676v1  http://arxiv.org/pdf/2401.06676v1   \n",
       "http://arxiv.org/abs/2401.06373v1  http://arxiv.org/pdf/2401.06373v1   \n",
       "http://arxiv.org/abs/2401.06643v1  http://arxiv.org/pdf/2401.06643v1   \n",
       "http://arxiv.org/abs/2401.06628v1  http://arxiv.org/pdf/2401.06628v1   \n",
       "http://arxiv.org/abs/2401.06603v1  http://arxiv.org/pdf/2401.06603v1   \n",
       "http://arxiv.org/abs/2401.06580v1  http://arxiv.org/pdf/2401.06580v1   \n",
       "http://arxiv.org/abs/2401.06568v1  http://arxiv.org/pdf/2401.06568v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06761v1  The massive adoption of large language models ...   \n",
       "http://arxiv.org/abs/2401.06692v1  Supervised finetuning (SFT) on instruction dat...   \n",
       "http://arxiv.org/abs/2401.06688v1  Neural machine translation systems estimate pr...   \n",
       "http://arxiv.org/abs/2401.06676v1  Recommendation systems are ubiquitous, from Sp...   \n",
       "http://arxiv.org/abs/2401.06373v1  Most traditional AI safety research has approa...   \n",
       "http://arxiv.org/abs/2401.06643v1  The latest generative large language models (L...   \n",
       "http://arxiv.org/abs/2401.06628v1  Advancing automated programming necessitates r...   \n",
       "http://arxiv.org/abs/2401.06603v1  Large Language Models (LLMs) have demonstrated...   \n",
       "http://arxiv.org/abs/2401.06580v1  Writing software tests is laborious and time-c...   \n",
       "http://arxiv.org/abs/2401.06568v1  Large Language Models (LLMs) have achieved rem...   \n",
       "\n",
       "                                                                            pdf_text  \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...  \n",
       "http://arxiv.org/abs/2401.06692v1  An Experimental Design Framework for Label-Eff...  \n",
       "http://arxiv.org/abs/2401.06688v1  Don’t Rank, Combine! Combining Machine Transla...  \n",
       "http://arxiv.org/abs/2401.06676v1   \\nWorkshop on Information Technology and Syst...  \n",
       "http://arxiv.org/abs/2401.06373v1  How Johnny Can Persuade LLMs to Jailbreak Them...  \n",
       "http://arxiv.org/abs/2401.06643v1  Effects of diversity incentives on sample dive...  \n",
       "http://arxiv.org/abs/2401.06628v1  OOP: Object-Oriented Programming Evaluation Be...  \n",
       "http://arxiv.org/abs/2401.06603v1  Mutual Enhancement of Large Language and\\nRein...  \n",
       "http://arxiv.org/abs/2401.06580v1  TestSpark: IntelliJ IDEA’s Ultimate Test Gener...  \n",
       "http://arxiv.org/abs/2401.06568v1  Lost in the Source Language: How Large Languag...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=arxiv.get_results()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e5ea2f2-f00e-4e84-9e85-0f434a9c2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv.store_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ed15054-6c60-4ffa-b4d2-0a526b1c1eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06761v1</th>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>2024-01-12 18:50:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06761v1</td>\n",
       "      <td>The massive adoption of large language models ...</td>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>951</td>\n",
       "      <td>43670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06692v1</th>\n",
       "      <td>An Experimental Design Framework for Label-Eff...</td>\n",
       "      <td>2024-01-12 16:56:54</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06692v1</td>\n",
       "      <td>Supervised finetuning (SFT) on instruction dat...</td>\n",
       "      <td>An Experimental Design Framework for Label-Eff...</td>\n",
       "      <td>1284</td>\n",
       "      <td>40531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06688v1</th>\n",
       "      <td>Don't Rank, Combine! Combining Machine Transla...</td>\n",
       "      <td>2024-01-12 16:52:41</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06688v1</td>\n",
       "      <td>Neural machine translation systems estimate pr...</td>\n",
       "      <td>Don’t Rank, Combine! Combining Machine Transla...</td>\n",
       "      <td>1358</td>\n",
       "      <td>71492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06676v1</th>\n",
       "      <td>LLMRS: Unlocking Potentials of LLM-Based Recom...</td>\n",
       "      <td>2024-01-12 16:33:17</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06676v1</td>\n",
       "      <td>Recommendation systems are ubiquitous, from Sp...</td>\n",
       "      <td>\\nWorkshop on Information Technology and Syst...</td>\n",
       "      <td>964</td>\n",
       "      <td>26069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06373v1</th>\n",
       "      <td>How Johnny Can Persuade LLMs to Jailbreak Them...</td>\n",
       "      <td>2024-01-12 16:13:24</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06373v1</td>\n",
       "      <td>Most traditional AI safety research has approa...</td>\n",
       "      <td>How Johnny Can Persuade LLMs to Jailbreak Them...</td>\n",
       "      <td>1199</td>\n",
       "      <td>120391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06643v1</th>\n",
       "      <td>Effects of diversity incentives on sample dive...</td>\n",
       "      <td>2024-01-12 15:46:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06643v1</td>\n",
       "      <td>The latest generative large language models (L...</td>\n",
       "      <td>Effects of diversity incentives on sample dive...</td>\n",
       "      <td>1002</td>\n",
       "      <td>53955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06628v1</th>\n",
       "      <td>OOP: Object-Oriented Programming Evaluation Be...</td>\n",
       "      <td>2024-01-12 15:21:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06628v1</td>\n",
       "      <td>Advancing automated programming necessitates r...</td>\n",
       "      <td>OOP: Object-Oriented Programming Evaluation Be...</td>\n",
       "      <td>1097</td>\n",
       "      <td>67956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06603v1</th>\n",
       "      <td>Mutual Enhancement of Large Language and Reinf...</td>\n",
       "      <td>2024-01-12 14:35:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06603v1</td>\n",
       "      <td>Large Language Models (LLMs) have demonstrated...</td>\n",
       "      <td>Mutual Enhancement of Large Language and\\nRein...</td>\n",
       "      <td>1260</td>\n",
       "      <td>13733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06580v1</th>\n",
       "      <td>TestSpark: IntelliJ IDEA's Ultimate Test Gener...</td>\n",
       "      <td>2024-01-12 13:53:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06580v1</td>\n",
       "      <td>Writing software tests is laborious and time-c...</td>\n",
       "      <td>TestSpark: IntelliJ IDEA’s Ultimate Test Gener...</td>\n",
       "      <td>1404</td>\n",
       "      <td>27565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06568v1</th>\n",
       "      <td>Lost in the Source Language: How Large Languag...</td>\n",
       "      <td>2024-01-12 13:23:21</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06568v1</td>\n",
       "      <td>Large Language Models (LLMs) have achieved rem...</td>\n",
       "      <td>Lost in the Source Language: How Large Languag...</td>\n",
       "      <td>1156</td>\n",
       "      <td>58720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06692v1  An Experimental Design Framework for Label-Eff...   \n",
       "http://arxiv.org/abs/2401.06688v1  Don't Rank, Combine! Combining Machine Transla...   \n",
       "http://arxiv.org/abs/2401.06676v1  LLMRS: Unlocking Potentials of LLM-Based Recom...   \n",
       "http://arxiv.org/abs/2401.06373v1  How Johnny Can Persuade LLMs to Jailbreak Them...   \n",
       "http://arxiv.org/abs/2401.06643v1  Effects of diversity incentives on sample dive...   \n",
       "http://arxiv.org/abs/2401.06628v1  OOP: Object-Oriented Programming Evaluation Be...   \n",
       "http://arxiv.org/abs/2401.06603v1  Mutual Enhancement of Large Language and Reinf...   \n",
       "http://arxiv.org/abs/2401.06580v1  TestSpark: IntelliJ IDEA's Ultimate Test Gener...   \n",
       "http://arxiv.org/abs/2401.06568v1  Lost in the Source Language: How Large Languag...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06761v1 2024-01-12 18:50:36   \n",
       "http://arxiv.org/abs/2401.06692v1 2024-01-12 16:56:54   \n",
       "http://arxiv.org/abs/2401.06688v1 2024-01-12 16:52:41   \n",
       "http://arxiv.org/abs/2401.06676v1 2024-01-12 16:33:17   \n",
       "http://arxiv.org/abs/2401.06373v1 2024-01-12 16:13:24   \n",
       "http://arxiv.org/abs/2401.06643v1 2024-01-12 15:46:43   \n",
       "http://arxiv.org/abs/2401.06628v1 2024-01-12 15:21:36   \n",
       "http://arxiv.org/abs/2401.06603v1 2024-01-12 14:35:57   \n",
       "http://arxiv.org/abs/2401.06580v1 2024-01-12 13:53:57   \n",
       "http://arxiv.org/abs/2401.06568v1 2024-01-12 13:23:21   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06761v1  http://arxiv.org/pdf/2401.06761v1   \n",
       "http://arxiv.org/abs/2401.06692v1  http://arxiv.org/pdf/2401.06692v1   \n",
       "http://arxiv.org/abs/2401.06688v1  http://arxiv.org/pdf/2401.06688v1   \n",
       "http://arxiv.org/abs/2401.06676v1  http://arxiv.org/pdf/2401.06676v1   \n",
       "http://arxiv.org/abs/2401.06373v1  http://arxiv.org/pdf/2401.06373v1   \n",
       "http://arxiv.org/abs/2401.06643v1  http://arxiv.org/pdf/2401.06643v1   \n",
       "http://arxiv.org/abs/2401.06628v1  http://arxiv.org/pdf/2401.06628v1   \n",
       "http://arxiv.org/abs/2401.06603v1  http://arxiv.org/pdf/2401.06603v1   \n",
       "http://arxiv.org/abs/2401.06580v1  http://arxiv.org/pdf/2401.06580v1   \n",
       "http://arxiv.org/abs/2401.06568v1  http://arxiv.org/pdf/2401.06568v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06761v1  The massive adoption of large language models ...   \n",
       "http://arxiv.org/abs/2401.06692v1  Supervised finetuning (SFT) on instruction dat...   \n",
       "http://arxiv.org/abs/2401.06688v1  Neural machine translation systems estimate pr...   \n",
       "http://arxiv.org/abs/2401.06676v1  Recommendation systems are ubiquitous, from Sp...   \n",
       "http://arxiv.org/abs/2401.06373v1  Most traditional AI safety research has approa...   \n",
       "http://arxiv.org/abs/2401.06643v1  The latest generative large language models (L...   \n",
       "http://arxiv.org/abs/2401.06628v1  Advancing automated programming necessitates r...   \n",
       "http://arxiv.org/abs/2401.06603v1  Large Language Models (LLMs) have demonstrated...   \n",
       "http://arxiv.org/abs/2401.06580v1  Writing software tests is laborious and time-c...   \n",
       "http://arxiv.org/abs/2401.06568v1  Large Language Models (LLMs) have achieved rem...   \n",
       "\n",
       "                                                                            pdf_text  \\\n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06692v1  An Experimental Design Framework for Label-Eff...   \n",
       "http://arxiv.org/abs/2401.06688v1  Don’t Rank, Combine! Combining Machine Transla...   \n",
       "http://arxiv.org/abs/2401.06676v1   \\nWorkshop on Information Technology and Syst...   \n",
       "http://arxiv.org/abs/2401.06373v1  How Johnny Can Persuade LLMs to Jailbreak Them...   \n",
       "http://arxiv.org/abs/2401.06643v1  Effects of diversity incentives on sample dive...   \n",
       "http://arxiv.org/abs/2401.06628v1  OOP: Object-Oriented Programming Evaluation Be...   \n",
       "http://arxiv.org/abs/2401.06603v1  Mutual Enhancement of Large Language and\\nRein...   \n",
       "http://arxiv.org/abs/2401.06580v1  TestSpark: IntelliJ IDEA’s Ultimate Test Gener...   \n",
       "http://arxiv.org/abs/2401.06568v1  Lost in the Source Language: How Large Languag...   \n",
       "\n",
       "                                   summary_length  pdf_text_length  \n",
       "http://arxiv.org/abs/2401.06761v1             951            43670  \n",
       "http://arxiv.org/abs/2401.06692v1            1284            40531  \n",
       "http://arxiv.org/abs/2401.06688v1            1358            71492  \n",
       "http://arxiv.org/abs/2401.06676v1             964            26069  \n",
       "http://arxiv.org/abs/2401.06373v1            1199           120391  \n",
       "http://arxiv.org/abs/2401.06643v1            1002            53955  \n",
       "http://arxiv.org/abs/2401.06628v1            1097            67956  \n",
       "http://arxiv.org/abs/2401.06603v1            1260            13733  \n",
       "http://arxiv.org/abs/2401.06580v1            1404            27565  \n",
       "http://arxiv.org/abs/2401.06568v1            1156            58720  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_data=arxiv.get_stored_data()\n",
    "stored_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0717ea3a-24d5-4f91-a628-3f711d3ae8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06761v1</th>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>2024-01-12 18:50:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06761v1</td>\n",
       "      <td>The massive adoption of large language models ...</td>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>951</td>\n",
       "      <td>43670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06692v1</th>\n",
       "      <td>An Experimental Design Framework for Label-Eff...</td>\n",
       "      <td>2024-01-12 16:56:54</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06692v1</td>\n",
       "      <td>Supervised finetuning (SFT) on instruction dat...</td>\n",
       "      <td>An Experimental Design Framework for Label-Eff...</td>\n",
       "      <td>1284</td>\n",
       "      <td>40531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06688v1</th>\n",
       "      <td>Don't Rank, Combine! Combining Machine Transla...</td>\n",
       "      <td>2024-01-12 16:52:41</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06688v1</td>\n",
       "      <td>Neural machine translation systems estimate pr...</td>\n",
       "      <td>Don’t Rank, Combine! Combining Machine Transla...</td>\n",
       "      <td>1358</td>\n",
       "      <td>71492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06676v1</th>\n",
       "      <td>LLMRS: Unlocking Potentials of LLM-Based Recom...</td>\n",
       "      <td>2024-01-12 16:33:17</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06676v1</td>\n",
       "      <td>Recommendation systems are ubiquitous, from Sp...</td>\n",
       "      <td>\\nWorkshop on Information Technology and Syst...</td>\n",
       "      <td>964</td>\n",
       "      <td>26069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06373v1</th>\n",
       "      <td>How Johnny Can Persuade LLMs to Jailbreak Them...</td>\n",
       "      <td>2024-01-12 16:13:24</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06373v1</td>\n",
       "      <td>Most traditional AI safety research has approa...</td>\n",
       "      <td>How Johnny Can Persuade LLMs to Jailbreak Them...</td>\n",
       "      <td>1199</td>\n",
       "      <td>120391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06643v1</th>\n",
       "      <td>Effects of diversity incentives on sample dive...</td>\n",
       "      <td>2024-01-12 15:46:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06643v1</td>\n",
       "      <td>The latest generative large language models (L...</td>\n",
       "      <td>Effects of diversity incentives on sample dive...</td>\n",
       "      <td>1002</td>\n",
       "      <td>53955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06628v1</th>\n",
       "      <td>OOP: Object-Oriented Programming Evaluation Be...</td>\n",
       "      <td>2024-01-12 15:21:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06628v1</td>\n",
       "      <td>Advancing automated programming necessitates r...</td>\n",
       "      <td>OOP: Object-Oriented Programming Evaluation Be...</td>\n",
       "      <td>1097</td>\n",
       "      <td>67956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06603v1</th>\n",
       "      <td>Mutual Enhancement of Large Language and Reinf...</td>\n",
       "      <td>2024-01-12 14:35:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06603v1</td>\n",
       "      <td>Large Language Models (LLMs) have demonstrated...</td>\n",
       "      <td>Mutual Enhancement of Large Language and\\nRein...</td>\n",
       "      <td>1260</td>\n",
       "      <td>13733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06580v1</th>\n",
       "      <td>TestSpark: IntelliJ IDEA's Ultimate Test Gener...</td>\n",
       "      <td>2024-01-12 13:53:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06580v1</td>\n",
       "      <td>Writing software tests is laborious and time-c...</td>\n",
       "      <td>TestSpark: IntelliJ IDEA’s Ultimate Test Gener...</td>\n",
       "      <td>1404</td>\n",
       "      <td>27565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06568v1</th>\n",
       "      <td>Lost in the Source Language: How Large Languag...</td>\n",
       "      <td>2024-01-12 13:23:21</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06568v1</td>\n",
       "      <td>Large Language Models (LLMs) have achieved rem...</td>\n",
       "      <td>Lost in the Source Language: How Large Languag...</td>\n",
       "      <td>1156</td>\n",
       "      <td>58720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06692v1  An Experimental Design Framework for Label-Eff...   \n",
       "http://arxiv.org/abs/2401.06688v1  Don't Rank, Combine! Combining Machine Transla...   \n",
       "http://arxiv.org/abs/2401.06676v1  LLMRS: Unlocking Potentials of LLM-Based Recom...   \n",
       "http://arxiv.org/abs/2401.06373v1  How Johnny Can Persuade LLMs to Jailbreak Them...   \n",
       "http://arxiv.org/abs/2401.06643v1  Effects of diversity incentives on sample dive...   \n",
       "http://arxiv.org/abs/2401.06628v1  OOP: Object-Oriented Programming Evaluation Be...   \n",
       "http://arxiv.org/abs/2401.06603v1  Mutual Enhancement of Large Language and Reinf...   \n",
       "http://arxiv.org/abs/2401.06580v1  TestSpark: IntelliJ IDEA's Ultimate Test Gener...   \n",
       "http://arxiv.org/abs/2401.06568v1  Lost in the Source Language: How Large Languag...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06761v1 2024-01-12 18:50:36   \n",
       "http://arxiv.org/abs/2401.06692v1 2024-01-12 16:56:54   \n",
       "http://arxiv.org/abs/2401.06688v1 2024-01-12 16:52:41   \n",
       "http://arxiv.org/abs/2401.06676v1 2024-01-12 16:33:17   \n",
       "http://arxiv.org/abs/2401.06373v1 2024-01-12 16:13:24   \n",
       "http://arxiv.org/abs/2401.06643v1 2024-01-12 15:46:43   \n",
       "http://arxiv.org/abs/2401.06628v1 2024-01-12 15:21:36   \n",
       "http://arxiv.org/abs/2401.06603v1 2024-01-12 14:35:57   \n",
       "http://arxiv.org/abs/2401.06580v1 2024-01-12 13:53:57   \n",
       "http://arxiv.org/abs/2401.06568v1 2024-01-12 13:23:21   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06761v1  http://arxiv.org/pdf/2401.06761v1   \n",
       "http://arxiv.org/abs/2401.06692v1  http://arxiv.org/pdf/2401.06692v1   \n",
       "http://arxiv.org/abs/2401.06688v1  http://arxiv.org/pdf/2401.06688v1   \n",
       "http://arxiv.org/abs/2401.06676v1  http://arxiv.org/pdf/2401.06676v1   \n",
       "http://arxiv.org/abs/2401.06373v1  http://arxiv.org/pdf/2401.06373v1   \n",
       "http://arxiv.org/abs/2401.06643v1  http://arxiv.org/pdf/2401.06643v1   \n",
       "http://arxiv.org/abs/2401.06628v1  http://arxiv.org/pdf/2401.06628v1   \n",
       "http://arxiv.org/abs/2401.06603v1  http://arxiv.org/pdf/2401.06603v1   \n",
       "http://arxiv.org/abs/2401.06580v1  http://arxiv.org/pdf/2401.06580v1   \n",
       "http://arxiv.org/abs/2401.06568v1  http://arxiv.org/pdf/2401.06568v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06761v1  The massive adoption of large language models ...   \n",
       "http://arxiv.org/abs/2401.06692v1  Supervised finetuning (SFT) on instruction dat...   \n",
       "http://arxiv.org/abs/2401.06688v1  Neural machine translation systems estimate pr...   \n",
       "http://arxiv.org/abs/2401.06676v1  Recommendation systems are ubiquitous, from Sp...   \n",
       "http://arxiv.org/abs/2401.06373v1  Most traditional AI safety research has approa...   \n",
       "http://arxiv.org/abs/2401.06643v1  The latest generative large language models (L...   \n",
       "http://arxiv.org/abs/2401.06628v1  Advancing automated programming necessitates r...   \n",
       "http://arxiv.org/abs/2401.06603v1  Large Language Models (LLMs) have demonstrated...   \n",
       "http://arxiv.org/abs/2401.06580v1  Writing software tests is laborious and time-c...   \n",
       "http://arxiv.org/abs/2401.06568v1  Large Language Models (LLMs) have achieved rem...   \n",
       "\n",
       "                                                                            pdf_text  \\\n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06692v1  An Experimental Design Framework for Label-Eff...   \n",
       "http://arxiv.org/abs/2401.06688v1  Don’t Rank, Combine! Combining Machine Transla...   \n",
       "http://arxiv.org/abs/2401.06676v1   \\nWorkshop on Information Technology and Syst...   \n",
       "http://arxiv.org/abs/2401.06373v1  How Johnny Can Persuade LLMs to Jailbreak Them...   \n",
       "http://arxiv.org/abs/2401.06643v1  Effects of diversity incentives on sample dive...   \n",
       "http://arxiv.org/abs/2401.06628v1  OOP: Object-Oriented Programming Evaluation Be...   \n",
       "http://arxiv.org/abs/2401.06603v1  Mutual Enhancement of Large Language and\\nRein...   \n",
       "http://arxiv.org/abs/2401.06580v1  TestSpark: IntelliJ IDEA’s Ultimate Test Gener...   \n",
       "http://arxiv.org/abs/2401.06568v1  Lost in the Source Language: How Large Languag...   \n",
       "\n",
       "                                   summary_length  pdf_text_length  \n",
       "http://arxiv.org/abs/2401.06761v1             951            43670  \n",
       "http://arxiv.org/abs/2401.06692v1            1284            40531  \n",
       "http://arxiv.org/abs/2401.06688v1            1358            71492  \n",
       "http://arxiv.org/abs/2401.06676v1             964            26069  \n",
       "http://arxiv.org/abs/2401.06373v1            1199           120391  \n",
       "http://arxiv.org/abs/2401.06643v1            1002            53955  \n",
       "http://arxiv.org/abs/2401.06628v1            1097            67956  \n",
       "http://arxiv.org/abs/2401.06603v1            1260            13733  \n",
       "http://arxiv.org/abs/2401.06580v1            1404            27565  \n",
       "http://arxiv.org/abs/2401.06568v1            1156            58720  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdefde0-42ea-43b1-aad3-5b58cf4c1993",
   "metadata": {},
   "source": [
    "## User Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "567d4fb8-39cd-40b3-b932-38630c360b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(ArxivParser):\n",
    "    # no get request\n",
    "    def __init__(self, topics: str=\"llm\", authors: str=None, keywords: list=None):\n",
    "        # Call the init method of the ArxivParser class\n",
    "        super().__init__()\n",
    "        \n",
    "        self.topics = topics\n",
    "        self.authors = authors\n",
    "        self.keywords = keywords\n",
    "        \n",
    "        self.extracted_data=None\n",
    "\n",
    "    def store_data(self, query: str = \"llm\", max_results: int = 1, days: int = 60) -> None:\n",
    "        # Construct a new query based on the user preferences\n",
    "        new_query = query\n",
    "        if self.topics:\n",
    "            new_query += f\"+cat:{self.topics}\"\n",
    "        if self.authors:\n",
    "            new_query += f\"+au:{self.authors}\"\n",
    "        if self.keywords:\n",
    "            new_query += f\"+ti:{'+'.join(self.keywords)}\"\n",
    "        \n",
    "        # Call the get_results method of the ArxivParser class with the new query\n",
    "        # self.extracted_data = super().get_results(new_query, max_results, days)\n",
    "        \n",
    "        self.extracted_data['summary_length'] = self.extracted_data.apply(lambda row: len(row['summary']), axis=1)\n",
    "        self.extracted_data['pdf_text_length'] = self.extracted_data.apply(lambda row: len(row['pdf_text']), axis=1)\n",
    "\n",
    "\n",
    "    # def get_stored_data(self) -> pd.DataFrame:\n",
    "    #   return self.extracted_data\n",
    "\n",
    "    def daily_feed():\n",
    "        pass\n",
    "    def search():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aef8174-8afb-4816-b9e7-af5e813a17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "subrata=User(\n",
    "    topics=\"time series\",\n",
    "    authors=None,\n",
    "    keywords=\"data augmentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba1e512d-b93a-418d-9837-0a70d02a997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://export.arxiv.org/api/query?search_query=all:llm\")\n",
    "# Parse the response\n",
    "entries = feedparser.parse(response.text).entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e480a0fe-723b-4b30-9910-23b9e02a108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'http://arxiv.org/abs/2311.10372v2',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2311.10372v2',\n",
       "  'updated': '2024-01-08T05:41:51Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2024, tm_mon=1, tm_mday=8, tm_hour=5, tm_min=41, tm_sec=51, tm_wday=0, tm_yday=8, tm_isdst=0),\n",
       "  'published': '2023-11-17T07:55:16Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=17, tm_hour=7, tm_min=55, tm_sec=16, tm_wday=4, tm_yday=321, tm_isdst=0),\n",
       "  'title': 'A Survey of Large Language Models for Code: Evolution, Benchmarking, and\\n  Future Trends',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'A Survey of Large Language Models for Code: Evolution, Benchmarking, and\\n  Future Trends'},\n",
       "  'summary': 'General large language models (LLMs), represented by ChatGPT, have\\ndemonstrated significant potential in tasks such as code generation in software\\nengineering. This has led to the development of specialized LLMs for software\\nengineering, known as Code LLMs. A considerable portion of Code LLMs is derived\\nfrom general LLMs through model fine-tuning. As a result, Code LLMs are often\\nupdated frequently and their performance can be influenced by the base LLMs.\\nHowever, there is currently a lack of systematic investigation into Code LLMs\\nand their performance. In this study, we conduct a comprehensive survey and\\nanalysis of the types of Code LLMs and their differences in performance\\ncompared to general LLMs. We aim to address three questions: (1) What LLMs are\\nspecifically designed for software engineering tasks, and what is the\\nrelationship between these Code LLMs? (2) Do Code LLMs really outperform\\ngeneral LLMs in software engineering tasks? (3) Which LLMs are more proficient\\nin different software engineering tasks? To answer these questions, we first\\ncollect relevant literature and work from five major databases and open-source\\ncommunities, resulting in 134 works for analysis. Next, we categorize the Code\\nLLMs based on their publishers and examine their relationships with general\\nLLMs and among themselves. Furthermore, we investigate the performance\\ndifferences between general LLMs and Code LLMs in various software engineering\\ntasks to demonstrate the impact of base models and Code LLMs. Finally, we\\ncomprehensively maintained the performance of LLMs across multiple mainstream\\nbenchmarks to identify the best-performing LLMs for each software engineering\\ntask. Our research not only assists developers of Code LLMs in choosing base\\nmodels for the development of more advanced LLMs but also provides insights for\\npractitioners to better understand key improvement directions for Code LLMs.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'General large language models (LLMs), represented by ChatGPT, have\\ndemonstrated significant potential in tasks such as code generation in software\\nengineering. This has led to the development of specialized LLMs for software\\nengineering, known as Code LLMs. A considerable portion of Code LLMs is derived\\nfrom general LLMs through model fine-tuning. As a result, Code LLMs are often\\nupdated frequently and their performance can be influenced by the base LLMs.\\nHowever, there is currently a lack of systematic investigation into Code LLMs\\nand their performance. In this study, we conduct a comprehensive survey and\\nanalysis of the types of Code LLMs and their differences in performance\\ncompared to general LLMs. We aim to address three questions: (1) What LLMs are\\nspecifically designed for software engineering tasks, and what is the\\nrelationship between these Code LLMs? (2) Do Code LLMs really outperform\\ngeneral LLMs in software engineering tasks? (3) Which LLMs are more proficient\\nin different software engineering tasks? To answer these questions, we first\\ncollect relevant literature and work from five major databases and open-source\\ncommunities, resulting in 134 works for analysis. Next, we categorize the Code\\nLLMs based on their publishers and examine their relationships with general\\nLLMs and among themselves. Furthermore, we investigate the performance\\ndifferences between general LLMs and Code LLMs in various software engineering\\ntasks to demonstrate the impact of base models and Code LLMs. Finally, we\\ncomprehensively maintained the performance of LLMs across multiple mainstream\\nbenchmarks to identify the best-performing LLMs for each software engineering\\ntask. Our research not only assists developers of Code LLMs in choosing base\\nmodels for the development of more advanced LLMs but also provides insights for\\npractitioners to better understand key improvement directions for Code LLMs.'},\n",
       "  'authors': [{'name': 'Zibin Zheng'},\n",
       "   {'name': 'Kaiwen Ning'},\n",
       "   {'name': 'Yanlin Wang'},\n",
       "   {'name': 'Jingwen Zhang'},\n",
       "   {'name': 'Dewu Zheng'},\n",
       "   {'name': 'Mingxi Ye'},\n",
       "   {'name': 'Jiachi Chen'}],\n",
       "  'author_detail': {'name': 'Jiachi Chen'},\n",
       "  'author': 'Jiachi Chen',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2311.10372v2',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2311.10372v2',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.SE',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.SE',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2307.09793v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2307.09793v1',\n",
       "  'updated': '2023-07-19T07:17:43Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=19, tm_hour=7, tm_min=17, tm_sec=43, tm_wday=2, tm_yday=200, tm_isdst=0),\n",
       "  'published': '2023-07-19T07:17:43Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=19, tm_hour=7, tm_min=17, tm_sec=43, tm_wday=2, tm_yday=200, tm_isdst=0),\n",
       "  'title': 'On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large\\n  Language Models',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large\\n  Language Models'},\n",
       "  'summary': 'Since late 2022, Large Language Models (LLMs) have become very prominent with\\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\\nare announced each week, many of which are deposited to Hugging Face, a\\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\\nGeneration models have been uploaded to the site. Given the huge influx of\\nLLMs, it is of interest to know which LLM backbones, settings, training\\nmethods, and families are popular or trending. However, there is no\\ncomprehensive index of LLMs available. We take advantage of the relatively\\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\\nand identify communities amongst LLMs using n-grams and term frequency-inverse\\ndocument frequency. Our methods successfully identify families of LLMs and\\naccurately cluster LLMs into meaningful subgroups. We present a public web\\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\\nConstellation rapidly generates a variety of visualizations, namely\\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\\nat the following link: https://constellation.sites.stanford.edu/.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Since late 2022, Large Language Models (LLMs) have become very prominent with\\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\\nare announced each week, many of which are deposited to Hugging Face, a\\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\\nGeneration models have been uploaded to the site. Given the huge influx of\\nLLMs, it is of interest to know which LLM backbones, settings, training\\nmethods, and families are popular or trending. However, there is no\\ncomprehensive index of LLMs available. We take advantage of the relatively\\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\\nand identify communities amongst LLMs using n-grams and term frequency-inverse\\ndocument frequency. Our methods successfully identify families of LLMs and\\naccurately cluster LLMs into meaningful subgroups. We present a public web\\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\\nConstellation rapidly generates a variety of visualizations, namely\\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\\nat the following link: https://constellation.sites.stanford.edu/.'},\n",
       "  'authors': [{'name': 'Sarah Gao'}, {'name': 'Andrew Kean Gao'}],\n",
       "  'author_detail': {'name': 'Andrew Kean Gao'},\n",
       "  'author': 'Andrew Kean Gao',\n",
       "  'arxiv_comment': '14 pages, 6 figures, 1 table',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2307.09793v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2307.09793v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.DL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.DL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None},\n",
       "   {'term': 'I.2.1; H.5.0',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2308.08241v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2308.08241v1',\n",
       "  'updated': '2023-08-16T09:16:02Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=16, tm_hour=9, tm_min=16, tm_sec=2, tm_wday=2, tm_yday=228, tm_isdst=0),\n",
       "  'published': '2023-08-16T09:16:02Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=16, tm_hour=9, tm_min=16, tm_sec=2, tm_wday=2, tm_yday=228, tm_isdst=0),\n",
       "  'title': \"TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for\\n  Time Series\",\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': \"TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for\\n  Time Series\"},\n",
       "  'summary': \"This work summarizes two strategies for completing time-series (TS) tasks\\nusing today's language model (LLM): LLM-for-TS, design and train a fundamental\\nlarge model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS\\ndata. Considering the insufficient data accumulation, limited resources, and\\nsemantic context requirements, this work focuses on TS-for-LLM methods, where\\nwe aim to activate LLM's ability for TS data by designing a TS embedding method\\nsuitable for LLM. The proposed method is named TEST. It first tokenizes TS,\\nbuilds an encoder to embed them by instance-wise, feature-wise, and\\ntext-prototype-aligned contrast, and then creates prompts to make LLM more open\\nto embeddings, and finally implements TS tasks. Experiments are carried out on\\nTS classification and forecasting tasks using 8 LLMs with different structures\\nand sizes. Although its results cannot significantly outperform the current\\nSOTA models customized for TS tasks, by treating LLM as the pattern machine, it\\ncan endow LLM's ability to process TS data without compromising the language\\nability. This paper is intended to serve as a foundational work that will\\ninspire further research.\",\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': \"This work summarizes two strategies for completing time-series (TS) tasks\\nusing today's language model (LLM): LLM-for-TS, design and train a fundamental\\nlarge model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS\\ndata. Considering the insufficient data accumulation, limited resources, and\\nsemantic context requirements, this work focuses on TS-for-LLM methods, where\\nwe aim to activate LLM's ability for TS data by designing a TS embedding method\\nsuitable for LLM. The proposed method is named TEST. It first tokenizes TS,\\nbuilds an encoder to embed them by instance-wise, feature-wise, and\\ntext-prototype-aligned contrast, and then creates prompts to make LLM more open\\nto embeddings, and finally implements TS tasks. Experiments are carried out on\\nTS classification and forecasting tasks using 8 LLMs with different structures\\nand sizes. Although its results cannot significantly outperform the current\\nSOTA models customized for TS tasks, by treating LLM as the pattern machine, it\\ncan endow LLM's ability to process TS data without compromising the language\\nability. This paper is intended to serve as a foundational work that will\\ninspire further research.\"},\n",
       "  'authors': [{'name': 'Chenxi Sun'},\n",
       "   {'name': 'Yaliang Li'},\n",
       "   {'name': 'Hongyan Li'},\n",
       "   {'name': 'Shenda Hong'}],\n",
       "  'author_detail': {'name': 'Shenda Hong'},\n",
       "  'author': 'Shenda Hong',\n",
       "  'arxiv_comment': '10 pages, 6 figures',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2308.08241v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2308.08241v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.AI',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2311.05656v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2311.05656v1',\n",
       "  'updated': '2023-11-09T00:05:27Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=9, tm_hour=0, tm_min=5, tm_sec=27, tm_wday=3, tm_yday=313, tm_isdst=0),\n",
       "  'published': '2023-11-09T00:05:27Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=9, tm_hour=0, tm_min=5, tm_sec=27, tm_wday=3, tm_yday=313, tm_isdst=0),\n",
       "  'title': 'Combating Misinformation in the Age of LLMs: Opportunities and\\n  Challenges',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Combating Misinformation in the Age of LLMs: Opportunities and\\n  Challenges'},\n",
       "  'summary': 'Misinformation such as fake news and rumors is a serious threat on\\ninformation ecosystems and public trust. The emergence of Large Language Models\\n(LLMs) has great potential to reshape the landscape of combating\\nmisinformation. Generally, LLMs can be a double-edged sword in the fight. On\\nthe one hand, LLMs bring promising opportunities for combating misinformation\\ndue to their profound world knowledge and strong reasoning abilities. Thus, one\\nemergent question is: how to utilize LLMs to combat misinformation? On the\\nother hand, the critical challenge is that LLMs can be easily leveraged to\\ngenerate deceptive misinformation at scale. Then, another important question\\nis: how to combat LLM-generated misinformation? In this paper, we first\\nsystematically review the history of combating misinformation before the advent\\nof LLMs. Then we illustrate the current efforts and present an outlook for\\nthese two fundamental questions respectively. The goal of this survey paper is\\nto facilitate the progress of utilizing LLMs for fighting misinformation and\\ncall for interdisciplinary efforts from different stakeholders for combating\\nLLM-generated misinformation.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Misinformation such as fake news and rumors is a serious threat on\\ninformation ecosystems and public trust. The emergence of Large Language Models\\n(LLMs) has great potential to reshape the landscape of combating\\nmisinformation. Generally, LLMs can be a double-edged sword in the fight. On\\nthe one hand, LLMs bring promising opportunities for combating misinformation\\ndue to their profound world knowledge and strong reasoning abilities. Thus, one\\nemergent question is: how to utilize LLMs to combat misinformation? On the\\nother hand, the critical challenge is that LLMs can be easily leveraged to\\ngenerate deceptive misinformation at scale. Then, another important question\\nis: how to combat LLM-generated misinformation? In this paper, we first\\nsystematically review the history of combating misinformation before the advent\\nof LLMs. Then we illustrate the current efforts and present an outlook for\\nthese two fundamental questions respectively. The goal of this survey paper is\\nto facilitate the progress of utilizing LLMs for fighting misinformation and\\ncall for interdisciplinary efforts from different stakeholders for combating\\nLLM-generated misinformation.'},\n",
       "  'authors': [{'name': 'Canyu Chen'}, {'name': 'Kai Shu'}],\n",
       "  'author_detail': {'name': 'Kai Shu'},\n",
       "  'author': 'Kai Shu',\n",
       "  'arxiv_comment': '9 pages for the main paper, 35 pages including 656 references, more\\n  resources on \"LLMs Meet Misinformation\" are on the website:\\n  https://llm-misinformation.github.io/',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2311.05656v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2311.05656v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CY',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CY',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2311.15759v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2311.15759v1',\n",
       "  'updated': '2023-11-27T12:29:20Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=27, tm_hour=12, tm_min=29, tm_sec=20, tm_wday=0, tm_yday=331, tm_isdst=0),\n",
       "  'published': '2023-11-27T12:29:20Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=27, tm_hour=12, tm_min=29, tm_sec=20, tm_wday=0, tm_yday=331, tm_isdst=0),\n",
       "  'title': 'Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage\\n  and Sharing in LLMs',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage\\n  and Sharing in LLMs'},\n",
       "  'summary': 'Recent advancements in multimodal large language models (MLLMs) have achieved\\nsignificant multimodal generation capabilities, akin to GPT-4. These models\\npredominantly map visual information into language representation space,\\nleveraging the vast knowledge and powerful text generation abilities of LLMs to\\nproduce multimodal instruction-following responses. We could term this method\\nas LLMs for Vision because of its employing LLMs for visual-language\\nunderstanding, yet observe that these MLLMs neglect the potential of harnessing\\nvisual knowledge to enhance overall capabilities of LLMs, which could be\\nregraded as Vision Enhancing LLMs. In this paper, we propose an approach called\\nMKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage\\nand Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a\\ncomponent integrated into the internal blocks of LLMs, designed to store\\nopen-world visual information efficiently. Additionally, we present a soft\\nMixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal\\nknowledge collaboration during generation. Our comprehensive experiments\\ndemonstrate that MKS2 substantially augments the reasoning capabilities of LLMs\\nin contexts necessitating physical or commonsense knowledge. It also delivers\\ncompetitive results on multimodal benchmarks.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Recent advancements in multimodal large language models (MLLMs) have achieved\\nsignificant multimodal generation capabilities, akin to GPT-4. These models\\npredominantly map visual information into language representation space,\\nleveraging the vast knowledge and powerful text generation abilities of LLMs to\\nproduce multimodal instruction-following responses. We could term this method\\nas LLMs for Vision because of its employing LLMs for visual-language\\nunderstanding, yet observe that these MLLMs neglect the potential of harnessing\\nvisual knowledge to enhance overall capabilities of LLMs, which could be\\nregraded as Vision Enhancing LLMs. In this paper, we propose an approach called\\nMKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage\\nand Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a\\ncomponent integrated into the internal blocks of LLMs, designed to store\\nopen-world visual information efficiently. Additionally, we present a soft\\nMixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal\\nknowledge collaboration during generation. Our comprehensive experiments\\ndemonstrate that MKS2 substantially augments the reasoning capabilities of LLMs\\nin contexts necessitating physical or commonsense knowledge. It also delivers\\ncompetitive results on multimodal benchmarks.'},\n",
       "  'authors': [{'name': 'Yunxin Li'},\n",
       "   {'name': 'Baotian Hu'},\n",
       "   {'name': 'Wei Wang'},\n",
       "   {'name': 'Xiaochun Cao'},\n",
       "   {'name': 'Min Zhang'}],\n",
       "  'author_detail': {'name': 'Min Zhang'},\n",
       "  'author': 'Min Zhang',\n",
       "  'arxiv_comment': '12 pages, 4 figures',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2311.15759v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2311.15759v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None},\n",
       "   {'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2306.05212v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2306.05212v1',\n",
       "  'updated': '2023-06-08T14:10:54Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=14, tm_min=10, tm_sec=54, tm_wday=3, tm_yday=159, tm_isdst=0),\n",
       "  'published': '2023-06-08T14:10:54Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=14, tm_min=10, tm_sec=54, tm_wday=3, tm_yday=159, tm_isdst=0),\n",
       "  'title': 'RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit'},\n",
       "  'summary': 'Although Large Language Models (LLMs) have demonstrated extraordinary\\ncapabilities in many domains, they still have a tendency to hallucinate and\\ngenerate fictitious responses to user requests. This problem can be alleviated\\nby augmenting LLMs with information retrieval (IR) systems (also known as\\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\\nfactual texts in response to user input according to the relevant content\\nretrieved by IR systems from external corpora as references. In addition, by\\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\\nquestions that cannot be answered by solely relying on the world knowledge\\nstored in parameters. To support research in this area and facilitate the\\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\\nto help researchers and users build their customized in-domain LLM-based\\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\\nprovides more plug-and-play modules to support better interaction between IR\\nsystems and LLMs, including {request rewriting, document retrieval, passage\\nextraction, answer generation, and fact checking} modules. Our toolkit is\\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Although Large Language Models (LLMs) have demonstrated extraordinary\\ncapabilities in many domains, they still have a tendency to hallucinate and\\ngenerate fictitious responses to user requests. This problem can be alleviated\\nby augmenting LLMs with information retrieval (IR) systems (also known as\\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\\nfactual texts in response to user input according to the relevant content\\nretrieved by IR systems from external corpora as references. In addition, by\\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\\nquestions that cannot be answered by solely relying on the world knowledge\\nstored in parameters. To support research in this area and facilitate the\\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\\nto help researchers and users build their customized in-domain LLM-based\\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\\nprovides more plug-and-play modules to support better interaction between IR\\nsystems and LLMs, including {request rewriting, document retrieval, passage\\nextraction, answer generation, and fact checking} modules. Our toolkit is\\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.'},\n",
       "  'authors': [{'name': 'Jiongnan Liu'},\n",
       "   {'name': 'Jiajie Jin'},\n",
       "   {'name': 'Zihan Wang'},\n",
       "   {'name': 'Jiehan Cheng'},\n",
       "   {'name': 'Zhicheng Dou'},\n",
       "   {'name': 'Ji-Rong Wen'}],\n",
       "  'author_detail': {'name': 'Ji-Rong Wen'},\n",
       "  'author': 'Ji-Rong Wen',\n",
       "  'arxiv_comment': 'Technical Report for RETA-LLM',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2306.05212v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2306.05212v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.IR',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.IR',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2311.07689v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2311.07689v1',\n",
       "  'updated': '2023-11-13T19:13:29Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=13, tm_hour=19, tm_min=13, tm_sec=29, tm_wday=0, tm_yday=317, tm_isdst=0),\n",
       "  'published': '2023-11-13T19:13:29Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=13, tm_hour=19, tm_min=13, tm_sec=29, tm_wday=0, tm_yday=317, tm_isdst=0),\n",
       "  'title': 'MART: Improving LLM Safety with Multi-round Automatic Red-Teaming',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'MART: Improving LLM Safety with Multi-round Automatic Red-Teaming'},\n",
       "  'summary': 'Red-teaming is a common practice for mitigating unsafe behaviors in Large\\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\\npotential flaws and addressing them with responsible and accurate responses.\\nWhile effective, manual red-teaming is costly, and existing automatic\\nred-teaming typically discovers safety risks without addressing them. In this\\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\\nincorporates both automatic adversarial prompt writing and safe response\\ngeneration, significantly increasing red-teaming scalability and the safety of\\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\\nwith each other in an iterative manner, where the adversarial LLM aims to\\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\\nwhile the target LLM is fine-tuned with safety aligned data on these\\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\\non the updated target LLM, while the target LLM also improves itself through\\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\\nachieving comparable performance to LLMs with extensive adversarial prompt\\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\\nthroughout iterations, indicating the target LLM maintains strong performance\\non instruction following.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Red-teaming is a common practice for mitigating unsafe behaviors in Large\\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\\npotential flaws and addressing them with responsible and accurate responses.\\nWhile effective, manual red-teaming is costly, and existing automatic\\nred-teaming typically discovers safety risks without addressing them. In this\\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\\nincorporates both automatic adversarial prompt writing and safe response\\ngeneration, significantly increasing red-teaming scalability and the safety of\\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\\nwith each other in an iterative manner, where the adversarial LLM aims to\\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\\nwhile the target LLM is fine-tuned with safety aligned data on these\\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\\non the updated target LLM, while the target LLM also improves itself through\\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\\nachieving comparable performance to LLMs with extensive adversarial prompt\\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\\nthroughout iterations, indicating the target LLM maintains strong performance\\non instruction following.'},\n",
       "  'authors': [{'name': 'Suyu Ge'},\n",
       "   {'name': 'Chunting Zhou'},\n",
       "   {'name': 'Rui Hou'},\n",
       "   {'name': 'Madian Khabsa'},\n",
       "   {'name': 'Yi-Chia Wang'},\n",
       "   {'name': 'Qifan Wang'},\n",
       "   {'name': 'Jiawei Han'},\n",
       "   {'name': 'Yuning Mao'}],\n",
       "  'author_detail': {'name': 'Yuning Mao'},\n",
       "  'author': 'Yuning Mao',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2311.07689v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2311.07689v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.12720v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.12720v1',\n",
       "  'updated': '2023-05-22T04:59:33Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=4, tm_min=59, tm_sec=33, tm_wday=0, tm_yday=142, tm_isdst=0),\n",
       "  'published': '2023-05-22T04:59:33Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=4, tm_min=59, tm_sec=33, tm_wday=0, tm_yday=142, tm_isdst=0),\n",
       "  'title': 'llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large\\n  Language Models and its Methodology',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large\\n  Language Models and its Methodology'},\n",
       "  'summary': 'This study constructed a Japanese chat dataset for tuning large language\\nmodels (LLMs), which consist of about 8.4 million records. Recently, LLMs have\\nbeen developed and gaining popularity. However, high-performing LLMs are\\nusually mainly for English. There are two ways to support languages other than\\nEnglish by those LLMs: constructing LLMs from scratch or tuning existing\\nmodels. However, in both ways, datasets are necessary parts. In this study, we\\nfocused on supporting Japanese in those LLMs and making a dataset for training\\nor tuning LLMs in Japanese. The dataset we constructed consisted of various\\ntasks, such as translation and knowledge tasks. In our experiment, we tuned an\\nexisting LLM using our dataset and evaluated the performance qualitatively. The\\nresults suggest that our dataset is possibly beneficial for LLMs. However, we\\nalso revealed some difficulties in constructing LLMs in languages other than\\nEnglish.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'This study constructed a Japanese chat dataset for tuning large language\\nmodels (LLMs), which consist of about 8.4 million records. Recently, LLMs have\\nbeen developed and gaining popularity. However, high-performing LLMs are\\nusually mainly for English. There are two ways to support languages other than\\nEnglish by those LLMs: constructing LLMs from scratch or tuning existing\\nmodels. However, in both ways, datasets are necessary parts. In this study, we\\nfocused on supporting Japanese in those LLMs and making a dataset for training\\nor tuning LLMs in Japanese. The dataset we constructed consisted of various\\ntasks, such as translation and knowledge tasks. In our experiment, we tuned an\\nexisting LLM using our dataset and evaluated the performance qualitatively. The\\nresults suggest that our dataset is possibly beneficial for LLMs. However, we\\nalso revealed some difficulties in constructing LLMs in languages other than\\nEnglish.'},\n",
       "  'authors': [{'name': 'Masanori Hirano'},\n",
       "   {'name': 'Masahiro Suzuki'},\n",
       "   {'name': 'Hiroki Sakaji'}],\n",
       "  'author_detail': {'name': 'Hiroki Sakaji'},\n",
       "  'author': 'Hiroki Sakaji',\n",
       "  'arxiv_comment': '12 pages',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.12720v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.12720v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.AI',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2309.17179v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2309.17179v1',\n",
       "  'updated': '2023-09-29T12:20:19Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=29, tm_hour=12, tm_min=20, tm_sec=19, tm_wday=4, tm_yday=272, tm_isdst=0),\n",
       "  'published': '2023-09-29T12:20:19Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=29, tm_hour=12, tm_min=20, tm_sec=19, tm_wday=4, tm_yday=272, tm_isdst=0),\n",
       "  'title': 'Alphazero-like Tree-Search can Guide Large Language Model Decoding and\\n  Training',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Alphazero-like Tree-Search can Guide Large Language Model Decoding and\\n  Training'},\n",
       "  'summary': \"Large language models (LLMs) typically employ sampling or beam search,\\naccompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and\\ndecoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via\\nPlanning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing\\ntree-search algorithms to guide multi-step reasoning. These methods mainly\\nfocus on LLMs' reasoning ability during inference and heavily rely on\\nhuman-designed prompts to activate LLM as a value function, which lacks general\\napplicability and scalability. To address these limitations, we present an\\nAlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically\\nillustrating how tree-search with a learned value function can guide LLMs'\\ndecoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a\\nlearned value function, our approach can be generally applied to different\\ntasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without\\nprompting advanced, large-scale models. (2) It can guide LLM's decoding during\\nboth inference and training. Empirical evaluations across reasoning, planning,\\nand RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees\\nwith a depth of 64.\",\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': \"Large language models (LLMs) typically employ sampling or beam search,\\naccompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and\\ndecoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via\\nPlanning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing\\ntree-search algorithms to guide multi-step reasoning. These methods mainly\\nfocus on LLMs' reasoning ability during inference and heavily rely on\\nhuman-designed prompts to activate LLM as a value function, which lacks general\\napplicability and scalability. To address these limitations, we present an\\nAlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically\\nillustrating how tree-search with a learned value function can guide LLMs'\\ndecoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a\\nlearned value function, our approach can be generally applied to different\\ntasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without\\nprompting advanced, large-scale models. (2) It can guide LLM's decoding during\\nboth inference and training. Empirical evaluations across reasoning, planning,\\nand RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees\\nwith a depth of 64.\"},\n",
       "  'authors': [{'name': 'Xidong Feng'},\n",
       "   {'name': 'Ziyu Wan'},\n",
       "   {'name': 'Muning Wen'},\n",
       "   {'name': 'Ying Wen'},\n",
       "   {'name': 'Weinan Zhang'},\n",
       "   {'name': 'Jun Wang'}],\n",
       "  'author_detail': {'name': 'Jun Wang'},\n",
       "  'author': 'Jun Wang',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2309.17179v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2309.17179v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.LG',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.LG',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None},\n",
       "   {'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2310.16343v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2310.16343v1',\n",
       "  'updated': '2023-10-25T03:58:49Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=25, tm_hour=3, tm_min=58, tm_sec=49, tm_wday=2, tm_yday=298, tm_isdst=0),\n",
       "  'published': '2023-10-25T03:58:49Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=25, tm_hour=3, tm_min=58, tm_sec=49, tm_wday=2, tm_yday=298, tm_isdst=0),\n",
       "  'title': 'A Comprehensive Evaluation of Constrained Text Generation for Large\\n  Language Models',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'A Comprehensive Evaluation of Constrained Text Generation for Large\\n  Language Models'},\n",
       "  'summary': \"Advancements in natural language generation (NLG) and large language models\\n(LLMs) have led to proficient text generation in various tasks. However,\\nintegrating intricate constraints into neural text generation, due to LLMs'\\nopacity, remains challenging. This study investigates constrained text\\ngeneration for LLMs, where predefined constraints are applied during LLM's\\ngeneration process. Our research examines multiple LLMs, including ChatGPT and\\nGPT-4, categorizing constraints into lexical, structural, and relation-based\\ntypes. We also present various benchmarks to facilitate fair evaluation. The\\nstudy addresses some key research questions, including the extent of LLMs'\\ncompliance with constraints. Results illuminate LLMs' capacity and deficiency\\nto incorporate constraints and provide insights for future developments in\\nconstrained text generation. Codes and datasets will be released upon\\nacceptance.\",\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': \"Advancements in natural language generation (NLG) and large language models\\n(LLMs) have led to proficient text generation in various tasks. However,\\nintegrating intricate constraints into neural text generation, due to LLMs'\\nopacity, remains challenging. This study investigates constrained text\\ngeneration for LLMs, where predefined constraints are applied during LLM's\\ngeneration process. Our research examines multiple LLMs, including ChatGPT and\\nGPT-4, categorizing constraints into lexical, structural, and relation-based\\ntypes. We also present various benchmarks to facilitate fair evaluation. The\\nstudy addresses some key research questions, including the extent of LLMs'\\ncompliance with constraints. Results illuminate LLMs' capacity and deficiency\\nto incorporate constraints and provide insights for future developments in\\nconstrained text generation. Codes and datasets will be released upon\\nacceptance.\"},\n",
       "  'authors': [{'name': 'Xiang Chen'}, {'name': 'Xiaojun Wan'}],\n",
       "  'author_detail': {'name': 'Xiaojun Wan'},\n",
       "  'author': 'Xiaojun Wan',\n",
       "  'arxiv_comment': 'Work in progress',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2310.16343v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2310.16343v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CL',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561432c-6450-4889-aa0b-f0275cad98c4",
   "metadata": {},
   "source": [
    "## Masterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a76a2ac5-5c82-4b38-a5e3-f9dccfa8e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import fitz # this is pymupdf\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class ArxivParser:\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    def __init__(self):\n",
    "        EntryData = Dict[str, str]\n",
    "        self.extracted_data: Dict[str, EntryData] = {} # store all the data present in Computer Science\n",
    "        \n",
    "    def get_results(self, max_results: int = 5, days: int = 60) -> pd.DataFrame:\n",
    "        # Construct the url with the query parameters\n",
    "        params = {\n",
    "            \"search_query\": f\"cat:cs*\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": max_results,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        url = self.base_url + \"?\" + requests.compat.urlencode(params)\n",
    "\n",
    "        # Send a GET request to the api endpoint\n",
    "        response = requests.get(url)\n",
    "        # Parse the response\n",
    "        entries = feedparser.parse(response.text).entries\n",
    "        # Loop through the entries\n",
    "        for entry in entries:\n",
    "            published_date = datetime.strptime(entry.published, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            current_date = datetime.now()\n",
    "            date_diff = (current_date - published_date).days\n",
    "            # Check if the date difference is less than or equal to the days parameter\n",
    "            if date_diff <= days:\n",
    "                id = entry.id\n",
    "                title = entry.title\n",
    "                link = entry.link\n",
    "                summary = entry.summary\n",
    "                # Get the pdf link by replacing the \"abs\" with \"pdf\" in the link\n",
    "                pdf_link = link.replace(\"abs\", \"pdf\")\n",
    "                # Get the pdf content by sending a GET request to the pdf link and opening it with fitz\n",
    "                pdf_content = requests.get(pdf_link).content\n",
    "                pdf_file = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "                # Extract the text from the pdf file\n",
    "                pdf_text = \"\"\n",
    "                for page in pdf_file:\n",
    "                    pdf_text += page.get_text()\n",
    "                # Store the extracted data in the dictionary with the id as the key\n",
    "                self.extracted_data[id] = {\n",
    "                    \"title\": title,\n",
    "                    \"published_date\": published_date,\n",
    "                    \"pdf_link\": pdf_link,\n",
    "                    \"summary\": summary,\n",
    "                    \"pdf_text\": pdf_text\n",
    "                }\n",
    "        # Convert the extracted data into a pandas dataframe\n",
    "        df = pd.DataFrame.from_dict(self.extracted_data, orient=\"index\")\n",
    "        return df\n",
    "        \n",
    "    def store_data(self, max_results: int = 10, days: int = 60) -> None:\n",
    "        # Call the get_results method and store the dataframe in the self.extracted_data attribute\n",
    "        self.extracted_data = self.get_results(max_results, days)\n",
    "        \n",
    "        # Create two new columns using lambda functions\n",
    "        self.extracted_data['summary_length'] = self.extracted_data.apply(lambda row: len(row['summary']), axis=1)\n",
    "        self.extracted_data['pdf_text_length'] = self.extracted_data.apply(lambda row: len(row['pdf_text']), axis=1)\n",
    "        self.extracted_data.to_pickle(\"master_data.pkl\")\n",
    "\n",
    "    def get_stored_data(self) -> pd.DataFrame:\n",
    "        # Return the self.extracted_data attribute\n",
    "        return self.extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c71344ed-655a-4a43-8d99-5047a7c545c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv=ArxivParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e563039-4fc6-4822-947f-d972072c7393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06769v1</th>\n",
       "      <td>Machine Translation Models are Zero-Shot Detec...</td>\n",
       "      <td>2024-01-12 18:59:02</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06769v1</td>\n",
       "      <td>Detecting the translation direction of paralle...</td>\n",
       "      <td>Machine Translation Models are\\nZero-Shot Dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06766v1</th>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>2024-01-12 18:58:26</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06766v1</td>\n",
       "      <td>Large language models demonstrate a remarkable...</td>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06765v1</th>\n",
       "      <td>Automated Test Case Repair Using Language Models</td>\n",
       "      <td>2024-01-12 18:56:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06765v1</td>\n",
       "      <td>Ensuring the quality of software systems throu...</td>\n",
       "      <td>1\\nAutomated Test Case Repair\\nUsing Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06763v1</th>\n",
       "      <td>Optimally Blending Honeypots into Production N...</td>\n",
       "      <td>2024-01-12 18:54:51</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06763v1</td>\n",
       "      <td>Honeypot is an important cyber defense techniq...</td>\n",
       "      <td>Optimally Blending Honeypots into Production\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06762v1</th>\n",
       "      <td>Seeing the roads through the trees: A benchmar...</td>\n",
       "      <td>2024-01-12 18:50:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06762v1</td>\n",
       "      <td>Fully understanding a complex high-resolution ...</td>\n",
       "      <td>SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Machine Translation Models are Zero-Shot Detec...   \n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06765v1   Automated Test Case Repair Using Language Models   \n",
       "http://arxiv.org/abs/2401.06763v1  Optimally Blending Honeypots into Production N...   \n",
       "http://arxiv.org/abs/2401.06762v1  Seeing the roads through the trees: A benchmar...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06769v1 2024-01-12 18:59:02   \n",
       "http://arxiv.org/abs/2401.06766v1 2024-01-12 18:58:26   \n",
       "http://arxiv.org/abs/2401.06765v1 2024-01-12 18:56:57   \n",
       "http://arxiv.org/abs/2401.06763v1 2024-01-12 18:54:51   \n",
       "http://arxiv.org/abs/2401.06762v1 2024-01-12 18:50:43   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06769v1  http://arxiv.org/pdf/2401.06769v1   \n",
       "http://arxiv.org/abs/2401.06766v1  http://arxiv.org/pdf/2401.06766v1   \n",
       "http://arxiv.org/abs/2401.06765v1  http://arxiv.org/pdf/2401.06765v1   \n",
       "http://arxiv.org/abs/2401.06763v1  http://arxiv.org/pdf/2401.06763v1   \n",
       "http://arxiv.org/abs/2401.06762v1  http://arxiv.org/pdf/2401.06762v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Detecting the translation direction of paralle...   \n",
       "http://arxiv.org/abs/2401.06766v1  Large language models demonstrate a remarkable...   \n",
       "http://arxiv.org/abs/2401.06765v1  Ensuring the quality of software systems throu...   \n",
       "http://arxiv.org/abs/2401.06763v1  Honeypot is an important cyber defense techniq...   \n",
       "http://arxiv.org/abs/2401.06762v1  Fully understanding a complex high-resolution ...   \n",
       "\n",
       "                                                                            pdf_text  \n",
       "http://arxiv.org/abs/2401.06769v1  Machine Translation Models are\\nZero-Shot Dete...  \n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...  \n",
       "http://arxiv.org/abs/2401.06765v1  1\\nAutomated Test Case Repair\\nUsing Language ...  \n",
       "http://arxiv.org/abs/2401.06763v1  Optimally Blending Honeypots into Production\\n...  \n",
       "http://arxiv.org/abs/2401.06762v1  SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9f07eca-22ef-437f-b6d2-89bf94dc159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv.store_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6756c82d-507d-4efe-94c2-0cd7c16434fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06769v1</th>\n",
       "      <td>Machine Translation Models are Zero-Shot Detec...</td>\n",
       "      <td>2024-01-12 18:59:02</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06769v1</td>\n",
       "      <td>Detecting the translation direction of paralle...</td>\n",
       "      <td>Machine Translation Models are\\nZero-Shot Dete...</td>\n",
       "      <td>917</td>\n",
       "      <td>43596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06766v1</th>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>2024-01-12 18:58:26</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06766v1</td>\n",
       "      <td>Large language models demonstrate a remarkable...</td>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>1219</td>\n",
       "      <td>79334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06765v1</th>\n",
       "      <td>Automated Test Case Repair Using Language Models</td>\n",
       "      <td>2024-01-12 18:56:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06765v1</td>\n",
       "      <td>Ensuring the quality of software systems throu...</td>\n",
       "      <td>1\\nAutomated Test Case Repair\\nUsing Language ...</td>\n",
       "      <td>1360</td>\n",
       "      <td>118802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06763v1</th>\n",
       "      <td>Optimally Blending Honeypots into Production N...</td>\n",
       "      <td>2024-01-12 18:54:51</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06763v1</td>\n",
       "      <td>Honeypot is an important cyber defense techniq...</td>\n",
       "      <td>Optimally Blending Honeypots into Production\\n...</td>\n",
       "      <td>1027</td>\n",
       "      <td>56963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06762v1</th>\n",
       "      <td>Seeing the roads through the trees: A benchmar...</td>\n",
       "      <td>2024-01-12 18:50:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06762v1</td>\n",
       "      <td>Fully understanding a complex high-resolution ...</td>\n",
       "      <td>SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...</td>\n",
       "      <td>1521</td>\n",
       "      <td>21421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06761v1</th>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>2024-01-12 18:50:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06761v1</td>\n",
       "      <td>The massive adoption of large language models ...</td>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>951</td>\n",
       "      <td>43670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06760v1</th>\n",
       "      <td>Navigating the Metrics Maze: Reconciling Score...</td>\n",
       "      <td>2024-01-12 18:47:40</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06760v1</td>\n",
       "      <td>Ten years ago a single metric, BLEU, governed ...</td>\n",
       "      <td>Navigating the Metrics Maze:\\nReconciling Scor...</td>\n",
       "      <td>1176</td>\n",
       "      <td>51549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06757v1</th>\n",
       "      <td>Synthetic Data Generation Framework, Dataset, ...</td>\n",
       "      <td>2024-01-12 18:44:01</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06757v1</td>\n",
       "      <td>Pedestrian intention prediction is crucial for...</td>\n",
       "      <td>Synthetic Data Generation Framework, Dataset, ...</td>\n",
       "      <td>1102</td>\n",
       "      <td>41723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06755v1</th>\n",
       "      <td>Solving the Discretised Multiphase Flow Equati...</td>\n",
       "      <td>2024-01-12 18:42:42</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06755v1</td>\n",
       "      <td>This paper solves the multiphase flow equation...</td>\n",
       "      <td>Solving the Discretised Multiphase Flow Equati...</td>\n",
       "      <td>1543</td>\n",
       "      <td>94492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06752v1</th>\n",
       "      <td>Stylometry Analysis of Multi-authored Document...</td>\n",
       "      <td>2024-01-12 18:36:41</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06752v1</td>\n",
       "      <td>In recent years, the increasing use of Artific...</td>\n",
       "      <td>Noname manuscript No.\\n(will be inserted by th...</td>\n",
       "      <td>1365</td>\n",
       "      <td>46903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Machine Translation Models are Zero-Shot Detec...   \n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06765v1   Automated Test Case Repair Using Language Models   \n",
       "http://arxiv.org/abs/2401.06763v1  Optimally Blending Honeypots into Production N...   \n",
       "http://arxiv.org/abs/2401.06762v1  Seeing the roads through the trees: A benchmar...   \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06760v1  Navigating the Metrics Maze: Reconciling Score...   \n",
       "http://arxiv.org/abs/2401.06757v1  Synthetic Data Generation Framework, Dataset, ...   \n",
       "http://arxiv.org/abs/2401.06755v1  Solving the Discretised Multiphase Flow Equati...   \n",
       "http://arxiv.org/abs/2401.06752v1  Stylometry Analysis of Multi-authored Document...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06769v1 2024-01-12 18:59:02   \n",
       "http://arxiv.org/abs/2401.06766v1 2024-01-12 18:58:26   \n",
       "http://arxiv.org/abs/2401.06765v1 2024-01-12 18:56:57   \n",
       "http://arxiv.org/abs/2401.06763v1 2024-01-12 18:54:51   \n",
       "http://arxiv.org/abs/2401.06762v1 2024-01-12 18:50:43   \n",
       "http://arxiv.org/abs/2401.06761v1 2024-01-12 18:50:36   \n",
       "http://arxiv.org/abs/2401.06760v1 2024-01-12 18:47:40   \n",
       "http://arxiv.org/abs/2401.06757v1 2024-01-12 18:44:01   \n",
       "http://arxiv.org/abs/2401.06755v1 2024-01-12 18:42:42   \n",
       "http://arxiv.org/abs/2401.06752v1 2024-01-12 18:36:41   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06769v1  http://arxiv.org/pdf/2401.06769v1   \n",
       "http://arxiv.org/abs/2401.06766v1  http://arxiv.org/pdf/2401.06766v1   \n",
       "http://arxiv.org/abs/2401.06765v1  http://arxiv.org/pdf/2401.06765v1   \n",
       "http://arxiv.org/abs/2401.06763v1  http://arxiv.org/pdf/2401.06763v1   \n",
       "http://arxiv.org/abs/2401.06762v1  http://arxiv.org/pdf/2401.06762v1   \n",
       "http://arxiv.org/abs/2401.06761v1  http://arxiv.org/pdf/2401.06761v1   \n",
       "http://arxiv.org/abs/2401.06760v1  http://arxiv.org/pdf/2401.06760v1   \n",
       "http://arxiv.org/abs/2401.06757v1  http://arxiv.org/pdf/2401.06757v1   \n",
       "http://arxiv.org/abs/2401.06755v1  http://arxiv.org/pdf/2401.06755v1   \n",
       "http://arxiv.org/abs/2401.06752v1  http://arxiv.org/pdf/2401.06752v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Detecting the translation direction of paralle...   \n",
       "http://arxiv.org/abs/2401.06766v1  Large language models demonstrate a remarkable...   \n",
       "http://arxiv.org/abs/2401.06765v1  Ensuring the quality of software systems throu...   \n",
       "http://arxiv.org/abs/2401.06763v1  Honeypot is an important cyber defense techniq...   \n",
       "http://arxiv.org/abs/2401.06762v1  Fully understanding a complex high-resolution ...   \n",
       "http://arxiv.org/abs/2401.06761v1  The massive adoption of large language models ...   \n",
       "http://arxiv.org/abs/2401.06760v1  Ten years ago a single metric, BLEU, governed ...   \n",
       "http://arxiv.org/abs/2401.06757v1  Pedestrian intention prediction is crucial for...   \n",
       "http://arxiv.org/abs/2401.06755v1  This paper solves the multiphase flow equation...   \n",
       "http://arxiv.org/abs/2401.06752v1  In recent years, the increasing use of Artific...   \n",
       "\n",
       "                                                                            pdf_text  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Machine Translation Models are\\nZero-Shot Dete...   \n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06765v1  1\\nAutomated Test Case Repair\\nUsing Language ...   \n",
       "http://arxiv.org/abs/2401.06763v1  Optimally Blending Honeypots into Production\\n...   \n",
       "http://arxiv.org/abs/2401.06762v1  SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...   \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06760v1  Navigating the Metrics Maze:\\nReconciling Scor...   \n",
       "http://arxiv.org/abs/2401.06757v1  Synthetic Data Generation Framework, Dataset, ...   \n",
       "http://arxiv.org/abs/2401.06755v1  Solving the Discretised Multiphase Flow Equati...   \n",
       "http://arxiv.org/abs/2401.06752v1  Noname manuscript No.\\n(will be inserted by th...   \n",
       "\n",
       "                                   summary_length  pdf_text_length  \n",
       "http://arxiv.org/abs/2401.06769v1             917            43596  \n",
       "http://arxiv.org/abs/2401.06766v1            1219            79334  \n",
       "http://arxiv.org/abs/2401.06765v1            1360           118802  \n",
       "http://arxiv.org/abs/2401.06763v1            1027            56963  \n",
       "http://arxiv.org/abs/2401.06762v1            1521            21421  \n",
       "http://arxiv.org/abs/2401.06761v1             951            43670  \n",
       "http://arxiv.org/abs/2401.06760v1            1176            51549  \n",
       "http://arxiv.org/abs/2401.06757v1            1102            41723  \n",
       "http://arxiv.org/abs/2401.06755v1            1543            94492  \n",
       "http://arxiv.org/abs/2401.06752v1            1365            46903  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.get_stored_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b71b1-7e8f-49d2-bc80-fe1cd818e1d7",
   "metadata": {},
   "source": [
    "**read data from pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7a91cce3-450e-4774-88d6-feb5cea00f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06769v1</th>\n",
       "      <td>Machine Translation Models are Zero-Shot Detec...</td>\n",
       "      <td>2024-01-12 18:59:02</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06769v1</td>\n",
       "      <td>Detecting the translation direction of paralle...</td>\n",
       "      <td>Machine Translation Models are\\nZero-Shot Dete...</td>\n",
       "      <td>917</td>\n",
       "      <td>43596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06766v1</th>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>2024-01-12 18:58:26</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06766v1</td>\n",
       "      <td>Large language models demonstrate a remarkable...</td>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>1219</td>\n",
       "      <td>79334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06765v1</th>\n",
       "      <td>Automated Test Case Repair Using Language Models</td>\n",
       "      <td>2024-01-12 18:56:57</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06765v1</td>\n",
       "      <td>Ensuring the quality of software systems throu...</td>\n",
       "      <td>1\\nAutomated Test Case Repair\\nUsing Language ...</td>\n",
       "      <td>1360</td>\n",
       "      <td>118802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06763v1</th>\n",
       "      <td>Optimally Blending Honeypots into Production N...</td>\n",
       "      <td>2024-01-12 18:54:51</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06763v1</td>\n",
       "      <td>Honeypot is an important cyber defense techniq...</td>\n",
       "      <td>Optimally Blending Honeypots into Production\\n...</td>\n",
       "      <td>1027</td>\n",
       "      <td>56963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06762v1</th>\n",
       "      <td>Seeing the roads through the trees: A benchmar...</td>\n",
       "      <td>2024-01-12 18:50:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06762v1</td>\n",
       "      <td>Fully understanding a complex high-resolution ...</td>\n",
       "      <td>SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...</td>\n",
       "      <td>1521</td>\n",
       "      <td>21421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06761v1</th>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>2024-01-12 18:50:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06761v1</td>\n",
       "      <td>The massive adoption of large language models ...</td>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>951</td>\n",
       "      <td>43670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06760v1</th>\n",
       "      <td>Navigating the Metrics Maze: Reconciling Score...</td>\n",
       "      <td>2024-01-12 18:47:40</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06760v1</td>\n",
       "      <td>Ten years ago a single metric, BLEU, governed ...</td>\n",
       "      <td>Navigating the Metrics Maze:\\nReconciling Scor...</td>\n",
       "      <td>1176</td>\n",
       "      <td>51549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06757v1</th>\n",
       "      <td>Synthetic Data Generation Framework, Dataset, ...</td>\n",
       "      <td>2024-01-12 18:44:01</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06757v1</td>\n",
       "      <td>Pedestrian intention prediction is crucial for...</td>\n",
       "      <td>Synthetic Data Generation Framework, Dataset, ...</td>\n",
       "      <td>1102</td>\n",
       "      <td>41723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06755v1</th>\n",
       "      <td>Solving the Discretised Multiphase Flow Equati...</td>\n",
       "      <td>2024-01-12 18:42:42</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06755v1</td>\n",
       "      <td>This paper solves the multiphase flow equation...</td>\n",
       "      <td>Solving the Discretised Multiphase Flow Equati...</td>\n",
       "      <td>1543</td>\n",
       "      <td>94492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06752v1</th>\n",
       "      <td>Stylometry Analysis of Multi-authored Document...</td>\n",
       "      <td>2024-01-12 18:36:41</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06752v1</td>\n",
       "      <td>In recent years, the increasing use of Artific...</td>\n",
       "      <td>Noname manuscript No.\\n(will be inserted by th...</td>\n",
       "      <td>1365</td>\n",
       "      <td>46903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Machine Translation Models are Zero-Shot Detec...   \n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06765v1   Automated Test Case Repair Using Language Models   \n",
       "http://arxiv.org/abs/2401.06763v1  Optimally Blending Honeypots into Production N...   \n",
       "http://arxiv.org/abs/2401.06762v1  Seeing the roads through the trees: A benchmar...   \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06760v1  Navigating the Metrics Maze: Reconciling Score...   \n",
       "http://arxiv.org/abs/2401.06757v1  Synthetic Data Generation Framework, Dataset, ...   \n",
       "http://arxiv.org/abs/2401.06755v1  Solving the Discretised Multiphase Flow Equati...   \n",
       "http://arxiv.org/abs/2401.06752v1  Stylometry Analysis of Multi-authored Document...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06769v1 2024-01-12 18:59:02   \n",
       "http://arxiv.org/abs/2401.06766v1 2024-01-12 18:58:26   \n",
       "http://arxiv.org/abs/2401.06765v1 2024-01-12 18:56:57   \n",
       "http://arxiv.org/abs/2401.06763v1 2024-01-12 18:54:51   \n",
       "http://arxiv.org/abs/2401.06762v1 2024-01-12 18:50:43   \n",
       "http://arxiv.org/abs/2401.06761v1 2024-01-12 18:50:36   \n",
       "http://arxiv.org/abs/2401.06760v1 2024-01-12 18:47:40   \n",
       "http://arxiv.org/abs/2401.06757v1 2024-01-12 18:44:01   \n",
       "http://arxiv.org/abs/2401.06755v1 2024-01-12 18:42:42   \n",
       "http://arxiv.org/abs/2401.06752v1 2024-01-12 18:36:41   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06769v1  http://arxiv.org/pdf/2401.06769v1   \n",
       "http://arxiv.org/abs/2401.06766v1  http://arxiv.org/pdf/2401.06766v1   \n",
       "http://arxiv.org/abs/2401.06765v1  http://arxiv.org/pdf/2401.06765v1   \n",
       "http://arxiv.org/abs/2401.06763v1  http://arxiv.org/pdf/2401.06763v1   \n",
       "http://arxiv.org/abs/2401.06762v1  http://arxiv.org/pdf/2401.06762v1   \n",
       "http://arxiv.org/abs/2401.06761v1  http://arxiv.org/pdf/2401.06761v1   \n",
       "http://arxiv.org/abs/2401.06760v1  http://arxiv.org/pdf/2401.06760v1   \n",
       "http://arxiv.org/abs/2401.06757v1  http://arxiv.org/pdf/2401.06757v1   \n",
       "http://arxiv.org/abs/2401.06755v1  http://arxiv.org/pdf/2401.06755v1   \n",
       "http://arxiv.org/abs/2401.06752v1  http://arxiv.org/pdf/2401.06752v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Detecting the translation direction of paralle...   \n",
       "http://arxiv.org/abs/2401.06766v1  Large language models demonstrate a remarkable...   \n",
       "http://arxiv.org/abs/2401.06765v1  Ensuring the quality of software systems throu...   \n",
       "http://arxiv.org/abs/2401.06763v1  Honeypot is an important cyber defense techniq...   \n",
       "http://arxiv.org/abs/2401.06762v1  Fully understanding a complex high-resolution ...   \n",
       "http://arxiv.org/abs/2401.06761v1  The massive adoption of large language models ...   \n",
       "http://arxiv.org/abs/2401.06760v1  Ten years ago a single metric, BLEU, governed ...   \n",
       "http://arxiv.org/abs/2401.06757v1  Pedestrian intention prediction is crucial for...   \n",
       "http://arxiv.org/abs/2401.06755v1  This paper solves the multiphase flow equation...   \n",
       "http://arxiv.org/abs/2401.06752v1  In recent years, the increasing use of Artific...   \n",
       "\n",
       "                                                                            pdf_text  \\\n",
       "http://arxiv.org/abs/2401.06769v1  Machine Translation Models are\\nZero-Shot Dete...   \n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06765v1  1\\nAutomated Test Case Repair\\nUsing Language ...   \n",
       "http://arxiv.org/abs/2401.06763v1  Optimally Blending Honeypots into Production\\n...   \n",
       "http://arxiv.org/abs/2401.06762v1  SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...   \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "http://arxiv.org/abs/2401.06760v1  Navigating the Metrics Maze:\\nReconciling Scor...   \n",
       "http://arxiv.org/abs/2401.06757v1  Synthetic Data Generation Framework, Dataset, ...   \n",
       "http://arxiv.org/abs/2401.06755v1  Solving the Discretised Multiphase Flow Equati...   \n",
       "http://arxiv.org/abs/2401.06752v1  Noname manuscript No.\\n(will be inserted by th...   \n",
       "\n",
       "                                   summary_length  pdf_text_length  \n",
       "http://arxiv.org/abs/2401.06769v1             917            43596  \n",
       "http://arxiv.org/abs/2401.06766v1            1219            79334  \n",
       "http://arxiv.org/abs/2401.06765v1            1360           118802  \n",
       "http://arxiv.org/abs/2401.06763v1            1027            56963  \n",
       "http://arxiv.org/abs/2401.06762v1            1521            21421  \n",
       "http://arxiv.org/abs/2401.06761v1             951            43670  \n",
       "http://arxiv.org/abs/2401.06760v1            1176            51549  \n",
       "http://arxiv.org/abs/2401.06757v1            1102            41723  \n",
       "http://arxiv.org/abs/2401.06755v1            1543            94492  \n",
       "http://arxiv.org/abs/2401.06752v1            1365            46903  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('master_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98952d0d-87b4-4f25-bdf0-9c6b36b32883",
   "metadata": {},
   "source": [
    "## User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b3dc8130-5e61-4c35-aadb-d2e6c373ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self):\n",
    "        self.master_data = pd.read_pickle('master_data.pkl')\n",
    "        self.user_data = None # this will store the filtered dataframe\n",
    "    \n",
    "    def search(self, query):\n",
    "        mask = self.master_data[\"pdf_text\"].str.contains(query) # create a boolean mask\n",
    "        self.user_data = self.master_data[mask] # filter the master_data using the mask\n",
    "        return self.user_data\n",
    "    \n",
    "    def feed(self):\n",
    "        return self.user_data.sort_values(\"published_date\", ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fdd2bfc8-bff1-4f06-952f-ca7a8efcca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1=User()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa97ed20-6054-432b-a0d8-d92263d6ab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06766v1</th>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>2024-01-12 18:58:26</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06766v1</td>\n",
       "      <td>Large language models demonstrate a remarkable...</td>\n",
       "      <td>Mind Your Format: Towards Consistent Evaluatio...</td>\n",
       "      <td>1219</td>\n",
       "      <td>79334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06762v1</th>\n",
       "      <td>Seeing the roads through the trees: A benchmar...</td>\n",
       "      <td>2024-01-12 18:50:43</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06762v1</td>\n",
       "      <td>Fully understanding a complex high-resolution ...</td>\n",
       "      <td>SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...</td>\n",
       "      <td>1521</td>\n",
       "      <td>21421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.06761v1</th>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>2024-01-12 18:50:36</td>\n",
       "      <td>http://arxiv.org/pdf/2401.06761v1</td>\n",
       "      <td>The massive adoption of large language models ...</td>\n",
       "      <td>APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...</td>\n",
       "      <td>951</td>\n",
       "      <td>43670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06762v1  Seeing the roads through the trees: A benchmar...   \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.06766v1 2024-01-12 18:58:26   \n",
       "http://arxiv.org/abs/2401.06762v1 2024-01-12 18:50:43   \n",
       "http://arxiv.org/abs/2401.06761v1 2024-01-12 18:50:36   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.06766v1  http://arxiv.org/pdf/2401.06766v1   \n",
       "http://arxiv.org/abs/2401.06762v1  http://arxiv.org/pdf/2401.06762v1   \n",
       "http://arxiv.org/abs/2401.06761v1  http://arxiv.org/pdf/2401.06761v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.06766v1  Large language models demonstrate a remarkable...   \n",
       "http://arxiv.org/abs/2401.06762v1  Fully understanding a complex high-resolution ...   \n",
       "http://arxiv.org/abs/2401.06761v1  The massive adoption of large language models ...   \n",
       "\n",
       "                                                                            pdf_text  \\\n",
       "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...   \n",
       "http://arxiv.org/abs/2401.06762v1  SEEING THE ROADS THROUGH THE TREES:\\nA BENCHMA...   \n",
       "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...   \n",
       "\n",
       "                                   summary_length  pdf_text_length  \n",
       "http://arxiv.org/abs/2401.06766v1            1219            79334  \n",
       "http://arxiv.org/abs/2401.06762v1            1521            21421  \n",
       "http://arxiv.org/abs/2401.06761v1             951            43670  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1.search(\"llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82b778-fc40-43b7-bf7b-d36226af75f9",
   "metadata": {},
   "source": [
    "## Test the Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760cee13-52f1-432e-97df-7b0d2506ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                               title  ... pdf_text_length\n",
      "http://arxiv.org/abs/2401.06766v1  Mind Your Format: Towards Consistent Evaluatio...  ...           79334\n",
      "http://arxiv.org/abs/2401.06762v1  Seeing the roads through the trees: A benchmar...  ...           21421\n",
      "http://arxiv.org/abs/2401.06761v1  APAR: LLMs Can Do Auto-Parallel Auto-Regressiv...  ...           43670\n",
      "\n",
      "[3 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "! python3 extract_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed86de-15b7-4bef-800b-02aa6fc0d758",
   "metadata": {},
   "source": [
    "## Testing with PyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d152854-12b2-4813-9ae0-49f046814553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py     extract_data.py master_data.pkl research.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343914ab-9a44-4d2c-975f-f53b0c939e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
