{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb673601-3114-44c5-b8a6-2e4775681f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Extraction from arxiv api\"\"\"\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "import requests\n",
    "import feedparser\n",
    "import json # import json instead of pandas\n",
    "import fitz  # this is pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "716b4588-d8e0-457e-8503-21c72269ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_SEARCH_QUERY:str = \"cat:cs.CV OR cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.NE OR cat:stat.ML OR cat:cs.IR\"\n",
    "\n",
    "\n",
    "class ArxivParser:\n",
    "    \"\"\"Extract & Parse data from the Arxiv API\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    def __init__(self, data_path=\"../data/\"):\n",
    "        self.extracted_data: List[Dict[str, str]] = [] # create an empty list instead of a dataframe\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            os.makedirs(data_path)\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def get_results(\n",
    "        self,\n",
    "        max_results: int = 5,\n",
    "        days: int = 60,\n",
    "        search_query: str = STANDARD_SEARCH_QUERY,\n",
    "    ) -> List[Dict[str, str]]: # return a list of dictionaries instead of a dataframe\n",
    "        # Construct the url with the query parameters\n",
    "        params = {\n",
    "            \"search_query\": search_query,\n",
    "            \"start\": 0,\n",
    "            \"max_results\": max_results,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\",\n",
    "        }\n",
    "        url = self.base_url + \"?\" + requests.compat.urlencode(params)\n",
    "\n",
    "        # Send a GET request to the api endpoint\n",
    "        response = requests.get(url)\n",
    "        # Parse the response\n",
    "        entries = feedparser.parse(response.text).entries\n",
    "\n",
    "        downloaded_data: List[Dict[str, str]] = [] # create an empty list instead of a dictionary\n",
    "\n",
    "        # Loop through the entries\n",
    "        for entry in tqdm(entries):\n",
    "            published_date = datetime.strptime(entry.published, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            current_date = datetime.now()\n",
    "            date_diff = (current_date - published_date).days\n",
    "\n",
    "            # Check if the date difference is less than or equal to the days parameter\n",
    "            if date_diff <= days:\n",
    "                id = entry.id\n",
    "                title = entry.title\n",
    "                link = entry.link\n",
    "                summary = entry.summary\n",
    "\n",
    "                # Get the pdf link by replacing the \"abs\" with \"pdf\" in the link\n",
    "                pdf_link = link.replace(\"abs\", \"pdf\")\n",
    "                # Get the pdf content by sending a GET request to the pdf link and opening it with fitz\n",
    "                pdf_content = requests.get(pdf_link).content\n",
    "                pdf_file = fitz.open(stream=pdf_content, filetype=\"pdf\")\n",
    "                # Extract the text from the pdf file\n",
    "                pdf_text = \"\"\n",
    "                for page in pdf_file:\n",
    "                    pdf_text += page.get_text()\n",
    "                # Store the extracted data in a dictionary and append it to the list\n",
    "                downloaded_data.append({\n",
    "                    \"id\": id,\n",
    "                    \"title\": title,\n",
    "                    \"published_date\": published_date,\n",
    "                    \"pdf_link\": pdf_link,\n",
    "                    \"summary\": summary,\n",
    "                    \"pdf_text\": pdf_text,\n",
    "                })\n",
    "        # Extend the extracted data list with the downloaded data list\n",
    "        self.extracted_data.extend(downloaded_data)\n",
    "        # Return the list as it is\n",
    "        return self.extracted_data\n",
    "\n",
    "\n",
    "    def store_data(\n",
    "        self,\n",
    "        save_file_name: str = \"master_data.json\",\n",
    "        max_results: int = 10,\n",
    "        days: int = 60,\n",
    "    ) -> None:\n",
    "        self.extracted_data = self.get_results(max_results, days)\n",
    "    \n",
    "        assert len(self.extracted_data) > 0, \"Got no results with the search query\"\n",
    "        # Convert the published_date to a string format\n",
    "        for data in self.extracted_data:\n",
    "            data[\"published_date\"] = data[\"published_date\"].strftime(\"%Y-%m-%d\")\n",
    "        # Save the list of dictionaries as a json file\n",
    "        save_location = os.path.join(self.data_path, save_file_name)\n",
    "        with open(save_location, \"w\") as f:\n",
    "            json.dump(self.extracted_data, f, indent=4)\n",
    "\n",
    "    def get_stored_data(self):\n",
    "        # Return the self.extracted_data attribute\n",
    "\n",
    "        assert len(self.extracted_data) != 0, \"Please store data first\"\n",
    "        return self.extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1867071-49cf-4cd4-b914-6ff45bf493a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ArxivParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa9824a-f682-4596-ba33-cb1c59fd7cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ArxivParser at 0x106bc14d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b691f1ee-3b72-48fc-b087-8d301ced4f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'http://arxiv.org/abs/2402.03312v1',\n",
       "  'title': 'Test-Time Adaptation for Depth Completion',\n",
       "  'published_date': datetime.datetime(2024, 2, 5, 18, 59, 52),\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2402.03312v1',\n",
       "  'summary': 'It is common to observe performance degradation when transferring models\\ntrained on some (source) datasets to target testing data due to a domain gap\\nbetween them. Existing methods for bridging this gap, such as domain adaptation\\n(DA), may require the source data on which the model was trained (often not\\navailable), while others, i.e., source-free DA, require many passes through the\\ntesting data. We propose an online test-time adaptation method for depth\\ncompletion, the task of inferring a dense depth map from a single image and\\nassociated sparse depth map, that closes the performance gap in a single pass.\\nWe first present a study on how the domain shift in each data modality affects\\nmodel performance. Based on our observations that the sparse depth modality\\nexhibits a much smaller covariate shift than the image, we design an embedding\\nmodule trained in the source domain that preserves a mapping from features\\nencoding only sparse depth to those encoding image and sparse depth. During\\ntest time, sparse depth features are projected using this map as a proxy for\\nsource domain features and are used as guidance to train a set of auxiliary\\nparameters (i.e., adaptation layer) to align image and sparse depth features\\nfrom the target test domain to that of the source domain. We evaluate our\\nmethod on indoor and outdoor scenarios and show that it improves over baselines\\nby an average of 21.1%.',\n",
       "  'pdf_text': 'Test-Time Adaptation for Depth Completion\\nHyoungseob Park\\nYale Vision Lab\\nhyoungseob.park@yale.edu\\nAnjali Gupta\\nYale Vision Lab\\nanjali.gupta@yale.edu\\nAlex Wong\\nYale Vision Lab\\nalex.wong@yale.edu\\nAbstract\\nIt is common to observe performance degradation when\\ntransferring models trained on some (source) datasets to tar-\\nget testing data due to a domain gap between them. Existing\\nmethods for bridging this gap, such as domain adaptation\\n(DA), may require the source data on which the model was\\ntrained (often not available), while others, i.e., source-free\\nDA, require many passes through the testing data. We pro-\\npose an online test-time adaptation method for depth com-\\npletion, the task of inferring a dense depth map from a single\\nimage and associated sparse depth map, that closes the per-\\nformance gap in a single pass. We first present a study on\\nhow the domain shift in each data modality affects model\\nperformance. Based on our observations that the sparse\\ndepth modality exhibits a much smaller covariate shift than\\nthe image, we design an embedding module trained in the\\nsource domain that preserves a mapping from features en-\\ncoding only sparse depth to those encoding image and sparse\\ndepth. During test time, sparse depth features are projected\\nusing this map as a proxy for source domain features and are\\nused as guidance to train a set of auxiliary parameters (i.e.,\\nadaptation layer) to align image and sparse depth features\\nfrom the target test domain to that of the source domain. We\\nevaluate our method on indoor and outdoor scenarios and\\nshow that it improves over baselines by an average of 21.1%.\\n1. Introduction\\nReconstructing the 3-dimensional (3D) structure of an envi-\\nronment can support a number of spatial tasks, from robotic\\nnavigation and manipulation to augmented and virtual reality.\\nMost systems addressing these tasks are built for sensor plat-\\nforms equipped with range (i.e., lidar or radar) or optics (i.e.,\\ncamera or sensors). While range sensors can measure the\\n3D coordinates of the surrounding space, they often yield\\npoint clouds that are sparse. Likewise, these coordinates\\ncan also be estimated from images by means of Structure-\\nfrom-Motion (SfM) or Visual Inertial Odometry (VIO). For\\nthe goal of dense mapping, depth completion is the task of\\nrecovering the dense depth of a 3D scene as observed from\\na sparse point cloud, which is often post-processed into a\\nsparse depth map by projecting the points onto the image\\nplane, and guided by a synchronized calibrated image.\\nTraining a depth completion model can be done in a su-\\npervised (using ground truth) or unsupervised (using SfM)\\nmanner. The former dominates in performance, but requires\\nexpensive annotations that are often unavailable; the latter\\nuses unannotated images, but they must satisfy SfM assump-\\ntions between frames, i.e., motion, covisibility, etc. Like\\nmost learning-based methods, models trained under both\\nparadigms typically experience a performance drop when\\ntested on a new dataset due to a covariate shift, i.e., domain\\ngap. As we can only assume that a single pair of image\\nand sparse depth map is available in the target domain for\\nthe depth completion, models belonging to either learning\\nparadigms cannot easily be trained or adapted to the new\\ndomain even when given the testing data. We focus on test-\\ntime adaptation (TTA) for depth completion, where one is\\ngiven access to the test data in a stream, i.e., one batch at a\\ntime, without being able to revisit previously-seen examples.\\nThe goal is to learn causally and to quickly adapt a set of\\npre-existing weights trained on a source domain to a target\\ntest domain, so one can reduce the performance gap.\\nWe begin with some motivating observations on the ef-\\nfects of the domain gap: (i) Errors in target domain tend\\nto be higher when feed both the image and sparse depth as\\ninput rather than sparse depth only, as shown in Fig. 1. This\\nimplies that the depth modality exhibits a smaller covariate\\nshift between the source and target domains than the image\\nmodality, to the extent that forgoing the image altogether\\noften yields superior results than using either both sparse\\ndepth and image or the image alone. (ii) Yet, when operating\\nin the source domain, we observe the opposite effect â€“ forgo-\\ning the image is detrimental to performance. Naturally, this\\nbegs the question: How should one leverage data modalities\\nthat are less sensitive to the domain shift (e.g., sparse depth)\\nto support alignment between source and target domains for\\nmodalities that are more sensitive (e.g., RGB image)?\\nTo answer this question, we investigate a test-time adap-\\ntation approach that learns an embedding for guiding the\\nmodel parameter update by exploiting the data modality\\n(sparse depth) that is less sensitive to the domain shift. The\\n1\\narXiv:2402.03312v1  [cs.CV]  5 Feb 2024\\nembedding module maps the latent features encoding sparse\\ndepth to the latent features encoding both image and sparse\\ndepth. The mapping is trained in the source domain and\\nfrozen when deployed to the target domain for adaptation.\\nDuring test time, sparse depth is first fed through the en-\\ncoder and mapped, through the embedding module, to yield\\na proxy for image and sparse depth embeddings from the\\nsource domain â€“ we refer to the embedded sparse depth fea-\\ntures as proxy embeddings. Note: As the mapping is learned\\nin the source domain, the proxy embeddings will also follow\\nthe distribution of source image and sparse depth embed-\\ndings. Next, both image and sparse depth from the target\\ntest domain are fed as input to the encoder. By maximiz-\\ning the similarity between test-time input embeddings and\\nthe proxy embeddings, we align the target distribution to\\nthat of the source to reduce the domain gap. When used in\\nconjunction with typical loss functions to penalize discrepan-\\ncies between predictions and input sparse depth, and abrupt\\ndepth transitions, i.e., Total Variation, the embeddings serve\\nas regularization to guide the model parameter update and\\nprevent excessive drift from those trained on the source data.\\nFollowing test-time adaptation conventions, we assume\\nlimited computational resources, and that inputs arrive in a\\nstream of small batches and must be processed within a time\\nbudget without access to the past data. To ensure fast model\\nupdating under these constraints, we deploy auxiliary param-\\neters, or a adaptation layer, to be updated while freezing the\\nrest of the network â€“ thus achieving low-cost adaptation. We\\ndemonstrate our method in both indoor (VIO) and outdoor\\n(lidar) settings across six datasets, where we not only target\\ntypical adaptation scenarios where the shift exists between\\nreal and synthetic data domains with similar scenes, i.e. from\\nKITTI [41] to Virtual KITTI [10], but also between differ-\\nent scene layouts, i.e., from VOID [46] to NYUv2 [27] and\\nSceneNet [26]. Our proxy embeddings consistently improve\\nover baselines by an average of 21.09% across all methods\\nand datasets. To the best of our knowledge, we are the first\\nto introduce test-time adaptation for depth completion.\\n2. Related work\\nTest Time Adaptation (TTA) aims to adapt a given model,\\npretrained on source data, to test data without access to the\\nsource training data. Related fields along this vein include\\nunsupervised domain adaptation [11, 29], which utilizes\\nsource domain data (in practice, this may not be available) for\\nadaptation, and source-free domain adaptation [20], which\\ndoes not assume access to source data, but allows access\\nto test data on multiple passes. In contrast, we focus on\\ntest-time adaptation where we do not have access to source\\ndata and must adapt to test data in a single pass.\\nPrevious studies have proposed strategies to select the\\nsource modelâ€™s component to be preserved, such as the class\\nprototypes extracted from the source data [5, 23, 34], the\\nsubset of source model parameters [17, 43], and the discrim-\\ninative feature from the self-supervised learning (SSL) [5].\\nFor instance, [43] proposes TENT, a simple but effective\\nbatch-norm layer adaptation with entropy minimization for\\nfully test-time adaptation. TTT [40] performs classification-\\nlayer adaptation by updating the last linear layer of the\\nsource model; [23] extends this with TTT++ and utilizes\\njoint task-specific and model-specific information based on\\nself-supervised learning. [17] presents T3A, an optimization-\\nfree classifier adjustment module. [5] uses shift-agnostic\\nweight regularization (SWR) to prevent an effect from the\\nerroneous signal in test time, jointly with the nearest source\\nprototype classifier and a self-supervised proxy task. [2]\\nproposes a contrastive learning with an online pseudo-label\\nrefinement while [34] proposes pseudo-label refinement and\\nmomentum update for 3D point cloud segmentation. [44]\\nproposes continual test-time adaptation based on stochastic\\nrestoration and weight-averaged pseudo-labels. [37] uses\\nefficient residual modules to realign the pretrained weights.\\nThe above methods largely focus on single-image-based\\ntasks, i.e., classification [2, 5, 23] and semantic segmenta-\\ntion [34], and rely on entropy constraints from [43].\\nUnlike prior work on classification [23, 40] and segmen-\\ntation [34], depth completion is a regression problem; hence,\\nexisting methods using entropy-based objectives [43], which\\noperate on logits, are not applicable in this task. Instead,\\nwe propose to minimize sparse depth reconstruction and lo-\\ncal smoothness objectives â€“ similar to that of some existing\\nunsupervised methods [45, 46] â€“ and to maximize cosine\\nsimilarity between the proxy embeddings and the test time\\nimage and sparse depth embeddings.\\nDepth Completion aims to output dense depth from a\\nsingle image and synchronized point cloud, i.e., from lidar\\nor tracked by VIO, projected onto a sparse depth map.\\nUnsupervised depth completion approaches rely on\\nStructure-from-Motion (SfM) for training and require ac-\\ncess to an auxiliary training dataset containing stereo image\\npairs [35] or monocular videos [25, 46â€“48] with synchro-\\nnized sparse depth maps. Typically, they minimize a linear\\ncombination of photometric reprojection consistency, sparse\\ndepth reconstruction error, and local smoothness [25, 46â€“48].\\nThese methods can support online training, but are limited\\nby the need for stereo or monocular videos with sufficient\\nparallax and co-visibility. In contrast, our approach does not\\nrely on SfM and can be used in more general scenarios.\\nSupervised depth completion trains the model by mini-\\nmizing a loss with respect to ground truth. [3, 16] focus\\non network operations and designs to effectively deal with\\nsparse inputs. [18, 25, 50] propose early and late fusion of\\nimage and depth encoder features while [15] uses separate\\nnetworks for each. [21] proposes a multi-scale cascaded\\nhourglass network to enhance the depth encoder with im-\\nage features. [4] proposes convolutional spatial propagation\\n2\\nSource Domain\\nTarget Domain\\nInputs\\nPredictions and error maps with different inputs\\n0.1\\n0.0\\n0.1m\\n8.0m\\nGround truth\\nImage\\nSparse depth\\nImage\\nSparse depth\\nImage only\\nSparse depth only\\nImage + sparse depth\\nRMSE: 496.65\\nRMSE: 597.91\\nRMSE: 1528.98\\nRMSE: 1046.28\\nRMSE: 2601.01\\nRMSE: 2462.63\\nFigure 1. Model sensitivity to input modalities. While utilizing both sparse depth and image as input, the best performance is achieved in the\\nsource domain (VOID). Yet, forgoing the image in the test domain (NYUv2) often yields lower error than using both as input.\\nnetwork; [28] extends it to non-local spatial propagation to\\nrefine an initial depth map based on confidence and learnable\\naffinity; [22] further extends it to dynamic spatial propaga-\\ntion. [7, 8, 31, 32] learn uncertainty of the depth estimates.\\n[42] utilizes confidence maps to combine depth predictions\\nwhile [30, 49, 52] use the surface normals to guide depth pre-\\ndiction. [19] incorporates cost volume for depth prediction.\\n[33] devises transformer architecture with cross-modal at-\\ntention, and [51] proposes a hybrid convolution-transformer\\narchitecture for depth completion.\\nWhile both unsupervised and supervised methods have\\ndemonstrated strong performance on benchmarks, they often\\nfail to generalize to test datasets with large domain discrep-\\nancies. Moreover, obtaining ground truth is unrealistic for\\nreal-time applications, and accumulating sufficient parallax\\nincurs large latencies â€“ presenting significant challenges for\\nonline adaptation. Unlike past works, we do not assume\\naccess to ground truth nor data outside of the input.\\nUnsupervised Domain Adaptation (UDA) is designed\\nto address the discrepancy between labeled source data and\\nunlabeled target data [11, 29, 39]. The only existing depth\\ncompletion UDA approach [24] models the domain discrep-\\nancy as the noise in sparse points and the appearance in RGB\\nimages. Unlike most UDA approaches that require source\\ndata during adaptation, we are only given the inputs neces-\\nsary for inference in a stream without the ability to revisit\\npast data, and must update the model online under a limited\\ncomputational budget.\\nOur Contributions. We present (i) a study on how the\\ndomain shift in each data modality (e.g., image and sparse\\ndepth) affects model performance when transferring it from\\nsource to target test domain. This study motivates (ii) our\\napproach to learn an embedding of sparse depth features\\n(which are less sensitive to the domain shift) that serves as\\nproxy to source features for guiding test-time adaptation.\\n(iii) To the best of our knowledge, we are the first to propose\\ntest-time adaptation for the depth completion task, and (iv)\\nwill release code, models, and dataset benchmarking setup\\nto make development accessible for the research community.\\n3. Method Formulation\\nFor ease of use, we assume access to a (source) pretrained\\ndepth completion model fÎ¸ that infers a dense depth map Ë†d\\nfrom a calibrated RGB image I âˆˆ RHÃ—W Ã—3 and its associ-\\nated sparse point cloud projected onto the image plane as a\\nsparse depth map z âˆˆ RHÃ—W\\n+\\n, i.e., fÎ¸(I, z) â†’ Ë†d âˆˆ RHÃ—W\\n+\\n.\\nFor simplicity, we assume that the model was trained to min-\\nimize a supervised loss between prediction and ground truth\\nd âˆˆ RHÃ—W\\n+\\non a source dataset Ds = {I(n)\\ns\\n, z(n)\\ns\\n, d(n)}Ns\\nn=1,\\nwhere Ns indicates the number of data samples. Following\\nconventions in TTA, we assume access to the source domain\\ndataset prior to deployment.\\nDuring test-time adaptation, we follow the protocol of\\n[23, 40], where we only have access to the target domain\\ndata Dt = {I(n)\\nt\\n, z(n)\\nt\\n}Nt\\nn=1 and utilize an online procedure\\nto adapt to unseen Nt target data samples. Note that we\\nmake no assumptions about supervision during test-time;\\nhence, while we present results on supervised methods for\\ncontrolled experiments, we see our method being applicable\\ntowards unsupervised methods as well.\\nOur method is split into three stages (Fig. 3): (a) Dur-\\ning an intialization stage, we augment the network encoder\\nwith a adaptation layer and train it using source domain\\ndata. (b) In the preparation stage, we learn a mapping from\\nsparse depth features to image and sparse depth (proxy) em-\\n3\\n0\\n800\\n1600\\n2400\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nMAE: VOID â†’ NYUv2\\n0\\n1000\\n2000\\n3000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nMAE: VOID â†’ ScanNet\\n0\\n1000\\n2000\\n3000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nRMSE: VOID â†’ ScanNet\\n0\\n8000\\n16000\\n24000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nMAE: KITTI â†’ Waymo\\n0\\n9000\\n18000\\n27000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nRMSE: KITTI â†’ Waymo\\n0\\n6000\\n12000\\n18000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nMAE: KITTI â†’ nuScenes\\n0\\n8000\\n16000\\n24000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nRMSE: KITTI â†’ nuScenes\\n0\\n1000\\n2000\\n3000\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nRMSE: VOID â†’ NYUv2\\nImage only\\nSparse depth only\\nImage + sparse depth\\nFigure 2. Model sensitivity to input modalities. Depth completion networks have a high reliance on sparse depth modality. Performing\\ninference in a novel domain without the RGB image, i.e., using just sparse depth as input, can improve over using both data modalities.\\nbeddings. (c) During test time, we do not need the source\\ndataset; we freeze the mapping and use its proxy embeddings\\nfor updating the adaptation layer parameters in the target test\\ndomain.\\n3.1. Sensitivity Study on Data Modalities\\nTo motivate our approach, we begin with a sensitivity study\\nof depth completion networks to input modalities, e.g. image,\\nsparse depth, and the effect of domain shift on them. To this\\nend, we alter the inputs by zeroing out either I or z to yield\\n(I, z), (I0, z), and (I, z0), where I0 and z0 indicate the zero\\nmatrices with identical size to I and z, respectively. We eval-\\nuate the pretrained models using (I, z), (I0, z), and (I, z0)\\nto highlight their dependence on each input modality and to\\ngauge their sensitivity when one modality gives no useful\\ninformation at all. Fig. 1 and Fig. 2 show qualitative (error\\nmaps) and quantitative results (bar graphs), respectively, of\\npretrained depth completion models when fed the different\\ninputs on the source dataset Ds and the target dataset Dt.\\nIn the source domain, inference using both image and\\nsparse depth as inputs, i.e., Ë†ds(I, z), shows the best perfor-\\nmance. Surprisingly, the inference using sparse depth alone\\n(i.e., with null-image) Ë†ds(I0, z) is comparable to Ë†ds(I, z).\\nThis shows the first intuition behind our approach: (i) Even\\nthough depth inputs are sparse, they are sufficient to sup-\\nport the reconstruction of the scene. Additionally, inference\\nwith image alone (i.e., null-depth) Ë†ds(I, z0) is worse than\\nË†ds(I0, z) and Ë†ds(I, z), which suggests that a depth com-\\npletion network relies heavily on sparse depth modality for\\ninference, and the image for guiding recovery of finer details.\\nIn the target test domain, expectedly, performance de-\\ngrades for inference using both image and sparse depth due\\nto a covariate shift. Remarkably, we observe that predic-\\ntions from sparse depth alone Ë†dt(I0, z) remain consistent in\\nperformance to those using both inputs Ë†dt(I, z). Moreover,\\nwe observe that in most cases Ë†dt(I0, z), in fact, outperforms\\nË†dt(I, z) across several methods and datasets, i.e., inference\\nwithout image information in the test domain is better than\\nwith it. Conversely, the performance gap between infer-\\nence with both inputs, Ë†dt(I, z), and just the image, Ë†dt(I, z0),\\nbecomes more evident under the domain shift. This observa-\\ntion illustrates another intuition: (ii) The domain shift largely\\naffects the image modality, and less so depth.\\nThe two intuitions above motivate our approach. As ob-\\nject shapes tend to persist across domains, and the measured\\nsparse points being a coarse representation of them, we aim\\nto leverage sparse depth modality to bridge the domain gap.\\nTo this end, we exploit the observation that depth comple-\\ntion networks are able to recover (coarse) 3D scenes from\\nsparse points alone and that the image serves to propagate\\nand refine depth for regions lacking points. This is done\\nby learning to map features encoding sparse depth inputs\\nto features encoding both modalities in the source domain\\nand, during test-time, recover the source domain features\\ncompatible with target domain sparse depth to guide model\\nadaptation. Specifically, as observed, the covariate shift is\\nlargely photometric, so we propose to adapt the RGB image\\nencoder branch by introducing a adaptation layer: a single\\nconvolutional layer designed to align target domain RGB\\nembeddings to those of the source domain. As the rest of the\\nnetwork is frozen, adapting just the adaptation layer allows\\nfor low-cost model updates.\\nIntuition for integrating adaptation layer. Guided by\\nour observations, the adaptation layer should be (i) placed\\nin the image encoder branch prior to the fusion of image\\nand depth features, and (ii) located within later layers to\\nmodulate higher level representations (i.e., object shapes,\\nas opposed to low-level edges). (iii) Ideally it should be\\nconnected as a skip connection to propagate its effects to the\\ndecoder.\\n3.2. Preparation Stage - Source Domain\\nInitialize adaptation layer from source domain. Updating\\nthe entire network is largely infeasible in test-time adap-\\n4\\n(a) Initialize Meta Layer\\n(b) Preparation â€“ Learning proxy embedding\\nLocal Smoothness\\nPenalty\\nSparse Depth\\nConsistency\\nProxy\\nConsistency\\nSupervised\\nLoss\\n(c) Adaptation\\nStopGrad\\nNull Image\\nProxy loss\\nProjection MLPs\\nFrozen param.\\nUpdated param.\\nWeight sharing\\nProjection MLPs\\nOutput depth \\nSparse depth\\nImage\\nImage\\nStopGrad\\nNull Image\\nImage\\nSparse depth\\nSparse depth\\nSparse depth\\nGround truth\\nSource Dataset \\nTarget Dataset \\nSparse depth\\nImage\\nPrediction\\nEncoder\\nDecoder\\nEncoder\\nEncoder\\nDecoder\\nAdaptation Layer\\nSource Dataset \\nSource Dataset \\nEMA update\\nð‘”!\\nâ„Ž\"\\nð‘”!#\\nð‘”!\\nâ„Ž\"\\nð‘§!\\nð¼!\\nð‘§\"\\nð¼\"\\nð¼#\\nð‘§!\\nð‘”!#\\nð¼\"\\nð¼#\\nð‘§\"\\nð‘§!\\nð¼!\\nð‘‘$!\\nð‘‘$\"\\nð‘‘\"\\nFigure 3. Overview. (a) The pretraining stage integrates a adaptation layer into a pretrained encoder and pretrains the adaptation layer on the\\nsource dataset. (b) The preparation stage learns the proxy mapping of features encoding sparse depth to those encoding both inputs. (c) The\\nadaptation stage deploys the model to the target domain and updates the adaptation layer by leveraging the proxy embeddings to guide\\nadaptation.\\ntation scenarios. For the sake of speed and efficiency, we\\nimplement a adaptation layer mÏ•, i.e., a convolutional layer,\\nwithin the encoder of a pretrained network. Note that the\\nentire network will be frozen during all stages of our method\\nwith the exception of the adaptation layer and proxy map-\\nping, where both will be initialized during preparation stage\\nin the source domain, and the proxy mapping will be frozen\\nand used to adapt the adaptation layer in the target domain.\\nTo ease the adaptation process, we initialize the adaptation\\nlayer mÏ• by minimizing a supervised loss over the source\\ndataset (Fig. 3-(a)). We denote the pretrained encoder inte-\\ngrated with mÏ• as eÏ•.\\nLearning proxy mapping from source domain. As ob-\\nserved in Fig. 1, the best results in the source domain are\\nachieved by feeding in both the image and sparse depth\\nmodalities for inference. However, the image is susceptible\\nto domain shift which degrades performance when the model\\nis transferred to an unseen test domain. Conversely, sparse\\ndepth is more resilient to the domain shift than RGB images,\\ni.e., the shape of a car (or another object) remains similar\\nregardless of (synthetic or real) domain. Our method aims\\nto leverage the sparse depth modality, which is less sensitive\\nto the domain shift, in the downstream adaptation process.\\nTo this end, we employ a soft mapping [13] from just the\\nencoded sparse depth features to sparse depth and image\\nfeatures to learn the photometric information that is captured\\nfrom the same scene as the sparse point cloud. This strategy\\nallows us to learn the mapping that projects the sparse depth\\nfeatures to â€œproxyâ€ embeddings close to those that also en-\\ncode the image. In other words, it fills in what is missing\\nin the image encoder branch by predicting the residual la-\\ntent image encoding that is compatible with the input sparse\\ndepth, i.e., 3D scene. As this is trained in the source domain,\\nthe mapping naturally yields proxy embeddings that encode\\nthe source domain image (and sparse depth), which can be\\nlater used to guide the adaptation layer mÏ• to transform test\\ndomain RGB features close to those of the source domain.\\nThis mapping is realized as MLPs denoted as gÏˆ(Â·) and\\nhÏ‰(Â·); to learn them, we first get two embeddings ppps and qqqs,\\nppps = hÏ‰(gÏˆ(StopGrad(eÏ•(I0, zs)))),\\nqqqs = StopGrad(gÏˆ(eÏ•(Is, zs)))\\n(1)\\nwhere eÏ• denotes the encoder augmented with the adap-\\ntation layer trained on source dataset, Is, zs, the image and\\nsparse depth from source domain, and I0 the null-image. The\\nembedding modules gÏˆ and hÏ‰ are updated to maximize the\\nsimilarity between ppps and qqqs. To learn them, we minimize:\\nâ„“prepare = 1 âˆ’ ( ppps\\nâˆ¥pppsâˆ¥ Â· qqqs\\nâˆ¥qqqsâˆ¥),\\n(2)\\nwhere âˆ¥ Â· âˆ¥ is L2-norm, and (aaa Â· bbb) indicates the dot product\\nof the vectors aaa and bbb. To this end, we first train the MLP\\nheads gÏˆ, hÏ‰ by minimizing Eqn. 2.\\nOnce the mapping is learned, we can freeze the embed-\\nding module and deploy it for test-time adaptation where\\nwe update the adaptation layer weights Ï• to maximize the\\nsimilarity between the embeddings of a test domain image\\nand sparse depth, and its proxy from the source domain. Nat-\\nurally, due to the domain shift, the embeddings will yield\\nlow similarity scores; hence, maximizing the scores through\\nour proxy embedding implicitly aligns the target RGB distri-\\nbution to that of the source distribution, i.e., minimizing the\\ncosine similarity between the source and target distributions.\\n5\\n3.3. Deploying Proxy Mapping to Target Domain\\nAdaptation stage aims to update the adaptation layer param-\\neters by minimizing a test-time loss function over the target\\ntest domain data {It, zt} âˆˆ Dt. To do so, we deploy the\\nlearned proxy mapping module (MLP heads {gâˆ—\\nÏˆ(Â·), hâˆ—\\nÏ‰(Â·)})\\nalong with the adaptation layer mÏ• integrated into the frozen\\nencoder.\\nAdaptation loss. For adaptation, our loss is composed of\\na linear combination of three loss terms:\\nLadapt = wzâ„“z + wsmâ„“sm + wproxyâ„“proxy,\\n(3)\\nwhere â„“z, â„“sm denote sparse depth consistency loss and lo-\\ncal smoothness loss, respectively, â„“proxy is proxy mapping\\nconsistency loss, and w indicates a weight of each loss term.\\nSparse Depth Consistency. Sparse point clouds capture\\na coarse structure of the 3D scene.To obtain metric scale\\npredictions consistent with the scene structure, we minimize\\nL1 error between the sparse depth zt and the prediction Ë†dt:\\nâ„“z =\\n1\\n|â„¦(zt)|\\nX\\nxâˆˆâ„¦(zt)\\n| Ë†dt(x) âˆ’ zt(x)|,\\n(4)\\nwhere x âˆˆ â„¦(zt) are the pixel locations where sparse points\\nwere projected onto the image plane.\\nLocal Smoothness. Based on the assumption of local\\nsmoothness and connectivity in a 3D scene, we impose the\\nsame in the predicted depth map Ë†dt. Specifically, we apply\\nan L1 penalty to its gradients in both the x- and y-directions\\n(i.e., âˆ‚X and âˆ‚Y ). We balance the weight of each term with\\nÎ»X and Î»Y , to allow discontinuities over object boundaries\\nbased on the image gradients, where Î»X(x) = eâˆ’|âˆ‚XIt(x)|,\\nÎ»Y (x) = eâˆ’|âˆ‚Y It(x)|, and â„¦ denotes the image domain.\\nâ„“sm = 1\\n|â„¦|\\nX\\nxâˆˆâ„¦\\nÎ»X(x)|âˆ‚X Ë†dt(x)| + Î»Y (x)|âˆ‚Y Ë†dt(x)|.\\n(5)\\nProxy Consistency. In order to regularize the adaptation\\nwith the learned mapping from the previous stage, we freeze\\nthe weight parameters of MLP heads {gâˆ—\\nÏˆ(Â·), hâˆ—\\nÏ‰(Â·)}, and\\nupdate the parameters of the adaptation layer mÏ•. First, we\\nobtain the features pppt and qqqt using the null-image I0 in one\\nand the given target test domain image It in the other:\\npppt = StopGrad(hâˆ—\\nÏ‰(gâˆ—\\nÏˆ(eÏ•(I0, zt)))), qqqt = gâˆ—\\nÏˆ(eÏ•(It, zt)).\\n(6)\\nWe maximize the cosine similarity between the feature qqqt\\nand pppt by using proxy loss â„“proxy to update adaptation layer\\nmÏ•:\\nâ„“proxy = 1 âˆ’ ( pppt\\nâˆ¥ppptâˆ¥ Â· qqqt\\nâˆ¥qqqtâˆ¥).\\n(7)\\n4. Experiments\\nWe demonstrate the effectiveness of our approach on a mix of\\nboth real and synthetic datasets including indoor SLAM/VIO\\nscenarios (VOID [46], NYUv2 [27], SceneNet [26], and\\nScanNet [6]) and outdoor driving scenarios using lidar sensor\\n(KITTI [41], Virtual KITTI [10], nuScenes [1], and Waymo\\nOpen Dataset [38]). We chose three representative architec-\\ntures of current depth completion methods to test our method:\\nMSG-CHN [21] (CNN-based), NLSPN [28] (SPN-based)\\nand CostDCNet [19] (cost volume-based). All reported re-\\nsults are averaged over 5 independent trials. We describe\\nimplementation details, hyper-parameters used, hardware\\nrequirements, evaluation metrics as well as additional exper-\\nimental results in the Supp. Mat.\\nMain Result. We use pretrained models (MSG-CHN,\\nNLSPN, and CostDCNet) from the two source datasets,\\nVOID for indoor, and KITTI for outdoor depth completion.\\nFor indoor, we adapt models pretrained on VOID to NYUv2,\\nSceneNet, and ScanNet; for outdoors, we adapt from KITTI\\nto Virtual KITTI (VKITTI) (with fog), nuScenes, and\\nWaymo. BN Adapt denotes updating the batch statistics\\n(i.e., running mean and variance). BN Adapt, â„“z, â„“sm is a\\nvariation of TENT [43] which minimizes Eqn. 4, 5 instead\\nof entropy by updating learnable scale factors. CoTTA de-\\nnotes replacing proxy loss with L1 consistency loss w.r.t. the\\npretrained prediction [44]. Ours-fast denotes our method\\nwithout batch norm update, which improves adaptation run-\\ntime by 25.32%.\\nOur method consistently improves over baselines and\\nvariants of BN Adapt (Table 1). Specifically, we improve\\nover BN Adapt, â„“z, â„“sm by 11.60% on average across all\\nmethods for indoor, 19.73% on outdoors, and 15.67% over-\\nall to achieve state-of-the-art performance. Qualitatively,\\nFig. 4 and Fig. 5 show that our method performs better in\\nboundary regions and homogeneous regions, thus exhibit-\\ning less oversmoothing on curtains in Fig. 4-(a) and car in\\nFig. 5-(b), and undersmoothing on blackboard in Fig. 4-(d)\\nand road in Fig. 5-(a), respectively, during adaptation. This\\ntrend is due to the proxy loss and the adaptation layer, which\\nallows us to adapt with minimum weight adjustments while\\npreserving high-level features (object shapes) learned from\\nthe source domain by mapping the target RGB modality to\\nthat of the source domain. Notably, Ours-fast still improves\\nover BN Adapt even though we only adapt our adaptation\\nlayer, which demonstrates the effectiveness of our design\\nchoice as well as our proposed proxy embeddings.\\nComparison to BN adaptation1 and CoTTA. To as-\\nsess the impact of our adaptation layer, we compared our\\napproach to the batch norm adaptation from TENT [43].\\nIn the BN Adapt scenario, we only update the batch norm\\n1BN adaptation cannot be applied to MSG-CHN due to the absence of a\\nbatch norm layer.\\n6\\nKITTI â†’ VKITTI-FOG\\nKITTI â†’ nuScenes\\nKITTI â†’ Waymo\\nMethod\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMSG-CHN\\nPretrained\\n2842.88\\n6557.38\\n3331.821\\n6449.094\\n1107.22\\n2962.45\\nCoTTA\\n730.6Â±11.67\\n3330.23Â±44.83\\n3157.69\\n6434.14\\n655.77Â±30.98\\n2213.27Â±98.80\\nOurs-fast\\n728.24Â±3.73\\n3087.36Â±15.92\\n2834.08Â±17.64\\n6096.56Â±21.08\\n608.91Â±1.74\\n1921.83Â±2.54\\nNLSPN\\nPretrained\\n1309.99\\n7423.48\\n2656.609\\n6146.590\\n1175.83\\n3078.377\\nBN Adapt\\n1140.21Â±35.89\\n4592.86Â±198.21\\n11291.57Â±21.32\\n16670.87Â±52.56\\n7283.33Â±104.58\\n9670.36Â±250.22\\nBN Adapt, â„“z, â„“sm\\n775.20Â±5.65\\n3465.05Â±32.73\\n2928.51Â±75.89\\n8209.24Â±164.31\\n494.94Â±3.08\\n1921.17Â±338.06\\nCoTTA\\n767.93Â±5.47\\n3799.88Â±17.29\\n2650.45Â±15.04\\n6242.52Â±33.14\\n933.41Â±4.31\\n2763.88Â±143.48\\nOurs-fast\\n732.61Â±29.57\\n3002.19Â± 52.29\\n2733.96Â±34.32\\n6099.48Â±82.32\\n875.01Â±15.8\\n2400.17Â±21.44\\nOurs\\n686.91Â±22.14\\n2666.70Â±56.64\\n2589.25Â±59.03\\n6006.18Â±90.66\\n477.28Â±3.32\\n1598.64Â±18.95\\nCostDCNet\\nPretrained\\n1042.98\\n6301.60\\n3064.724\\n6630.649\\n1093.79\\n2798.25\\nBN Adapt\\n1476.57Â±1.38\\n5428.20Â±8.15\\n2306.04Â±28.86\\n6391.98Â±48.97\\n596.08Â±5.55\\n1877.91Â±45.56\\nBN Adapt, â„“z, â„“sm\\n729.67Â±3.14\\n3413.76Â±14.59\\n2288.85Â±14.02\\n6338.38Â±31.31\\n469.97Â±2.47\\n1572.95Â±10.63\\nCoTTA\\n756.32Â±3.59\\n3686.69Â±14.75\\n2676.83Â±68.92\\n6099.49Â±66.79\\n689.94Â±1.95\\n2140.23Â±16.12\\nOurs-fast\\n756.98Â±31.07\\n3091.78Â±105.42\\n2595.81Â±12.13\\n6373.01Â±7.74\\n606.10Â±11.10\\n1817.79Â±19.14\\nOurs\\n512.72Â±0.74\\n2735.01Â±3.53\\n2062.28Â±11.24\\n5509.96Â±23.41\\n466.44Â±1.63\\n1580.38Â±11.48\\nVOID â†’ NYUv2\\nVOID â†’ SceneNet\\nVOID â†’ ScanNet\\nMSG-CHN\\nPretrained\\n1040.934\\n1528.983\\n281.28\\n645.01\\n687.988\\n1201.747\\nCoTTA\\n876.93Â±146.95\\n1148.62Â±173.53\\n223.19Â±14.77\\n498.46Â±28.21\\n619.37Â±4.14\\n1141.04Â±7.35\\nOurs-fast\\n699.60Â±6.00\\n1120.37Â±9.76\\n192.74Â±1.72\\n424.49Â±4.58\\n302.21Â±4.10\\n480.08Â±8.03\\nNLSPN\\nPretrained\\n388.87\\n702.80\\n167.250\\n438.71\\n233.33\\n431.20\\nBN Adapt\\n250.13Â±5.23\\n447.18Â±10.32\\n143.61Â±6.34\\n385.56Â±9.84\\n207.00Â±0.57\\n401.41Â±2.84\\nBN Adapt, â„“z, â„“sm\\n147.55Â±1.36\\n271.10Â±2.17\\n120.48Â±1.94\\n345.91Â±7.14\\n82.76Â±0.47\\n181.97Â±1.21\\nCoTTA\\n390.50Â±8.29\\n704.72Â±16.74\\n205.02Â±1.79\\n540.01Â±4.08\\n234.77Â±1.52\\n496.18Â±2.75\\nOurs-fast\\n168.43Â±3.46\\n309.48Â±6.92\\n124.67Â±1.33\\n357.56Â±2.59\\n104.06Â±11.03\\n232.84Â±20.46\\nOurs\\n124.41Â±2.27\\n240.73Â±5.72\\n113.93Â±1.49\\n333.41Â±4.32\\n74.77Â±0.31\\n166.61Â±0.45\\nCostDCNet\\nPretrained\\n189.10\\n446.71\\n173.37\\n443.22\\n144.31\\n458.69\\nBN Adapt\\n160.31Â±2.7\\n410.55Â±10.70\\n176.62Â±0.72\\n446.32Â±8.52\\n159.65Â±4.63\\n399.14Â±13.92\\nBN Adapt, â„“z, â„“sm\\n136.80Â±5.35\\n338.59Â±22.36\\n134.22Â±2.33\\n385.9Â±6.68\\n68.44Â±0.46\\n164.59Â±2.82\\nCoTTA\\n147.69Â±5.3\\n376.87Â±21.25\\n136.42Â±3.41\\n405.38Â±11.63\\n101.98Â±1.53\\n322.63Â±5.04\\nOurs-fast\\n131.93Â±2.58\\n269.02Â±5.61\\n129.99Â±3.88\\n353.86Â±7.91\\n128.12Â±3.41\\n244.62Â±7.53\\nOurs\\n95.87Â±2.16\\n203.83Â±4.72\\n125.75Â±1.93\\n357.12Â±4.13\\n68.17Â±0.44\\n162.35Â±1.12\\nTable 1. Test-time adaptation for depth completion. For indoors, we adapt from VOID to NYUv2, SceneNet, and ScanNet; for outdoors,\\nfrom KITTI to VKITTI with fog, nuScenes, and Waymo. Bold denotes the best result and Italics the second-best. Ours-fast denotes our\\nmethod without updating BatchNorm layers.\\nlayerâ€™s scale and shift factor based on the loss function. On\\naverage, BN Adapt with â„“z, â„“sm improves the pretrained\\nmodel by 32.77%; whereas, updating just our adaptation\\nlayer (Ours-fast) improves it more by 34.07% (Table 1). The\\nimprovement of Ours-fast over BN adapt demonstrates the\\neffectiveness of updating adaptation layer, which directly ad-\\njusts the high-level features from the RGB branch guided by\\nproxy loss, where BN adapt realigns the learned source fea-\\ntures from both RGB and range sensors by updating feature\\nstatistics.\\nNonetheless, the best results are achieved when we in-\\nclude batch norm update, which improves the pretrained\\nmodel by 44.53%, but at the cost â‰ˆ33.16% of extra time.\\nThe improvement of ours over BN adapt implies that adapt-\\ning only BN parameters may not bridge the large domain\\ndiscrepancy and may not be solved with scaling and shift-\\ning, whereas our method explicitly adjusts RGB features by\\nupdating the adaptation layer.\\nWe also compared our approach to CoTTA [44], which\\nadapts the whole model parameters using the prediction from\\nthe teacher model updated by exponential moving average of\\npretrained weight and the model prediction. We combined\\nadditional loss â„“z, â„“sm on top of CoTTA loss, since we ob-\\nserved that the models cannot be adapted with CoTTA alone.\\nSpecifically, our method without proxy shows a 25.26% aver-\\nage improvement on the CoTTA method. CoTTA updates the\\nwhole parameters including RGB and sparse depth branch,\\nwhich causes a drift from the learned model parameters. On\\nthe other side, our method only updates additional layer at\\nRGB branch, based on the study from the most domain dis-\\ncrepancy comes from RGB modality as studied in Sec. 3.1,\\nand this prevents the model from a drift from learned domain.\\nAlso, CoTTA assumes the test-time augmentation can miti-\\ngate the domain shift. However, the results shows test-time\\naugmentation on RGB image, causing a small distributional\\nshift, may not solve a large domain discrepancy.\\nAlso, our method with batch normalization layer update\\nshows 26.52% average improvement, while using 25.05%\\nless adaptation time. Note: CoTTA costs not only additional\\nmemory for teacher model but also inference time to get the\\nteacher model prediction, even if CoTTA does not require\\nany preparation process. Overall, our method shows 21.09%\\n7\\nBN Adapt\\nOurs\\nSparse Depth\\nPredicted Depth\\nImage\\nError Map\\nCoTTA\\n(a)\\n(b)\\n(c)\\n(d)\\nBN Adapt\\nOurs\\nCoTTA\\nFigure 4. Representative results for test-time adaptation on NYUv2. Boxes highlight detailed comparisons. Notably, our method performs\\nbetter in boundary regions displaying the discontinuity in depth (e.g., curtains, (a)), as well as homogeneous regions (e.g., blackboard, (d)).\\nBN Adapt\\nOurs\\n(a)\\n(b)\\nImage\\nSparse Depth\\nCoTTA\\nPredicted Depth\\nError Map\\nFigure 5. Test-time adaptation results on NuScenes dataset. Our method is consistent indoors and outdoors. Boxes highlight detailed\\ncomparisons. As seen in Fig. 4, our method performs better than BN Adapt and CoTTA, notably in both depth-discontinuous regions (e.g.,\\ncar in (b)) and homogeneous regions (e.g., road in (a) and (b)).\\naverage improvement over BN adapt and CoTTA methods.\\n5. Discussion\\nWe have proposed a method for test-time adaptation for\\ndepth completion that leverages the strength of complemen-\\ntary multi-sensor setup in the presence of domain shift. By\\nstudying model sensitivity to each input modality as well as\\nthe data under domain shift, we designed a way to exploit\\nthe modality (sparse depth) that is less sensitive to guide\\nadaptation. We do so through a proxy embedding that learns\\nthe photometric information from the source domain that is\\ncompatible with the sparse depth depicting a 3D scene. Our\\nproxy embedding works well as a regularizer for scenarios\\nwhere there exists covariate shifts in photometry (i.e., KITTI\\nto VKITTI) as well as scene layouts (i.e., VOID to NYUv2\\nand SceneNet). While one may surmise that the applica-\\ntion of the embeddings are specific to scene distributions,\\nwe show otherwise. VOID (classrooms, laboratories, and\\ngardens), NYUv2 (households and shopping centers), and\\nSceneNet (randomly arranged synthetic rooms) all differ in\\nlayouts. The proxy embedding captures latent photomet-\\nric features of the object shapes populating them; the same\\nproxy embedding can be transferred across domains even\\nwhen scene differ, but share objects within them.\\nThis leads to possible limitations in the scenarios where\\nthe source dataset is sampled from scenes that do not share\\nany objects with the target test dataset; in this case, the proxy\\nembeddings should give little to no gain and one must rely\\non generic regularizers like local smoothness. Additionally,\\nwhile we follow the conventions in TTA and assume access\\nto the source dataset prior to deployment, in reality, many\\nmodels are trained on private datasets, so adapting â€œoff-the-\\nshelfâ€ models remains a challenge. In such cases, one must\\nincorporate our preparation pipeline into their model train-\\ning and release the adaptation layer and proxy embedding\\nmodule together with network weights. Nonetheless, this is\\nthe first test-time adaptation work in depth completion; in\\naddition to our findings, we plan to release models, dataset,\\nadapation, and evaluation code, and hope to further motivate\\ninterest in TTA for multi-modal tasks like depth completion.\\n8\\nReferences\\n[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\\nmodal dataset for autonomous driving. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 11621â€“11631, 2020. 6, 12\\n[2] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna\\nEbrahimi. Contrastive test-time adaptation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 295â€“305, 2022. 2\\n[3] Yun Chen, Bin Yang, Ming Liang, and Raquel Urtasun. Learn-\\ning joint 2d-3d representations for depth completion. In Pro-\\nceedings of the IEEE/CVF International Conference on Com-\\nputer Vision, pages 10023â€“10032, 2019. 2\\n[4] Xinjing Cheng, Peng Wang, Chenye Guan, and Ruigang Yang.\\nCspn++: Learning context and resource aware convolutional\\nspatial propagation networks for depth completion. In Pro-\\nceedings of the AAAI Conference on Artificial Intelligence,\\npages 10615â€“10622, 2020. 2, 11\\n[5] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack\\nYun. Improving test-time adaptation via shift-agnostic weight\\nregularization and nearest source prototypes. In ECCV, pages\\n440â€“458. Springer, 2022. 2\\n[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\\nber, Thomas Funkhouser, and Matthias NieÃŸner. Scannet:\\nRichly-annotated 3d reconstructions of indoor scenes. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition, pages 5828â€“5839, 2017. 6, 12\\n[7] Abdelrahman Eldesokey, Michael Felsberg, and Fahad Shah-\\nbaz Khan. Propagating confidences through cnns for sparse\\ndata regression. arXiv preprint arXiv:1805.11913, 2018. 3\\n[8] Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist,\\nand Michael Persson. Uncertainty-aware cnns for depth com-\\npletion: Uncertainty from beginning to end. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 12014â€“12023, 2020. 3\\n[9] Xiaohan Fei, Alex Wong, and Stefano Soatto. Geo-supervised\\nvisual depth prediction. IEEE Robotics and Automation Let-\\nters, 4(2):1661â€“1668, 2019. 11\\n[10] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig.\\nVirtual worlds as proxy for multi-object tracking analysis. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition, pages 4340â€“4349, 2016. 2, 6, 12\\n[11] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\\nadaptation by backpropagation. In International conference\\non machine learning, pages 1180â€“1189. PMLR, 2015. 2, 3\\n[12] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\\nUrtasun. Vision meets robotics: The kitti dataset. The Inter-\\nnational Journal of Robotics Research, 32:1231 â€“ 1237, 2013.\\n11\\n[13] Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach\\nto self-supervised learning. Advances in neural information\\nprocessing systems, 33:21271â€“21284, 2020. 5, 13\\n[14] Christopher G. Harris and M. J. Stephens. A combined corner\\nand edge detector. In Alvey Vision Conference, 1988. 11, 12\\n[15] Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and Xiao-\\njin Gong. Penet: Towards precise and efficient image guided\\ndepth completion. In 2021 IEEE International Conference on\\nRobotics and Automation (ICRA), pages 13656â€“13662. IEEE,\\n2021. 2\\n[16] Zixuan Huang, Junming Fan, Shenggan Cheng, Shuai Yi,\\nXiaogang Wang, and Hongsheng Li. Hms-net: Hierarchi-\\ncal multi-scale sparsity-invariant network for sparse depth\\ncompletion. IEEE Transactions on Image Processing, 29:\\n3429â€“3441, 2019. 2\\n[17] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier\\nadjustment module for model-agnostic domain generalization.\\n34:2427â€“2440, 2021. 2\\n[18] Maximilian Jaritz, Raoul De Charette, Emilie Wirbel, Xavier\\nPerrotton, and Fawzi Nashashibi. Sparse and dense data with\\ncnns: Depth completion and semantic segmentation. In 2018\\nInternational Conference on 3D Vision (3DV), pages 52â€“60.\\nIEEE, 2018. 2\\n[19] Jaewon Kam, Jungeon Kim, Soongjin Kim, Jaesik Park, and\\nSeungyong Lee. Costdcnet: Cost volume based depth com-\\npletion for a single rgb-d image. In European Conference on\\nComputer Vision, pages 257â€“274. Springer, 2022. 3, 6\\n[20] Youngeun\\nKim,\\nDonghyeon\\nCho,\\nKyeongtak\\nHan,\\nPriyadarshini Panda, and Sungeun Hong. Domain adaptation\\nwithout source data.\\nIEEE Transactions on Artificial\\nIntelligence, 2(6):508â€“518, 2021. 2\\n[21] Ang Li, Zejian Yuan, Yonggen Ling, Wanchao Chi, Chong\\nZhang, et al. A multi-scale guided cascade hourglass network\\nfor depth completion. In Proceedings of the IEEE/CVF Winter\\nConference on Applications of Computer Vision, pages 32â€“40,\\n2020. 2, 6\\n[22] Yuankai Lin, Tao Cheng, Qi Zhong, Wending Zhou, and Hua\\nYang. Dynamic spatial propagation network for depth com-\\npletion. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, pages 1638â€“1646, 2022. 3\\n[23] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste\\nBellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++:\\nWhen does self-supervised test-time training fail or thrive?\\n34:21808â€“21820, 2021. 2, 3\\n[24] Adrian Lopez-Rodriguez, Benjamin Busam, and Krystian\\nMikolajczyk. Project to adapt: Domain adaptation for depth\\ncompletion from noisy and sparse sensor data. In Proceedings\\nof the Asian Conference on Computer Vision, 2020. 3\\n[25] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac\\nKaraman. Self-supervised sparse-to-dense: Self-supervised\\ndepth completion from lidar and monocular camera.\\nIn\\n2019 International Conference on Robotics and Automation\\n(ICRA), pages 3288â€“3295. IEEE, 2019. 2\\n[26] John McCormac, Ankur Handa, Stefan Leutenegger, and An-\\ndrew J Davison. Scenenet rgb-d: 5m photorealistic images of\\nsynthetic indoor trajectories with ground truth. arXiv preprint\\narXiv:1612.05079, 2016. 2, 6, 12\\n[27] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\\nFergus. Indoor segmentation and support inference from rgbd\\nimages. In ECCV, 2012. 2, 6\\n9\\n[28] Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In\\nSo Kweon. Non-local spatial propagation network for depth\\ncompletion. In Computer Visionâ€“ECCV 2020: 16th European\\nConference, Glasgow, UK, August 23â€“28, 2020, Proceedings,\\nPart XIII 16, pages 120â€“136. Springer, 2020. 3, 6, 11\\n[29] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\\nSaenko, and Bo Wang. Moment matching for multi-source\\ndomain adaptation. In Proceedings of the IEEE/CVF inter-\\nnational conference on computer vision, pages 1406â€“1415,\\n2019. 2, 3\\n[30] Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang,\\nShuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deepli-\\ndar: Deep surface normal guided depth prediction for outdoor\\nscene from sparse lidar data and single color image. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 3313â€“3322, 2019. 3\\n[31] Chao Qu, Ty Nguyen, and Camillo Taylor. Depth completion\\nvia deep basis fitting. In Proceedings of the IEEE/CVF Winter\\nConference on Applications of Computer Vision, pages 71â€“80,\\n2020. 3\\n[32] Chao Qu, Wenxin Liu, and Camillo J Taylor. Bayesian deep\\nbasis fitting for depth completion with uncertainty. In Proceed-\\nings of the IEEE/CVF international conference on computer\\nvision, pages 16147â€“16157, 2021. 3\\n[33] Kyeongha Rho, Jinsung Ha, and Youngjung Kim. Guide-\\nformer: Transformers for image guided depth completion.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 6250â€“6259, 2022. 3\\n[34] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schul-\\nter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk-Jin Yoon.\\nMm-tta: multi-modal test-time adaptation for 3d semantic\\nsegmentation. In CVPR, pages 16928â€“16937, 2022. 2\\n[35] Shreyas S Shivakumar, Ty Nguyen, Ian D Miller, Steven W\\nChen, Vijay Kumar, and Camillo J Taylor. Dfusenet: Deep\\nfusion of rgb and sparse depth information for image guided\\ndense depth completion. In 2019 IEEE Intelligent Transporta-\\ntion Systems Conference (ITSC), pages 13â€“20. IEEE, 2019.\\n2\\n[36] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\\nFergus. Indoor segmentation and support inference from rgbd\\nimages. In European Conference on Computer Vision, 2012.\\n11\\n[37] Junha Song, Jungsoo Lee, In So Kweon, and Sungha Choi.\\nEcotta: Memory-efficient continual test-time adaptation via\\nself-distilled regularization. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 11920â€“11929, 2023. 2\\n[38] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\\nYuning Chai, Benjamin Caine, et al. Scalability in perception\\nfor autonomous driving: Waymo open dataset. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, pages 2446â€“2454, 2020. 6, 12\\n[39] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros.\\nUnsupervised domain adaptation through self-supervision.\\narXiv preprint arXiv:1909.11825, 2019. 3\\n[40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei\\nEfros, and Moritz Hardt.\\nTest-time training with self-\\nsupervision for generalization under distribution shifts. In\\nICML, pages 9229â€“9248. PMLR, 2020. 2, 3\\n[41] Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke,\\nThomas Brox, and Andreas Geiger. Sparsity invariant cnns.\\nIn 2017 international conference on 3D Vision (3DV), pages\\n11â€“20. IEEE, 2017. 2, 6, 11, 12\\n[42] Wouter Van Gansbeke, Davy Neven, Bert De Brabandere,\\nand Luc Van Gool. Sparse and noisy lidar completion with\\nrgb guidance and uncertainty. In 2019 16th international\\nconference on machine vision applications (MVA), pages 1â€“6.\\nIEEE, 2019. 3\\n[43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-\\nshausen, and Trevor Darrell. Tent: Fully test-time adaptation\\nby entropy minimization. arXiv preprint arXiv:2006.10726,\\n2020. 2, 6\\n[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai.\\nContinual test-time domain adaptation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 7201â€“7211, 2022. 2, 6, 7\\n[45] Alex Wong and Stefano Soatto. Unsupervised depth comple-\\ntion with calibrated backprojection layers. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision,\\npages 12747â€“12756, 2021. 2\\n[46] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano Soatto.\\nUnsupervised depth completion from visual inertial odome-\\ntry. IEEE Robotics and Automation Letters, 5(2):1899â€“1906,\\n2020. 2, 6, 11\\n[47] Alex Wong, Safa Cicek, and Stefano Soatto. Learning topol-\\nogy from synthetic data for unsupervised depth completion.\\nIEEE Robotics and Automation Letters, 6(2):1495â€“1502,\\n2021.\\n[48] Alex Wong, Xiaohan Fei, Byung-Woo Hong, and Stefano\\nSoatto. An adaptive framework for learning unsupervised\\ndepth completion. IEEE Robotics and Automation Letters, 6\\n(2):3120â€“3127, 2021. 2\\n[49] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun\\nBao, and Hongsheng Li. Depth completion from sparse lidar\\ndata with depth-normal constraints. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\npages 2811â€“2820, 2019. 3\\n[50] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense depth\\nposterior (ddp) from single image and sparse range. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 3353â€“3362, 2019. 2\\n[51] Zhang Youmin, Guo Xianda, Poggi Matteo, Zhu Zheng,\\nHuang Guan, and Mattoccia Stefano. Completionformer:\\nDepth completion with convolutions and vision transformers.\\narXiv preprint arXiv:2304.13030, 2023. 3\\n[52] Yinda Zhang and Thomas Funkhouser. Deep depth comple-\\ntion of a single rgb-d image. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 175â€“185, 2018. 3\\n10\\nSupplementary Materials\\nSummary of contents\\nâ€¢ In Section A, we present the GPU time of each adaptation\\nmethod to show the effectiveness of our method.\\nâ€¢ In Section B, we present the preliminary observations with\\nimage and range inputs of varying sparsity.\\nâ€¢ In Section C, we describe the datasets used.\\nâ€¢ In Section D, we present the hyperparameter settings for\\nresult reproduction and we elucidate evaluation details.\\nâ€¢ In Section E, we provide a study on the learned proxy\\nembedding with a visualization.\\nâ€¢ In Section F, we present an ablation study of the loss\\ncomponents in our method.\\nâ€¢ In Section G, we present the results on KITTI â†’ VKITTI\\nadaptation.\\nâ€¢ In Section H, we present the results on a different source\\ndataset (Waymo â†’ VKITTI).\\nâ€¢ In Section I, we show a qualitavive result of the prelimi-\\nnary observation.\\nA. Adaptation speed\\nWe compare the GPU time of our adaptation method with\\nthe baselines (BN Adapt, CoTTA) on VKITTI in Table 2.\\nCompared to CoTTA, our adaptation method does not\\nrequire multiple inferences to get the pseudo-prediction (de-\\nrived from averaging teacher model predictions with differ-\\nent RGB augmentations) used to adapt the student model.\\nYet, our method requires an additional computation for the\\nproxy embedding. Thus, the proxy layerâ€™s size relative to\\nthe model size causes the adaptation time difference. For\\nexample, CoTTA reduced the total time by 38.9% over Ours-\\nfast on MSGCHN, which is a light-weight depth completion\\nmodel. In this case, the proxy layer is relatively larger than\\nin other models, where multiple inferences require less com-\\nputation than the proxy layer. As a result, the total time\\nis increased in MSGCHN. However, for large models (NL-\\nSPN, CostDCNet), Ours reduced total time by 56.6% over\\nCoTTA; our proxy layer size is relatively smaller than the\\nlarge models, while still improving performance by 26.52%.\\nCompared to BN Adapt, our method requires additional pa-\\nrameters for the adaptation layer and the proxy layer. Hence,\\nour method is 38.18% slower in adaptation time, 19.36%\\nslower in evaluation time, and 33.16% in total. Yet, our\\nmethod improves errors by 15.67% over BN Adapt.\\nB. Further observations on image/range inputs\\nWe present additional preliminary observations of the image\\nand range sensor inputs with varying sparsity. Since pre-\\nvious works [4, 28] state that the depth completion model\\npropagates the sparse depth to the dense depth guided by\\nimage features, one can raise a question on our preliminary\\nModel\\nMethod\\nAdaptation time\\nEvaluation time\\nTotal time\\nMSGCHN\\nCoTTA\\n88.9 (-38.9%)\\n8.66 (-1.0%)\\n81.2 (-41.3%)\\nOurs-fast\\n136.6\\n8.8\\n145.4\\nNLSPN\\nCoTTA\\n717.5 (+67.4%)\\n75.3 (-10.9%)\\n792.8 (+60.0%)\\nBN Adapt\\n185.0 (-20.8%)\\n82.8 (-0.8%)\\n267.8 (-15.6%)\\nOurs-fast\\n168.2 (-28.0%)\\n83.4 (-0.1%)\\n251.6 (-20.66%)\\nOurs\\n233.6\\n83.5\\n317.1\\nCostDCNet\\nCoTTA\\n329.1 (+78.2%)\\n33.6 (-51.0%)\\n369.1 (+43.2%)\\nBN Adapt\\n82.1 (-55.5%)\\n42.5 (-37.9%)\\n125.6 (-50.8%)\\nOurs-fast\\n141.9 (-23.2%)\\n68.7 (+0.3%)\\n210.6 (-16.8%)\\nOurs\\n184.7\\n68.5\\n253.2\\nTable 2. GPU time for various methods and models, tested on\\nVirtual KITTI. Time is in milliseconds (ms). â€˜Adaptation timeâ€™\\ndenotes the time required to adapt (or train) each method for a\\nsingle test data point. â€˜Evaluation timeâ€™ denotes the time taken to\\ntest each method for a test data instance.â€˜Total timeâ€™ is the sum of\\nthe Adaptation and Evaluation times.\\nresults in the main paper without the lidar input, such as\\nthereâ€™s no sparse point to propagate to the near pixels. We\\nclarify that the results are intended to highlight the domain\\ndistrepancy. Therefore, we show additional results with 1%,\\n5%, and 10% of sparse points in the range input on indoor\\ndatasets, as shown in Table 3. As we increase the range\\npoints, the performance is improved yet still worse than the\\nsparse-depth-only results in Tab. 9.\\nC. Datasets\\nKITTI [12] is composed of calibrated RGB images with\\nsynchronized point clouds from Velodyne lidar, inertial, and\\nGPS information, and from more than 61 driving scenes.\\nThere are â‰ˆ80K raw image frames and associated sparse\\ndepth maps, both with â‰ˆ5% density, available for depth\\ncompletion [41]. Semi-dense depth is available for the lower\\n30% of the image space, and 11 neighboring raw lidar scans\\ncomprise the ground-truth depth. We did not use a test or\\nvalidation set, and the training set contains â‰ˆ86K single\\nimages.\\nVOID [46] contains synchronized 640Ã—480 RGB images\\nand sparse depth maps from indoor scenes of laboratories\\nand classrooms and from outdoor scenes of gardens. Sparse\\ndepth maps (of â‰ˆ0.5% density and containing â‰ˆ1,500 sparse\\ndense points) are obtained by the VIO system XIVO [9], and\\ndense ground-truth depth maps are obtained by active stereo.\\nVOID uses rolling shutter to capture challenging 6 DoF\\nmotion for 56 sequences - as opposed to KITTIâ€™s typically\\nplanar motion. We use a training set of â‰ˆ46K images to\\nprepare the model.\\nNYUv2 [36] contains 372K synchronized 640Ã—480 RGB\\nimages and depth maps (via Microsoft Kinect) from 464\\nindoor scenes of household, office, and commercial types.\\nTo generate sparse depth maps in the style of SLAM/VIO,\\nwe used the Harris corner detector [14] to sample â‰ˆ1,500\\npoints from the depth maps. We use a set of 654 test set\\n11\\nMethod\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nDataset\\nVOIDâ†’NYUv2\\nVOIDâ†’ScanNet\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nImage + sparse depth (1%)\\n1643.34 2177.71 602.17 858.19 809.36 1144.91 1597.41 2240.43 490.13 738.77 665.57 982.32\\nImage + sparse depth (5%)\\n996.54\\n1599.14 379.45 638.55 427.69\\n736.23\\n809.38\\n1455.69 240.55 441.70 337.39 620.53\\nImage + sparse depth (10%)\\n785.65\\n1376.93 327.41 591.99 339.31\\n622.75\\n581.93\\n1165.63 191.75 379.10 264.66 516.74\\nSparse depth only\\n734.13\\n1046.28 237.47 402.47 147.76\\n354.57\\n211.86\\n444.62\\n162.29 276.29\\n88.25\\n205.46\\nTable 3. Model sensitivity to input modalities with varying sparsity.\\nimages for adaptation.\\nScanNet [6] contains 2.5 million images and dense depth\\nmaps for 1,513 indoor scenes. To generate sparse depth\\nmaps in the style of SLAM/VIO, we used the Harris corner\\ndetector [14] to sample â‰ˆ1,500 points from the depth maps.\\nWe use a set of â‰ˆ21K test images for adaptation.\\nVirtual\\nKITTI\\n(VKITTI)\\n[10]\\ncontains\\nâ‰ˆ17K\\n1242Ã—375 images from 35 synthetic videos created by ap-\\nplying 7 variations in weather, lighting, or camera angle to\\neach of 5 cloned KITTI [41] videos. There exists a large\\ndomain gap between RGB images from VKITTI and KITTI,\\neven though the virtual worlds created in Unity by [10] are\\nsimilar to KITTI scenes. Thus, we only use the dense depth\\nmaps of VKITTI to avoid the domain gap in photometric\\nvarations. The sparse depth maps are obtained by simulating\\nKITTIâ€™s lidar-generated sparse depth measurements such\\nthat the marginal distribution of VKITTIâ€™s sparse points\\nmimics that of KITTIâ€™s. We use a set of â‰ˆ2,300 test images\\nfor the adaptation.\\nnuScenes [1] consists of 1600Ã—900 calibrated RGB im-\\nages and synchronized sparse point clouds, 27.4K images\\nfrom 1000 outdoor driving scenes for training, and 5.8K im-\\nages from 150 scenes for testing. We set up the ground truth\\nfor the test images by merging projected sparse depth from\\nforward-backward frames. The setup code will be released\\nto clarify further details and reproducibility.\\nSceneNet [26] contains 5 million 320Ã—240 RGB images\\nand depth maps from indoor trajectories of randomly ar-\\nranged rooms. We use a single split (out of 17 available)\\ncontaining 1000 subsequences of 300 images each, gener-\\nated by recording the same scene over a trajectory. Because\\nthere are no sparse depth maps provided, we sampled from\\nthe depth map via Harris corner detector [14] to mimic the\\nsparse depth produced by SLAM/VIO. The final 375 corners\\nare obtained by using k-means to subsample the resulting\\npoints, representing 0.49% of the total pixels. We use a set\\nof â‰ˆ2,300 test images for adaptation.\\nWaymo Open Dataset [38] contains 1920Ã—1280 RGB\\nimages and lidar scans from autonomous vehicles. The\\ntraining set contains â‰ˆ158K images from 798 scenes and\\nthe validation set â‰ˆ40K images from 202 scenes, collected\\nDataset\\nLearning Rate\\nwsm\\nwz\\nwproxy\\nInner Iter.\\nMSG-CHN\\nVKITTI\\n2e-3\\n1.0\\n1.0\\n0.2\\n1\\nVKITTI-FOG\\n5e-3\\n3.0\\n1.0\\n0.1\\n1\\nnuScenes\\n3e-3\\n9.0\\n1.0\\n0.2\\n1\\nSceneNet\\n2e-3\\n8.0\\n1.0\\n0.1\\n3\\nNYUv2\\n2e-4\\n0.8\\n1.0\\n0.4\\n3\\nScanNet\\n5e-3\\n8.0\\n1.0\\n0.3\\n3\\nNLSPN\\nVKITTI\\n2e-3\\n0.8\\n1.0\\n0.4\\n1\\nVKITTI-FOG\\n1e-3\\n1.0\\n1.0\\n0.2\\n1\\nnuScenes\\n1e-3\\n1.0\\n1.0\\n0.1\\n1\\nSceneNet\\n2e-3\\n0.7\\n1.0\\n2.0\\n3\\nNYUv2\\n4e-3\\n5.0\\n1.0\\n1.0\\n3\\nScanNet\\n1e-4\\n2.0\\n1.0\\n0.3\\n3\\nCostDCNet\\nVKITTI\\n4e-3\\n4.5\\n1.0\\n0.1\\n1\\nVKITTI-FOG\\n5e-3\\n3.0\\n1.0\\n0.04\\n1\\nnuScenes\\n5e-3\\n3.0\\n1.0\\n0.1\\n1\\nSceneNet\\n7e-3\\n2.0\\n1.0\\n0.2\\n3\\nNYUv2\\n6e-3\\n4.0\\n1.0\\n0.1\\n3\\nScanNet\\n3e-3\\n1.0\\n1.0\\n0.2\\n3\\nTable 4. Hyperparameters. For MSG-CHN, NLSPN, and CostDC-\\nNet methods for meta-initialization, preparation, and adaptation.\\nat 10Hz. Objects are annotated across the full 360â—¦ field.\\nWe obtain our validation set by sampling from the whole\\nvalidation dataset every 0.6 seconds. Range sensor inputs are\\nobtained by projecting the top lidarâ€™s point cloud scan to the\\ncamera frame. We obtained the ground truth by projecting\\n10 forward and backward frames from front lidar and top\\nlidar to the image frame, which approximately counts for\\n1 second of capture. To assume that the reprojected scenes\\nare static, we removed the moving objects in the scenes\\nusing object annotations. Also, outlier removal is utilized\\nfor filtering out errorenous depth points.\\nD. Implemetation details\\nHyperparameter. We specifically note the hyperparameters\\nof three methods for meta-initialization, preparation, and\\n12\\nKITTI â†’ Waymo\\nKITTI â†’ VKITTI-FOG\\nKITTI â†’ nuScenes\\nMethod\\nâ„“z\\nâ„“sm\\nâ„“proxy\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMSG-CHN\\nâœ“\\n951.25Â±3.14\\n3512.07Â±6.40\\n978.84Â±3.36\\n3561.40Â±15.48\\n3164.46Â±11.32\\n6453.54Â±17.31\\nâœ“\\nâœ“\\n613.01Â±1.99\\n1935.43Â±9.14\\n732.61Â±6.02\\n3113.11Â±21.78\\n2865.15Â±9.96\\n6144.48Â±24.14\\nâœ“\\nâœ“\\n608.91Â±1.74\\n1921.83Â±2.54\\n728.24Â±3.73\\n3087.36Â±15.92\\n2834.08Â±17.64\\n6096.56Â±21.08\\nNLSPN\\nâœ“\\n837.66Â± 8.73\\n3668.94Â± 25.90\\n715.86Â±26.36\\n3034.21Â± 57.65\\n5076.83Â±53.85\\n9710.88Â± 89.76\\nâœ“\\nâœ“\\n489.46Â±5.45\\n1613.66Â±30.04\\n705.14Â±16.86\\n3059.64Â±97.85\\n2783.61Â±159.62\\n6313.4Â±276.09\\nâœ“\\nâœ“\\nâœ“\\n477.28Â±3.32\\n1598.64Â±18.95\\n686.91Â±22.14\\n2666.70Â±56.64\\n2589.25Â±59.03\\n6006.18Â±90.66\\nCostDCNet\\nâœ“\\n816.33Â±32.01\\n3431.96Â±55.34\\n807.62Â±69.12\\n3254.83Â±179.90\\n3135.11Â±81.76\\n7596.49Â±159.16\\nâœ“\\nâœ“\\n469.52Â±2.54\\n1594.38Â±6.10\\n516.93Â±1.62\\n2751.21Â±17.42\\n2067.42Â±10.23\\n5487.85Â±37.21\\nâœ“\\nâœ“\\nâœ“\\n466.44Â±1.63\\n1580.38Â±11.48\\n512.72Â±0.74\\n2735.01Â±3.53\\n2062.28Â±11.24\\n5509.96Â±23.41\\nVOID â†’ NYUv2\\nVOID â†’ SceneNet\\nVOID â†’ ScanNet\\nMSG-CHN\\nâœ“\\n971.64Â±66.86\\n1291.45Â±45.67\\n242.11Â±4.24\\n491.48Â±10.49\\n462.95Â±34.84\\n659.9Â±37.93\\nâœ“\\nâœ“\\n1005.49Â±25.97\\n1329.76Â±25.01\\n194.60Â±3.64\\n425.16Â±10.58\\n330.20Â±48.46\\n503.73Â±57.14\\nâœ“\\nâœ“\\nâœ“\\n699.60Â±6.00\\n1120.37Â±9.76\\n192.74Â±1.72\\n424.49Â±4.58\\n302.21Â±4.10\\n480.08Â±8.03\\nNLSPN\\nâœ“\\n145.72 Â±6.55\\n271.78Â± 9.91\\n130.49Â±13.64\\n337.14Â±28.38\\n112.38Â±1.72\\n234.60Â±3.46\\nâœ“\\nâœ“\\n128.17Â±4.13\\n240.97Â±3.86\\n118.65Â±2.24\\n337.63Â±2.58\\n77.84Â±0.28\\n169.81Â±0.50\\nâœ“\\nâœ“\\nâœ“\\n124.41Â±2.27\\n240.73Â±5.72\\n113.93Â±1.49\\n333.41Â±4.32\\n74.77Â±0.31\\n166.61Â±0.45\\nCostDCNet\\nâœ“\\n152.43Â±13.07\\n432.20Â±54.51\\n213.4Â±19.52\\n597.22Â±49.78\\n91.13Â±1.40\\n286.17Â±9.07\\nâœ“\\nâœ“\\n101.31Â±1.67\\n217.77Â±6.00\\n134.51Â±4.23\\n360.33Â±9.67\\n69.02Â±0.51\\n164.90Â±2.38\\nâœ“\\nâœ“\\nâœ“\\n95.87Â±2.16\\n203.83Â±4.72\\n125.75Â±1.93\\n357.12Â±4.13\\n68.17Â±0.44\\n162.35Â±1.12\\nTable 5. Ablation study of each loss term. Note that NLSPN and CostDCNet update the adaptation layer and batch normalization layers, yet\\nMSGCHN only updates the adaptation layer.\\nadaptation on Table 4.\\nEpochs and training details Adaptation occurs in a sin-\\ngle epoch, with â€˜the number of iterations per data pointâ€™\\n(inner-iter) specified in Tab. 4. During meta-initialization\\nand preparation stages, the meta- and proxy layers are trained\\nfor 6 epochs. Batch sizes for all methods are: 48 for prepara-\\ntion stage, 16 for meta-initialization and adaptation stages,\\nwith the exception of ScanNet [6], using a batch size of 36.\\nTo prevent collapse during preparation stage, we follow the\\nprotocol of [13]; we exploit the projection / prediction layers\\nand divide online / target branch, and update target projec-\\ntion layer with exponential moving average of online branch.\\nWe used embedding dimension and hidden dimension of 512\\nfor MSGCHN, and 1024 for CostDCNet and NLSPN. The\\nlearning rates for meta-initialization and preparation stage\\nwill be released with the code release.\\nEvaluation. We evaluate our adaptation models on bottom-\\ncropped regions in the outdoor dataset, where the sparse\\ndepth exists. For outdoor dataset, models are evaluated on\\nthe bottom cropped region of the test split, 1242 Ã— 240 for\\nVirtual KITTI, and 1600 Ã— 544 for nuScenes. For indoor\\ndataset, we evaluated the models on the entire region. The\\ndefinition of the error metrics in evaluation are described in\\nTable 6. We evaluate our model on depth range from 0.0 to\\n80.0 meters for the ourdoor, and 0.2 to 5.0 meters for the\\nindooor.\\nCode release We will release the code, training scripts, and\\ntrained models with meta layer and proxy layers.\\nMetric\\nDefinition\\nMAE\\n1\\n|â„¦|\\nP\\nxâˆˆâ„¦ | Ë†d(x) âˆ’ dgt(x)|\\nRMSE\\nKITTI â†’ VKITTI\\nMethod\\nMAE\\nRMSE\\nMSG-CHN\\nPretrained\\n2433.46\\n6675.16\\nCoTTA\\n839.19Â±12.78\\n3625.38Â±39.35\\nOurs-fast\\n800.88Â±1.86\\n3268.26Â±4.12\\nNLSPN\\nPretrained\\n1469.19\\n8060.97\\nBN Adapt\\n1016.87Â±8.84\\n3453.00Â±3.21\\nBN Adapt, â„“z, â„“sm\\n855.12Â±14.56\\n3516.85Â±58.63\\nCoTTA\\n775.09Â±3.63\\n3585.37Â±13.31\\nOurs-fast\\n849.43Â±3.61\\n3540.44Â±3.57\\nOurs\\n639.19Â±5.68\\n2934.36Â±33.80\\nCostDCNet\\nPretrained\\n845.35\\n3774.01\\nBN Adapt\\n1248.35Â±0.25\\n4267.64Â±0.62\\nBN Adapt, â„“z, â„“sm\\n1016.87Â±8.84\\n3453.00Â±3.21\\nCoTTA\\n698.42Â±9.93\\n3324.59Â±30.21\\nOurs-fast\\n822.49Â±13.55\\n3331.24Â±55.30\\nOurs\\n639.91Â±8.92\\n2951.21Â±30.93\\nTable 7. Additional results for test-time adaptation for depth com-\\npletion on KITTI â†’ VKITTI.\\nF. Ablation study\\nHere, we ablate the effect of each loss term denoted with\\nthe checkmarks in Table 5. Using sparse depth consistency\\nloss â„“z (Eqn. 4) alone can improve the pretrained model as\\nit learns the shapes of the test domain. However, because\\nof the sparsity, the supervision signal is weak, leading the\\nmodel to exhibit artifacts and distortions in the depth map.\\nIncluding a local smoothness loss â„“sm (Eqn. 5) mitigates this\\nby propagating depth to nearby regions. However, without\\nknowledge of 3D shapes compatible with the sparse points,\\nthe wrong predictions are sometimes propagated as in the\\nleft bounding box region from Row 1, Column 4 of Fig. 4.\\nThe best-performing method employs the proposed proxy\\nembeddings as a regularizer to guide the adaptation layer\\nupdate. As the proxy mapping produces test-time features\\nthat follow the distribution of the source domain, minimizing\\nour proxy consistency loss (Eqn. 7) implicitly aligns the test\\ndomain features to those of the source domain that are com-\\npatible with the 3D scene observed by the test-time sparse\\npoint cloud. Not only does this improve overall performance,\\nbut it also reduces standard deviation in error, which can\\nbe interpreted as an increase in the stability of the adapta-\\ntion. We show qualitative comparisons against BN Adapt\\nin Fig. 4, where boxes highlight improvements by fixing\\nerroneous propagation by local smoothness (e.g., bleeding\\neffect, which is not mitigated by using image gradients as\\nguidance in Eqn. 5). Quantitatively, we improve over the\\nbaseline by an average of 21.09% across all methods and\\ndatasets, demonstrating the efficacy of our proxy embedding.\\nWaymo â†’ VKITTI-FOG\\nMethod\\nMAE\\nRMSE\\nMSG-CHN\\nPretrained\\n1473.14\\n4676.19\\nCoTTA\\n1348.02Â±38.03\\n4016.67Â±28.16\\nOurs-fast\\n1052.78Â±5.74\\n3891.05Â±17.34\\nNLSPN\\nPretrained\\n2734.27\\n37621.10\\nBN Adapt, â„“z, â„“sm\\n1205.96Â±40.14\\n3857.88Â±101.15\\nCoTTA\\n2485.66Â±18.05\\n6307.96Â±48.64\\nOurs\\n808.16Â±7.86\\n3536.58Â±91.15\\nCostDCNet\\nPretrained\\n1261.00\\n4360.37\\nBN Adapt, â„“z, â„“sm\\n742.99Â±2.17\\n3403.00Â±3.62\\nCoTTA\\n1150.16Â±5.69\\n4134.16Â±9.15\\nOurs\\n724.77Â±5.18\\n3349.21Â±29.00\\nTable 8. Additional results for test-time adaptation for depth com-\\npletion on Waymo â†’ VKITTI-FOG.\\nG. KITTI â†’ VKITTI results\\nHere, we present additional results on KITTI â†’ VKITTI\\nadaptation. Test-time adaptation results are shown in Table 7.\\nConsistent with the trends observed in the main paper, our\\nmethod outperforms over both BN Adapt and CoTTA, with\\na 21.82% improvement compared to BN Adapt and 12.6%\\nimprovement over CoTTA.\\nH. Experiment with different source dataset\\nIn our main paper, the only source dataset for outdoor adap-\\ntation scenario was KITTI which is the most popular outdoor\\ndepth completion dataset. To validate our methodâ€™s appli-\\ncability to models trained on diverse source datasets, we\\ninclude additional results from adaptation scenarios using\\na model trained on the Waymo dataset, as shown in Table\\n8. Our method shows an improvement over CoTTA and BN\\nAdapt by 21.70%.\\nA noteworthy observation from the Waymo adaptation\\nresults, when compared to the KITTI â†’ VKITTI-fog results\\nfrom the main paper, is that the adaptation result of KITTI\\noutperforms that of Waymo. This difference is caused by\\nfrom the domain discrepancies between KITTI and VKITTI-\\nfog datasets versus the domain gap between Waymo and\\nVKITTI-fog. For example, VKITTIâ€™s object appearances\\nand resolution (1226Ã—370 for KITTI, and 1242Ã—375 for\\nVKITTI) are more akin to those in the KITTI dataset.\\nConversely, the Waymo dataset features higher resolution\\n(1920Ã—1280) and different object shapes compared to KITTI\\nand VKITTI. Hence, the adaptation result is influenced by\\nthe extent of domain discrepancy between the source and\\ntarget datasets.\\nI. Quantitative preliminary results\\nTo provide a precise observation, we provide the quantitative\\nresults of model sensitive study in Tab. 9.\\n14\\nMethod\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nMSG-CHN\\nNLSPN\\nCostDCNet\\nDataset\\nVOIDâ†’NYUv2\\nVOIDâ†’ScanNet\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nMAE\\nRMSE\\nImage only\\n2072.78\\n2462.63\\n969.14\\n1228.44\\n1359.16\\n1619.40\\n2001.90\\n2451.681\\n899.41\\n1151.12\\n1216.17\\n1459.46\\nSparse depth only\\n734.13\\n1046.28\\n237.47\\n402.47\\n147.76\\n354.57\\n211.86\\n444.62\\n162.29\\n276.29\\n88.25\\n205.46\\nImage + sparse depth\\n1040.93\\n1528.98\\n387.36\\n704.66\\n189.10\\n446.71\\n316.646\\n698.633\\n232.332\\n431.199\\n144.311\\n458.692\\nDataset\\nKITTIâ†’Waymo\\nKITTIâ†’nuScenes\\nImage only\\n12766.791 18324.83 18829.96 24495.73 13598.50 18376.15 11823.061 17244.44 15835.04 22613.78 12794.65 16744.15\\nSparse depth only\\n861.13\\n2706.75\\n1290.28\\n3571.26\\n1210.93\\n3102.49\\n3943.97\\n7306.33\\n2540.58\\n6203.66\\n2996.28\\n6773.06\\nImage + sparse depth\\n1103.33\\n2969.39\\n1173.26\\n3092.02\\n1084.18\\n2819.42\\n3331.82\\n6449.09\\n2656.61\\n6146.59\\n3064.72\\n6630.65\\nTable 9. Model sensitivity to input modalities. Depth completion networks have a high reliance on sparse depth modality. Performing\\ninference in a novel domain without the RGB image, i.e., using just sparse depth as input, can improve over using both data modalities.\\n15\\n'},\n",
       " {'id': 'http://arxiv.org/abs/2402.03311v1',\n",
       "  'title': 'HASSOD: Hierarchical Adaptive Self-Supervised Object Detection',\n",
       "  'published_date': datetime.datetime(2024, 2, 5, 18, 59, 41),\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2402.03311v1',\n",
       "  'summary': 'The human visual perception system demonstrates exceptional capabilities in\\nlearning without explicit supervision and understanding the part-to-whole\\ncomposition of objects. Drawing inspiration from these two abilities, we\\npropose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a\\nnovel approach that learns to detect objects and understand their compositions\\nwithout human supervision. HASSOD employs a hierarchical adaptive clustering\\nstrategy to group regions into object masks based on self-supervised visual\\nrepresentations, adaptively determining the number of objects per image.\\nFurthermore, HASSOD identifies the hierarchical levels of objects in terms of\\ncomposition, by analyzing coverage relations between masks and constructing\\ntree structures. This additional self-supervised learning task leads to\\nimproved detection performance and enhanced interpretability. Lastly, we\\nabandon the inefficient multi-round self-training process utilized in prior\\nmethods and instead adapt the Mean Teacher framework from semi-supervised\\nlearning, which leads to a smoother and more efficient training process.\\nThrough extensive experiments on prevalent image datasets, we demonstrate the\\nsuperiority of HASSOD over existing methods, thereby advancing the state of the\\nart in self-supervised object detection. Notably, we improve Mask AR from 20.2\\nto 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:\\nhttps://HASSOD-NeurIPS23.github.io.',\n",
       "  'pdf_text': 'HASSOD: Hierarchical Adaptive Self-Supervised\\nObject Detection\\nShengcao Cao1\\nDhiraj Joshi2\\nLiang-Yan Gui1\\nYu-Xiong Wang1\\n1University of Illinois at Urbana-Champaign\\n2IBM Research\\n1{cao44,lgui,yxw}@illinois.edu\\n2djoshi@us.ibm.com\\nAbstract\\nThe human visual perception system demonstrates exceptional capabilities in learn-\\ning without explicit supervision and understanding the part-to-whole composition\\nof objects. Drawing inspiration from these two abilities, we propose Hierarchical\\nAdaptive Self-Supervised Object Detection (HASSOD), a novel approach that\\nlearns to detect objects and understand their compositions without human supervi-\\nsion. HASSOD employs a hierarchical adaptive clustering strategy to group regions\\ninto object masks based on self-supervised visual representations, adaptively de-\\ntermining the number of objects per image. Furthermore, HASSOD identifies\\nthe hierarchical levels of objects in terms of composition, by analyzing coverage\\nrelations between masks and constructing tree structures. This additional self-\\nsupervised learning task leads to improved detection performance and enhanced\\ninterpretability. Lastly, we abandon the inefficient multi-round self-training process\\nutilized in prior methods and instead adapt the Mean Teacher framework from\\nsemi-supervised learning, which leads to a smoother and more efficient training\\nprocess. Through extensive experiments on prevalent image datasets, we demon-\\nstrate the superiority of HASSOD over existing methods, thereby advancing the\\nstate of the art in self-supervised object detection. Notably, we improve Mask\\nAR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:\\nhttps://HASSOD-NeurIPS23.github.io.\\n1\\nIntroduction\\nThe development of human visual perception is remarkable for two key abilities: 1) Humans begin\\nlearning to perceive objects in their environment through observation alone [25], without needing\\nto learn the names of these objects from external supervision. 2) Moreover, human perception\\noperates in a hierarchical manner, enabling individuals to recognize the part-to-whole composition of\\nobjects [2, 23]. These characteristic capabilities offer valuable insights into the learning processes\\nof object detectors, which still heavily rely on the availability and quality of fine-grained training\\ndata. For example, the state-of-the-art detection/segmentation model, Segment Anything Model\\n(SAM) [18], is developed on a dataset of 11 million images and 1 billion object masks. It remains an\\nopen question how to effectively learn to detect objects and recognize their compositions from even\\nlarger-scale datasets (e.g., LAION-5B [26]) without such object-level annotations.\\nIn prior work on self-supervised object detection [37, 38], a two-stage discover-and-learn paradigm is\\nadopted: 1) Self-supervised visual representations [5, 15] are obtained, and a saliency-based method\\nis employed to extract the most prominent one or few objects. 2) Subsequently, an object detector\\nis trained based on these pseudo-labels, sometimes involving multiple rounds of self-training for\\nrefinement. However, despite such attempts to eliminate the need for external supervision, several\\nweaknesses persist in these approaches: 1) Narrow coverage of objects. The focus on only one or few\\nobjects per image in previous methods undermines their ability to fully exploit the learning signals in\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\narXiv:2402.03311v1  [cs.CV]  5 Feb 2024\\nInput\\nCutLER\\nHASSOD (Ours)\\nWhole Objects\\nHASSOD (Ours)\\nWhole + Part + Subpart\\nLVIS\\nObjects365\\nSA-1B\\nFigure 1: Fully self-supervised object detection and instance segmentation on prevalent image datasets.\\nOur approach, HASSOD, demonstrates a significant improvement over the previous state-of-the-art\\nmethod, CutLER [38], by discovering a more comprehensive range of objects. Moreover, HASSOD\\nunderstands the part-to-whole object composition like humans do, while previous methods cannot.\\nnatural scene images containing dozens of objects, such as those in the MS-COCO dataset [20]. This\\nnarrow focus also restricts the capability of these methods to accurately detect and segment multiple\\nobjects within an image. 2) Lack of composition. Prior work often overlooks the composition of\\nobjects, neglecting the identification of hierarchical levels for whole objects, part objects, and subpart\\nobjects (e.g., considering an image of a bicycle; the bicycle is a whole object, its wheels and handles\\nare parts, and the spokes and tires are subparts). This oversight not only limits the interpretability of\\nlearned object detectors, but also hinders the modelâ€™s ability to tackle the intrinsic ambiguity in the\\ntask of segmentation. 3) Inefficiency. The reliance on multi-round self-training in earlier methods\\ncan result in inefficient and non-smooth training processes, which further constrains the potential of\\nself-supervised object detection and comprehension of object composition.\\nInspired by the unsupervised, hierarchical human visual perception system, we propose Hierarchi-\\ncal Adaptive Self-Supervised Object Detection (HASSOD), aiming to address these limitations\\nmentioned above and better harness the potential of self-supervised object detection, as depicted in\\nFigure 1. First, unlike previous methods that limit their focus to one or few prominent objects per\\nimage, HASSOD employs a hierarchical adaptive clustering strategy to group regions into object\\nmasks, based on self-supervised visual representations. By adjusting the threshold for terminating the\\nclustering process, HASSOD is capable of effectively determining the appropriate number of objects\\nper image, thus better leveraging the learning signals in images with multiple objects.\\nThe second key component of HASSOD is its ability to identify hierarchical levels of objects in terms\\nof composition. By analyzing the coverage relations between masks and building tree structures,\\nour approach successfully classifies objects as whole objects, part objects, or subpart objects. This\\nnovel self-supervised learning task not only improves detection performance, but also enhances the\\ninterpretability and controllability of the learned object detector, a property that prior self-supervised\\ndetectors lack. Therefore, HASSOD users can comprehend how the detected whole objects are\\nassembled from smaller constituent parts. Simultaneously, they can control HASSOD to perform\\ndetection at their preferred hierarchical level, thereby catering more effectively to their needs.\\nFinally, HASSOD abandons the multi-round self-training used in previous methods which lacks\\nefficiency and smoothness. Instead, we take inspiration from the Mean Teacher [22, 31] framework\\nin semi-supervised learning, employing a teacher model and a student model that mutually learn\\nfrom each other. This innovative adaptation facilitates a smoother and more efficient training process,\\nresulting in a more effective self-supervised object detection approach.\\nIn summary, the key contributions of HASSOD include:\\n2\\nâ€¢ A hierarchical adaptive clustering strategy that groups regions into object masks based on self-\\nsupervised visual representations, adaptively determining the number of objects per image and\\neffectively discover more objects from natural scenes.\\nâ€¢ The ability to identify hierarchical levels of objects in terms of composition (whole/part/subpart)\\nby analyzing coverage relations between masks and building tree structures, leading to improved\\ndetection performance and enhanced interpretability.\\nâ€¢ A novel adaptation of the Mean Teacher framework from semi-supervised learning, which replaces\\nthe multi-round self-training in prior methods, leading to smoother and more efficient training.\\nâ€¢ State-of-the-art performance in self-supervised object detection, enhancing Mask AR from 20.2 to\\n22.5 on LVIS [11], and from 17.0 to 26.0 on SA-1B [18]. Remarkably, these results are achieved\\nthrough training with only 1/5 of the images and 1/12 of the iterations required by prior work.\\n2\\nRelated Work\\nUnsupervised object detection/discovery. Identifying and locating objects in images without using\\nany human annotations is a challenging task, as it requires learning the concept of objects from\\nimage data without any external supervision. OSD [33] formulates this task as an optimization\\nproblem on a graph, where the nodes are object proposals generated by selective search, and the\\nedges are constructed based on visual similarities. rOSD [34] improves the scalability of OSD\\nwith a saliency-based region proposal algorithm and a two-stage strategy. LOD [35] formulates\\nunsupervised object discovery as a ranking optimization problem for improved computation efficiency.\\nFollowing the observation that DINO [5], a self-supervised pre-training method, can segment the most\\nprominent object in each image, LOST [29], FOUND [30], and FreeSOLO [37] train object detectors\\nusing saliency-based pseudo-labels. TokenCut [39] and CutLER [38] also use self-supervised\\nrepresentations, but generate pseudo-labels by extending Normalized Cuts [28]. Saliency-based\\nregion proposal and Normalized Cuts are both focused on the prominent objects in each image, and\\nusually only propose one or few objects per image. Different from these approaches, HASSOD\\nproduces initial pseudo-labels using a hierarchical adaptive clustering strategy, which can adaptively\\ndetermine the number of objects depending on the image contents.\\nObject detection by parts. Detecting objects by identifying their composing parts has been widely\\nstudied in computer vision. Deformable Parts Model (DPM) [9] is a seminal approach that utilizes\\ndiscriminatively trained part-based models for object detection, which effectively models complex\\nobject structures and improves over monolithic detectors. A following method [6] not only detects\\nthe objects, but also simultaneously represents them using body parts, highlighting the importance of\\nboth holistic models and part-based representations. This idea is extended by leveraging both whole\\nobject and part detections to infer human actions and attributes [10], suggesting the advantage of a\\ncombined approach. In this work, we revisit this classic idea of representing and detecting whole\\nobjects as well as their parts in the context of self-supervised learning.\\n3\\nApproach\\nIn this section, we introduce the learning process in our proposed approach, Hierarchical Adap-\\ntive Self-Supervised Object Detection (HASSOD). Following prior work on unsupervised object\\ndetection [29, 30, 37â€“39], HASSOD adopts a two-stage discover-and-learn process to learn a self-\\nsupervised object detector, as illustrated in Figure 2. In the first stage, we discover objects from\\nunlabeled images using self-supervised representations, and generate a set of initial pseudo-labels.\\nThen in the second stage, we learn an object detector based on the initial pseudo-labels, and smoothly\\nrefine the model by self-training. The first stage is based on pre-trained, fixed visual features, and the\\nsecond stage learns an object detector to improve over the fixed visual features and pseudo-labels. In\\nthe following subsections, we describe the three core components of HASSOD in detail.\\n3.1\\nHierarchical Adaptive Clustering\\nIn the first stage, HASSOD creates a set of pseudo-labels as the initial self-supervision source.\\nWe propose a hierarchical adaptive clustering strategy to discover object masks as pseudo-labels,\\nusing only unlabeled images and a frozen self-supervised visual backbone. Figure 3 provides an\\noverview of this procedure. Our hierarchical adaptive clustering algorithm extends agglomerative\\n3\\nStage 2: Object Detector Learning\\n(Sections 3.2 & 3.3)\\nStage 1: Initial Pseudo-Label Discovery\\n(Section 3.1)\\nUnlabeled\\nRaw Images\\nFrozen\\nDINO ViT\\nInitial Pseudo-\\nLabeled Masks\\nObject Detector\\nCascade Mask R-CNN\\nMean Teacher\\nSelf-Training\\nFigure 2: Two-stage discover-and-learn process in HASSOD. Stage 1 uses a frozen, self-supervised\\nDINO [5] ViT backbone to discover initial pseudo-labels from unlabeled images. Stage 2 learns an\\nobject detector to improve over the pre-trained features and initial pseudo-labels.\\nDINO \\nViT\\nPatch-Level\\nFeatures\\nMerge\\nðœƒ!\\n\"#$%# = 0.2\\nðœƒ&\\n\"#$%# = 0.4\\nðœƒ\\'\\n\"#$%# = 0.1\\nMerge\\nMerge\\nPost-\\nProcess\\nPost-\\nProcess\\nPost-\\nProcess\\nAll Initial\\nPseudo-Labels\\nEnsem-\\nble\\nWhole\\nPart\\nSubpart\\nSplit\\nHierarchical\\nLevels of Objects\\nFigure 3: Hierarchical adaptive clustering and hierarchical levels of objects. The procedure of creating\\ninitial pseudo-labels for training the object detector without any human annotations includes the\\nfollowing steps: (Initialize) Visual features are extracted from the given image by a ViT pre-trained\\nwith DINO [5], and each 8 Ã— 8 patch is initialized as one individual region. (Merge) Adjacent regions\\nwith the highest feature similarities are progressively merged into object masks, until the pre-set\\nthresholds Î¸merge\\ni\\nare reached. (Post-Process) Object masks are selected and refined using simple\\npost-processing techniques. (Ensemble) Results from multiple thresholds {Î¸merge\\ni\\n}3\\ni=1 are combined\\nto ensure better coverage of potential objects. (Split) Analysis of coverage relations divides objects\\ninto three hierarchical levels: whole, part, and subpart. The example on the right illustrates the tree\\nstructure of object composition: The whole aircraft is composed of an upper and a lower part. The\\nupper part further consists of a left wing, a right wing, and a person standing on it.\\nclustering [12], grouping adjacent image patches into semantically coherent masks based on the\\nsimilarity of self-supervised visual representations. More specifically, we use a frozen ViT-B/8\\nmodel [8] pre-trained on unlabeled ImageNet [7] by DINO [5], a self-supervised representation\\nlearning method, to extract visual features. For each image, we take the feature map generated by\\nthis model at its final Transformer layer [32]. Each spatial element in the feature map corresponds to\\na 8 Ã— 8 patch in the original image.\\nTo initiate the hierarchical adaptive clustering process, we treat each patch as an individual region.\\nWe then compute the pairwise cosine similarity between the features of adjacent regions to measure\\ntheir closeness in the semantic feature space. The regions are gradually merged into masks that\\nrepresent objects by iteratively performing the following steps: 1) Identify the pair of adjacent regions\\nwith the highest feature similarity. 2) If the similarity is smaller than the pre-set threshold Î¸merge, stop\\nthe merging process. 3) Merge the two regions, and compute the feature of the merged region by\\naveraging all the patch-level features within it. 4) Update the pairwise similarity between this newly\\nmerged region and its neighbors. This merging process is visualized in Figure 3, columns 1-3.\\nOnce the merging process is complete, we perform a series of automated post-processing steps to\\nrefine and select the masks, including Conditional Random Field (CRF) [19] and filtering out masks\\nthat are smaller than 100 pixels or contain more than two corners of the image. These steps are based\\non standard practices in previous work [38] and require no manual intervention. Our hierarchical\\nadaptive clustering strategy effectively groups regions into object masks based on self-supervised\\nvisual representations, adaptively determining the appropriate number of objects per image. In\\n4\\nimages containing multiple objects with heterogeneous semantic features, the merging process stops\\nearlier, resulting in a larger number of regions corresponding to different objects. Conversely, in\\nhighly homogeneous images, more regions are merged, leading to fewer object masks. This adaptive\\napproach enables HASSOD to cover more objects for self-supervised learning, rather than being\\nlimited by one or a few prominent objects in each image in prior work [38, 39].\\nIn practice, we are not restricted to one single fixed threshold Î¸merge to determine the stopping\\ncriterion for the clustering process. Instead, we find it beneficial to ensemble results from multiple\\n(e.g., 3) pre-set thresholds {Î¸merge\\ni\\n}3\\ni=1. When the currently highest feature similarity reaches one of\\nthese thresholds, we record the derived object masks from the merged regions at that step. Utilizing\\nmultiple thresholds allows us to capture objects of various sizes and at different hierarchical levels\\nof composition, enabling a more comprehensive coverage of objects in scene images. The post-\\nprocessing and ensemble are visualized in Figure 3, columns 4-5.\\n3.2\\nHierarchical Level Prediction\\nIn the following second stage, HASSOD learns an object detection and instance segmentation model,\\ne.g., Cascade Mask R-CNN [4], using the initial pseudo-labels generated in the first stage. By training\\non such pseudo-labels, the model learns to recognize common objects across different training images,\\nand thus achieves enhanced generalization to images which the model has not seen during training.\\nIn addition to the standard object detection objective, we aim to equip our detector with the ability\\nto understand the hierarchical structure among objects and their constituent parts. In HASSOD, we\\nincorporate the concept of hierarchical levels into object masks by leveraging the coverage relations\\nbetween them. Formally, we say mask A is covered by mask B when three conditions are satisfied\\n(with respect to a pre-set coverage threshold Î¸cover%): 1) More than Î¸cover% of pixels in mask A are\\nalso in mask B. 2) Less than Î¸cover% of pixels in mask B are in mask A. 3) Mask B is the smallest\\namong all masks satisfying the previous two conditions. Intuitively, if mask B covers mask A, it\\nsuggests that A is a part of B and B is at a higher level than A. If we consider A and B as tree\\nnodes, A should be a child of B. Using all such coverage relations, we can construct a forest of trees\\nthat contain all masks in an image. Ultimately, the roots of all trees in this image are considered\\nas â€œwholeâ€ objects, their direct children are â€œpartâ€ objects, and all the remaining descendants are\\nâ€œsubpartâ€ objects. An example is shown on the right side of Figure 3.\\nAfter identifying the hierarchical levels of object masks in the pseudo-labels, we attach a new\\nclassification head to the object detector for level prediction, which classifies each predicted object as\\na whole object, a part object, or a subpart object. This new component enables HASSOD to model\\nobject composition effectively, resulting in improved object detection performance and enhanced\\ninterpretability compared with previous self-supervised object detection methods. The hierarchical\\nlevel prediction head is added alongside the existing foreground/background classification head, box\\nregression head, and mask prediction head. Subsequently, we train the object detector using the initial\\nset of object mask pseudo-labels obtained from the hierarchical adaptive clustering process, as well\\nas the additional level prediction task.\\n3.3\\nMean Teacher Training with Adaptive Targets\\nNotably, the initial pseudo-labels derived in the first stage contain noise and are not perfectly aligned\\nwith real objects. To improve over such noisy pseudo-labels, prior work [37, 38] usually employs\\nmulti-round self-training to refine the model, i.e., using a well-trained detector to re-generate pseudo-\\nlabels and re-train a new detector. For the first time, HASSOD refines the object detector efficiently\\nand smoothly by adapting the Mean Teacher learning paradigm [22, 31] from semi-supervised\\nlearning to the fully self-supervised setting.\\nBefore introducing our innovative adaptation of Mean Teacher in the self-supervised setting, we\\nfirst briefly summarize the mutual-learning process in Mean Teacher (see Figure 4). Mean Teacher\\nemploys two models, a teacher and a student, which learn from each other. The teacher takes weakly-\\naugmented, unlabeled images as input and provides detection outputs as training targets for the student.\\nThe studentâ€™s weights are updated to minimize the discrepancy between its predictions and the targets\\ngiven by the teacher on the same unlabeled images but with strong augmentation. In the semi-\\nsupervised setting, the student receives supervision from two sources simultaneously. One source\\nis the â€œteacher-to-studentâ€ branch mentioned above, and the other is the â€œlabel-to-studentâ€ branch\\n5\\nLabel-to-Student Branch\\nTeacher-to-Student Branch\\nUnlabeled\\nImage Batch A\\nUnlabeled\\nImage Batch B\\nTeacher\\nDetector\\nStudent\\nDetector\\nTeacher\\'s \\nPredictions A\\nInitial\\nPseudo-Labels B\\nStudent\\'s \\nPredictions A\\nStudent\\'s \\nPredictions B\\nð¿!\"#$%\"&\\nð¿\\'#(\"\\'\\nð¿!)!#\\'\\nStrong \\nAug.\\nWeak \\nAug.\\nDiscover\\nPredict\\nPredict\\nEMA\\nÃ—ð›¼!\"#$%\"& â†—\\nÃ—ð›¼\\'#(\"\\' â†˜\\nPredict\\nSupervise\\nFigure 4: Mean Teacher self-training with adaptive targets in HASSOD. Two detectors of the same\\narchitecture, the teacher and the student, learn from each other to improve over the initial pseudo-\\nlabels. The teacher is updated as the exponential moving average (EMA) of the student. The\\nstudent receives supervision from two branches: The teacher-to-student branch (top) encourages\\nthe student to mimic the teacherâ€™s predictions; the label-to-student branch (bottom) minimizes the\\ndiscrepancy between the studentâ€™s predictions and the initial pseudo-labels. During training, our\\nproposed adaptive target strategy increases the weight for the teacher-to-student branch, Î±teacher, and\\ndecreases the weight for the label-to-student branch, Î±label, since the teacher becomes a more and\\nmore reliable self-supervision source compared with the initial pseudo-labels.\\nwhere the student learns from images with ground-truth labels. Both branches compute standard\\ndetection losses (e.g., bounding box classification and regression), and the student is optimized to\\nminimize the total loss. The teacherâ€™s weights are an exponential moving average of the studentâ€™s\\nweights, ensuring smooth and stable training targets.\\nIn HASSOD, we do not have any labeled images from human supervision but instead utilize two\\nsources of self-supervision. One source is the initial pseudo-labels obtained from our hierarchical\\nadaptive clustering, which functions similarly to the labels-to-student branch in the semi-supervised\\nsetting. The other source is the detection predictions made by the teacher model, which corresponds\\nto the teacher-to-student branch in Mean Teacher. Different from standard Mean Teacher, our method\\nemploys adaptive training targets, as we gradually adjust the loss weights for the two branches.\\nThis is because the initial pseudo-labels may not effectively cover all possible objects, while the\\nteacher model will progressively improve as a better source of supervision. Consequently, during\\nMean Teacher self-training, we continuously decrease the loss weight Î±label for the branch that uses\\nthe initial pseudo-labels and increase the loss weight Î±teacher for the branch based on the teacherâ€™s\\npredictions, following a cosine schedule.\\n4\\nExperiments\\nIn this section, we conduct extensive experiments to evaluate HASSOD in comparison with previous\\nmethods. We first describe the training details and efficiency in Section 4.1. Section 4.2 introduces\\nthe datasets and metrics used for evaluation. Section 4.3 presents our main results of self-supervised\\nobject detection and instance segmentation. Section 4.4 provides some qualitative results and analysis.\\nSection 4.5 conducts further experiments to verify the effects of each component in HASSOD.\\nAdditional quantitative and qualitative results are included in the appendix.\\n4.1\\nData-Efficient and Computation-Efficient Training\\nWe train a Cascade Mask R-CNN [4] with a ResNet-50 [13] backbone on MS-COCO [20] images.\\nThe backbone is initialized from DINO [5] self-supervised pre-training. We use both the train and\\nunlabeled splits of MS-COCO, totaling to about 0.24 million images. Notably, this amount of\\nimages is only 1/5 of ImageNet used by prior work CutLER [38]. Compared with ImageNet-like [7]\\niconic images, images in MS-COCO are mostly captured in complex scenes containing multiple\\nobjects with diverse layouts and compositions. Therefore, each image offers richer learning resources\\nfor object detectors, enabling effective detector training with significantly fewer images. The whole\\n6\\ntraining process spans 40,000 iterations, taking about 20 hours on 4 NVIDIA A100 GPUs. The\\nefficiency and smoothness introduced by the Mean Teacher self-training approach reduces the training\\niterations to 1/12 of that required by CutLER [38], highlighting the computation efficiency of our\\ntraining strategy. Implementation details are included in Appendix J.\\n4.2\\nEvaluation Datasets and Metrics\\nWe mainly conduct our experiments in a zero-shot manner on the validation sets of three benchmark\\ndatasets, namely Objects365 [27], LVIS [11], and SA-1B [18]. Given that self-supervised object\\ndetection methods, including HASSOD, do not utilize class labels as a form of supervision, we follow\\nprior work [37â€“39] and evaluate these models as class-agnostic detectors, comparing them only\\nagainst the bounding boxes and masks provided in the dataset annotations.\\nâ€¢ Objects365 [27] is a large-scale object detection dataset containing 365 object categories. The\\ncombined validation sets of Objects365 v1 and v2 include 80,000 images in total.\\nâ€¢ LVIS [11] is a dataset that features a wide variety of over 1,200 object classes, using the same\\nimages as MS-COCO [20]. LVIS v1.0 validation set has 19,809 images, each annotated with\\nobject masks for instance segmentation.\\nâ€¢ SA-1B [18] is a recent dataset that includes 11 million images and 1 billion fine-grained, model-\\ngenerated object masks. SA-1B provides a more comprehensive coverage of all potential objects,\\nfacilitating a more robust evaluation of self-supervised object detectors. As SA-1B does not provide\\na validation split, we utilize a random subset of 50,000 images for our assessment.\\nIn terms of evaluation metrics, we focus primarily on average recall (AR) rather than average precision\\n(AP). The choice of AR over AP is motivated by the nature of the self-supervised task. In a dataset\\nwith a fixed number of classes, objects not labeled by humans â€“ simply because they do not fall under\\nthe designated classes â€“ may still be detected by a self-supervised detection model. Standard AP\\ncalculation would penalize such predictions as false positives, despite them being valid detections.\\nIn contrast, AR does not suffer from this issue, making it a more appropriate metric for our context.\\nBy prioritizing recall, we can more accurately assess the ability of our model to identify all relevant\\nobjects in an image, which aligns with the goal of the self-supervised object detection task. We\\nevaluate AR based on both bounding boxes for object detection (â€œBox ARâ€) and masks for instance\\nsegmentation (â€œMask ARâ€). Appendix A discusses the evaluation metrics in detail.\\n4.3\\nSelf-Supervised Detection and Segmentation\\nAfter we use HASSOD to train the object detection and instance segmentation model, Cascade\\nMask R-CNN, on MS-COCO images, we evaluate our model on Objects365, LVIS, and SA-1B\\ndatasets in a zero-shot manner, i.e., no further training on these three datasets. The whole training and\\nevaluation process is repeated for three times, and we report the mean performance for conciseness.\\nThe standard deviation of AR is less than 0.6 on all three datasets. Complete evaluation results are\\nincluded in Appendix H.\\nWe compare HASSOD with prior state-of-the-art self-supervised object detection methods, including\\nFreeSOLO [37] and CutLER [38]. We also include results from SAM [18], the latest supervised class-\\nagnostic detection/segmentation model, to gain understanding of the gap between self-supervised and\\nsupervised models, and how HASSOD is effectively closing this gap. To be consistent with other\\nmodels, we provide SAM with only the raw images but no bounding boxes or points as prompts.\\nFor prior methods FreeSOLO, CutLER, and SAM, we directly evaluate the publicly available model\\ncheckpoints on the given datasets. Considering that the number of ground-truth labels per image may\\nbe greater than 100, we allow all models to output up to 1,000 predictions per image.\\nAs shown in the main results summarized in Table 1, HASSOD significantly improves the detection\\nand segmentation performance over previous self-supervised models FreeSOLO and CutLER. On\\nObjects365, we improve the Box AR by 3.2; on LVIS, we improve Box AR by 3.3, and Mask AR by\\n2.3. The most remarkable performance gain is observed on SA-1B. We improve Box AR from 18.8\\nto 29.0 (relatively +54%) and improve Mask AR from 17.0 to 26.0 (relatively +53%).\\nWe gain improved recalls for objects of all scales, but small and medium-sized objects relatively\\nbenefit more than large objects. For instance, our ARS is 2.6Ã— as CutLERâ€™s ARS on SA-1B. It is\\nworth noting that detecting small objects is intrinsically harder than large objects â€“ even though the\\n7\\nTable 1: Comparison of self-supervised object detection and instance segmentation methods on\\nprevalent image datasets. We consider the average recall (AR) instead of average precision (AP)\\nas the main metric, because valid detection of objects outside the categories defined by human\\nannotations is penalized by AP. HASSOD significantly outperforms the previously best methods\\nFreeSOLO [37] and CutLER [38] in terms of AR at all object scales (Small, Medium, and Large). To\\nunderstand the extent of improvements, we also include results from state-of-the-art supervised model\\nSAM [18]. HASSOD leads to a reduced gap between fully self-supervised models and supervised\\nSAM. Notably, HASSOD only uses 1/5 of training images and 1/12 of training iterations as CutLER.\\nBox\\nMask\\nMethod\\nAR\\nARS\\nARM\\nARL\\nAP\\nAR\\nARS\\nARM\\nARL\\nAP\\nObjects365 [27]\\nSAM [18]\\n54.9\\n32.1\\n60.5\\n67.6\\n11.9\\nFreeSOLO [37]\\n10.2\\n0.2\\n5.8\\n23.4\\n3.4\\nNo ground-truth mask\\nCutLER [38]\\n35.8\\n17.6\\n36.1\\n50.5\\n11.5\\nannotations in Objects365\\nHASSOD (Ours)\\n39.0\\n21.4\\n40.4\\n52.1\\n11.0\\nLVIS [11]\\nSAM [18]\\n42.7\\n27.7\\n66.3\\n75.5\\n6.1\\n46.1\\n31.1\\n71.3\\n74.6\\n6.7\\nFreeSOLO [37]\\n6.4\\n0.3\\n9.7\\n34.6\\n1.9\\n5.9\\n0.2\\n9.2\\n31.7\\n1.9\\nCutLER [38]\\n23.6\\n13.1\\n36.2\\n55.6\\n4.5\\n20.2\\n11.3\\n31.1\\n46.2\\n3.6\\nHASSOD (Ours)\\n26.9\\n15.6\\n42.2\\n56.9\\n4.9\\n22.5\\n12.7\\n36.1\\n47.8\\n4.2\\nSA-1B [18]\\nSAM [18]\\n60.5\\n19.8\\n59.8\\n81.5\\n38.2\\n60.8\\n20.0\\n59.9\\n82.2\\n38.9\\nFreeSOLO [37]\\n2.4\\n0.0\\n0.1\\n7.4\\n1.5\\n2.2\\n0.0\\n0.2\\n6.9\\n1.5\\nCutLER [38]\\n18.8\\n5.1\\n14.6\\n32.8\\n9.0\\n17.0\\n4.9\\n13.9\\n28.5\\n7.8\\nHASSOD (Ours)\\n29.0\\n13.3\\n25.1\\n43.8\\n15.5\\n26.0\\n12.9\\n22.8\\n38.3\\n13.8\\nlabels in SA-1B are produced by the same SAM model, when the bounding box prompts are no\\nlonger available, SAM can only reach a 20.0 Mask AR for small objects. Meanwhile, we halve the\\nperformance gap between self-supervised CutLER and supervised SAM from 15.1 Mask ARS\\nto 7.1 Mask ARS. By learning from hierarchical levels of object compositions, HASSOD more\\neffectively captures small objects which are part of whole objects.\\n4.4\\nQualitative Results\\nIn this section, we analyze some qualitative results on images from LVIS. The visualization is shown\\nin Figure 5. Qualitative results on other datasets are included in Appendix K.\\nAs shown in the examples, our proposed HASSOD exhibits a more comprehensive coverage of\\nall objects in complex scenes, compared with the previous state-of-the-art self-supervised object\\ndetection method CutLER [38]. This advantage originates from the pseudo-labels generated by our\\nhierarchical adaptive clustering, which includes a proper number of candidate objects per image\\naccording to the image contents, rather than only focusing on a fixed number of objects. Furthermore,\\nHASSOD can predict the hierarchical level of each detected object. Being a fully self-supervised\\nmodel, HASSOD has surprisingly gained the human-like ability to comprehend the composition\\nof objects. This ability leads to better interpretability and controllability: Users of HASSOD can\\nunderstand the composition of each object detected by the model. Meanwhile, users can also control\\nthe segmentation granularity by selecting the predictions at the desired hierarchical level.\\nThe qualitative results also show some limitations of HASSOD. By comparing the last two columns,\\nit can be observed that HASSOD produces relatively fewer predictions for â€œsubpartâ€ objects. This is\\ndue to the distribution imbalance in the initial pseudo-labels, in which only about 10% objects are\\nsubparts. Also, the hierarchical levels learned by HASSOD are sometimes inconsistent with human\\nperception. For example, instead of recognizing the person as a whole object in the last example\\nimage, HASSOD detects the upper and lower parts of the body as whole objects. Due to the lack of\\nhuman supervision, the hierarchical prediction of HASSOD is not always aligned with humans. We\\nfurther analyze this limitation in Appendix I.\\n8\\nInput\\nCutLER\\nHASSOD (Ours)\\nWhole Objects\\nHASSOD (Ours)\\nWhole + Part\\nHASSOD (Ours)\\nWhole + Part + Subpart\\nFigure 5: Qualitative results on LVIS images. Overall, our HASSOD successfully detects more\\nobjects compared with CutLER [38]. CutLER tends to detect only one or few prominent objects\\nin the image, while HASSOD captures other objects as well (e.g., bread in row 1, and traffic sign\\nin row 3). Moreover, HASSOD learns the composition of objects (e.g., cat-face-eye in row 2, and\\nvehicle-wheel-tire in row 4), which is similar to human perception.\\n4.5\\nAblation Study\\nIn this section, we conduct an ablation study to understand the effects of each component in HASSOD.\\nTo evaluate the performance more robustly against a larger set of human-annotated object-level labels,\\nwe combine the annotations of MS-COCO [20] and LVIS [11] on the val2017 split, because they\\nare complementary to each other: LVIS uses the same images as MS-COCO, but labels more object\\nclasses. However, LVIS annotations are not as exhaustive as MS-COCO, meaning that objects within\\nLVIS categories may not be labeled on all images. After combining the two sets of annotations and\\nremoving duplicates, there are around 20 object-level labels per image. We use this combined dataset\\nfor evaluation in all the ablation study experiments, unless otherwise specified.\\nQuality of initial pseudo-labels. We first examine how the design choices in our hierarchical\\nadaptive clustering influence the quality of initial pseudo-labels. The results are summarized in\\nTable 2. Each threshold Î¸merge\\ni\\nâˆˆ {0.1, 0.2, 0.4} leads to a different trade-off between the number of\\nlabels per image and the recall. A higher threshold stops the merging process earlier, and thus results\\nin more pseudo-labels. With post-processing, we can improve pseudo-label quality by removing\\nabout half of the labels, and increase AP significantly without losing much AR. Finally, the ensemble\\nof multiple merging thresholds Î¸merge\\ni\\nâˆˆ {0.1, 0.2, 0.4} brings the best overall pseudo-label quality.\\nAppendices D, E, and F present more details regarding the choice of Î¸merge, computation costs, and\\nViT backbones in this stage.\\nEffects of hierarchical level prediction and Mean Teacher. After generating the initial pseudo-\\nlabels with hierarchical adaptive clustering, we train the object detector with several techniques,\\nincluding hierarchical level prediction, Mean Teacher self-training, and adaptively adjusting learning\\ntargets. The contribution of each technique is summarized in Table 3. Each component brings\\nan additional 0.3 - 0.5 Mask AR improvement, and when they function together, the best overall\\nperformance can be achieved.\\n9\\nTable 2: Ablation study on factors influencing the quality of initial pseudo-labels. The threshold\\nÎ¸merge controls the stopping criterion of the merging process, and affects AR and the number of\\npseudo-labels. Applying post-processing can remove low-quality pseudo-labels without decreasing\\nAR by a large margin. Ensemble of multiple pseudo-label sources leads to the best overall quality.\\nPost-\\nLabels\\nMask\\nMethod\\nÎ¸merge\\nProcess\\nper Img\\nAR\\nARS\\nARM\\nARL\\nAP\\nMaskCut [38]\\nâ€“\\nâœ“\\n1.85\\n3.5\\n0.0\\n2.0\\n20.0\\n1.5\\n0.1\\n5.33\\n4.4\\n0.8\\n5.1\\n16.6\\n1.2\\n0.1\\nâœ“\\n2.58\\n4.1\\n0.6\\n5.1\\n15.6\\n1.8\\nHierarchical\\n0.2\\n8.36\\n5.5\\n1.2\\n7.0\\n18.9\\n1.3\\nadaptive\\n0.2\\nâœ“\\n4.20\\n5.3\\n0.9\\n7.0\\n18.4\\n1.8\\nclustering\\n0.4\\n23.33\\n7.9\\n2.1\\n12.0\\n21.7\\n0.7\\n(Ours)\\n0.4\\nâœ“\\n11.61\\n7.8\\n1.7\\n12.1\\n22.1\\n1.3\\nensemble\\nâœ“\\n12.69\\n8.9\\n1.7\\n12.4\\n29.1\\n1.7\\nTable 3: Ablation study on factors influencing the training of the object detector. Hierarchical level\\nprediction is introduced as an auxiliary task for self-supervision. Mean Teacher self-training replaces\\nthe vanilla multi-round self-training and brings a smoother and more efficient training process. The\\nweights of two learning targets, initial pseudo-labels and teacher predictions, are adaptively adjusted\\nto build a more effective curriculum. All the three key designs are combined for the best overall\\nperformance of HASSOD.\\nLevel\\nMean\\nAdaptive\\nMask\\nPrediction\\nTeacher\\nTargets\\nAR\\nARS\\nARM\\nARL\\nAP\\n20.2\\n9.2\\n30.4\\n42.5\\n5.7\\nâœ“\\n20.6\\n9.7\\n30.1\\n43.9\\n6.1\\nâœ“\\nâœ“\\n22.1\\n10.9\\n32.9\\n44.5\\n5.5\\nâœ“\\nâœ“\\nâœ“\\n22.4\\n11.3\\n32.9\\n45.0\\n6.3\\nImprovement over initial pseudo-labels. Although the initial pseudo-labels are produced by a\\nfrozen DINO backbone and they tend to be noisy and coarse, HASSOD is not upper-bounded by the\\nquality of the fixed initial pseudo-labels or the pre-trained backbone. This is because of the following\\nreasons: 1) While the initially discovered pseudo-labels are noisy, in the learning stage we train a\\ndetection model to learn common objects and their hierarchical relations for enhanced generalization\\nto unseen images. By learning this detector, we boost the detection AR and AP from 8.9 and 1.7\\n(the last row in Table 2) to 20.6 and 6.1 (the second row in Table 3), respectively. Meanwhile, the\\npre-trained backbone features are adapted for the detection task in an end-to-end manner. 2) We\\nfurther leverage Mean Teacher for continual self-enhancement, and gradually minimize the negative\\nimpact of noisy initial pseudo-labels. The evolving teacher detector and its features provide improved\\npseudo-labels to the student. Notably, we can directly read out the predictions from the teacher as the\\nrefined hierarchical pseudo-labels, instead of inefficiently running the clustering algorithm using the\\nenhanced backbone. Consequently, we further improve the detection AR and AP to 22.4 and 6.3 (the\\nlast row in Table 3), respectively.\\n5\\nConclusion\\nWe present Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), an approach inspired\\nby human visual perception that learns to detect objects and understand object composition in a self-\\nsupervised manner. HASSOD uses a hierarchical adaptive clustering strategy to propose a varying\\nnumber of objects per image, and learns hierarchical levels of objects by analyzing geometric relations\\nbetween objects. Mean Teacher self-training with adaptive targets facilitates the detector training\\nprocess with smooth learning objectives and improved training efficiency. Empirical evaluation\\non recent large-scale image datasets Objects365, LVIS, and SA-1B demonstrates our significant\\nimprovement over prior self-supervised detectors. We detail the limitations and broader impacts of\\nHASSOD in Appendix I.\\n10\\nAcknowledgments\\nThis work was supported in part by the IBM-Illinois Discovery Accelerator Institute, NSF Grant\\n#2106825, NIFA Award #2020-67021-32799, the Jump ARCHES endowment through the Health\\nCare Engineering Systems Center, the National Center for Supercomputing Applications (NCSA)\\nat the University of Illinois at Urbana-Champaign through the NCSA Fellows program, the Illinois-\\nInsper Partnership, and the Amazon Research Award. This work used NVIDIA GPUs at NCSA Delta\\nthrough allocations CIS220014, CIS230012, and CIS230013 from the Advanced Cyberinfrastructure\\nCoordination Ecosystem: Services & Support (ACCESS) program [3], which is supported by NSF\\nGrants #2138259, #2138286, #2138307, #2137603, and #2138296.\\nReferences\\n[1] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran. Zero-shot object\\ndetection. In ECCV, 2018. 15\\n[2] Irving Biederman. Recognition-by-components: A theory of human image understanding. Psychological\\nreview, 94(2):115, 1987. 1\\n[3] Timothy J. Boerner, Stephen Deems, Thomas R. Furlani, Shelley L. Knuth, and John Towns. ACCESS:\\nAdvancing innovation: NSFâ€™s advanced cyberinfrastructure coordination ecosystem: Services & support.\\nIn Practice and Experience in Advanced Research Computing, 2023. 11\\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detection. In\\nCVPR, 2018. 5, 6, 15, 16, 21\\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, and Armand\\nJoulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 1, 3, 4, 6, 15, 18, 20, 21\\n[6] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what\\nyou can: Detecting and representing objects using holistic models and body parts. In CVPR, 2014. 3\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical\\nimage database. In CVPR, 2009. 4, 6, 15, 20\\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR,\\n2021. 4, 20\\n[9] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with\\ndiscriminatively trained part-based models. TPAMI, 32(9):1627â€“1645, 2009. 3\\n[10] Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Actions and attributes from wholes and parts. In\\nICCV, 2015. 3\\n[11] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation.\\nIn CVPR, 2019. 3, 7, 8, 9, 14, 19\\n[12] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of statistical\\nlearning: data mining, inference, and prediction. Springer, 2009. 4\\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\\nIn CVPR, 2016. 6, 21\\n[14] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 16\\n[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders\\nare scalable vision learners. In CVPR, 2022. 1\\n[16] Tarun Kalluri, Weiyao Wang, Heng Wang, Manmohan Chandraker, Lorenzo Torresani, and Du Tran.\\nOpen-world instance segmentation: Top-down learning with bottom-up supervision. arXiv preprint\\narXiv:2303.05503, 2023. 15\\n[17] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and Weicheng Kuo. Learning open-world\\nobject proposals without learning to classify. IEEE Robotics and Automation Letters, 7(2):5453â€“5460,\\n2022. 15\\n11\\n[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\\nSpencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, and Ross Girshick. Segment anything.\\nIn ICCV, 2023. 1, 3, 7, 8, 14, 17, 18, 19, 21, 23\\n[19] Philipp KrÃ¤henbÃ¼hl and Vladlen Koltun. Efficient inference in fully connected CRFs with Gaussian edge\\npotentials. In NeurIPS, 2011. 4, 20\\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r,\\nand C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 2, 6, 7, 9, 14, 15,\\n17, 20, 21\\n[21] Yang Liu, Idil Esen Zulfikar, Jonathon Luiten, Achal Dave, Deva Ramanan, Bastian Leibe, AljoÅ¡a OÅ¡ep,\\nand Laura Leal-TaixÃ©. Opening up open world tracking. In CVPR, 2022. 15\\n[22] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt\\nKira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. In ICLR, 2021. 2, 5, 21\\n[23] Stephen E Palmer. Hierarchical structure in perceptual representation. Cognitive psychology, 9(4):441â€“474,\\n1977. 1\\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Z. Yang,\\nZach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\\nand Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS,\\n2019. 21\\n[25] Jean Piaget. The construction of reality in the child. Journal of Consulting Psychology, 19(1):77, 1955. 1\\n[26] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi\\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R\\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B:\\nAn open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and\\nBenchmarks Track, 2022. 1\\n[27] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.\\nObjects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019. 7, 8, 17, 19, 21, 22\\n[28] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. TPAMI, 22(8):888â€“905, 2000. 3\\n[29] Oriane SimÃ©oni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick PÃ©rez,\\nRenaud Marlet, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. In\\nBMVC, 2021. 3\\n[30] Oriane SimÃ©oni, ChloÃ© Sekkat, Gilles Puy, AntonÃ­n Vobeck`y, Ã‰loi Zablocki, and Patrick PÃ©rez. Unsuper-\\nvised object localization: Observing the background to discover objects. In CVPR, 2023. 3\\n[31] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency\\ntargets improve semi-supervised deep learning results. In NeurIPS, 2017. 2, 5\\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[33] Huy V Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick PÃ©rez, and Jean Ponce. Unsupervised\\nimage matching and object discovery as optimization. In CVPR, 2019. 3\\n[34] Huy V Vo, Patrick PÃ©rez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-scale\\nimage collections. In ECCV, 2020. 3\\n[35] Van Huy Vo, Elena Sizikova, Cordelia Schmid, Patrick PÃ©rez, and Jean Ponce. Large-scale unsupervised\\nobject discovery. In NeurIPS, 2021. 3\\n[36] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and Du Tran. Open-world instance segmentation:\\nExploiting pseudo ground truth from learned pairwise affinity. In CVPR, 2022. 15\\n[37] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and Jose M\\nAlvarez. FreeSOLO: Learning to segment objects without annotations. In CVPR, 2022. 1, 3, 5, 7, 8\\n[38] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object\\ndetection and instance segmentation. In CVPR, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 15, 16, 20, 21\\n12\\n[39] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L Crowley, and\\nDominique Vaufreydaz.\\nTokenCut: Segmenting objects in images and videos with self-supervised\\ntransformer and normalized cut. In CVPR, 2022. 3, 5, 7\\n[40] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https:\\n//github.com/facebookresearch/detectron2, 2019. 21\\n13\\nAppendix\\nIn this appendix, Section A first explains the deficiency of traditional MS-COCO AP evaluation in\\nthe self-supervised setting and reasons for adopting AR on more class-extensively-annotated datasets\\nlike LVIS. Sections B and C address potential concerns about unfair comparison with CutLER [38]\\nregarding the training data and the detector architecture. Sections D, E, and F present additional\\nablation study regarding the threshold choice, computation cost, and patch size of the ViT backbone\\nin our hierarchical adaptive clustering algorithm. Section G studies different behaviors of SAM and\\nour proposed HASSOD in detecting parts of objects. Sections H and K present more comprehensive\\nquantitative and qualitative evaluation results for completeness. Section I provides the failure cases\\nof HASSOD and analyzes its current limitations. Section J describes the detailed hyper-parameter\\nand implementation setup.\\nA\\nDeficiency of MS-COCO AP Evaluation in Self-Supervised Object\\nDetection\\nTraditionally, the Average Precision (AP) metric on the MS-COCO dataset [20] has been a gold\\nstandard for assessing the performance of supervised object detection and instance segmentation\\nmodels, which are trained and evaluated on a fixed set of object categories pre-defined by human\\nannotators. However, we find this metric misleading in the context of self-supervised object detection,\\nwhere class labels are not available to the model and class-agnostic predictions are necessary. In this\\nwork, we advocate for evaluating Average Recall (AR) on datasets with extensive class annotations\\n(e.g., LVIS [11]) as a more reliable metric for comparing different methods. In this section, we\\ndiscuss the inherent deficiencies of MS-COCO AP evaluation using an illustrative example.\\nTo demonstrate this problem objectively, we compare two previous methods, CutLER [38] and\\nSAM [18]. Both models function as class-agnostic object detectors, but SAM, trained with human\\nsupervision, evidently outperforms CutLER in terms of detecting and segmenting objects, as depicted\\nin Figure 6. Surprisingly, when evaluated on MS-COCO annotations, SAM attains a mere 6.7 AP,\\nwhich is approximately half of CutLERâ€™s 12.3 AP. In this case, the AP metric is contradicting with\\nthe observed performance of the two models in terms of accurately detecting and segmenting as many\\nobjects as possible.\\nInput\\nCutLER\\n12.3 AP on MS-COCO\\nSAM\\n6.7 AP on MS-COCO\\nFigure 6: MS-COCO average precision (AP) evaluation may not accurately reflect the perfor-\\nmance of class-agnostic object detectors. We compare two previous class-agnostic object detec-\\ntion/segmentation models, CutLER [38] and SAM [18]. Despite SAM, a supervised model, detecting\\nmore objects with superior localization than CutLER, it achieves only half the AP of CutLER.\\nThe primary reason for this discrepancy lies in the annotations of MS-COCO. MS-COCO labels only\\n80 object categories, and model predictions are considered as true positives only if they fall within\\n14\\nthese categories. Consequently, correct predictions for objects outside the 80 categories are unjustly\\ndeemed false positives and penalized by the AP metric, contradicting the objective of class-agnostic\\nobject detection and revealing the shortcomings of AP evaluation.\\nTo correct these deficiencies in the evaluation metric, we need two changes: 1) Adopt a dataset\\nwith comprehensive annotations that include as many object categories as possible. If we substitute\\nMS-COCO ground-truth annotations with LVIS, which labels over 1,200 object categories despite\\nusing the same images, the AP comparison is reversed: CutLER scores 4.5 AP on LVIS, while SAM\\nattains a higher 6.1 AP. 2) Replace AP with the AR metric. Even with LVIS annotations, not all\\nobject categories are labeled in every image, resulting in certain valid object detection predictions still\\nbeing penalized. Conversely, AR does not penalize such predictions and exhibits a more pronounced\\ndifference between CutLER and SAM (23.6 AR vs. 42.7 AR).\\nIn summary, we advocate for AR on class-extensively-annotated datasets as the primary comparison\\nmetric, which can more accurately reflect the actual performance of class-agnostic object detectors\\nwith reduced bias. This choice of metric aligns with prior work on open-world detection [1, 17],\\nsegmentation [16, 36], and tracking [21] as well.\\nB\\nComparison of CutLER and HASSOD with Equal Training Data\\nAs described in the main paper, HASSOD utilizes a ResNet-50 backbone initialized with DINO [5]\\nweights pre-trained in a self-supervised manner on ImageNet [7], while subsequent training is\\nconducted on MS-COCO [20] images. We choose MS-COCO for its compact size and richness in\\nobjects per image. Due to limited computation resources, we are unable to perform HASSOD training\\non ImageNet, and we leave large-scale training as one interesting future direction. This distinction in\\nthe training dataset may lead to concerns regarding the fairness of comparison with prior work, such\\nas CutLER [38], which trains exclusively on ImageNet data. However, it is important to note that\\nusing both ImageNet and MS-COCO data in HASSOD does not grant HASSOD additional benefit\\ncompared with CutLER for two main reasons: 1) The two datasets are leveraged in separate stages\\nand not in a blended manner. Specifically, ImageNet data are only used in the DINO pre-training\\nstage, while MS-COCO is employed for detector training. 2) ImageNet contains approximately 5Ã—\\nas many images as MS-COCO. Thus, if ImageNet is employed in the detector training stage, as is the\\ncase with CutLER, this would actually lead to a stronger detector.\\nTable 4: Comparison between CutLER [38] and HASSOD concerning the training image dataset.\\nAlthough both CutLER and HASSOD leverage DINO [5] weights pre-trained on ImageNet, employ-\\ning MS-COCO images in the detector training stage does not lead to superior performance. CutLER\\ntrained on ImageNet surpasses CutLER trained on MS-COCO across all metrics. Simultaneously,\\nHASSOD outperforms both CutLER models, despite using MS-COCO training data and requiring\\nfewer training iterations.\\nTraining\\nTraining\\nMask\\nMethod\\nImages\\nIterations\\nAR\\nARS\\nARM\\nARL\\nAP\\nCutLER [38]\\nMS-COCO [20]\\n160,000\\n17.3\\n6.2\\n24.2\\n46.1\\n5.8\\nCutLER [38]\\nImageNet [7]\\n160,000\\n18.8\\n7.2\\n27.6\\n46.6\\n6.2\\nHASSOD (Ours)\\nMS-COCO [20]\\n40,000\\n22.4\\n11.3\\n32.9\\n45.0\\n6.3\\nIn order to address these concerns more effectively and ensure an equal usage of training data, we\\nconduct an additional experiment. Specifically, we train a Cascade Mask R-CNN [4] detector using\\nthe CutLER approach on MS-COCO images for one round (160,000 iterations), with the ResNet-50\\nbackbone initialized with DINO pre-trained weights. We adopt CutLERâ€™s original implementation,\\nwith the sole modification being the use of MS-COCO images for training. We compare this model\\nwith a CutLER model trained on ImageNet for one round and our HASSOD model trained on\\nMS-COCO. The evaluation is conducted against MS-COCO+LVIS annotations, as described in\\nSection 4.5. The results are presented in Table 4.\\nBy comparing the two CutLER models trained with MS-COCO and ImageNet, we observe that\\nutilizing distinct datasets for backbone pre-training and detector training does not universally improve\\nperformance. The CutLER model, exclusively trained on ImageNet, surpasses its counterpart that uses\\n15\\nImageNet for DINO pre-training and MS-COCO for detector training, exhibiting gains of 1.5 Mask\\nAR and 0.4 Mask AP. Meanwhile, HASSOD, despite being trained on the smaller MS-COCO dataset\\nand for a shorter duration, outperforms CutLER by 3.6 Mask AR. This remarkable performance is\\nattributed to HASSODâ€™s comprehensive object coverage in images and its efficient usage of training\\ndata.\\nC\\nAdditional Results on Mask R-CNN\\nIn HASSOD, we train a Cascade Mask R-CNN [4] object detection and instance segmentation model.\\nWe choose this architecture following CutLER [38] and we ensure a fair comparison with this prior\\nwork. In fact, HASSOD can be applied on different detector architectures. As an example, we also\\ntrain a Mask R-CNN [14] and compare its LVIS performance with CutLER in Table 5. With the\\nsame detector architecture, HASSOD produces a better detector than CutLER. More impressively,\\nour Mask R-CNN (weaker architecture) outperforms CutLERâ€™s Cascade Mask R-CNN (stronger\\narchitecture) on LVIS, highlighting the advantage of our approach.\\nTable 5: Comparison between CutLER [38] and HASSOD with different detector architectures on\\nthe LVIS dataset. When training models of the same architecture, HASSOD outperforms CutLER\\n(row 1 vs. 2, row 3 vs. 4). Notably, HASSOD has a stronger AR even with a weaker architecture (row\\n2 vs. 3) as compared with CutLER.\\nBox\\nMask\\nArchitecture\\nMethod\\nAR\\nARS\\nARM\\nARL\\nAP\\nAR\\nARS\\nARM\\nARL\\nAP\\nMask R-CNN\\nCutLER [38]\\n20.7\\n10.4\\n33.3\\n52.0\\n4.1\\n18.5\\n9.6\\n29.1\\n44.9\\n3.4\\nHASSOD (Ours)\\n23.8\\n13.5\\n38.3\\n50.9\\n4.3\\n21.5\\n11.8\\n35.1\\n46.6\\n4.1\\nCas. Mask R-CNN\\nCutLER [38]\\n23.6\\n13.1\\n36.2\\n55.6\\n4.5\\n20.2\\n11.3\\n31.1\\n46.2\\n3.6\\nHASSOD (Ours)\\n26.9\\n15.6\\n42.2\\n56.9\\n4.9\\n22.5\\n12.7\\n36.1\\n47.8\\n4.2\\nD\\nChoosing Thresholds for Hierarchical Adaptive Clustering\\nWhen determining the merging thresholds {Î¸merge\\ni\\n}, we mainly consider our computational constraints\\nand empirical observations. The decision of merging thresholds is not made based on validation\\nperformance (Table 2), ensuring that HASSOD is fully self-supervised.\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nMerging threshold\\n0\\n20\\n40\\n60\\n80\\n100\\nNumber of pseudo-labels per image\\nFigure 7: Relation between the number of pseudo-labels per image after post-processing (y-axis)\\nand the merging threshold Î¸merge (x-axis). When Î¸merge â‰¥ 0.5, the number of pseudo-labeled masks\\ngrows rapidly. Empirically, we find that when the number of masks per image exceeds 20, generating,\\nloading, and transforming such pseudo-labels becomes a major bottleneck in HASSOD. Therefore, we\\nchoose three thresholds {Î¸merge\\ni\\n} = {0.1, 0.2, 0.4}, mainly guided by a computational consideration.\\n16\\nGuidance by number of pseudo-masks. Our choice for {Î¸merge\\ni\\n} is primarily guided by the number\\nof pseudo-label masks produced per image. Figure 7 shows the relationship between the number\\nof masks per image and different thresholds. When Î¸merge â‰¥ 0.5, the number of masks per image\\nescalates rapidly. This steep increase incurs significant computational costs, both during the initial\\ngeneration of pseudo-labels and the subsequent data loading and pre-processing procedures during\\nmodel training. To strike a balance between computational efficiency and the desired mask granularity,\\nthresholds of {Î¸merge\\ni\\n} = {0.1, 0.2, 0.4} are chosen.\\nTable 6: Generalizability of the merging thresholds {Î¸merge\\ni\\n}. With the same merging threshold, the\\nnumber of generated pseudo-labels is not significantly changing across different image datasets.\\nÎ¸merge\\nNumber of pseudo-labels per image\\nMS-COCO [20]\\nObjects365 [27]\\nSA-1B [18]\\n0.1\\n2.58\\n3.49\\n2.91\\n0.2\\n4.20\\n5.78\\n4.88\\n0.4\\n11.61\\n12.15\\n12.70\\nThreshold generalization across datasets. Another noteworthy observation is the generalizability of\\nthese thresholds across various datasets. In Table 6, we present the number of generated pseudo-labels\\nper image on three datasets. With the merging threshold Î¸merge fixed, the number of generated labels\\nis relatively stable, regardless of the source image dataset. Therefore, our pre-set thresholds are\\ngeneralizable and require no further tuning when transferred to other datasets. Meanwhile, our\\ndetection model was trained on MS-COCO images with pseudo-labels generated using our pre-set\\n{Î¸merge\\ni\\n}, and could generalize well to other datasets in a zero-shot manner, as shown in Table 1. This\\nfact shows that the thresholds {Î¸merge\\ni\\n} are effective regardless of evaluation datasets.\\nE\\nComputational Costs in Hierarchical Adaptive Clustering\\nWe adopt the hierarchical adaptive clustering strategy to generate our initial pseudo-labels. In this\\nsection, we provide additional details regarding computation costs in this procedure.\\nAs shown in Figure 3, the hierarchical adaptive clustering contains four steps â€œmerge,â€ â€œpost-process,â€\\nâ€œensemble,â€ and â€œsplit.â€ Among them, the â€œmergeâ€ step accounts for the major computation costs.\\nWe can analyze its time complexity: Suppose we have n patches in the beginning, then there are\\nat most n merging steps before stopping. Each merging step requires retrieving the most similar\\npair of adjacent regions. The collection of adjacent pairs has size at most O(n), and each operation\\nrequires time O(log n) if this collection is organized as a balanced binary tree. Therefore, the merging\\nprocess has time complexity O(n log n). In our practice, the input image has resolution 480 Ã— 480,\\nso n = 480\\n8 Ã— 480\\n8 = 3, 600. This time complexity is affordable.\\nMore concretely, we list the time costs in the hierarchical adaptive clustering on our computation\\nplatform in Table 7. Since the procedure is learning-free, we can parallelize the processing of\\nimages using more than one worker. On our computation nodes equipped with 4 NVIDIA A100\\nGPUs, we can reduce the processing time to 1.59 sec/image. With 4 such nodes, we can complete\\nthe pseudo-label generation for MS-COCO train and unlabeled splits (0.24 million images) in\\n1.59Ã—0.24Ã—106\\n4Ã—86400\\nâ‰ˆ 1 day.\\nTable 7: Time costs in the steps of the hierarchical adaptive clustering. Notably, with multiple parallel\\nworkers, we can reduce the total processing time of MS-COCO [20] images to one day.\\nStep\\nTime Cost\\nWorkers\\nParallelized Cost\\n(sec/image)\\n(sec/image)\\nMerge and Post-Process\\n11.7\\n8\\n1.46\\nEnsemble and Split\\n2.1\\n16\\n0.13\\nTotal\\n13.8\\n-\\n1.59\\n17\\nF\\nImpact of Patch Size in Hierarchical Adaptive Clustering\\nWe use the DINO [5] pre-trained ViT-B/8 backbone to generate the initial pseudo-labels through\\nour hierarchical adaptive clustering. We observe that the small patch size 8 Ã— 8 leads to better\\npseudo-label quality. In Table 8, we compare the mask quality of pseudo-labels generated by ViT-B/8\\n(patch size 8 Ã— 8) vs. ViT-B/16 (patch size 16 Ã— 16). For a fair comparison, we use 480 Ã— 480\\ninput resolution for ViT-B/8 and 960 Ã— 960 for ViT-B/16, so that they have the same number of\\ninitial patches. With the same merging threshold Î¸merge, ViT-B/16 leads to slightly fewer labels per\\nimage, and the quality is significantly worse than ViT-B/8, especially for small and medium objects.\\nTherefore, we apply ViT-B/8 in our experiments for its localized visual features and subsequent\\nhigh-quality pseudo-labels.\\nTable 8: Comparison between DINO ViT backbones with different patch sizes 8Ã—8 and 16Ã—16. The\\nbackbone with the smaller patch size leads to higher-quality initial pseudo-labels in the hierarchical\\nadaptive clustering procedure.\\nDINO\\nÎ¸merge\\nLabels\\nMask\\nBackbone\\nper Img\\nAR\\nARS\\nARM\\nARL\\nAP\\nViT-B/8\\n0.1\\n2.58\\n4.1\\n0.6\\n5.1\\n15.6\\n1.8\\n0.2\\n4.20\\n5.3\\n0.9\\n7.0\\n18.4\\n1.8\\n0.4\\n11.61\\n7.8\\n1.7\\n12.1\\n22.1\\n1.3\\nViT-B/16\\n0.1\\n1.97\\n3.0\\n0.3\\n2.3\\n14.6\\n1.1\\n0.2\\n3.19\\n3.8\\n0.4\\n3.3\\n17.4\\n1.2\\n0.4\\n10.15\\n5.7\\n0.9\\n6.6\\n21.7\\n1.3\\nG\\nDifferent Behaviors of SAM and HASSOD in Object Part Detection\\nSegment Anything Model (SAM) [18], a supervised segmentation model, has demonstrated its ability\\nin creating high-quality, fine-grained segmentation of images. Directly comparing the performance\\nbetween SAM and HASSOD would be unbalanced, considering SAM requires extensive human-\\nlabeled images for training, while HASSOD operates entirely under self-supervision. Despite this, it\\nremains intriguing to understand their different behaviors. In this section, we delve into a qualitative\\ncomparison between SAM and our proposed HASSOD approach, paying particular attention to their\\nrespective abilities to detect constituent parts of whole objects. Figure 8 presents a visualization for\\nthis comparison.\\nBoth SAM and HASSOD can perform fine-grained segmentation within complex scenes, successfully\\ndetecting individual object parts that constitute a whole entity. However, a clear distinction arises in\\ntheir respective approaches towards the segmentation of these object parts. For scenes in which objects\\nfollow a grid pattern, where the whole entity is partitioned by regular boundary lines into semantically\\nsimilar pieces, SAM tends to perceive each grid as a distinct object. Conversely, HASSOD adheres to\\na holistic perspective for such cases, grouping the grid parts together even at the subpart level, due to\\ntheir semantically analogous features.\\nIn particular, HASSOD excels at distinguishing object parts that contain different contents within\\nwhole objects. This skill of HASSOD is especially advantageous in certain real-world applications.\\nFor example, in medical imaging, HASSODâ€™s fine-grained segmentation can potentially assist in\\nidentifying and separating different tissues or anatomical structures within a scan. Additionally, in the\\nmanufacturing industry, HASSOD could be used in quality control to check and compare individual\\ncomponents of an assembled product. While both segmentation strategies of SAM and HASSOD are\\nvalid, HASSODâ€™s unique ability to distinguish semantically different parts within objects provides\\ndistinct advantages in a wide range of practical scenarios. Comprehensively quantifying and analyzing\\nsuch an ability is crucial for advancement of self-supervised object detection and segmentation\\napproaches, and we consider it as an important future direction.\\n18\\nInput\\nSAM\\nHASSOD (Ours)\\nWhole Objects\\nHASSOD (Ours)\\nWhole + Part + Subpart\\nFigure 8: Analysis of different behaviors in object part detection between supervised SAM [18] and\\nself-supervised HASSOD through qualitative visualization. While both models generate fine-grained\\nsegmentation, they exhibit distinct preferences concerning object parts. SAM inclines towards\\nseparating objects following a grid pattern (e.g., comforters, keyboards, tiles), whereas HASSOD\\ndiscriminates objects into semantically diverse parts (e.g., eyes and beard of a cat, handle and button\\nof a cabinet).\\nH\\nAdditional Evaluation Results for Completeness\\nAs an addition to Table 1 in the main paper, we present a comprehensive evaluation of HASSOD\\non the Objects365 [27], LVIS [11], and SA-1B [18] datasets in Table 9. For a robust measure of\\nperformance, we incorporate the standard deviation, estimated from three independently trained\\nmodels.\\nTable 9: Comprehensive evaluation results of HASSOD across three datasets. We set the maximum\\nnumber of predictions per image to 1,000. The number superscript on AR (e.g., AR10) denotes\\nthe number of most confident predictions taken into account when computing AR. The subscript\\non AP (e.g., AP50) represents the Intersection-over-Union (IoU) threshold utilized when matching\\npredictions with ground truth labels. Size-specific metrics for small, medium, and large objects are\\nindicated by the subscripts S, M, and L, respectively. We include the standard deviation, estimated\\nfrom three independent runs.\\nDataset\\nAR10\\nAR100\\nAR1000\\nARS\\nARM\\nARL\\nAP\\nAP50\\nAP75\\nAPS\\nAPM\\nAPL\\nBox (Object Detection)\\nObjects365 [27]\\n15.20\\n36.63\\n39.03\\n21.40\\n40.43\\n52.10\\n10.97\\n20.33\\n10.27\\n2.90\\n10.37\\n19.47\\nÂ±0.10\\nÂ±0.06\\nÂ±0.06\\nÂ±0.10\\nÂ±0.12\\nÂ±0.17\\nÂ±0.15\\nÂ±0.31\\nÂ±0.15\\nÂ±0.10\\nÂ±0.25\\nÂ±0.25\\nLVIS [11]\\n10.58\\n25.01\\n26.87\\n15.64\\n42.22\\n56.90\\n4.94\\n9.03\\n4.75\\n2.82\\n7.90\\n12.19\\nÂ±0.07\\nÂ±0.02\\nÂ±0.03\\nÂ±0.05\\nÂ±0.08\\nÂ±0.17\\nÂ±0.09\\nÂ±0.09\\nÂ±0.10\\nÂ±0.01\\nÂ±0.14\\nÂ±0.20\\nSA-1B [18]\\n5.52\\n23.92\\n29.02\\n13.34\\n25.12\\n43.79\\n15.47\\n26.20\\n15.90\\n5.39\\n15.06\\n21.81\\nÂ±0.02\\nÂ±0.03\\nÂ±0.18\\nÂ±0.27\\nÂ±0.21\\nÂ±0.12\\nÂ±0.08\\nÂ±0.09\\nÂ±0.03\\nÂ±0.06\\nÂ±0.13\\nÂ±0.08\\nMask (Instance Segmentation)\\nLVIS [11]\\n9.69\\n21.11\\n22.50\\n12.68\\n36.14\\n47.79\\n4.21\\n7.99\\n3.95\\n1.88\\n6.96\\n13.67\\nÂ±0.06\\nÂ±0.05\\nÂ±0.01\\nÂ±0.01\\nÂ±0.11\\nÂ±0.17\\nÂ±0.20\\nÂ±0.25\\nÂ±0.24\\nÂ±0.19\\nÂ±0.21\\nÂ±0.18\\nSA-1B [18]\\n5.27\\n21.62\\n25.99\\n12.90\\n22.76\\n38.33\\n13.85\\n24.78\\n13.67\\n4.09\\n13.45\\n20.36\\nÂ±0.01\\nÂ±0.07\\nÂ±0.22\\nÂ±0.37\\nÂ±0.24\\nÂ±0.21\\nÂ±0.09\\nÂ±0.08\\nÂ±0.11\\nÂ±0.04\\nÂ±0.10\\nÂ±0.07\\n19\\nI\\nLimitations and Broader Impacts\\nLimitations. Due to the self-supervised nature of HASSOD, the learned object hierarchical levels\\nmay not be perfectly aligned with human perception. This mismatch may lead to over-segment or\\nunder-segment of objects in real-world applications.\\nInput\\nCutLER\\nHASSOD (Ours)\\nFigure 9: Failure cases of HASSOD. Top: Overlapping and similar objects are hard to distinguish.\\nMiddle: The whole object comprising diverse parts is not identified. Bottom: Text contents are not\\nprecisely localized.\\nWe observe that HASSOD performs unsatisfactorily in certain scenes due to this lack of human\\nsupervision. Figure 9 provides a visualization of the failure cases. In the first row, HASSOD\\nmistakenly treats the lower parts of the two players as a single coherent object. Due to their similar\\ncolor and texture, multiple overlapping instances of the same class can sometimes be perceived\\nas one object. In the second row, HASSOD fails to predict a mask that encompasses the entire\\nmotorcycle. This object consists of highly heterogeneous parts, making it challenging to recognize\\nthem as components of a single entity. In the third row, HASSOD fails to detect the text â€œKoelnmesse,â€\\nand the boundaries for other text are not clear. We also observe that such errors are not unique to\\nHASSOD; they appear in prior self-supervised object detection methods like CutLER as well. We\\nbelieve that further human supervision would be necessary for correcting these mistakes.\\nBroader impacts. Detecting object parts with self-supervision may be beneficial to real-world\\napplications including robotic manipulation and inspection. As a general object detection method, we\\nshare risks associated with applying recognition models such as abuse of surveillance systems.\\nJ\\nHyper-Parameters and Implementation Details\\nIn the initial pseudo-label generation process, we use a frozen ViT-B/8 model [8] pre-trained on\\nunlabeled ImageNet [7] by DINO [5], a self-supervised representation learning method, to extract\\nvisual features of train and unlabeled images in MS-COCO [20]. Following prior work Cut-\\nLER [38], we resize the resolution of each image to 480 Ã— 480, leading to 60 Ã— 60 patches as initial\\nregions. The merging process stops at three thresholds Î¸merge\\n1\\n= 0.4, Î¸merge\\n2\\n= 0.2, Î¸merge\\n3\\n= 0.1, and\\nresults from these three thresholds are ensembled after post-processing. The post-processing steps\\ninclude Conditional Random Field (CRF) [19] and filling the holes in each mask. We also filter out\\nlow-quality masks that 1) have an Intersection-over-Union (IoU) smaller than 0.5 before and after\\nCRF, 2) are smaller than 100 pixels, or 3) contain more than two corners of the image (which are likely\\n20\\nbackground). These post-processing steps are also used in prior work including CutLER [38]. After\\nensembling results from the three thresholds {Î¸merge\\ni\\n}3\\ni=1, we identify the hierarchical levels of each\\nobject mask based on the coverage relation analysis. The coverage threshold is set to Î¸cover% = 90%.\\nIn the detector training stage, we train a Cascade Mask R-CNN [4] with a ResNet-50 [13] backbone\\non the same MS-COCO [20] images. The backbone is initialized from DINO [5] self-supervised\\npre-training. Our hyper-parameter setting for Mean Teacher mostly follows the practice of Unbiased\\nTeacher [22]. The whole training process starts with a â€œburn-inâ€ stage, during which the student\\nmodel is only trained on the initial pseudo-labels with a fixed learning rate 0.01 and fixed loss weights.\\nAfter the burn-in stage, the teacher model is introduced, and we gradually adjust the learning rate\\nfrom 0.01 to 0, the loss weight in the label-to-student branch from 1.0 to 0.0, and the loss weight in\\nthe teacher-to-student branch from 2.0 to 3.0, all following a cosine schedule. The whole training\\nprocess spans 40,000 iterations with a batch size of 16 images. The training is performed on 4Ã—\\nNVIDIA A100 GPUs. Our code is developed based on PyTorch [24] and Detectron2 [40].\\nK\\nAdditional Qualitative Results\\nIn this section, we present visualization of detection results by CutLER [38] and HASSOD on\\nObjects365 [27] in Figure 10 and SA-1B [18] in Figure 11 (see next pages). Qualitative results on\\nLVIS has been included in the main paper, Figure 5.\\n21\\nInput\\nCutLER\\nHASSOD (Ours)\\nWhole Objects\\nHASSOD (Ours)\\nWhole + Part + Subpart\\nFigure 10: Qualitative results on Objects365 [27]. In each row, we show detection results of prior\\nstate-of-the-art self-supervised object detector CutLER, whole objects predicted by HASSOD, and\\nall object predicted by HASSOD.\\n22\\nInput\\nCutLER\\nHASSOD (Ours)\\nWhole Objects\\nHASSOD (Ours)\\nWhole + Part + Subpart\\nFigure 11: Qualitative results on SA-1B [18]. In each row, we show detection results of prior\\nstate-of-the-art self-supervised object detector CutLER, whole objects predicted by HASSOD, and\\nall object predicted by HASSOD.\\n23\\n'},\n",
       " {'id': 'http://arxiv.org/abs/2402.03310v1',\n",
       "  'title': 'V-IRL: Grounding Virtual Intelligence in Real Life',\n",
       "  'published_date': datetime.datetime(2024, 2, 5, 18, 59, 36),\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2402.03310v1',\n",
       "  'summary': 'There is a sensory gulf between the Earth that humans inhabit and the digital\\nrealms in which modern AI agents are created. To develop AI agents that can\\nsense, think, and act as flexibly as humans in real-world settings, it is\\nimperative to bridge the realism gap between the digital and physical worlds.\\nHow can we embody agents in an environment as rich and diverse as the one we\\ninhabit, without the constraints imposed by real hardware and control? Towards\\nthis end, we introduce V-IRL: a platform that enables agents to scalably\\ninteract with the real world in a virtual yet realistic environment. Our\\nplatform serves as a playground for developing agents that can accomplish\\nvarious practical tasks and as a vast testbed for measuring progress in\\ncapabilities spanning perception, decision-making, and interaction with\\nreal-world data across the entire globe.',\n",
       "  'pdf_text': 'V-IRL: Grounding Virtual Intelligence in Real Life\\nJihan Yang1*\\nRunyu Ding1\\nEllis Brown2\\nXiaojuan Qi1\\nSaining Xie2\\n1The University of Hong Kong\\n2New York University\\nhttps://virl-platform.github.io\\nâ€œSeamless schedules, \\nsignature service!â€\\nâ€œBeep beep! Clean \\nstreet ahead!â€\\nSan Francisco\\nNew York\\nLondon\\nParis\\nTokyo\\nMelbourne\\nBuenos Aires\\nRio de Janeiro\\nLagos\\nMumbai\\nHong Kong\\nâ€œBuilding the future, one \\nblueprint at a time!â€\\nâ€œDash, deliver, done!â€\\nâ€œFrom star-ratings to \\nyour dining!â€\\nâ€œNew city, new chapter, \\nendless curiosity!â€\\nâ€œLost in wonder. \\nShow me the way?â€\\nYour perfect home\\nawaits here!\\nFigure 1. V-IRL agents leverage real-world geospatial information and street view imagery to navigate urban terrains, execute complex\\ntasks, and interact in real-time scenarios. From recommending relevant destinations to assessing city infrastructure to collaboratively\\ngiving & following verbal directionsâ€”we develop agents that illustrate V-IRLâ€™s current capabilities, flexibility, and utility. Above all else,\\nwe present a flexible platform for researchers to harness abundant data from across the globe to create and test diverse autonomous agents.\\nAbstract\\nThere is a sensory gulf between the Earth that humans\\ninhabit and the digital realms in which modern AI agents\\nare created. To develop AI agents that can sense, think, and\\nact as flexibly as humans in real-world settings, it is im-\\nperative to bridge the realism gap between the digital and\\nphysical worlds. How can we embody agents in an envi-\\nronment as rich and diverse as the one we inhabit, without\\nthe constraints imposed by real hardware and control? To-\\nwards this end, we introduce V-IRL: a platform that enables\\nagents to scalably interact with the real world in a virtual\\nyet realistic environment. Our platform serves as a play-\\nground for developing agents that can accomplish various\\npractical tasks and as a vast testbed for measuring progress\\nin capabilities spanning perception, decision-making, and\\ninteraction with real-world data across the entire globe.\\n*Work conducted during a visit to NYU.\\n1. Introduction\\nThe advent of large language models (LLMs) has breathed\\nnew life into autonomous agent research by offering a uni-\\nversal interface for diverse capabilities, ranging from basic\\nreasoning to complex planning and tool use [72]. While\\nthese developments are promising, most of these agents\\nremain confined to text-based environments or simplis-\\ntic simulations. Visual components in existing agents are\\neither rudimentaryâ€”such as simulated tabletop environ-\\nments [11, 28]â€”or rely on abstracted representations using\\nground-truth APIs [27, 67]. Furthermore, the prevalent vi-\\nsual models employed by these agents are trained on photo-\\ngenic, object-centric Internet images, which fail to capture\\nthe unpredictability and diversity of real-world scenes.\\nThis paper aims to bridge this gap between AI agents\\nand the sensory world by grounding them in rich, real-\\nworld environmentsâ€”a crucial step towards developing au-\\ntonomous agents that can effectively operate in real-life sce-\\n1\\narXiv:2402.03310v1  [cs.AI]  5 Feb 2024\\nnarios. Our novel setting for AI agents necessitates rich sen-\\nsory grounding and perception: virtual embodiment within\\ncities around the globe using real visual and geospatial data.\\nTo this end, we introduce V-IRL, a versatile platform for\\nbuilding and testing virtual agents within this novel virtual-\\nreal-world setting.\\nV-IRL harnesses the power of map-\\nping and street view data, enabling agents to navigate real-\\nworld locations, access up-to-date information about their\\nsurroundings, and perform practical tasks. With geospatial\\ncoordinates at its core, V-IRL is flexible and extensible, inte-\\ngrating with arbitrary geospatial platforms and APIs. More-\\nover, V-IRL opens up a vast sea of visual data, allowing a\\nsimple and extensible way for researchers to evaluate vision\\nmodels on realistic data distributions.\\nWe demonstrate the versatility and adaptability of V-IRL\\nby developing a series of diverse exemplar agents, each\\nsolving a unique and practical task. As these agents hinge\\nupon foundational language and vision models, it is critical\\nto evaluate these models within this setting and their impact\\non agent performance. We leverage the vast data available\\nthrough our platform to develop global scale benchmarks\\nmeasuring the performance of underlying vision models\\non images from diverse geographic and cultural contextsâ€”\\nevaluating their adaptability to shifting environmental, ar-\\nchitectural, and language-specific elements. Furthermore,\\nwe evaluate the contributions of models to agent perfor-\\nmance on challenging tasks. Our results illustrate the po-\\ntential of V-IRL in bridging the gap between virtual agents\\nand visually rich real-world environments, paving the way\\nfor future research in this direction.\\nIn summary, our contributions are:\\nâ€¢ V-IRL: an open-source platform for building and testing\\nagents in a real-world setting that necessitates rich sen-\\nsory grounding and perceptionâ€”embodiment using real\\ngeospatial data and street-view imagery.\\nâ€¢ Development of diverse exemplar agents that showcase\\nthe platformâ€™s versatility and adaptability.\\nâ€¢ Global benchmarks measuring the performance of foun-\\ndational language and vision models (1) in isolation us-\\ning our platformâ€™s real-world data and (2) on end-to-end\\nagent performance in challenging tasks. In addition, we\\ndiscuss the robustness of â€œopen-worldâ€ vision models\\nto real-world data from across the globe.\\nWe are excited to see how the research community will\\nleverage V-IRL to develop and test agents that can under-\\nstand and interact with the real world.\\n2. Related Work\\nHere, we ground V-IRL to three streams of research.\\nAI Agents. Agents are autonomous entities capable of per-\\nceiving their environment and acting to achieve goals [69].\\nHistorically, agent development has leveraged symbolic and\\nreinforcement learning methods [9, 30, 48], which face is-\\nsues of scalability and real-world utility. In contrast, the\\nnew wave of LLM-driven agents overcomes these chal-\\nlenges with text as a universal interface, enabling natural\\nhuman interaction and adaptability to various tasks [49,\\n62, 63, 68, 77].\\nMoreover, these models equip agents\\nwith complex capabilities, such as tool use and collabora-\\ntion [26, 35, 50, 55, 67, 71, 84]. Yet a critical limitation\\npersists: the agents in this new wave are entirely text-based,\\ndevoid of any tangible connection to the visual or sensory\\naspects of the real world.\\nEmbodied AI. Embodied AI studies intelligent agents &\\nrobots perceiving and interacting with their environment. A\\nsignificant challenge in this field is the acquisition of large\\nquantities of realistic data. Consequently, robots are primar-\\nily trained in simulated environments [12, 46, 54, 73, 74]\\nto develop skills such as navigation [4, 5, 13] and manip-\\nulation [25, 79]. Recent advancements in LLMs [2, 6, 66]\\nhave enabled embodied agents to perform long-horizon and\\nopen-end tasks in game-engines [27, 28, 39, 45, 60] or hu-\\nman rooms [10, 11, 19, 29, 38]. However, the diversity of\\ntasks and data is still too narrow and simplistic to enable\\nthem to operate flexibly in diverse real-world environments.\\nOpen-World Computer Vision.\\nMotivated by the suc-\\ncess of vision-language models [3, 8, 51, 80] pre-trained\\non large-scale web-crawled data [16, 32, 56, 61, 65, 75],\\nopen-world computer vision has received increasing atten-\\ntion in recent years [23, 33, 34, 37, 47, 76, 82]. However,\\nimages and benchmarks sourced from the Internet [7, 18,\\n21, 31, 33, 53] are unavoidably biased towards specific dis-\\ntributions rather than truly reflecting the real world [52].\\nBecause they are trained and evaluated entirely on Inter-\\nnet data, existing â€œopen-worldâ€ models are effectively more\\nopen-Internet than open-world.\\n3. Virtual Intelligence in Real Life\\nTo demonstrate the versatility of V-IRL, we use it to instan-\\ntiate several exemplar agents in our virtual real-world envi-\\nronment. In this section, we engage these agents with tasks\\nthat highlight various capabilities of our platform. In Sec. 4,\\nwe discuss the technical details of our platform and how it\\nenables agents to interact with the real world.\\nFor illustration, we give V-IRL agents character meta-\\ndata, including an 8-bit avatar, a name, a short bio, and an\\nintention they are trying to accomplish. More concretely,\\nagents are defined by pipelines that use this character meta-\\ndata along with our platformâ€™s API and pretrained models to\\naddress complex tasks (see Sec. 4). Here we provide a high-\\nlevel overview of the tasks, highlight the V-IRL capabilities\\nthey require, and visualize the agents solving them.\\nWe highlight the specific V-IRL capabilities being em-\\nployed throughout using tags and corresponding colored\\nunderlines:\\nENV\\nMap â†’ action,\\nLM\\nLLM â†’ reasoning,\\nCV\\nVision â†’ perception, &\\nCOL\\nColab â†’ collaboration.\\n2\\n3.1. Earthbound Agents\\nV-IRL agents inhabit virtual representations of real cities\\naround the globe. At the core of this representation are geo-\\ngraphic coordinates corresponding to points on the Earthâ€™s\\nsurface.\\nUsing these coordinates, V-IRL allows virtual\\nagents to ground themselves in the real world using maps,\\nstreet view imagery, information about nearby destinations,\\nand additional data from arbitrary geospatial APIs.\\nRoute Optimizer\\nENV\\nMap\\nName: Peng\\nAge: 21\\nLoc: NYC\\nBio:\\nOriginally from Chengdu, Sichuan, Peng is a student at PKU. He just\\narrived for a semester abroad at NYC, and is couch surfing until he gets settled.\\nIntention:\\nPeng needs to visit five locations around the city: his University\\nCard Center, Residence Hall, Research Center, Library, and Student Center.\\nTask:\\nGiven a starting address and a list of waypoints, plan the\\nshortest route to all waypoints and then follow it on street view.\\nTakeaway:\\nV-IRL instantiates agents with real geospatial infor-\\nmation, and enables useful tasks like route optimization.\\nPeng needs to visit several locations throughout the city to\\nget documents signed for registration as a visiting student. . .\\nLeveraging Geolocation & Mapping capabilities, Peng\\nsaves 7 minutes by walking along the shortest path as op-\\nposed to in order waypoint visitation as shown in Fig. 2.\\nSTART\\nEND\\n1\\n2\\n3\\n4\\nSTART\\nEND\\n3\\n1\\n2\\n4\\n42.7 min, 3.1 km\\n36.0 min, 2.6 km\\n9.7 min, 2.9 km\\n4\\n2\\n1\\n3\\nEND\\nSTART\\nShortest path routing\\nSequential routing\\nShortest path routing\\nFigure 2. Finding the shortest path for Peng to travel to five places.\\n3.2. Language-Driven Agents\\nTo tackle more complex tasks, we follow the pattern of\\nlanguage-driven agents [72]. LLMs enable agents to flex-\\nibly reason, plan, and use external tools & APIs.\\nPlace Recommender\\nENV\\nMap\\nLM\\nLLM\\nName: Aria\\nAge: 26\\nLoc: NYC\\nBio:\\nA 3rd year graduate student who loves to try new restaurants. She is\\nalways looking for new places to try, and shares her favorite spots on her blog!\\nIntention:\\nPick out a lunch spot that Peng might like.\\nName: Vivek\\nAge: 35\\nLoc: NYC\\nBio:\\nA tech-savvy estate agent who combines his local knowledge with online\\ntools like Zillow to find the perfect homes for his clients in the bustling city.\\nIntention:\\nHelp Peng find a place to live for the semester.\\nTask:\\nGiven specific location, background, and intention, synthe-\\nsize reviews of nearby businesses to provide a recommendation.\\nTakeaway:\\nV-IRL exposes rich real-world information to agents\\nthat they can use for real-world tasks.\\nPeng is starving for some lunch but doesnâ€™t know where to\\neat. . . Luckily, he met a nice grad student Aria during his er-\\nrands who might be able to help him find a good spot.. .\\nPersonalized Rating: 8 ðŸ‘\\nPersonalized Rating: 2.5ðŸ‘Ž\\nPersonalized Rating: 7.5ðŸ‘\\nå¤¸çˆ¶ç‚¸ä¸² Kwa Food \\nDeep Fried Skewers\\nTartinery CafÃ© â€“ Bar\\n| Greenwich Village\\nDos Toros Taqueria\\nMexican\\nSTART\\nPersonalized Rating: 8 ðŸ‘\\nPersonalized Rating: 2.5 ðŸ‘Ž\\nPersonalized Rating: 7.5 ðŸ‘\\nå¤¸çˆ¶ç‚¸ä¸² Kwa Food \\nDeep Fried Skewers\\nTartinery CafÃ© â€“ Bar\\n| Greenwich Village\\nDos Toros Taqueria\\nMexican\\nSTART\\nAria searches for possible\\nrestaurants nearby. She then\\nsynthesizes public reviews\\nto make final recommenda-\\ntions via GPT-4.\\nAs Peng\\nis new to the city and orig-\\ninally from Sichuan, she recommends a spicy Chinese joint\\nKwa Food Deep Fried Skewers to give him a taste of home.\\nPeng hires Vivek to help him find an apartment in East Vil-\\nlage, Jersey City, or Long Island City for $1kâ€“$3k per month\\nclose to a gym, supermarket, and public transit. . .\\nRecommendations\\nPersonalized rating: 8/10 ðŸ‘\\nThe apartment is well-located near a\\nsupermarket and gym, which aligns with\\nPeng\\'s lifestyle. Multiple bus stations are\\nnearby, but the lack of a close subway\\nstation may affect his commute.\\n\"address\": 155 Washington St, \\nJersey City, NJ 07302, â€ rent\": \\n$2643, \"type\": Apartment, â€sqft\": \\n571, \"bedrooms\": 0, \"bathrooms\": \\n1, \"year built\": 1992,\\nPersonalized rating: 7.5/10 ðŸ‘\\nThe apartment is well-located with easy\\naccess to supermarkets, public transport,\\nand a gym, which aligns with Peng\\'s\\nrequirements. However, the price may\\nnot be cost-effective for a student.\\nRental Information\\n\"address\": 42-18 28th St, Unit \\n12E, New York, NY 11101, \\nâ€rent\": $2904, \"type\": Apartment, \\nâ€sqft\": 450, \"bedrooms\": 0, \\n\"bathrooms\": 1,\\n\"address\": 37-14 32nd St, Unit \\n508, New York, NY 11101,  \\nâ€rent\": $1986,  \"type\": Apartment,  \\nâ€sqft\": 800,  \"bedrooms\": 1,  \\n\"bathrooms\": 1,\\nPersonalized rating: 2/10 ðŸ‘Ž\\nThe estate lacks nearby supermarkets,\\nbus, subway stations, and gyms, which\\nare essential for Peng\\'s requirements.\\nVivek uses real es-\\ntate APIs to find po-\\ntential apartments in\\nPengâ€™s desired regions\\nand price range.\\nFor\\neach candidate, he re-\\nsearches its proxim-\\nity to the places Peng\\ncares about.\\nSynthe-\\nsizing these factors, Vivek provides a holistic rating and ac-\\ncompanying reasoning using GPT-4. His top recommenda-\\ntion is a cost-effective 1 bedroom apartment for $1986/mo,\\nwhich is close to a supermarket, 2 bus stations, and a gym.\\n3.3. Visually Grounded Agents\\nAlthough language-driven agents can address some real-\\nworld tasks using external tools, their reliance solely on\\ntext-based information limits their applicability to tasks\\nwhere visual grounding is required. In contrast, real sensory\\ninput is integral to many daily human activitiesâ€”allowing a\\ndeep connection to and understanding of the world around\\nus. Agents can leverage street view imagery through the\\nV-IRL platform to visually ground themselves in the real\\nworldâ€”opening up a wide range of perception-driven tasks.\\nUrban Assistance Robot\\nENV\\nMap\\nCV\\nVision\\nName: RX-399\\nAge: Unk.\\nLoc: HK/NYC\\nBio:\\nThis urban robotâ€™s advanced object detection, localization, and naviga-\\ntional telemetry systems allow it to perform perceptive tasks in busy city streets.\\nIntention:\\nReport the locations of trash bins to the sanitation dept.\\nTask:\\nTravel along a specified route and detect instances of a spec-\\nified object (e.g., trash bins, hydrants, benches, etc.).\\nTakeaway:\\nV-IRL agents can use perceptive input to understand\\nand interact with their environment.\\nRX-399 is a state-of-the-art robot agent with advanced navi-\\ngation and sensing capabilities. Its manufacturer is running\\na pilot program with sanitation departments in Hong Kong\\nand New York City to assess its readiness for garbage duty. . .\\nRX-399 navigates along pre-defined city routes, tagging all\\ntrash bins using its open-world detector and geolocation\\nmodule as depicted in Fig. 4. RX-399 can actively adjust\\nits camera pose to the optimal view for each potential ob-\\n3\\nHydrant\\nBench\\nBench\\nHydrant\\nTrash bin\\nTrash bin\\nTrash bin\\nTrash bin\\nFigure 3. Imaniâ€™s visualization of trash bins, fire hydrants, & park benches in NYCâ€™s Central Park using data collected by RX-399.\\nject thanks to our interactive embodied environment and the\\nsensor-rich visual input. During the pilot in Hong Kong,\\nRX-399 locates eight trash bins, correctly identifying five\\nbut overlooking one. In New York, it accurately detects all\\nfive trash bins but mistakenly reports two mailboxes.\\nNYC\\nHong Kong\\nFigure 4. Portions of RX-399â€™s system records in HK and NYC.\\nRX-399 can avoid double-counting previously seen ob-\\njects by using feature matching to check for duplicates\\namong prior detections (see Fig. 5).\\nFigure 5. RX-399 avoids double-counting trash cans by identify-\\ning duplicates across different viewpoints using feature matching.\\nUrban Planner\\nENV\\nMap\\nCV\\nVision\\nName: Imani\\nAge: 42\\nLoc: NYC\\nBio:\\nA sustainable urban development graduate, Imani is passionate about\\nmaintaining a harmonious balance between nature and urban ecosystems.\\nIntention:\\nUse RX-399 to collect first-person data for her studies.\\nTask:\\nRecord the location of all instances of any specified objects\\n(e.g., trash bins, hydrants, benches, etc.) in a specified region.\\nTakeaway:\\nV-IRL enables realistic open-world applications re-\\nquiring vast geospatial and first-person visual information.\\nImani needs to analyze the distribution of trash bins, fire hy-\\ndrants, and park benches in New Yorkâ€™s Central Park for a\\nproject with the NYC Parks & Recreation department. . .\\nImani sets routes spanning Central Park and objects of in-\\nterest for RX-399, who traverses the routes and records all\\ndetected instances. After RX-399 finishes its route, Imani\\nanalyzes the collected data at different levels of detail. As\\ndepicted in Fig. 3, the coarsest level shows general distribu-\\ntions of trash bins, hydrants, and benches in the park. Imani\\ncan also zoom in to specific regions, where lighter colors\\nrepresent positions with more unique instances identified.\\nThe following table presents RX-399â€™s counting report:\\nCategory Trash Bin\\nFire Hydrant\\nPark Benchâˆ—\\nCount\\n1059\\n727\\n1015\\nTable 1. RX-399â€™s counting report in Central Park, New York City.\\n(âˆ—Note: contiguous benches counted as one instance).\\nBy retrieving geotagged sensory-rich data within RX-399,\\nImani can also inspect the detection results for each object\\nto help her verify the reliability of RX-399â€™s reports as il-\\nlustrated by the bottom level in Fig. 3.\\n4\\nIntentional Explorer\\nENV\\nMap\\nLM\\nLLM\\nCV\\nVision\\nName: Hiro\\nAge: 22\\nLoc: HK\\nBio:\\nA seasoned traveler, Hiro thrives in unknown territories. He enjoys get-\\nting lost in new places instead of following the travel guide.\\nIntention:\\nHiro is looking for an authentic lunch spot that is not too spicy.\\nTask:\\nExplore on foot (in street view) looking for a destination that\\nfulfills a certain intention (e.g., lunch, shopping, etc.)\\nTakeaway:\\nAgents can utilize visual detectors, VLMs and LLMs\\nto iteratively perceive, decide, and interact in the environment.\\nHiro is starting a new journey in Hong Kong. He decides to\\nexplore without a specific destination in mind, looking for a\\ngood local lunch spot with food thatâ€™s not too spicy...\\nAs depicted in Fig. 6, starting at\\n, Hiro walks down the\\nstreet and encounters the first intersection. Thanks to the\\ninteractive and sensory-rich environment, he can adjust his\\npose to fetch street views for each possible path. Using\\nVQA on these views, he decides to turn left:\\nResidential buildings on the left road indicate cozy and\\nfamily-run local food...A better choice than the others!\\nThen, after exploring for a block, he encounters the second\\nintersection where he looks around and decides to turn right:\\nLooks like there are some local food spots this way. . .\\nAfter a few steps, Hiro finds â€œA One Chinese Noodles é˜¿ä¸€\\nè±¬æ‰’é…¸è¾£ç±³ç·šâ€ using his open-world detector. He retrieves\\ninformation, ratings, and reviews for the restaurant using\\nour platform, which connects street views to places. Hiro\\nultimately decides to pass on it and keep exploring because:\\nMost reviews mention the spicy pork chop noodles. . .\\nFinally, at the end of the block\\n, Hiro discovers another\\nlunch spot called â€œXintianfa æ–°å¤©ç™¼â€. He decides to dine\\nthere after reading numerous online reviews praising its au-\\nthentic cuisine and diverse menu.\\n[ACTION 2]\\nâ€œI should turn right.â€\\nrestaurant\\n[ACTION 3]\\nâ€œIâ€™ll pass. Keep exploring.â€\\n[ACTION 1]\\nâ€œI should turn left.â€\\nrestaurant\\n[EXPLORATION ENDS]\\nâ€œLetâ€™s dine here!â€\\n[EXPLORATION STARTS]\\nâ€œLetâ€˜s grab a bite to eat, Iâ€™m hungry.â€\\nA One\\né˜¿â¼€è±¬æ‰’é…¸è¾£â½¶ç¶«\\nChinese Noodles\\næ–°å¤©ç™¼\\nChinese Takeout\\nFigure 6. Visualization for Hiroâ€™s lunch exploration in HK.\\n3.4. Collaborative Agents\\nHumans often work together to solve complex real-world\\ntasks. This collaboration promotes efficiency and effective-\\nness by decomposing a complex task into simpler sub-tasks,\\nallowing each to be handled by an expert in its domain.\\nGrounded in the world via our platform, V-IRL agents can\\nleverage geospatial data and street view imagery to collab-\\norate with other agents as well as with human users.\\n3.4.1\\nAgent-Agent Collaboration\\nAs with previous agents, collaborative agents are designed\\nfor specific tasks; however, they can handle objectives be-\\nyond their expertise through collaboration with each other.\\nTourist\\nENV\\nMap\\nLM\\nLLM\\nCV\\nVision\\nCOL\\nColab\\nName: Ling\\nAge: 25\\nLoc: NYC/SF/HK\\nBio:\\nLing is a spirited traveler from Taipei who is always eager to explore new\\ncities and cultures. She is unafraid of asking locals for help when sheâ€™s lost!\\nIntention:\\nNYC: find gifts for friends back home; go to a famous restaurant.\\nSF: find a store to repair a broken iPhone. HK: try some authentic local food.\\nTask:\\n(i) Ask a nearby Local agent for directions to a specific loca-\\ntion. The Locals will preview the route on the map and in streetview\\nand then provide walking directions in natural language, mentioning\\nmajor intersections and landmarks.\\n(ii) Follow these directions in streetview, and if lost, ask\\nanother Local agent for assistance.\\nTakeaway:\\nAgents can collaborate to solve complex tasks that\\nare beyond their individual expertise.\\nLing travels to cities around the world. She seeks out authen-\\ntic experiences and is always unafraid to ask for help from\\nLocals whenever she finds herself lost. . .\\nAfter obtaining route descriptions from Locals, Ling starts\\nher journeyâ€”as shown in Fig. 7. Grounded in our embod-\\nied platform, Ling can adjust her pose and identify visual\\nlandmarks along the streets using open-world recognition\\nand her map. Correctly recognizing these landmarks helps\\nGPT-4 to make correct decisions about where to change di-\\nrection, move forward, and stop, as seen in the top two New\\nYork City cases in Fig. 7. The success of these decisions\\nmade by GPT-4 relies on the real-sensory input for visual\\ngrounding and the interactive environment from V-IRL.\\nNevertheless, Ling may occasionally fail to find the des-\\ntination. In the bottom left San Francisco example in Fig. 7,\\nLing passes by the Apple Store because only its stainless\\nsteel wall is visible from her viewpoint. In the bottom right\\nHong Kong example, Ling mistakes another restaurant for\\nher destination and stops prematurely. Fortunately, when\\nshe makes these mistakes, Ling can ask another Local agent\\nfor new directions and start another round of navigation,\\nwhich eventually leads her to the destination.\\n5\\nI still canâ€™t ï¬nd the Apple \\nStore. Maybe I should ï¬nd \\nsome more helpâ€¦\\nHmm. I donâ€™t see the Apple \\nStore on my right. Maybe \\nitâ€™s a bit further ahead? \\nAhh, I see the Apple \\nStore on my left now!\\nYou just went past the \\nApple Store. You need \\nto turn around. ðŸ’¬\\nOh no, my iPhone \\nscreen just cracked!\\nThereâ€˜s an Apple Store just \\na short walk from here. I \\ncan guide you there: â­\\nSTART\\nâ­ Try 1: First, turn around and head west. Continue straight until you reach the\\nintersection, with a Starbucks on your left. Next, turn left to head south. Walk a \\nbit further, and your destination, Apple Union Square, will be on your right.\\nðŸ’¬ Try 2: Turn around and head north. Walk straight for a short distance until \\nyou reach the intersection. You will see the Apple Union Square, on your left.\\nSan Francisco\\nThere are two restaurants \\non my left. Iâ€™m guessing \\none of them is Ka Hing?\\nI eat at Ka Hing \\nevery day! â­\\nIs Ka Hing \\nRestaurant here?\\nNo, you should keep \\nwalking to the west. ðŸ’¬\\nAny good burger spots \\naround here?\\nBlack Tap is worth checking out! \\nI\\'ll tell you how to get there: â­\\nArrived!\\nSTART\\nâ­ First, turn right to face southeast and walk a short distance until you reach \\nthe next intersection. You should see Famous Benâ€™s Pizza on your left. Next, turn \\nright to head southwest. Continue walking for a while until the next \\nintersection. Janovic Paint & Decorating Center SoHo should be behind you on \\nyour right. Finally, turn right again to face northwest and walk just a bit further. \\nYour destination, Black Tap Craft Burgers & Beer - SoHo, will be on your left.\\nNYC\\nIâ€™ve found Ka\\nHing, ï¬nally!\\nIs there a local \\nrestaurant youâ€™d \\nrecommend?\\nSTART\\nâ­ Try 1: First, turn to south and walk until you reach the intersection. You will \\nnotice the McDonald on your left front. Then, take a right turn, and continue \\nwalking west. Proceed until you see Ka Hing Restaurant on your left-hand side.\\nNYC\\nHong Kong\\nWhereâ€™s a good place to buy \\nsome gifts for my friends?\\nThe MoMA Design Store is \\nan excellent option! Let me \\ntell you how to get there: â­\\nIâ€™ve found the\\nMoMA Store!\\nSTART\\nâ­ First, turn left to face northwest and walk a short distance until you reach the \\nnext intersection, with Prince St in front of you. Next, make a sharp left turn to \\nhead southwest. Continue straight for a while until you reach the next \\nintersection, where you\\'ll see Aritzia on your left front. Finally, make a sharp left \\nturn to head southeast and walk a bit further. You will find your destination, the \\nMoMA Design Store, on your left.\\nðŸ’¬ Try 2: Facing west, walk a short distance until you spot Ka Hing Restaurant on \\nyour left.\\nMcDonaldâ€™s\\néº¥ç•¶å‹ž\\nFast Food $ \\nKa Hing \\nå˜‰èˆˆé¤å»³\\nBlack Tap Cra6 Burgers \\n& Beer â€“ SoHo\\nAmerican - $$ \\nFamous Benâ€™s Pizza\\nStarbucks\\nApple Union Square\\nElectronics Store\\nMoMA Design Store\\nGi5 Shop\\nAritzia\\nWomenâ€™s clothing store\\nN\\nN\\nN\\nN\\nFigure 7. Ling and Local collaboration examples. Trajectories in red and green mean Lingâ€™s first and second attempts, respectively.\\n6\\nEnergy\\nHunger\\nJoy\\nStress\\nPain\\nSadness\\n0%\\n0%\\n80%\\n20%\\n30%\\n50%\\n90%\\n25%\\n45%\\n40%\\n0%\\n0%\\n0%\\n0%\\n30%\\n60%\\n30%\\n80%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n25%\\n20%\\n15%\\n10%\\n5%\\n0%\\n0%\\n0%\\n0%\\n95%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n100%\\n1%\\n45%\\n15%\\n15%\\n25%\\n35%\\n45%\\n50%\\n30%\\n20%\\n85%\\n75%\\n95%\\n90%\\n80%\\n75%\\n65%\\n63%\\n83%\\n73%\\nMorning walk in the Washington Square Park.\\n10:00 - 10:52\\n09:00 - 10:00\\n14:00 - 14:28\\nTravel from Washington Square Park to The Cloisters. \\nExplore The Cloisters and the surrounding Fort Tryon Park. \\n12:30 - 13:30\\n10:52 - 12:30\\nHaving food in Jochyâ€˜s Cafe near Fort Tryon Park.\\n14:28 - 16:00\\nTravel from Jochyâ€˜s Cafe to the Little Red Lighthouse.\\nVisit Little Red Lighthouse and enjoy views of the Hudson River.\\nTravel from the Little Red Lighthouse to Wave Hill.\\n16:00 - 17:06\\nExplore the gardens and art exhibitions at Wave Hill.\\nTravel from Wave Hill to the Riverdale neighborhood.\\n19:00 - 19:13\\nHaving food in Floridita Restaurant, Riverdale.\\nTravel from Floridita Restaurant in Riverdale to the university.\\n19:13 - 20:00\\n20:00 - 20:40\\nInteractive Concierge\\nBudget\\n$120\\n$117.25\\n$92.25\\n$72.25\\n$69.5\\n$69.5\\n$64.5\\n$54.5\\n$52.5\\n$22.5\\n$19.75\\n$120\\n1\\n5\\n6\\n7\\n4\\n2\\n3\\n8\\nSTART\\n17:06 - 19:00\\nFigure 8. The Perfect Day Itinerary: Crafted by Diego, our iterative concierge agent, this schedule is meticulously tailored, accounting for\\nyour mental and physical well-being and budget variations as your day unfolds.\\nGeo: [40.8647205, -73.9325163]\\nRating: 7.2\\nðŸ‘\\nGeo: [40.8653388, -73.9322499]\\nRating : 6.5\\nðŸ‘\\nGeo: [40.8609142,-73.9324818]\\nRating: 4.2\\nðŸ‘Ž\\nGeo: [40.8642401,-73.9325958]\\nRating: 3.5\\nðŸ‘Ž\\nGeo: [40.8649162,-73.9311561]\\nRating : 7.5\\nðŸ‘\\nFigure 9. Diego traverses regions of interest to find scenic locations to add to your itinerary.\\n3.4.2\\nHuman-Agent Collaboration\\nGrounded in the same environment we humans inhabit, V-\\nIRL agents can collaborate with and assist real human users.\\nInteractive Concierge\\nENV\\nMap\\nLM\\nLLM\\nCV\\nVision\\nCOL\\nColab\\nName: Diego\\nAge: 62\\nLoc: NYC\\nBio:\\nDiego is an expert concierge at a hotel. Heâ€™s a master at creating intri-\\ncate itineraries and providing valuable local advice.\\nIntention:\\nPlan personalized and practical itinerary for customer!\\nTask:\\nGiven a userâ€™s location, background, and intention for a day,\\nplan a full itinerary balancing their mental/physical state & budget.\\nTakeaway:\\nV-IRL agents can collaborate with users to solve\\ncomplex tasks that require understanding the userâ€™s internal state.\\nAs a university student in NYC, you are excited to spend a\\nday exploring lesser-known and tranquil places. Your friend\\nrecommended Diego, who is known for his professionalism\\nin planning practical and personalized itineraries.\\nAs depicted in Fig. 8, Diegoâ€™s itinerary is tailored to your\\n(the userâ€™s) needs. Diego not only considers your physical\\nand mental interoception status, budget for each activity, but\\nalso anticipates your status changes and cost when you fol-\\nlow each event. He is able to take into account real travel\\ntimes from the V-IRL platform and select suitable destina-\\ntions by collaborating with another recommendation agent.\\nIn contrast, Fig. 10 shows that a simpler â€œungroundedâ€\\nLLM-only concierge agent is unable to consider the real dis-\\nSingle pass results\\nMorning walk at The High Line.\\n08:00 - 9:00\\nBreakfast at a local cafÃ© in Chelsea. \\nSubway ride from Chelsea to Green-Wood Cemetery.\\n12:45 - 13:45\\n10:00 - 10:30\\nExplore Green-Wood Cemetery.\\n14:00 - 15:30\\nLunch at a quiet restaurant in Brooklyn.\\nVisit the Brooklyn Botanic Garden.\\nTravel from Brooklyn Botanic Garden to Wave Hill.\\n15:30 - 16:00\\nStroll around Wave Hill.\\n16:00 - 18:00\\nDinner at a cozy restaurant in Riverdale.\\nTravel back to the university.\\n18:15 - 19:00\\n19:00 - 20:00\\n10:30 - 12:30\\n09:15 - 10:00\\nSTART\\n1 2\\n3\\n4\\n5\\n6\\nFigure 10. An ungrounded LLM-only concierge agentâ€™s itinerary.\\ntance and travel time between locations without access to V-\\nIRL, resulting in an impractical itinerary. For example, lack-\\ning real geospatial information, the ungrounded concierge\\nallocates only 30 minutes for travel between the â€œBrook-\\nlyn Botanic Gardenâ€ and â€œWave Hillâ€ in the Bronx, which\\nactually requires 60â€“100 minutes*. The hallucinated travel\\ntimes overlook geospatial realities and result in a plan with\\nexcessively distant destinations.\\nAlso, as shown in Fig. 11, you can intervene in Diegoâ€™s\\n*(per Google Maps https://maps.app.goo.gl/SW1r5GSx3ZVo7BTr7).\\n7\\nplanning process by adjusting your interoceptive status or\\nby providing verbal feedback. In response, Diego promptly\\nrevises his original plan to accommodate your demands, and\\nre-estimates your state changes after his revision.\\n35%\\nRevised Plan 2:\\nTravel from the Little Red Lighthouse back to the university.\\n16:00 - 17:20\\n$66.75\\n30%\\n5%\\n0%\\n0%\\n10%\\n0%\\n25%\\n20%\\n80%\\n70%\\n14:28 - 16:00\\nVisit Little Red Lighthouse and enjoy views of the Hudson River.\\nOriginal Plan:\\nTravel from the Little Red Lighthouse to Wave Hill.\\n16:00 - 17:06\\nRevised Plan 1:\\nEat at Buunni Coffee near Little Red Lighthouse for a short \\nbreak and refreshment.\\n16:00 - 16:30\\n$69.5\\n$69.5\\n$49.5\\nEnergy\\nHunger\\nStress\\nSadness Budget\\n40%\\n0%\\n10%\\n80%\\nHuman Intervention (Option 1):\\nAdjusting interoceptive states\\nHuman Intervention (Option 2):\\nProviding verbal feedback\\nâ€œOh no, I totally forgot! There\\'s an assignment due tonight \\nand I need to change my plans immediately to get it done!â€\\nâ€¦\\nâ€¦\\nâ€¦\\nâ€¦\\n70%\\nFigure 11. Diego adapts original plan to suit userâ€™s intervention.\\nFinally, using V-IRLâ€™s street views and Map, Diego can\\ntraverse regions of interest scouting for potential scenic\\nviewpoints for you to visit as shown in Fig. 9. He uses\\nVQA to rate and assess each captured view, and adds the\\nhighest-rated locations to your itinerary.\\n4. System Fundamentals\\nThis section introduces our systemâ€™s core: a platform de-\\nsigned for perception-driven agents that transforms real-\\nworld cities around the world into a vast virtual play-\\nground where agents can be constructed to solve practi-\\ncal tasks.\\nAt its heart, V-IRL is comprised of a hier-\\narchical architecture (see Fig. 12).\\nThe platform lies at\\nthe foundationâ€”providing the underlying components and\\ninfrastructure for agents to employ.\\nHigher level capa-\\nbilities of\\nCV\\nPerception ,\\nLM\\nReasoning ,\\nENV\\nAction , and\\nCOL\\nCollaboration emerge from the platformâ€™s components.\\nFinally, agents leverage these capabilities and user-defined\\nmetadata in task-specific routines to solve tasks.\\n4.1. Agent Definition\\nIn our system, agent behavior is shaped by user-defined\\nmetadata, including a background, an intended goal, and\\nan interoceptive state. The background provides the context\\nnecessary to instantiate the agent in the real world (loca-\\ntion), and to guide its reasoning and decision-making (biog-\\nraphy). Intentions outline agentsâ€™ purpose within the envi-\\nronment. An agentâ€™s interoceptive state reflects its internal\\nmental and physical statusâ€”varying over time and influenc-\\ning its behavior. This novel concept is crucial to AI agents\\nfor enhancing collaboration with humans (see Sec. 3.4.2).\\nConcretely, agents are developed by writing task-specific\\nrun() routines that leverage the various components of\\nour platform and the agentâ€™s metadata to solve tasks.\\n4.2. Platform Components\\nNext, we delve into the platform components, which pro-\\nvide the infrastructure to instantiate capabilities, execute\\nagent actions, and ground agents in the real world.\\nReasoning\\nPerception\\nAction\\nCollaboration\\nBackground\\nIntention\\nComputer Vision\\nLanguage Model\\nEnvironment\\nInteroceptive State\\nAgent\\nCapabilities\\nPlatform\\nOpen-World \\nRecognition\\nLocalization\\nStreet View \\nImagery\\nGeolocation\\nMapping\\nPlace Info & \\nSearch\\nMovement\\nFeature \\nMatching\\nHuman / Agent \\nInteraction\\nTool & API\\nUse\\nPhysical \\nMental\\nGoal\\nTask\\nBiography\\nVQA\\nLocation\\nFigure 12. Hierarchical V-IRL architecture described in Sec. 4.\\n4.2.1\\nEnvironment (Action)\\nENV\\nEnvironment components are responsible for ground-\\ning agents in the world around them: providing a navigable\\nrepresentation of real cities (see Sec. 3.1). Geographic co-\\nordinates serve as the link between the world and our virtual\\nrepresentation of it. Leveraging the Google Maps Platform\\n(GMP) [24], V-IRL enables agents to access street view im-\\nagery, query valid movements, retrieve information about\\nnearby locations, and plan routes. As these coordinates and\\nlocation information are bound to the real world, they also\\nprovide a natural interface with external tools that leverage\\ngeolocationâ€”such as real estate APIs (see Sec. 3.2). Tech-\\nnical designs of environment are detailed in Appendix C.\\n4.2.2\\nVision (Perception)\\nCV\\nPerception components enable agents to process the\\nsensory-rich data provided by the environment, especially\\nstreet view imagery. Pretrained localization models [37]\\ngive agents a precise spatial understanding of their environ-\\nment. This allows RX-399 to identify and count instances of\\nobjects, and Hiro to pick out specific businesses to look up\\nwith the GMP (Sec. 3.3). While localization models allow\\nfor precise interaction with perceptive input, open-world\\nrecognition models [51] are more general, and allow agents\\nto detect a wider range of objects in their field of view (e.g.,\\nTourist searches for the Apple Store). Pretrained feature\\nmatching models [40] provide an understanding of conti-\\nnuity across views of the same location, and enable agents\\nto identify & deduplicate instances of the same object from\\ndifferent viewpoints (Sec. 3.3). Multimodal models with\\nVQA & Captioning capabilities [36] bridge the perceptual\\nworld with natural language, and are essential for integra-\\ntion with reasoning (Sec. 3.3).\\n8\\nWorking\\nMemory\\nInteractive\\nConcierge\\nLocal Agents\\nRevising Loop\\nHierarchical \\nCoordinator \\nInteroceptive\\nEstimator\\nSupervisor\\nPlanning Iterations\\nEnvironment\\nHuman\\nBackground\\nIntention\\nInteroceptive State\\nBudget\\nHuman-Agent\\nInteraction\\nAgent-Agent\\nCollaboration\\nInformation\\nRetrieval\\nAudit Revise Approve\\nFigure 13. Architecture overview of interactive concierge agent Diego (Sec. 3.4.2). See pipeline description in Sec. 4.4.\\n4.2.3\\nLanguage (Reasoning & Collaboration)\\nLM\\nReasoning components allow decision making based on\\ninformation from perception and the environment. LLMs\\nsuch as GPT-4 [2] and Llama 2 [66] interface across various\\nAPIs (Sec. 3.2), transforming environmental data and per-\\nceptual outputs into actionable insights. They also enable\\nCOL\\nCollaboration between agents or with humans through\\nnatural language (Sec. 3.4) Custom prompts facilitate this\\ninteraction (see Sec. 4.4).\\n4.3. V-IRL Capabilities\\nOur platformâ€™s components can be flexibly combined to ex-\\nhibit a vast array of capabilities. In Sec. 3, we present agents\\nthat exhibit increasingly complex behaviors, each requiring\\nmore components of the platform. From simple combina-\\ntions, like the Route Optimizer (Sec. 3.1), to more com-\\nplex arrangements, like the Tourist (Sec. 3.4.1), our system\\nshowcases the versatility and potential of the V-IRL plat-\\nform to be applied to various real-world scenarios. Next,\\nwe perform a high-level case study of how V-IRLâ€™s com-\\nponents are combined to create our most complex agent; in\\nAppendix D, we delve deeper into the low-level platform\\ndetails that underpin creating a V-IRL agent.\\n4.4. High-Level System Case Study:\\nInteractive\\nConcierge â€œDiegoâ€\\nBy studying Diego (Sec. 3.4.2), we illustrate how our plat-\\nformâ€™s components are combined to create complex agents.\\nBehind Diegoâ€™s proficiency in developing itineraries is\\nhis iterative planning pipeline (depicted in Fig. 13). The\\nprocess begins with Diego creating an initial draft plan\\nfor the first activity using GPT-4, taking into account the\\nuserâ€™s biography, requirements, and previous activities in\\nworking memory. This draft is then meticulously refined.\\nFirst, a hierarchical coordination module re-\\ntrieves real transportation time and asks a recommenda-\\ntion agent for dining recommendations. Subsequently, an\\ninteroceptive estimation module evaluates the\\neffect of the proposed activity on the userâ€™s mental/physi-\\ncal state and budget.\\nThe\\ncrucial\\nfinal\\nstep\\ninvolves\\na\\nsupervisor\\nmodule,\\nwhich reviews (â€œauditsâ€) the incoming ac-\\ntivity\\nin\\nlight\\nof\\nthe\\ncurrent\\nuser\\nstatus,\\nremain-\\ning budget, and potential interactions (exemplified in\\nFig. 11).\\nIf the supervisor deems the plan unsuit-\\nable, it initiates revisions.\\nThe revised plan is then\\nlooped back to the hierarchical coordinator\\nand interoceptive estimator for reliability, fol-\\nlowed by another review from the supervisor (see\\nthe revising loop in Fig. 13).\\nThis iterative pro-\\ncess between the hierarchical coordinator, the\\ninteroceptive estimator, and the supervisor\\ncontinues until the supervisor approves the activity and\\nadds it to its working memory.\\nAfter finalizing an activity, Diego proceeds to plan the\\nsubsequent activity by repeating this process until the dayâ€™s\\nitinerary is complete.\\n5. V-IRL Benchmarks\\nIn the previous sections, we illustrate the primary benefit of\\nthe V-IRL platform: seamless access to first-person street-\\nview imagery and descriptive information about real-world\\ncities across the globe. This scalable source of truly open-\\nworld data can be harnessed to test core component mod-\\nels and agent capabilities. We propose three V-IRL bench-\\nmarks: two evaluating vision-language models on open-\\nworld vision tasks (Secs. 5.2 and 5.3), and one evaluating\\nend-to-end agent performance (Sec. 5.4). Benchmark de-\\ntails are in Appendix E.\\n5.1. Automated Data and Annotation Collection\\nTo allow our V-IRL benchmarks to scale globally, we de-\\nvelop an automatic data/annotation construction pipeline\\ninstead of crawling and manually annotating limited data.\\nThis allows models to be conveniently tested worldwide,\\nprovided there is access to Google Street Views [24].\\nRegion Selection. Though our benchmark is feasible across\\nall regions covered by the GMP, we select 14 districts across\\n12 cities from 6 continents to ensure coverage of a diverse\\n9\\ndata distribution while keeping inference costs affordable.\\nThe detailed locations of these regions are listed in Tab. 2.\\nPlace Types.\\nWe collect place information in each re-\\ngion for all 96 places types annotated by GMPâ€ . Our V-IRL\\nplace: localization, recognition and VQA benchmarks are\\nbuilt upon all or part of these place types.\\nVision and Place Data Collection. Within each region, we\\ncollect geolocations with available street views, place infor-\\nmation, and place-centric images. Data Cleaning. Though\\nscalable, automated data collection can introduce noise due\\nto the absence of human supervision. To this end, we design\\nthree automatic data cleaning strategies: i) distance-based\\nfiltering to exclude places not easily visible from any street\\nviews due to their distance; ii) human-review filtering to\\nremove â€œzombieâ€ places with no reviews which might no\\nlonger be valid or relevant; and iii) CLIP-based filtering to\\nretain only place-centric images with a high CLIP likeli-\\nhood of being storefronts.\\nContinent\\nCity\\nDistrict\\nAfrica\\nJohannesburg\\nRosebank\\nLagos\\nSurulere\\nAsia\\nMumbai\\nKhar\\nNew Delhi\\nLajpat Nagar\\nHong Kong\\nPrince Edward\\nTokyo\\nShinjuku\\nAustralia\\nMelbourne\\nCBD\\nMelbourne\\nSouthBank\\nEurope\\nMilan\\nBrera\\nLondon\\nOxford St\\nNorth America\\nNew York City\\nChinatown, Manhattan\\nNew York City\\nSoHo, Manhattan\\nSan Francisco\\nUnion Square\\nSouth America\\nBuenos Aires\\nMonserrat\\nTable 2. Region list for global V-IRL benchmarks.\\n5.2. V-IRL Place: Localization\\nEvery day, humans traverse cities, moving between varied\\nplaces to fulfill a range of goals, like the Intentional Ex-\\nplorer agent (Sec. 3.3). We assess the performance of vision\\nmodels on the everyday human activity of localizing places\\nusing street view imagery and associated place data.\\nSetups. We modify RX-399 (Sec. 3.3) to traverse polygonal\\nareas while localizing & identifying 20 types of places. We\\nsubsample 28 polygonal areas from the 14 districts.\\nBenchmarked Models. We evaluate three prominent open-\\nworld detection models: GroundingDINO [43], GLIP [37]\\nand Owl-ViT [47]. We also implement a straightforward\\nâ€ https://developers.google.com/maps/documentation/places/web-\\nservice/supported_types/#table1\\nbaseline, CLIP (w/ GLIP proposal), which involves reclas-\\nsifying the categories of GLIP proposals with CLIP [51].\\nEvaluation.\\nWe evaluate the models based on localiza-\\ntion recall, which is quantified as\\nNtp\\nNtp+Nfn , where Ntp and\\nNfn represents the number of correctly localized places and\\nmissed places, respectively.\\nMatching between Object Proposals and Places.\\nAs\\nmentioned in Sec. 5.1, we do not annotate bounding boxes\\nfor places on each potential street view image. Such human\\nannotation diverges from our initial motivation of providing\\nplug-and-play and sensor-rich (V-IRL) benchmarks. To as-\\nsign ground truth for each object proposal in this scenario,\\nwe develop a simple matching strategy to assign object pro-\\nposals from street view object detections to nearby places.\\nAs illustrated in Fig. 14, we first project the bounding\\nbox of each object proposal onto a frustum in the 3D space,\\nsubject to a radius. We then determine if any nearby places\\nfall within this frustum and radius. If any nearby place is\\nfound, the closest one is assigned as the ground truth for\\nthe object proposal. Otherwise, the object proposal is re-\\ngarded as a false positive. When multiple places are inside\\nthe frustum, we consider the nearest one as the ground truth\\nsince it would likely block the others in the image. This\\nprocess is also used in Intentional Explorer agent Hiro to\\nparse object proposals on image to place information.\\nFigure 14. Matching between 2D object proposal and street place.\\nResults.\\nTab. 3 shows that open-world detectors like\\nGroundingDINO [43], Owl-ViT [47] and GLIP [37] are bi-\\nased towards certain place types such as school, cafe,\\nand convenience store, respectively.\\nIn contrast,\\nCLIP (w/ GLIP proposal) can identify a broader spectrum\\nof place types. This is mainly caused by the category bias in\\nobject detection datasets with a limited vocabulary. Hence,\\neven if detectors like Owl-ViT are initialized with CLIP,\\ntheir vocabulary space narrows down due to fine-tuning.\\nThese results suggest that cascading category-agnostic ob-\\nject proposals to zero-shot recognizers appears promising\\nfor â€œrealâ€ open-world localizationâ€”especially for less com-\\nmon categories in object detection datasets.\\n10\\nPlace Types\\nAR10 AR20\\nGroundingDINO [43]\\n0.0\\n0.0\\n0.0\\n0.0\\n0.0\\n7.8\\n0.0\\n0.0 16.8 0.0\\n2.5\\n1.2\\nOwl-ViT [47]\\n0.0 58.0 0.0\\n0.0\\n6.4\\n1.6\\n0.9\\n0.0\\n0.0\\n0.0\\n6.7\\n4.4\\nGLIP [37]\\n24.6 0.0 19.2 0.0\\n0.0\\n0.0 16.6 0.0\\n0.0\\n0.0\\n6.0\\n3.7\\nCLIP [51] (w/ GLIP proposal) 58.5 8.8 28.8 41.2 33.6 23.0 13.0 25.0 0.0 14.5 24.6\\n20.1\\nTable 3. Benchmark results on V-IRL Place Localization. AR10\\nand AR20 denote average recall on subsampled 10 and all 20 place\\ncategories, respectively. Full results in Appendix E.1.\\n5.3. V-IRL Place: Recognition and VQA\\nIn contrast to the challenging V-IRL place localization task\\nusing street view imagery alone, in real life, humans can\\nrecognize businesses by taking a closer, place-centric look.\\nWe assess existing vision models in this manner on two per-\\nception tasks based on place-centric images: i) recognizing\\nspecific place types; ii) identifying human intentions via Vi-\\nsion Question Answering (VQA), dubbed â€œintention VQAâ€.\\nSetups. For recognition, we assess 10 open-world recogni-\\ntion models on identifying a placeâ€™s type (from 96 options)\\nusing place-centric images (see Tab. 4). For intention VQA,\\nwe evaluate 8 multi-modal large language models (MM-\\nLLM) to determine viable human intentions from a four-\\noption multiple-choice. The V-IRL Place VQA process is\\nillustrated in Fig. 15, where the candidate and true choices\\nare generated by GPT-4 [2] given the place types and place\\nnames corresponding to the image.\\nQuestion: Which human intentions can be accomplished here?\\nChoices: A. Learning how to cook authentic Australian food.\\nB. Applying for a reduction on parking fines.\\nC. Reporting a crime or lost property.\\nD. Attending a yoga session.\\nFigure 15. Example of V-IRL Place VQA process.\\nPlace-centric Images vs. Street View Images. In contrast\\nto the street view imagery utilized in the V-IRL Place local-\\nization benchmark, the V-IRL Place recognition and VQA\\nbenchmarks use place-centric images. To illustrate the dis-\\ntinction between these image types, we present examples in\\nFig. 16. The figure shows that street view images, sourced\\nfrom the Google Street View databaseâ€¡, are taken from the\\nstreet and encompass a broader view of the surroundings,\\nincluding multiple buildings and possible occluding object-\\nâ€¡https://developers.google.com/maps/documentation/streetview/request-\\nstreetview\\nFigure 16. Top row: examples of street view imagery. Bottom\\nrow: corresponding place-centric images.\\ns/vehicles. In contrast, place-centric images, drawn from\\nthe Google Place databaseÂ§, are taken on foot and focus\\nmore closely on the specific placeâ€”providing a more con-\\ncentrated view.\\nEvaluation. We adopt mean accuracy (mAcc) to evaluate\\nboth place recognition and VQA tasks. For place VQA, we\\nfollow MMBench [44] to conduct circular evaluation and\\nGPT-assisted answer parsing.\\nModel\\n#Param\\nmAcc (%)\\nV-IRL Place Recognition\\nCLIP [51]\\nViT-B/32\\n151M\\n18.2\\nCLIP [51]\\nViT-L/14\\n428M\\n37.2\\nCLIP [51]\\nViT-L/14@336px\\n428M\\n41.3\\nOpenCLIP [16]\\nViT-B/32\\n151M\\n21.2\\nOpenCLIP [16]\\nViT-L/14\\n428M\\n31.0\\nEva-02-CLIP [64]\\nViT-B/16\\n150M\\n19.5\\nEva-02-CLIP [64]\\nViT-L/14\\n428M\\n34.2\\nEva-02-CLIP [64]\\nViT-L/14@336px\\n428M\\n40.7\\nSigLIP [81]\\nViT-B/16\\n203M\\n29.5\\nSigLIP [81]\\nViT-L/16@384px\\n652M\\n37.3\\nV-IRL Place VQA\\nMiniGPT-4 [83]\\nVicuna-13B-v0\\n14.0B\\n3.9\\nmPLUG-Owl [78]\\nLLaMA-7B\\n7.2B\\n5.5\\nShikra [15]\\nVicuna-7B\\n7.2B\\n10.9\\nBLIP-2 [36]\\nFlanT5XXL\\n12.1B\\n69.6\\nInstructBLIP [17]\\nFlanT5XXL\\n12.0B\\n68.0\\nLLaVA [42]\\nVicuna-13B-v1.3\\n13.4B\\n23.5\\nLLaVA-1.5 [41]\\nVicuna-7B-v1.5\\n7.2B\\n60.1\\nLLaVA-1.5 [41]\\nVicuna-13B-v1.5\\n13.4B\\n61.9\\nTable 4. Benchmark results on V-IRL Place recognition and V-IRL\\nPlace VQA. Green indicates increased resolution models, while\\nBlue denotes model parameter scaling.\\nResults. Tab. 4 shows that CLIP (L/14@336px) outper-\\nforms even the biggest version of Eva-02-CLIP and SigLIP\\nin the V-IRL recognition task, highlighting the high-quality\\ndata used to train CLIP [51]. The bottom of the table shows\\nthat BLIP2 [36], InstructBLIP [17], and LLaVA-1.5 [41] ex-\\ncel at intention VQA, whereas others struggle. We note that\\nÂ§https://developers.google.com/maps/documentation/places/web-\\nservice/photos\\n11\\nJohannesburg\\nLagos\\nMumbaiHK\\nNew DelhiTokyo\\nMelbourne\\nLondon\\nMilan\\nNYC\\nSF\\nBs. Aires\\n0.38\\n0.4\\n0.42\\n0.44\\n0.46\\nAcc.\\nVisual Question Answering\\n20\\n40\\n60\\n80\\n100\\n% Eng. Speakers\\nEnglish Speakers\\nJohannesburg\\nLagos\\nMumbaiHK\\nNew DelhiTokyo\\nMelbourne\\nLondon\\nMilan\\nNYC\\nSF\\nBs. Aires\\n0.3\\n0.35\\n0.4\\nAcc.\\nRecognition\\nJohannesburg\\nLagos\\nMumbaiHK\\nNew DelhiTokyo\\nMelbourne\\nLondon\\nMilan\\nNYC\\nSF\\nBs. Aires\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0.1\\nAcc.\\nLocalization\\nFigure 17. City-level visualization of V-IRL benchmark results.\\nthese three top-performing MM-LLMs provide consistent\\nanswers in the circular evaluation, while others frequently\\nfail due to inconsistent selections. Moreover, vision models\\nperform better on intention VQA over place-type recogni-\\ntion, suggesting direct prompts about human intention could\\nbe more effective for intention-driven tasks. We provide\\nplace-type perspective analysis in Appendix E.2.\\n5.4. V-IRL Vision-Language Navigation\\nAs discussed in Sec. 3.3, Intentional Explorer and Tourist\\nagents require coordination between vision models and lan-\\nguage models to accomplish vision-language tasks. To in-\\nvestigate the effect of various models on end-to-end agent\\nperformance, we develop an embodied task that jointly tests\\nvision and language models: Vision-Language Navigation\\n(VLN). In VLN, agents navigate to a desired destination by\\nfollowing textual directions using only raw street views.\\nSetup. We adopt the Tourist implementation from Sec. 3.4\\nand swap its recognition component with the various bench-\\nmarked models. These models are used to identify visual\\nlandmarks during navigation. Subsequently, GPT-4 [2] pre-\\ndicts the next action according to the recognition results.\\nNavigation instructions are generated using the Local agent.\\nRecent work VELMA [59] attempts to enhance VLN by\\nleveraging LLMs on existing datasets [14, 58].\\nIn con-\\ntrast, our V-IRL VLN benchmark evaluates vision models\\nand their coordination with language models across a global\\ndata scale. See more details in Appendix E.3.\\nBenchmarked methods.\\nFour approaches are evaluated\\nto recognize landmarks during navigation: (i) Oracle that\\nsearches nearby landmarks with GMP [24]; (ii) Zero-shot\\nrecognizers CLIP [51] & EVA-CLIP [64]; (iii) Multi-modal\\nLLM LLaVA-1.5 [41]; (iv) An OCR model [20] to extract\\ntext in street views followed by GPT answer parsing. Im-\\nplementation details are provided in Appendix E.3.\\nEvaluation. We primarily measure navigation success rate\\n(Success), defining success as the navigator stopping within\\nMethod\\nStart Intersection\\nStop\\nSuccess Reac Arr\\nReac\\nArr Reac\\nOracle (No Vision)\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\nCLIP (B/32) [51]\\n0.22\\n1.0\\n0.86\\n0.84\\n0.83\\n0.22\\nCLIP (L/14@336px) [51]\\n0.44\\n0.83\\n0.73\\n0.94\\n0.67\\n0.44\\nEVA-02-CLIP (BigE/14-plus) [64]\\n0.39\\n0.89\\n0.77\\n0.94\\n0.72\\n0.39\\nEVA-02-CLIP (L/14@336px) [64]\\n0.22\\n1.0\\n0.82\\n0.83\\n0.78\\n0.22\\nLLaVA-1.5-13B [41]\\n0.11\\n0.61\\n0.55\\n1.0\\n0.56\\n0.11\\nPP-OCR [20] (+ GPT3.5)\\n0.28\\n0.89\\n0.73\\n0.94\\n0.72\\n0.28\\nTable 5. Results on V-IRL VLN-mini. We test various CLIP-based\\nmodels, MM LLM , and OCR model with GPT postprocessing.\\n25 meters of the destination. In addition, as navigation suc-\\ncess is mainly influenced by the agentâ€™s actions at key posi-\\ntions (i.e., start positions, intersections and stop positions),\\nwe also evaluate the arrival ratio (Arr) and reaction accu-\\nracy (Reac) for each route. Arr denotes the percentage of\\nkey positions reached, while Reac measures the accuracy\\nof the agentâ€™s action predictions at these key positions. To\\nsave GPT-4 resources, we mainly compare vision modules\\non a 10% mini-set comprising 18 routes from 9 regions. See\\nAppendix E.3 for full-set results with CLIP and Oracle.\\nResults.\\nTable 5 shows that, with oracle landmark in-\\nformation, powerful LLMs can impressively comprehend\\nnavigation instructions and thus make accurate decisions.\\nHowever, when relying on vision models to fetch land-\\nmark information from street views, the success rate drops\\ndramaticallyâ€”suggesting that the perception of vision\\nmodels is noisy and misguides LLMsâ€™ decision-making.\\nAmong these recognizers, larger variants of CLIP [51] and\\nEVA-02-CLIP [64] perform better, highlighting the benefits\\nof model scaling. LLaVA-1.5 [41] shows inferior perfor-\\nmance with CLIP (L/14@336px) as its vision encoder, pos-\\nsibly due to the alignment tax [2] introduced during instruc-\\ntion tuning. Further, PP-OCR [20] (+ GPT-3.5) achieves a\\n28% success rate, signifying that OCR is crucial for visual\\nlandmark recognition.\\n5.5. Geographic Diversity\\nSpanning 12 cities across the globe, our V-IRL benchmarks\\nprovide an opportunity to analyze the inherent model bi-\\nases across different regions. As depicted in Fig. 17, vi-\\nsion models demonstrate subpar performance on all three\\nbenchmark tasks in Lagos, Tokyo, Hong Kong, and Buenos\\nAires. Vision models might struggle in Lagos due to its non-\\ntraditional street views relative to more developed cities (see\\nstreet views in Fig. 1). For cities like Tokyo, Hong Kong,\\nand Buenos Aires, an intriguing observation is their primary\\nuse of non-English languages in street views, as shown in\\nFig. 17 bottom right Â¶ and Fig. 1. This suggests that exist-\\ning vision models may face challenges when deployed in\\nnon-English-dominant countries.\\nÂ¶Source: https://en.wikipedia.org/wiki/List_of_countries_by_English-\\nspeaking_population\\n12\\n6. Discussion: Ethics & Privacy\\nOur platform serves as a tool for AI development and as a\\ncrucible for ethical discourse and preparation. As AI is in-\\nevitably being integrated into societyâ€”e.g., via augmented\\nreality wearables, spatial computing platforms, or mobile\\nrobots navigating city streetsâ€”it is imperative to confront\\nand discuss ethical and privacy concerns now. Unlike these\\nimpending real-time systems, the data accessed by V-IRL is\\nâ€œstaleâ€ and preprocessedâ€”providing a controlled environ-\\nment to study these concerns.\\nNotably, V-IRL exclusively utilizes preexisting, read-\\nily available APIs; it does not capture or make available\\nany previously inaccessible data. Our primary source of\\nstreet-view imagery, Google Maps [24], is subject to major\\nprivacy-protection measures, including blurring faces and\\nlicense plates [22].\\nMoreover, V-IRL complies with the\\nGoogle Maps Platform license||, similarly to notable exist-\\ning works that also leverage Googleâ€™s street views [1, 14].\\nWe believe V-IRL is an invaluable tool for researching\\nbias. As discussed in Sec. 5.5, V-IRLâ€™s global scale pro-\\nvides a lens to study linguistic, cultural, and other geo-\\ngraphic biases inherent in models. By using V-IRL to study\\nsuch questions, we aim to preemptively tackle the ethi-\\ncal dilemmas that will arise with deploying real-time sys-\\ntems rather than being blindsided by them. We hope our\\nwork helps spur proactive discussion of future challenges\\nthroughout the community.\\n7. Conclusion\\nIn this work, we introduce V-IRL, an open-source platform\\ndesigned to bridge the sensory gap between the digital and\\nphysical worlds, enabling AI agents to interact with the real\\nworld in a virtual yet realistic environment. Through V-IRL,\\nagents can develop rich sensory grounding and perception,\\nutilizing real geospatial data and street-view imagery. We\\ndemonstrate the platformâ€™s versatility by creating diverse\\nexemplar agents and developing benchmarks measuring the\\nperformance of foundational language and vision models on\\nopen-world visual data from across the globe.\\nThis platform opens new avenues for advancing AI capa-\\nbilities in perception, decision-making, and real-world data\\ninteraction. As spatial computing and robotic systems be-\\ncome increasingly prevalent, the demand for and possibili-\\nties of AI agents will only grow. From personal assistants to\\npractical applications like urban planning to life-changing\\ntools for the visually impaired, we hope V-IRL helps usher\\nin a new era of perceptually grounded agents.\\n||https://cloud.google.com/maps-platform/terms\\nReferences\\n[1] Image geo-localization based on multiplenearest neighbor\\nfeature matching usinggeneralized graphs. TPAMI, 2014. 13\\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\\nGPT-4 technical report. arXiv preprint arXiv:2303.08774,\\n2023. 2, 9, 11, 12, 18\\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\\nvisual language model for few-shot learning. In NeurIPS,\\n2022. 2\\n[4] Peter Anderson, Angel Chang, Devendra Singh Chaplot,\\nAlexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana\\nKosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,\\net al. On evaluation of embodied navigation agents. arXiv\\npreprint arXiv:1807.06757, 2018. 2\\n[5] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\\nJohnson, Niko SÃ¼nderhauf, Ian Reid, Stephen Gould, and\\nAnton Van Den Hengel. Vision-and-Language Navigation:\\nInterpreting visually-grounded navigation instructions in real\\nenvironments. In CVPR, 2018. 2\\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2\\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2\\n[7] Yuki M Asano, Christian Rupprecht, Andrew Zisserman, and\\nAndrea Vedaldi. PASS: An imagenet replacement for self-\\nsupervised pretraining without humans. In NeurIPS, 2021.\\n2\\n[8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\\nSamir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-\\nsource framework for training large autoregressive vision-\\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\\n2\\n[9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki\\nCheung, PrzemysÅ‚aw DË›ebiak, Christy Dennison, David\\nFarhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al.\\nDota 2 with large scale deep reinforcement learning. arXiv\\npreprint arXiv:1912.06680, 2019. 2\\n[10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\\nDanny Driess, Avinava Dubey, Chelsea Finn, Pete Florence,\\nChuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakr-\\nishnan, Kehang Han, Karol Hausman, Alex Herzog, Jas-\\nmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan\\nJulian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,\\nLisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,\\nHenryk Michalewski, Igor Mordatch, Karl Pertsch, Kan-\\nishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar,\\nPannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait\\nSingh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan\\nVuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin\\nWu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,\\nand Brianna Zitkovich. RT-2: Vision-language-action mod-\\n13\\nels transfer web knowledge to robotic control. arXiv preprint\\narXiv:2307.15818, 2023. 2\\n[11] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\\nHausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex\\nIrpan, Eric Jang, Ryan Julian, et al. Do As I Can, Not As I\\nSay: Grounding language in robotic affordances. In CoRL,\\n2023. 1, 2\\n[12] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\\nber, Matthias Niessner, Manolis Savva, Shuran Song, Andy\\nZeng, and Yinda Zhang. Matterport3D: Learning from rgb-d\\ndata in indoor environments. In 3DV, 2017. 2\\n[13] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Ab-\\nhinav Gupta, and Russ R Salakhutdinov. Object goal naviga-\\ntion using goal-oriented semantic exploration. In NeurIPS,\\n2020. 2\\n[14] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely,\\nand Yoav Artzi. TOUCHDOWN: Natural language naviga-\\ntion and spatial reasoning in visual street environments. In\\nCVPR, 2019. 12, 13\\n[15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\\nFeng Zhu, and Rui Zhao.\\nShikra:\\nUnleashing multi-\\nmodal llmâ€™s referential dialogue magic.\\narXiv preprint\\narXiv:2306.15195, 2023. 11\\n[16] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\\ning laws for contrastive language-image learning. In CVPR,\\n2023. 2, 11\\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi.\\nInstructBLIP: Towards General-\\npurpose Vision-Language Models with Instruction Tuning.\\nIn NeurIPS, 2023. 11, 20\\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\\ndatabase. In CVPR, 2009. 2\\n[19] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\\nand Pete Florence. PaLM-E: An Embodied Multimodal Lan-\\nguage Model. In ICML, 2023. 2\\n[20] Y Du, C Li, R Guo, X Yin, W Liu, J Zhou, Y Bai, Z Yu, Y\\nYang, Q Dang, et al. PP-OCR: A practical ultra lightweight\\nocr system. arxiv 2020. arXiv preprint arXiv:2009.09941,\\n2020. 12, 23\\n[21] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland,\\nand Dhruv Mahajan. Adaptive methods for real-world do-\\nmain generalization. In CVPR, 2021. 2\\n[22] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco\\nZennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam,\\nHartmut Neven, and Luc Vincent. Large-scale privacy pro-\\ntection in google street view. In ICCV, 2009. 13\\n[23] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-\\ning open-vocabulary image segmentation with image-level\\nlabels. In ECCV, 2022. 2\\n[24] Google Map Team.\\nGoogle Map Platform.\\nhttps://\\nmapsplatform.google.com/. 8, 9, 12, 13\\n[25] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey\\nLevine.\\nDeep reinforcement learning for robotic manipu-\\nlation with asynchronous off-policy updates. In ICRA, 2017.\\n2\\n[26] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,\\nYuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang,\\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu\\nRan, Lingfeng Xiao, Chenglin Wu, and JÃ¼rgen Schmidhuber.\\nMetaGPT: Meta Programming for A Multi-Agent Collabora-\\ntive Framework. In ICLR, 2023. 2\\n[27] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\\nMordatch. Language Models As Zero-Shot Planners: Ex-\\ntracting actionable knowledge for embodied agents.\\nIn\\nICML, 2022. 1, 2\\n[28] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor\\nMordatch, Yevgen Chebotar, et al. Inner Monologue: Em-\\nbodied reasoning through planning with language models. In\\nCoRL, 2022. 1, 2\\n[29] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,\\nJiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3d value\\nmaps for robotic manipulation with language models. arXiv\\npreprint arXiv:2307.05973, 2023. 2\\n[30] Heinrich KÃ¼ttler, Nantas Nardelli, Alexander Miller, Roberta\\nRaileanu, Marco Selvatici, Edward Grefenstette, and Tim\\nRocktÃ¤schel. The nethack learning environment. In NeurIPS,\\n2020. 2\\n[31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-\\nfan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\\nThe Open Images Dataset V4: Unified image classification,\\nobject detection, and visual relationship detection at scale.\\nIJCV, 2020. 2\\n[32] Hugo LaurenÃ§on, Lucile Saulnier, LÃ©o Tronchon, Stas Bek-\\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\\nOBELICS: An open web-scale filtered dataset of interleaved\\nimage-text documents. In NeurIPS, 2023. 2\\n[33] Alexander C Li, Ellis Brown, Alexei A Efros, and Deepak\\nPathak. Internet explorer: Targeted representation learning\\non the open web. In International Conference on Machine\\nLearning, pages 19385â€“19406. PMLR, 2023. 2\\n[34] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\\nKoltun, and Rene Ranftl.\\nLanguage-driven semantic seg-\\nmentation. In ICLR, 2022. 2\\n[35] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,\\nDmitrii Khizbullin, and Bernard Ghanem. CAMEL: Com-\\nmunicative agents for\" mind\" exploration of large language\\nmodel society. In NeurIPS, 2023. 2\\n[36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\nBLIP-2:\\nbootstrapping language-image pre-training with\\nfrozen image encoders and large language models. In ICML,\\n2023. 8, 11\\n[37] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\\nYuan, Lei Zhang, Jenq-Neng Hwang, et al.\\nGrounded\\nlanguage-image pre-training. In CVPR, 2022. 2, 8, 10, 11\\n14\\n[38] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\\nHausman, Brian Ichter, Pete Florence, and Andy Zeng. Code\\nas Policies: Language model programs for embodied control.\\nIn ICRA, 2023. 2\\n[39] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco\\nPavone, and Jeannette Bohg.\\nText2Motion: From natu-\\nral language instructions to feasible plans. arXiv preprint\\narXiv:2303.12153, 2023. 2\\n[40] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-\\nfeys. LightGlue: Local Feature Matching at Light Speed. In\\nICCV, 2023. 8\\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\\nLee.\\nImproved baselines with visual instruction tuning.\\narXiv:2310.03744, 2023. 11, 12, 23\\n[42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. In NeurIPS, 2023. 11, 20\\n[43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding DINO: Marrying dino with grounded\\npre-training for open-set object detection.\\narXiv preprint\\narXiv:2303.05499, 2023. 10, 11\\n[44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\\nZiwei Liu, et al. MMBench: Is Your Multi-modal Model an\\nAll-around Player? arXiv preprint arXiv:2307.06281, 2023.\\n11\\n[45] Zeyi Liu, Arpit Bahety, and Shuran Song. REFLECT: Sum-\\nmarizing robot experiences for failure explanation and cor-\\nrection. In CoRL, 2023. 2\\n[46] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,\\nMichelle Lu, Kier Storey, Miles Macklin, David Hoeller,\\nNikita Rudin, Arthur Allshire, Ankur Handa, et al.\\nIsaac\\nGym: High performance gpu-based physics simulation for\\nrobot learning. In NeurIPS, 2021. 2\\n[47] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\\nShen, et al. Simple Open-Vocabulary Object Detection with\\nVision Transformers. In ECCV, 2022. 2, 10, 11\\n[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\\nRiedmiller. Playing atari with deep reinforcement learning.\\narXiv preprint arXiv:1312.5602, 2013. 2\\n[49] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse, Shantanu\\nJain, Vineet Kosaraju, William Saunders, et al. WebGPT:\\nBrowser-assisted question-answering with human feedback.\\narXiv preprint arXiv:2112.09332, 2021. 2\\n[50] Joon Sung Park, Joseph Oâ€™Brien, Carrie Jun Cai, Mered-\\nith Ringel Morris, Percy Liang, and Michael S Bernstein.\\nGenerative Agents: Interactive simulacra of human behav-\\nior. In UIST, 2023. 2\\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-\\nvision. In ICML, 2021. 2, 8, 10, 11, 12\\n[52] Vikram\\nV\\nRamaswamy,\\nSing\\nYu\\nLin,\\nDora\\nZhao,\\nAaron Bryan Adcock, Laurens van der Maaten, Deepti\\nGhadiyaram, and Olga Russakovsky. GeoDE: a geograph-\\nically diverse evaluation dataset for object recognition. In\\nNeurIPS, 2023. 2\\n[53] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan\\nKini, David Kanter, Vijay Janapa Reddi, and Cody Cole-\\nman.\\nThe Dollar Street Dataset: Images representing the\\ngeographic and socioeconomic diversity of the world.\\nIn\\nNeurIPS, 2022. 2\\n[54] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A plat-\\nform for embodied ai research. In ICCV, 2019. 2\\n[55] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\\ncedda, and Thomas Scialom. Toolformer: Language models\\ncan teach themselves to use tools. In NeurIPS, 2023. 2\\n[56] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\\nJitsev. LAION-5b: An open large-scale dataset for training\\nnext generation image-text models. In NeurIPS, 2022. 2\\n[57] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,\\nJacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam\\nFedus, Luke Metz, Michael Pokorny, et al. ChatGPT: Opti-\\nmizing language models for dialogue. OpenAI blog, 2022.\\n23\\n[58] Raphael Schumann and Stefan Riezler. Generating landmark\\nnavigation instructions from maps as a graph-to-text prob-\\nlem. In ACL, 2020. 12, 18\\n[59] Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu,\\nStefan Riezler, and William Yang Wang. VELMA: Verbal-\\nization embodiment of llm agents for vision and language\\nnavigation in street view. arXiv preprint arXiv:2307.06082,\\n2023. 12, 18, 23\\n[60] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander,\\nYu Liu, and Hongsheng Li.\\nLMDrive: Closed-loop end-\\nto-end driving with large language models. arXiv preprint\\narXiv:2312.07488, 2023. 2\\n[61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\\nSoricut. Conceptual Captions: A cleaned, hypernymed, im-\\nage alt-text dataset for automatic image captioning. In ACL,\\n2018. 2\\n[62] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflex-\\nion: an autonomous agent with dynamic memory and self-\\nreflection. In NeurIPS, 2023. 2\\n[63] Significant Gravitas. AutoGPT. https://github.com/\\nSignificant-Gravitas/AutoGPT, 2023. 2\\n[64] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\\nCao. EVA-CLIP: Improved Training Techniques for CLIP at\\nScale. arXiv preprint arXiv:2303.15389, 2023. 11, 12\\n[65] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\\nLi-Jia Li. YFCC100M: The new data in multimedia research.\\nCommunications of the ACM, 2016. 2\\n15\\n[66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\\nLLAMA 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288, 2023. 2, 9\\n[67] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\\nmar. Voyager: An open-ended embodied agent with large\\nlanguage models. arXiv preprint arXiv:2305.16291, 2023.\\n1, 2\\n[68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\\nChain-of-thought prompting elicits reasoning in large lan-\\nguage models. In NeurIPS, 2022. 2\\n[69] Michael Wooldridge and Nicholas R Jennings. Intelligent\\nAgents: Theory and practice. The knowledge engineering\\nreview, 1995. 2\\n[70] Penghao Wu and Saining Xie. V*: Guided Visual Search\\nas a Core Mechanism in Multimodal LLMs. arXiv preprint\\narXiv:2312.14135, 2023. 17\\n[71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun\\nZhang, and Chi Wang.\\nAutoGen: Enabling next-gen llm\\napplications via multi-agent conversation framework. arXiv\\npreprint arXiv:2308.08155, 2023. 2\\n[72] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding,\\nBoyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu\\nZhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864,\\n2023. 1, 3\\n[73] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra\\nMalik, and Silvio Savarese. Gibson Env: Real-world percep-\\ntion for embodied agents. In CVPR, 2018. 2\\n[74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao\\nZhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,\\nHe Wang, et al. SAPIEN: A simulated part-based interactive\\nenvironment. In CVPR, 2020. 2\\n[75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,\\nRussell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,\\nLuke Zettlemoyer, and Christoph Feichtenhofer. Demystify-\\ning clip data. arXiv preprint arXiv:2309.16671, 2023. 2\\n[76] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,\\nThomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT:\\nSemantic segmentation emerges from text supervision. In\\nCVPR, 2022. 2\\n[77] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\\nKarthik Narasimhan, and Yuan Cao.\\nReAct: Synergizing\\nreasoning and acting in language models. In ICLR, 2023. 2\\n[78] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\\nYaya Shi, et al. mPLUG-Owl: Modularization empowers\\nlarge language models with multimodality. arXiv preprint\\narXiv:2304.14178, 2023. 11\\n[79] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav,\\nAustin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen\\nYang, Vidhi Jain, Alexander William Clegg, John Turner,\\net al. HomeRobot: Open-vocabulary mobile manipulation.\\nIn CoRL, 2023. 2\\n[80] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\\nBoxin Li,\\nChunyuan Li,\\net al.\\nFlorence:\\nA new\\nfoundation model for computer vision.\\narXiv preprint\\narXiv:2111.11432, 2021. 2\\n[81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\\nLucas Beyer. Sigmoid loss for language image pre-training.\\narXiv preprint arXiv:2303.15343, 2023. 11\\n[82] Xingyi Zhou,\\nRohit Girdhar,\\nArmand Joulin,\\nPhilipp\\nKrÃ¤henbÃ¼hl, and Ishan Misra.\\nDetecting twenty-thousand\\nclasses using image-level supervision. In ECCV, 2022. 2\\n[83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language\\nUnderstanding with Advanced Large Language Models.\\narXiv preprint arXiv:2304.10592, 2023. 11\\n[84] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie\\nSu, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang\\nWang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost\\nin the Minecraft: Generally capable agents for open-world\\nenvironments via large language models with text-based\\nknowledge and memory. arXiv preprint arXiv:2305.17144,\\n2023. 2\\n16\\nA. Appendix Outline\\nIn these supplementary materials, we provide additional de-\\ntails for our V-IRL platform, including:\\nâ€¢ Designs behind V-IRL Agents (Appendix B);\\nâ€¢ Technical details and challenges in the V-IRL environ-\\nment (Appendix C).\\nâ€¢ A low-level case study of Intentional Explorer agent Hiro,\\ndelving into implementation details of our system such as\\nLLM prompts (Appendix D);\\nâ€¢ More detailed setups and results for our V-IRL bench-\\nmarks (Appendix E).\\nB. Technical Details of V-IRL Agents\\nIn Sec. 3, our discussion mainly focuses on the innovative\\ncapabilities and behaviors of V-IRL agents empowered by\\nour platform. We avoid in-depth discussions about technical\\ndetails in the main paper due to the concern of readability.\\nIn this section, we go through our main technical designs\\nfor each agent. More comprehensive technical implementa-\\ntions are available in our released code.\\nB.1. Peng: Route Optimizer\\nPeng is designed to showcase the utilization of real geo-\\ngraphic coordinates within our platform.\\nBy processing\\na sequence of real addresses, Peng calculates the shortest\\npath for traversing them using various modes of transporta-\\ntion, such as walking, driving, and bicycling, among oth-\\ners.\\nThis capability is powered by the mapping module\\ndescribed in Appendix C.3. After that, Peng proceeds to\\nnavigate through the destinations along the predetermined\\npath, employing the point navigation procedure outlined in\\nAppendix C.2.2.\\nB.2. Aria: Place Recommender\\nAria leverages the realistic place information provided by\\nour Place Info & Search module (see Appendix C.4) to en-\\nhance LLMsâ€™ reasoning capability in the geographic aspect.\\nSpecifically, Aria evaluates Pengâ€™s intention to determine\\nthe suitable type of place and searches all possible places\\nin the vicinity. For each searched place, Aria considers its\\nreviews and user ratings from Google to summarize a place\\noverview. Subsequently, we customize prompts for Aria to\\namalgamate Pengâ€™s biography, intentions, and the summa-\\nrized place overviews to rate each place between 0 and 10,\\naccompanied by justifications.\\nWithout such technical designs, LLMs could recom-\\nmend some places that are either too distant or permanently\\nclosed. This issue arises because LLMs struggle to accu-\\nrately understand geospatial relationships and often depend\\non outdated databases.\\nB.3. Vivek: Estate Agent\\nThe process employed by Vivek is similar to that of Aria,\\nas both are designed to recommend places. However, Vivek\\nshowcases the versatility of our V-IRL platform by demon-\\nstrating how it can seamlessly integrate a wide range of re-\\nalistic information beyond the Google Maps Platform, with\\na standardized definition of geographic coordinates. This\\ncapability enables the creation of even more sophisticated\\nand intriguing agents.\\nB.4. RX-399: Urban Assistance Robot\\nDifferent from previous example agents, RX-399 intro-\\nduces visual perception capabilities such as open-world de-\\ntection and feature matching. There are two fundamental\\nsystems inside it â€“ navigation and perception.\\nIn terms\\nof navigation, RX-399 can automatically navigate from the\\ncurrent position to the pre-defined destination step by step.\\nThis navigation process is elaborated in Appendix C.2.2,\\nand thus, will not be extensively discussed here.\\nWhen it comes to its perception system, RX-399 is de-\\nsigned to simulate human visual perception by capturing\\nstreet views within a 90-degree horizontal angle to both its\\nleft and right. For each captured view, an open-world detec-\\ntion process is conducted. Leveraging the interactive capa-\\nbilities of our environment, we further propose an active\\ndetection strategy to dynamically adjust the agentâ€™s ego-\\npose and focal length according to the scale and position\\nof potential objects.\\nThis significantly improves its per-\\nformance as illustrated in Tab. 6. In the future, more ad-\\nvanced approaches such as visual search [70] could also\\nbe considered. In the subsequent de-duplication procedure,\\nwhich aims to avoid double-counting objects across differ-\\nent viewpoints, we have tried a few strategies including\\nmeasuring with multi-view geometry, object tracking, and\\nfeature matching. We choose feature matching because of\\nits accuracy and efficiency.\\nCity\\nHong Kong\\nNew York City\\nw/ active detection\\n0.63 / 0.83\\n0.71 / 1.00\\nw/o active detection\\n0.10 / 0.33\\n0.30 / 0.60\\nTable 6. RX-399 detection performance with or without active\\ndetection manner. Metrics are accuracy / recall.\\nB.5. Imani: Urban Planner\\nThe visual perception system of Imani mirrors that of RX-\\n399. The primary distinction between them lies in their\\nnavigation systems. Imani possesses the capability to plan\\na navigation route in the given polygonal region, enabling\\nRX-399 to traverse that region. This functionality is named\\nâ€œregion navigationâ€ and elaborated in Appendix C.2.2. Ad-\\nditionally, within the Imani agent, we develop a heatmap\\nvisualization tool to visualize and verify the data collected\\nby RX-399 (see Fig. 3).\\n17\\nB.6. Hiro: Intentional Explorer\\nHiro is a representative agent equipped with geographical,\\nperceptual, and reasoning abilities, to address a daily human\\ntask: randomly exploring to find a suitable restaurant. In\\nthis regard, we have dedicated a separate section to offer an\\nin-depth case study, including the detailed methodology and\\nprompts in Appendix D.\\nB.7. Ling: Tourist\\nOur vision language navigation pipeline of Ling is sim-\\nilar to [59], leveraging vision models, the map, and\\nLLMs.\\nAt each position, we start by capturing eight\\nstreet views around the agent, corresponding to front,\\nleft front, left, left behind, behind, right\\nbehind, right and right front. Vision models use\\nthese street views to identify landmarks mentioned in route\\ndescriptions, which are then verbalized as landmark obser-\\nvations. Also, intersection information is retrieved from the\\nmover to formulate an intersection observation. LLMs play\\na crucial role in processing landmark & intersection obser-\\nvations along with the agentâ€™s previous working history to\\ndetermine the next action. After each action, current obser-\\nvations and actions are stored into the agentâ€™s working his-\\ntory. This auto-regressive process continues until the agent\\ndecides to stop.\\nB.8. Local Agent\\nThe primary mission of the Local agent is to generate\\nhuman-like and easily followable navigation instructions on\\na global scale (refer to 3.4.1). This task is known as navi-\\ngation instruction generation [58]. Contrary to most exist-\\ning research, which depends on human-annotated data for\\nlimited geographic areas, our â€œLocalâ€ agent automatically\\nselects suitable landmarks taking account into real-world\\nplaces and generates human-like route descriptions using\\nLLMs across the globe. Remarkably, it achieves this with-\\nout the need for any training data, relying solely on our tai-\\nlored prompts and a few in-context examples. The effective-\\nness of its generated instructions has been verified through\\ncollaboration with â€Lingâ€. To the best of our knowledge,\\nthis is a first in the field. There are massive technical de-\\ntails on selecting easily noticeable landmarks and prompt\\nengineering, which are available in our released code.\\nB.9. Diego: Interactive Concierge\\nIn Sec. 4.4, we have already presented the technical designs\\nof Diegoâ€™s itinerary. Here, we detail how Diego can find\\nscenic locations as shown in Fig. 9. For any given des-\\ntination, such as â€œFort Tryon Parkâ€, Diego will sample a\\nrectangle region around it and traverse all navigable posi-\\ntions within it. At each position, he will capture a photo-\\ngraph (i.e. street view imagery) using pre-defined headings,\\npitches, and FOVs. Each photograph will then be evaluated\\nusing GPT-4(V) [2], where it receives a rating between 0\\nand 10 along with explanatory reasons.\\nC. Technical Details of Environment\\nIn Sec. 4.2.1, we provide an overview of our systemâ€™s envi-\\nronment, which grounds agents in real life. Here, we delve\\ninto the technical designs beyond mere leveraging Google\\nMap Platform system calls. Concrete implementations can\\nbe found in our open-sourced code.\\nC.1. Geolocation & Street View Imagery\\nAt the core of V-IRL lies its innovative use of sensor-rich en-\\nvironment, including street view imagery and geolocations.\\nThey enable agents to gather surrounding place and vision\\ninformation.\\nGeolocation. Agents in the V-IRL platform inhabit virtual\\nrepresentations of real cities around the globe. At the core\\nof this representation are geographic coordinates (i.e. ge-\\nolocation) corresponding to points on the Earthâ€™s surface.\\nThe initial geolocation of each agent is specified by its â€œLo-\\ncationâ€ configuration, as illustrated in Fig. 12. Whenever\\nagents require access to surrounding information (e.g. street\\nviews, places or maps), geolocation serves as a crucial pa-\\nrameter for querying the related Google Map APIs.\\nStreet view imagery. Google Map Platform specifies each\\nstreet view imagery with multiple key parameters: geolo-\\ncation, heading (the horizontal angle ranging from 0â—¦ to\\n360â—¦), pitch (a vertical angle ranging from -90â—¦ to 90â—¦), and\\nField of View (FOV, ranging from 20 âˆ¼ 120). Itâ€™s notewor-\\nthy that adjusting the FOV here is similar to changing the\\ncameraâ€™s focal length, rather than simply zooming in on an\\nimage, which ensures that image resolution remains high,\\neven as the FOV decreases to a low value. By modifying\\nthe heading, pitch, and FOV, we can simulate the human\\nsensory process of adjusting oneâ€™s pose and concentrating\\non a specific area.\\nAlignment between street view imagery and geolocation.\\nWithin our sensor-rich platform, a fundamental challenge is\\nto ensure agents are positioned at geolocations where street\\nview imagery is available. To address this issue, we design\\na custom operation named â€œrelocateâ€. Specifically, when\\nan agent is initialized at a location lacking street view im-\\nagery, the â€œrelocateâ€ operation will identify and transition\\nthe agent to the nearest viable geolocation where street view\\ndata is available. Notice that, this operation is indispensable\\nto our platform, as the positions with available street views\\nare relatively sparse in comparison to the vast continuous\\nspace of all possible coordinates.\\nC.2. Movement\\nEnabling agents to move along city streets is a core func-\\ntionality of our platform, allowing interaction between\\n18\\nagents and the real world.\\nWhenever an agent needs to\\nmove, this module powers all related processes, from route\\nplanning and direction selection to the continuous update\\nof the agentâ€™s geolocation during its moving. Since Google\\nMap Platform does not provide APIs to access nearby navi-\\ngable positions and directions, the design of this movement\\nmodule is a significant technical challenge and a substantial\\ncontribution from our team. We discuss its low-level imple-\\nmentations in Appendix C.2.1 and the enabled high-level\\nactions in Appendix C.2.2.\\nC.2.1\\nMover\\nMove by controlling the web interface. A straightforward\\nsolution is to let the agent control the web front-end Google\\nStreet View to select moving directions and move. Never-\\ntheless, there are three key challenges for this solution:\\n(i) How can Python-implemented agents control the\\nmovement via the interaction to the webpage? We use a\\nPython package Selenium** to locate web elements respon-\\nsible for movement. After determining a movement direc-\\ntion, the agent uses Selenium to simulate a click action on\\nthe web element corresponding to the chosen direction.\\n(ii) How can the agent acquire the necessary informa-\\ntion to decide moving direction? Although agents can ac-\\ncess all potential movement directions from web elements,\\nthey cannot identify these directions without prior knowl-\\nedge of what each represents. We find that the â€œtransformâ€\\nattribute in the web element corresponding to each direction\\ncan be leveraged to calculate their represented heading an-\\ngles. The heading angle also allows us to collect street view\\nimagery for each movement direction. Agentâ€™s movement\\ndecision-making is then based on these heading angles and\\nthe visual data from street view imagery.\\n(iii) How to track the agentâ€™s geolocation along its\\nmovement? To accomplish this, we customize a webpage\\nelement to display the geolocation of the current street view\\npanorama. As the agents move and trigger updates to the\\nstreet view panorama, this customized element concurrently\\nrefreshes to reflect the new geolocation. By using Selenium,\\nwe can then extract this updated geolocation data, enabling\\ncontinuous tracking of the agentâ€™s geolocation changes.\\nMove by grid-based relocating. In our test of the above\\nweb-based mover, a critical limitation emerged: the web-\\nembedded Google Street View panoramas display only a\\nsubset of navigable directions. This constraint significantly\\nrestricts our agentsâ€™ mobility, often preventing them from\\nsuccessfully navigating to their intended destinations due to\\nthe incomplete coverage of potential routes.\\nTo overcome this obstacle, we develop an alternative\\nmethod: a grid-based relocating mover.\\nThis approach\\ninvolves performing a grid search for geolocations in the\\n**https://www.selenium.dev/\\nvicinity of the agent and employing the â€œrelocateâ€ opera-\\ntion to sift through these locations, identifying those that\\nare navigable. While this method offers a more comprehen-\\nsive view of navigable positions, it is markedly more time-\\nconsuming than the web-based approach due to the exten-\\nsive number of Google Maps API calls required.\\nIn our practical applications, we design a heuristic strat-\\negy that combines web-based controlling and grid-based re-\\nlocation. This hybrid approach aims to balance the trade-\\noffs between the speed and the completeness of navigable\\nposition data, optimizing our agentsâ€™ capabilities and effi-\\nciency in real-world scenarios.\\nC.2.2\\nNavigator\\nHere, we introduce the high-level action of agents powered\\nby the mover â€“ navigation. Unlike the mover, which con-\\ncentrates on enabling agent mobility in the environment, the\\nfocus here shifts to determining the direction of movement.\\nIn our platform, we group different navigators according to\\ntheir usages into four types:\\n(i) Point navigator is designed to tackle navigation\\ntasks that clearly define single or multiple destinations\\n(represented in addresses or geolocations).\\nThis naviga-\\ntor employs the route planning function described in Ap-\\npendix C.3 to obtain a series of key positions for naviga-\\ntion. At each location, the agent utilizes a greedy algorithm\\nto select the most optimal direction towards the next key\\nposition that has not yet been reached. Exemplary agents,\\nsuch as â€œPengâ€, â€œRX-399â€ and â€œLocalâ€, use this type of\\nnavigator in their implementation.\\n(ii) Region navigator is tailored for agents like â€œImaniâ€\\nand â€œDiegoâ€, who need to traverse every position within a\\npolygonal region. This navigator first employs a grid search\\ncombined with our â€œrelocateâ€ operation to identify all navi-\\ngable positions within the specified region. Subsequently, it\\nadopts a heuristic algorithm designed to solve the traveling\\nsalesman problem, planning an efficient order for visiting\\nthese positions. The agentsâ€™ task is to simply follow this\\npredetermined route, visiting each navigable position in the\\nplanned order.\\n(iii) Vision-language navigator is specifically devel-\\noped for the tourist agent â€œLingâ€, as well as for tasks within\\nthe V-IRL vision-language navigation benchmark. Its pri-\\nmary function is to guide the agent in selecting a proper\\ndirection based on navigation instructions.\\nThe detailed\\npipeline is presented in Appendix B.7.\\n(iv) Intention navigator is utilized in intentional ex-\\nplorer agent â€Hiroâ€œ to select the most suitable direction\\nthat aligns with the agentâ€™s specific intentions. The detailed\\nmethodology and prompt are detailed in Appendix D.2.\\n19\\nC.3. Mapping\\nThe mapping module in our environment is designed to\\nequip agents with functionalities such as route planning and\\ntransportation time estimation. It mainly utilizes the â€œDi-\\nrections APIâ€â€ â€  from the Google Map Platform to facilitate\\nthese capabilities. Given the complex nature of this APIâ€™s\\ninterface, our principal focus has been on parsing its out-\\nput and adapting it into various user-friendly interfaces for\\nagents.\\nC.4. Place Info & Search\\nPlace Info & Search module hosts another important in-\\nformation source in our platform beyond the visual street\\nview imagery, enabling agents to interact with real-world\\nâ€œplacesâ€. It provides various attributes of places, including\\ntype, name, location, imagery, reviews, etc. In this module,\\nour technical efforts are primarily focused on understand-\\ning, comparing, and integrating the most suitable functions\\nfrom the vast array of Google Maps Platform APIs related\\nto place information and nearby place searches. Addition-\\nally, we devise some post-processing strategies to identify\\nand eliminate invalid or conflicting data sources from the\\nGoogle Maps Platform.\\nAnother essential capability enabled by this module is to\\nassociate object proposals in street view imagery and their\\ncorresponding places in the real city. This function is vital\\nto enhance the reality of our platform by connecting street\\nview and geolocation. It also powers the â€œHiroâ€ agent and\\nthe evaluation of the V-IRL Place localization benchmark.\\nThe implementation is detailed in Sec. 5.2.\\nD. Low-Level System Case Study:\\nIntentional Explorer â€œHiroâ€\\nThis section delves deeper into the low-level implemen-\\ntation details of the Intentional Explorer agent â€œHiroâ€\\n(Sec. 3.3), focusing on the prompts utilized to interact with\\nvarious parts of our system.\\nConcretely, we present the\\nprompts in four subparts: identifying a type of place to\\nsearch using the user-defined intention (Appendix D.1), se-\\nlecting appropriate roads (Appendix D.2), summarizing re-\\nviews of places (Appendix D.3), and making action deci-\\nsions (Appendix D.4). These four components jointly en-\\nable Hiro to explore in our interactive embodied environ-\\nment driven by his initial intention.\\nD.1. Intention to Place Type\\nStarting with a user-defined agent intention, Hiro first deter-\\nmines the type of place that could fulfill this intention using\\nGPT-4 and the following prompt:\\nâ€ â€ https : / / developers . google . com / maps /\\ndocumentation/directions\\n[Role]\\nYou are PlaceSuggesterGPT, an expert\\nin recommending types of places\\nbased on user-specified intentions.\\n[Task Description]\\nGiven a user-specified intention,\\ndetermine the type of \"place\"\\none should seek to fulfill the\\nintention.\\nYour response should\\nbe in the following JSON format:\\n{\"place\":\\n\"Desired Place Type\"}\\n[Example]\\nInput: \"Intention:\\n<buy a book>\"\\nOutput: {\"place\":\\n\"bookstore\"}\\n[Input]\\nIntention:\\n<{agent_intention}>\\n[Output]\\nYour recommended place type based on\\nthe user-specified intention, in the\\nrequired JSON format:\\nUsing this prompt with the intention\\nHiro is hungry and looking for a place where he\\ncan try some good local food. He cannot handle\\nspicy food.\\nreturns the result\\n{\"place\":\\n\"restaurant\"}.\\nThe identified place type (here, restaurant) is extracted\\nand set as the target category for Hiroâ€™s open-world detector\\nduring his exploration.\\nD.2. Road Selection\\nWhenever Hiro is at a crossroads, he determines the best\\nroad to follow using his multi-modal LLM and GPT-4. The\\nprimary goal of the road selection process is to identify the\\nroad most likely to lead to the desired place type that aligns\\nwith Hiroâ€™s intention. First, Hiro fetches the street view\\ntowards each potential road using the V-IRL environment.\\nThen he utilizes his multi-modal LLM (such as Instruct-\\nBLIP [17] or LLaVA [42]) to generate captions for each\\nroad using the following prompt:\\nI am looking for a {place_type}.\\nPlease detail information that might\\nbe helpful for me along this road:\\nCaptions for each road are then formatted in the style of\\n{road_idx}:\\n{road_description}\\n20\\nand concatenated to form all_road_descriptions.\\nThese road captions, along with Hiroâ€™s user-defined inten-\\ntion, are then fed into GPT-4 to determine the most promis-\\ning road to follow using the following prompt:\\n[Role]\\nYou are PathSelectorGPT, an expert\\nin choosing the optimal road from\\nmultiple candidates based on a\\nuser-specified intention.\\n[Task Description]\\nGiven an intention, the road\\npreviously traveled, and\\ndescriptions of available candidate\\nroads, select the best road from the\\ncrossroad.\\nYour response must be in\\nthe following JSON format:\\n{\"idx\":\\n\"Selected road index\",\\n\"reason\":\\n\"Justification for your\\nselection\"}\\n[Example]\\nFor the intention \"find a grocery\\nstore\", the road previously traveled\\nas \"1\", and with candidates \"2:\\nLeads to residential area, 3:\\nLeads\\nto a shopping district\", the output\\nmight be:\\n{\"idx\":\\n\"3\", \"reason\":\\n\"Road 3 leads to a shopping district\\nwhich is more likely to have a\\ngrocery store.\"}\\n[Input]\\nUser Intention:\\n<{agent_intention}>\\nRoad Descriptions:\\n{all_road_descriptions}\\nPreviously Traveled Road:\\nRoad\\n{from_road_idx}\\n[Output]\\nYour chosen road index and the\\nreasoning behind your selection,\\nin the required JSON format:\\nWe design such a two-stage captioning and decision-\\nmaking pipeline for road selection because Multi-modal\\nLLMs cannot process multiple images simultaneously.\\nHowever, with the recent advancements of GPT-4V, it may\\nbe possible to perform road selection using several road im-\\nages with a single prompt at once. Empirical findings sug-\\ngest that GPT-4V yields more reasonable choices with the\\nfollowing prompt:\\n[Role]\\nYou are PathSelectorGPT, an expert\\nin choosing the optimal road from\\nmultiple road images according to a\\nuser-specified intention.\\n[Task Description]\\nGiven a set of road images, select\\nthe best road from the crossroad.\\nYour answer must be in the following\\nJSON format:\\n{\"idx\":\\n\"Selected road index (start\\nby 0)\", \"reason\":\\n\"Justification\\nfor your selection\"}\\n[Input]\\nUser Intention:\\n<{agent_intention}>\\n[Output]\\nPlease answer with the road index\\nand the reasoning behind your\\nselection, in the required JSON\\nformat:\\nAn example road selection response for the first crossroad\\nselection in Fig. 6 is as follows:\\nâ€œidxâ€: â€œ0â€, â€œreasonâ€: â€œChoosing road 0 promises a\\ngenuine taste of local cuisine in a less commercialized set-\\nting. Itâ€™s likely to have family-run eateries where I can re-\\nquest non-spicy dishes and savor authentic flavors. This road\\noffers a tranquil dining atmosphere, allowing for a more en-\\ngaged and leisurely culinary exploration among the locals.â€\\nD.3. Summarize Place Reviews\\nWhen Hiro discovers a place in the street view imagery, he\\nretrieves its corresponding name and Google reviews from\\nthe V-IRL environment. There is a complex algorithm be-\\nhind projecting the 2D box on street view imagery to a con-\\ncrete place in the real world, which is detailed in â€œmatch-\\ning between object proposal and placesâ€ of Sec. 5.2. After\\nHiro obtains these place reviews, he summarizes them into\\na place overview (to aid in decision-making) using the fol-\\nlowing prompt:\\n[Role]\\nYou are SummarizeGPT, skilled at\\ncondensing multiple reviews into a\\nconcise overview of a location.\\n[Task Description]\\nGiven multiple reviews with ratings,\\ncraft a brief overview of the place.\\nYour response should be in the\\nfollowing JSON format:\\n{\"summarization\":\\n\"Concise\\ndescription (limited to 80 words)\"}\\n[Example]\\nFor reviews \"Great ambiance but\\naverage food (Rating:\\n3)\" and\\n\"Loved the decor, food could be\\n21\\nbetter (Rating:\\n3.5)\", the output\\nmight be:\\n{\"summarization\":\\n\"The place\\nboasts great ambiance and decor,\\nbut the food quality receives mixed\\nreviews.\"}\\n[Input]\\nReviews:\\n{all_reviews}\\n[Output]\\nYour concise overview (max 80 words)\\nbased on the provided reviews, in\\nthe prescribed JSON format:\\nD.4. Action Decision\\nAfter obtaining the overview of the identified place, Hiro\\ndecides to visit the place or keep exploration using GPT-4\\nand the following prompt:\\n[Role]\\nYou are ActionSelectorGPT,\\nproficient in choosing the most\\nappropriate action based on a\\nuserâ€™s background, intention, and\\nan overview of a place.\\n[Task Description]\\nEvaluate the provided user\\nbackground, intention, and place\\noverview to select the most suitable\\naction from the list.\\nYour response\\nshould be in the following JSON\\nformat:\\n{\"action\":\\n\"Selected Action\",\\n\"reason\":\\n\"Justification for your\\nchoice\"}\\nPossible actions:\\n- enter_place():\\nEnter the\\ndesignated place.\\n- continue():\\nContinue searching\\nfor another appropriate place.\\n[Example]\\nFor the background \"loves historical\\nsites\", intention \"discover local\\nhistory\", and place overview\\n\"This is a 200-year-old preserved\\nmansion\", the output might be:\\n\"action\":\\n\"enter_place()\",\\n\"reason\":\\n\"The historical mansion\\naligns with the userâ€™s interest in\\nhistorical sites.\"\\n[Input]\\nUser Background:\\n<{background}>\\nUser Intention:\\n<{intention}>\\nPlace Overview:\\n<{place_intro}>\\n[Output]\\nYour chosen action and the rationale\\nbehind your decision in the\\nprescribed JSON format:\\nHiroâ€™s\\nexploration\\nwill\\ncontinue\\nif\\nhe\\ndecides\\nto\\ncontinue()\\nand\\nwill\\nterminate\\nif\\nhe\\nopts\\nfor\\nenter_place().\\nE. V-IRL Benchmarks: Details\\nE.1. V-IRL Places: Localization (Details)\\nAll category results.\\nDue to the page limit of the main\\npaper, we only present the results of 10 categories in Tab. 3.\\nHere, we present the place recall for all 20 categories in\\nFig. 18.\\nRecall\\nconvenience store\\ncafe\\nclothing store\\npark\\nbank\\npharmacy\\nlodging\\nbook store\\nrestaurant\\njewelry store\\nlibrary\\nlaundry\\nschool\\nbakery\\nhospital\\nsupermarket\\nbar\\ngym\\nspa\\nmovie theater\\n0\\n20\\n40\\n60\\nGrounding DINO\\nOwl-ViT\\nGLIP\\nCLIP (w/ GLIP proposal) \\nFigure 18. Recalls in V-IRL Place localization\\nExample illustrations.\\nTo facilitate the understanding of\\nV-IRL Place localization benchmark, we present some ex-\\namples of CLIP (w/ GLIP proposals) in Fig. 21.\\nE.2. V-IRL Places: Recognition and VQA (Details)\\nPlace types performance for recognition.\\nIn Figure 19,\\nwe present the averaged accuracy for each place type across\\n10 benchmarked vision models. The size and the x-axis po-\\nsition of each bubble correspond to the number of places\\nwithin each type. A clear trend emerges: accuracy tends\\nto correlate with the frequency. Common categories such\\nas clothing store, cafe exhibit higher accuracy,\\nwhereas vision models often struggle with infrequent place\\ntypes like bowling alley or mosque.\\nPlace types performance for VQA.\\nThe place types\\nperformance of the V-IRL place VQA in Fig. 20 further\\n22\\naccounting\\nart gallery\\natm\\nbakery\\nbank\\nbar\\nbeauty salon\\nbicycle store\\nbook store\\nbus station\\ncafe\\ncar dealer\\ncar rental\\ncar repair\\ncar wash\\nchurch\\nclothing store\\nconvenience store\\ndoctor\\ndrugstore\\nelectronics store\\nembassy\\nflorist\\nfuneral home\\nfurniture store\\ngas station\\ngym\\nhair care\\nhardware store\\nhome goods store\\nhospital\\ninsurance agency\\njewelry store\\nlaundry\\nlawyer\\nlibrary\\nliquor store\\nlocal government office\\nlocksmith\\nlodging\\nmeal delivery\\nmeal takeaway\\nmovie theater\\nmoving company\\nmuseum\\nnight club\\npark\\nparking\\npharmacy\\npolice\\npost office\\nprimary school\\nreal estate agency\\nrestaurant\\nschool\\nsecondary school\\nshoe store\\nshopping mall\\nspa\\nstorage\\nstore\\nsupermarket\\ntourist attraction\\ntransit station\\ntravel agency\\nuniversity\\nveterinary care\\npet store\\ndepartment store\\nphysiotherapist\\nplumber\\ndentist\\ncourthouse\\nmovie rental\\npainter\\nelectrician\\nhindu temple\\nPlace count\\nAccuracy (100%)\\n0\\n25\\n50\\n75\\n100\\n1\\n10\\n100\\n1000\\ncasino\\nbowling alley\\nmosque\\nFigure 19. Category-wise accuracy and numbers for V-IRL Place\\nRecognition benchmark.\\nverifies the correlation between accuracy and frequency\\nfrom a human intention perspective. The top-10 categories\\nare closely aligned with the most common human activ-\\nities, purchasing and dining.\\nIn contrast, the bottom-10\\nplace types relate to places that are less frequently encoun-\\ntered and serve a more diverse purpose, such as mosque,\\nplumber and embassy.\\nbicycle store\\nbakery\\ndepartment store\\nsupermarket\\nshopping mall\\nconvenience store\\nflorist\\nclothing store\\ncafe\\nshoe store\\nlawyer\\ninsurance agency\\ngovernment office\\nsecondary school\\nhindu temple\\nmosque\\ncourthouse\\nbowling alley\\nembassy\\nplumber\\nPlace Type\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nAccuracy (%)\\nTop 10 place types\\nBottom 10 place types\\nFigure 20. Top-10 and bottom-10 place types averaged on four\\nvision models of V-IRL Place VQA.\\nE.3. V-IRL Vision-Language Navigation (Details)\\nNavigation pipeline.\\nAs mentioned in Appendix B.7, our\\nVLN pipeline is similar to [59], however, our benchmark\\noffers greater scalability through the worldwide V-IRL plat-\\nform and an automated data collection pipeline, as opposed\\nto the manual annotation of a specific region. Furthermore,\\nour benchmark emphasizes the analysis of the vision com-\\nponent in the VLN pipeline, as opposed to [59], which aims\\nto enhance performance on existing VLN datasets using\\nLLMs.\\nImplementation Details.\\nHere, we introduce the imple-\\nmentation details for LLaVA-1.5 [41] and PP-OCR [20] (+\\nGPT-3.5). For LLaVA-1.5 [41], we transform the landmark\\nrecognition task to a multiple choice VQA problem, asking\\nWhich of the following landmarks\\ncan be identified with a high\\ndegree of confidence?\\nThe VQA options include all potential landmarks men-\\ntioned in the route description, along with a â€œNone of\\naboveâ€ choice. The modelâ€™s response to this question is\\nthen parsed as the landmark observation.\\nFor PP-OCR [20] (+ GPT-3.5), we first extract all recog-\\nnized text using PP-OCR [20] for each street view image.\\nThen, GPT-3.5 [57] determines the presence of each land-\\nmark in this street view image, jointly considering the OCR\\ntext and landmark name.\\nFull set results.\\nApart from the mini-set results presented\\nin Sec. 5.4, we also provide the full set results of Oracle and\\nCLIP (L/14@336px) in Tab. 7. The Oracle results, interest-\\ningly, do not achieve a 100% success rate, due to incorrect\\ndecisions made by the LLM at stop positions. This is evi-\\ndenced by the high arrival ratio and low reaction accuracy\\nat stop positions. Empirically, we observe that the LLM oc-\\ncasionally decides to keep moving, despite clear destination\\nindications in the observations.\\nWhen we substitute the map in oracle with the CLIP\\nmodel to gather landmark observations from street view im-\\nagery, we observe a significant drop in the success rate,\\ndue to the inevitable model prediction errors. To improve\\nthe success rate in VLN, we can focus on two important\\nfactors: (i) designing better vision models; (ii) develop-\\ning LLMs and prompt techniques that are robust to vision-\\nrelated noise. Especially, our empirical findings suggest that\\nsophisticated prompt designs significantly improve the ro-\\nbustness of LLMs to visual observation noise.\\nMethod\\nStart Intersection\\nStop\\nSuccess Reac Arr\\nReac\\nArr Reac\\nOracle (No Vision)\\n0.88\\n1.0\\n0.95\\n0.99\\n0.96\\n0.88\\nCLIP (L/14@336px)\\n0.22\\n0.84\\n0.66\\n0.90\\n0.61\\n0.22\\nTable 7. Results of V-IRL VLN-full.\\n23\\nFigure 21. Samples of V-IRL Place localization using CLIP (w/ GLIP proposals).\\n24\\n'},\n",
       " {'id': 'http://arxiv.org/abs/2402.03309v1',\n",
       "  'title': 'AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion',\n",
       "  'published_date': datetime.datetime(2024, 2, 5, 18, 59, 31),\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2402.03309v1',\n",
       "  'summary': 'Underwater perception and 3D surface reconstruction are challenging problems\\nwith broad applications in construction, security, marine archaeology, and\\nenvironmental monitoring. Treacherous operating conditions, fragile\\nsurroundings, and limited navigation control often dictate that submersibles\\nrestrict their range of motion and, thus, the baseline over which they can\\ncapture measurements. In the context of 3D scene reconstruction, it is\\nwell-known that smaller baselines make reconstruction more challenging. Our\\nwork develops a physics-based multimodal acoustic-optical neural surface\\nreconstruction framework (AONeuS) capable of effectively integrating\\nhigh-resolution RGB measurements with low-resolution depth-resolved imaging\\nsonar measurements. By fusing these complementary modalities, our framework can\\nreconstruct accurate high-resolution 3D surfaces from measurements captured\\nover heavily-restricted baselines. Through extensive simulations and in-lab\\nexperiments, we demonstrate that AONeuS dramatically outperforms recent\\nRGB-only and sonar-only inverse-differentiable-rendering--based surface\\nreconstruction methods. A website visualizing the results of our paper is\\nlocated at this address: https://aoneus.github.io/',\n",
       "  'pdf_text': 'AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion\\nMohamad Qadriâˆ—1\\nKevin Zhangâˆ—2\\nAkshay Hinduja1\\nMichael Kaess1\\nAdithya Pediredla3\\nChristopher Metzler2\\nCarnegie Mellon University1\\nUniversity of Maryland, College Park2\\nDartmouth College3\\n{mqadri,ahinduja}@andrew.cmu.edu, {kzhang24,metzler}@umd.edu, kaess@cmu.edu,\\nAdithya.K.Pediredla@dartmouth.edu\\nhttps://aoneus.github.io/\\ncamera and sonar ground truth\\nNeuS \\n(Optical)\\nNeuSIS \\n(Acoustic)\\nAONeus \\n(Acoustic-optical)\\nFigure 1. AONeuS Experimental Results: Under the restricted baseline operating conditions commonly encountered in underwater\\nconstruction and navigation, camera-only reconstruction techniques (NeuS [46]) and sonar-only reconstruction techniques (NeuSIS [39])\\nstruggle to accurately recover 3D surface geometry. This is due to the highly underdetermined nature of their respective measurement\\nprocesses; cameras lack depth information, and imaging sonars do not capture elevation information. We have developed a multimodal\\nacoustic-optical neural surfaces reconstruction framework (AONeuS) that effectively combines data from these complementary modalities.\\nAbstract\\nUnderwater perception and 3D surface reconstruction\\nare challenging problems with broad applications in con-\\nstruction, security, marine archaeology, and environmen-\\ntal monitoring.\\nTreacherous operating conditions, frag-\\nile surroundings, and limited navigation control often dic-\\ntate that submersibles restrict their range of motion and,\\nthus, the baseline over which they can capture measure-\\nments.\\nIn the context of 3D scene reconstruction, it\\nis well-known that smaller baselines make reconstruction\\nmore challenging.\\nOur work develops a physics-based\\nmultimodal acoustic-optical neural surface reconstruction\\nframework (AONeuS) capable of effectively integrating\\nhigh-resolution RGB measurements with low-resolution\\ndepth-resolved imaging sonar measurements.\\nBy fusing\\nthese complementary modalities, our framework can recon-\\nstruct accurate high-resolution 3D surfaces from measure-\\nments captured over heavily-restricted baselines. Through\\nextensive simulations and in-lab experiments, we demon-\\n*: equal contribution.\\nstrate that AONeuS dramatically outperforms recent RGB-\\nonly and sonar-only inverse-differentiable-renderingâ€“based\\nsurface reconstruction methods.\\n1. Introduction\\nThe 3D reconstruction of underwater environments is an\\nimportant problem with applications in myriad fields, in-\\ncluding underwater construction, marine ecology, archaeol-\\nogy, mapping, inspection, and surveillance [1, 25, 33, 45].\\nThe underwater robots applied to this task are typically\\nequipped with both imaging sonars (i.e., acoustic cameras)\\nand optical cameras [23, 28]. These sensors capture com-\\nplementary information about their operating environments.\\nForward-look imaging sonars consist of a uniform linear\\narray of transducers which, through beamforming, recover\\nboth range and azimuth information (but not elevation). (3D\\nimaging sonars which record both azimuth and elevation\\nalso exist, but can be prohibitively expensive.) Unlike light-\\nbased sensors, imaging sonars are highly robust to scatter-\\ning and low-light conditions. Unfortunately, imaging sonars\\n1\\narXiv:2402.03309v1  [cs.CV]  5 Feb 2024\\ngenerally have poor spatial resolution; sonar images of an\\nobject of interest often appear textureless and hard to rec-\\nognize; and imaging sonar measurements can suffer from\\ncomplex artifacts caused by multipath reflections and the\\nvariable speed of sound passing through inhomogeneous\\nwater [44].\\nBy contrast, optical cameras have high spatial resolution\\nand can resolve object appearance in great detail. How-\\never, in turbid water light scattering and absorption can\\nseverely restrict the range and contrast of optical cam-\\neras [18].\\nMoreover, to recover depth information pas-\\nsive optical sensors rely on large displacements/baselines\\nbetween measurements. In constrained operating environ-\\nments, such measurements are often inaccessible.\\nBy leveraging the complementary strengths and weak-\\nnesses of cameras and imaging sonars, acoustic-optical sen-\\nsor fusion promises to enable robust and high-resolution un-\\nderwater perception and scene reconstruction [16, 31]. Ex-\\nisting contour matching based acoustic-optical reconstruc-\\ntion methods based can already reconstruct accurate high-\\nresolution 3D surfaces [8]. Unfortunately, these methods\\nrequire a 360-degree view of the scene and are inappli-\\ncable in the small-baseline operating conditions prevalent\\nin real-world unmanned underwater vehicle operation. Al-\\nternatively, one can reconstruct the scene from optical and\\nacoustic measurements independently and then fuse the re-\\nsult [20]. However, this simple approach provides limited\\nbenefits over camera-only surface reconstruction.\\nIn this paper, we develop an inverse-differentiable-\\nrenderingâ€“based approach to acoustic-optical sensor fusion\\nthat can form dense 3D surface reconstructions from camera\\nand sonar measurements captured across a small baseline.\\nOur work consists of four key contributions.\\nâ€¢ We develop a physics-based multimodal acoustic-optical\\nneural surface framework which simultaneously inte-\\ngrates RGB and imaging sonar measurements. Our ap-\\nproach extends the neural surfaces 3D reconstruction\\nframework [46] by combining a unified representation of\\nthe scene geometry with modality-specific (acoustic and\\noptical) representations of appearance.\\nâ€¢ We\\nconduct\\nexperiments\\non\\nboth\\nsynthetic\\nand\\nexperimentally-captured\\ndatasets\\nand\\ndemonstrate\\nour method can effectively reconstruct high-fidelity\\nsurface geometry from noisy measurements captured\\nover limited baselines.\\nâ€¢ We theoretically support our strong empirical perfor-\\nmance by analyzing the conditioning of the acoustic-\\noptical forward model. We show that the forward process\\nassociated with triangulating a point in 3D from acoustic-\\noptical measurements is better conditioned and easier to\\ninvert the unimodal forward models.\\nâ€¢ We release a public dataset and open-source implementa-\\ntion of our method.\\n2. Related Work\\nSonar Imaging\\n3D reconstruction from sonar imagery is\\nan important and widely studied problem. Over the last\\ndecade a variety of 3D reconstruction methods have been\\nproposed based on space carving [5, 6], classical point-\\ncloud processing algorithms [43, 51], generative model-\\ning [5, 7, 35, 50], convex optimization [52], graph-based\\nprocessing [47, 48], and supervised machine learning [3,\\n15, 49].\\nLast year, two research groups employed neural ren-\\ndering to enable breakthrough 3D sonar imaging perfor-\\nmance. Qadri et al. [39] developed a Neural Implicit Sur-\\nface Reconstruction Using Imaging Sonar (NeuSIS) method\\nwhich forms high-fidelity 3D surface reconstructions from\\nforward imaging sonar measurements by combining neu-\\nral surface representations with a novel acoustic differen-\\ntiable volumetric renderer. Similarly, Reed et al. [40] em-\\nployed neural rendering to recover 3D volumes from syn-\\nthetic aperture sonar measurements. The former method re-\\nlies upon a large number of sonar images captured over a\\nlarge baseline while the latter applies to synthetic aperture\\nsonar, not forward imaging sonar, and relies on access to\\nraw time-based sonar measurements. These methods rep-\\nresent the state-of-the-art in 3D surface reconstruction with\\nsonar.\\nTo date, no method has effectively recovered a dense 3D\\nscene from 2D sonar images captured over a limited base-\\nline. Without additional constrains, e.g., optical measure-\\nments, or strong priors the reconstruction problem is hope-\\nlessly underdetermined.\\nNeural Rendering\\nIn their breakthrough neural radiance\\nfields (NeRF) paper, Mildenhall et al. [32] combined neural\\nsignal representations with differentiable volume rendering\\nto perform novel view synthesis. The underlying differen-\\ntiable volume rendering concept has since been extended to\\nrepresent and recover scene geometry. The Implicit Differ-\\nentiable Renderer (IDR) approach, introduced in [53], rep-\\nresents geometry as the zero-level set of a neural network\\nand uses differentiable surface rendering to fit the parame-\\nters of a neural network. IDR requires object masks for su-\\npervision. Later methods, like Neural Surfaces (NeuS) [46],\\nUnified Surfaces (UNISURF) [37], and Volume Signed Dis-\\ntance Functions (VolSDF) [54] combine an implicit surface\\nrepresentation with differentiable volume rendering to re-\\ncover 3D geometry from images without the need for ob-\\nject masks. Recent work has sought to reduce the number\\nof training images required [29] and to accelerate rendering\\nto enable real-time applications [55].\\nThe inverse-differentiable-rendering framework has also\\nbeen extended to handle measurements from a diverse range\\nof sensors. Cross-spectral radiance fields (X-NeRF) were\\n2\\nproposed in [38] to model multispectral, infrared, and RGB\\nimages.\\nTransient neural radiance fields were proposed\\nin [30] to model the measurements from a single-photon\\nlidar. Time-of-flight radiance fields were proposed in [4] to\\nmodel the measurements from a continuous wave time-of-\\nflight sensor. Polarization-aided decomposition of radiance,\\nor PANDORA, was proposed in [14] to model polarimetric\\nmeasurements of light. Radar neural radiance fields (RaN-\\neRF) were proposed in [27] to model inverse synthetic aper-\\nture radar measurements.\\nSeveral recent works have modeled light scattering\\nwithin the neural rendering framework to improve recon-\\nstructions through water [24, 42], haze [12], and fog [2].\\nMultimodal Imaging\\nTo overcome the disadvantages in-\\nherent to using a single sensing modality, numerous multi-\\nmodal sensing algorithms have been developed [10, 22, 26,\\n36]. Most related to our work, Babaee and Negahdaripour\\n[8] reconstruct 3D objects from RGB and sonar imagery by\\nmatching occluding contours across RGB images and imag-\\ning sonar measurements, performing stereo matching, and\\ninterpolating the curves in 3D space. Unfortunately, this\\nmethod is inapplicable to the small-baseline setting; it fun-\\ndamentally requires 360-degree views of the scene. Another\\nwork that reconstructions 3D from RGB images and imag-\\ning sonar is [20]. This work reconstructs a scene from op-\\ntical and acoustic measurements independently using clas-\\nsical methods like COLMAP [41] and then fuses the re-\\nsult. As we will demonstrate in Sec. 6.2, this approach pro-\\nvides limited benefits over a purely optical approach and is\\nnot competitive with sate-of-the-art neural rendering based\\nmethods.\\nOutside of sonar, several neural-rendering based ap-\\nproaches to sensor fusion have recently been developed.\\nIn Multimodal Neural Radiance Field, Zhu et al. [56] use\\nneural rendering to combine RGB, thermal, and point cloud\\ndata. Similarly, [21] use neural rendering to combine mul-\\ntispectral measurements of different polarizations and Carl-\\nson et al. [11] fuse sparse lidar and RGB measurements to\\nbuild 3D occupancy grid of unbounded scenes,\\nTo our knowledge, ours is the first work to perform\\nacoustic-optical sensor fusion with neural rendering.\\n3. Background\\n3.1. Imaging Sonars\\nImaging sonars are active sensors that emit acoustic pulses\\nand measure the intensity of the reflected wave. They pro-\\nduce a 2D acoustic image in which the range and azimuth\\nof the imaged object are resolved. However, the objectâ€™s el-\\nevation remains ambiguous. I.e., the reflecting object can\\nbe located anywhere on the elevation arc (fig. 2) and the in-\\ntensity of a pixel in a sonar image is proportional to the cu-\\n(b) Sonar image formation model\\n(a) Camera image formation model\\n(d) Sample sonar image\\n(c) Sample camera image\\nFigure 2. Acoustic-Optical Measurement Processes. (a) RGB\\nmeasurement process and example measurement. Pixels along a\\ncommon ray passing through the camera center map to the same\\nimage pixel on the image plane. (b) Sonar measurement process\\nand example measurement. In a sonar image, the azimuth Î¸ and\\nrange r of the imaged object are resolved. However, the elevation\\ninformation Ï• is lost; all objects located along the elevation arc in\\nblue map to the same pixel.\\nmulative reflected acoustic energy from all reflecting points\\nalong the elevation arc.\\n3.2. Image Formation Model of an Imaging Sonar\\nSimilar to [39], we use the following sonar image formation\\nmodel:\\nIson(ri, Î¸i)=\\nZ Ï•max\\nÏ•min\\nZ ri+Ïµ\\nriâˆ’Ïµ\\nEe\\nr T(r, Î¸i, Ï•)Ïƒ(r, Î¸i, Ï•)drdÏ•,\\n(1)\\nwhere Ï•min, Ï•max are the minimum and maximum elevation\\nangles, Ee is the acoustic energy emitted by the sonar. T =\\neâˆ’\\nR ri\\n0\\nÏƒ(râ€²,Î¸i,Ï•i)drâ€² is the transmittance term, and Ïƒ is the\\nparticle density. (See [39] for more details.)\\n3.3. Image Formation Model of an Optical Camera\\nWe adopt the optical camera image formation model pro-\\nposed by [46] where a pixel intensity at (x, y) is approxi-\\nmated by:\\nIcam(x, y) =\\nZ âˆž\\n0\\nT(t)Ïƒ(t)c(p(t), v)dt,\\n(2)\\nwhere the integral is over the ray starting at the camera cen-\\nter and passing through pixel (x, y). T, Ïƒ are the transmit-\\ntance and density values at point p(t), and c(p(t), v) is the\\ncolor of a point viewed from direction v.\\n4. Problem Statement\\nOur goal in this work is to reconstruct the 3D surface\\nof an underwater object using a small collection of RGB\\n3\\nLocal Coordinates\\nWorld \\nCoordinates\\nWorld \\nCoordinates\\nLocal Coordinates\\nElevation\\nAmbiguity\\nDepth\\nAmbiguity\\n(a)\\n(b)\\nFigure 3. Acoustic-Optical Measurement Ambiguities. (a) Two\\nRGB measurements captured over a limited baseline struggle to\\nlocalize a point along the depth-axis. (b) Two sonar measurements\\ncaptured over a limited baseline struggle to localize a point along\\nthe x-axis. Because they have orthogonal ambiguities, RGB and\\nsonar measurements are highly complementary.\\nand sonar measurements captured over a limited base-\\nline.\\nSpecifically, we assume access to two datasets,\\nDcam = {Icam\\ni\\n, P son\\ni\\n} and Dson = {Ison\\ni\\n, P son\\ni\\n}, consisting\\nof RGB/sonar images and their respective poses.\\nGiven a large dataset captured over a sufficiently diverse\\nrange of poses (e.g., thousands of images captured from\\n360-degrees [39]), existing unimodal (camera-only/sonar-\\nonly) surface reconstruction methods are already effec-\\ntive [39, 46]. In this work, we focus on the small baseline\\noperating conditionsâ€”pervasive in underwater roboticsâ€”\\nwhere optical cameras record insufficient information to re-\\ncover depth information (see Fig. 3(a)) and imaging sonars\\nrecord insufficient information to recover elevation infor-\\nmation (see Fig. 3(b)).\\nSpecifically, we introduce a physics-based multimodal\\ninverse-differentiable-rendering framework that integrates\\ninformation from both acoustic and optical sensors to gen-\\nerate accurate 3D reconstructions.\\nOur approach auto-\\nmatically exploits the complementary information (eleva-\\ntion/range) provided by each sensor.\\nBecause our shared pool-based testing facility does not\\nallow us to introduce turbidity, in this work we focus on the\\nclear-water setting. Modeling the effects of light scattering\\nin our forward model [2, 12, 24, 42] would likely improve\\nour systemâ€™s in-the-wild performance.\\n5. Method\\n5.1. Acoustic-Optical NeuS\\nOur AONeuS reconstruction framework is illustrated\\nin Fig. 4. Following Qadri et al. [39], Wang et al. [46], we\\nrepresent the objectâ€™s surface using a Signed Distance Func-\\ntion (SDF), N(x), which outputs the distance of each 3D\\npoint x = (X, Y, Z) to the nearest surface. Distinct from\\nthese works, we use two separate rendering neural networks\\n(Mcam and Mson) that approximate the optical and acoustic\\noutgoing radiance at each spatial coordinate x. This choice\\nis motivated by the fact that different materials have differ-\\nent acoustic and optical reflectance properties. For example,\\nglass is invisible to optical cameras but visible to imaging\\nsonar, and PVC is invisible to imaging sonar but visible to\\noptical cameras.\\nIn this work, we sample and sum points along acoustic\\nand optical rays to approximate the rendering integrals de-\\nfined by Eq. (1) and Eq. (2). Our rendering functions can be\\nexpressed as\\nË†Ison(r, Î¸) =\\nX\\nxâˆˆApson\\n1\\nr(x)T[x]Î±[x]Mson(x), and\\n(3)\\nË†Icam(x, y) =\\nX\\nxâˆˆRpcam\\nT[x]Î±[x]Mcam(x),\\n(4)\\nwhere Apson is the set sampled points along the acoustic arc\\nat pixel pson and Rpcam is the set of sampled points along\\noptical ray passing through pixel pcam. Mson and Mcam are\\nthe predicted radiance at x.\\nThe computation of the discrete transmittance and opac-\\nity terms in Eq. (3) and Eq. (4) requires sampling along both\\nacoustic and optical rays. For any such spatial sample xs,\\n(i.e., any point along an acoustic or optical ray), the discrete\\nopacity at xs can be approximated as\\nÎ±[xs] = max\\n\\x12Î¦s(N(xs)) âˆ’ Î¦s(N(xs+1))\\nÎ¦s(N(xs))\\n, 0\\n\\x13\\n,\\n(5)\\nwhere Î¦s(x) = (1+eâˆ’sx)âˆ’1 is the Sigmoid function and s\\nis a trainable parameter. The discrete transmittance is mod-\\neled as\\nT[xs] =\\nY\\nxr | r<s\\n(1 âˆ’ Î±[xr]).\\n(6)\\n5.1.1\\nLoss Function\\nOur loss function comprises the sonar and camera intensity\\nlosses:\\nLson\\nint â‰¡\\n1\\nNPson\\nX\\npâˆˆPson\\n||Ë†I(p) âˆ’ I(p)||1, and\\n(7)\\nLcam\\nint â‰¡\\n1\\nNPcam\\nX\\npâˆˆPcam\\n||Ë†I(p) âˆ’ I(p)||1,\\n(8)\\nwhere Pcam and Pson is the set of sampled pixels in the\\ncamera and sonar images respectively. We additionally use\\nthe eikonal loss as an implicit regularization to encourage\\nsmooth reconstructions:\\nLeik â‰¡\\n1\\n|X|\\nX\\nxâˆˆX\\n(||âˆ‡N(x)||2 âˆ’ 1)2,\\n(9)\\nwhere X is the set of all sampled points.\\n4\\n64\\n64\\n64\\n64\\n64\\n64\\n64\\n64\\n3\\n1\\n64\\n3\\n1\\n64\\n64\\n64\\n64\\n3\\nNeural Implicit Surface\\nRepresentation  \\nNeural Renderer (Sonar)\\nNeural Renderer (Camera)\\n64\\nPositional Encoding\\nFigure 4. AONeuS Reconstruction Framework. A shared sur-\\nface geometry SDF network N is used in combination with ren-\\ndering specific neural rendering modules. For each sampled point\\nx along an acoustic or optical ray, N outputs its signed distance,\\nits gradient as well as 2 features vectors Fson and Fcam all serving\\nas input to their respective rendering networks.\\nWe also utilize an â„“1 loss term as an additional prior term\\nwhich biases the network towards reconstructions that min-\\nimize the total opacity of the scene (for example in cases\\nwhere the object is on the seafloor and only specific sides\\ncan be imaged):\\nLreg â‰¡\\n1\\n|X|\\nX\\nxâˆˆX\\n||Î±[x]||1.\\n(10)\\nHence, our total loss is\\nL = Î±(t)Lson\\nint + (1 âˆ’ Î±(t))Lcam\\nint + Î»eikLeik + Î»regLreg.\\n(11)\\nThe network is trained with the ADAM optimizer.\\n5.1.2\\nWeight Scheduling\\nThe weights assigned to the sonar and camera intensity\\nlosses (respectively Î±(t) and 1 âˆ’ Î±(t) in eq. 11) impact\\nthe reconstruction quality as they determine which measure-\\nments the network should emphasize throughout training.\\nWe adopt a simple two-step weighting scheme:\\nÎ±(t) =\\n(\\n1 if t < Et,\\nÎ» if Et < t < Ee.\\n(12)\\nIn the early iterations, t < Et, the sonar measurements are\\nused exclusively and serve to â€maskâ€œ the object; i.e., update\\nCamera\\nSonar\\nWorld \\nCoordinates\\nSonar image \\nplane\\nCamera image \\nplane\\nFigure 5. Simulation setup. We simulate capturing sonar and\\ncamera measurements over a limited baseline.\\nthe weights of the SDF network N to bias it towards re-\\nconstructions in which the geometry of the object are better\\nconstrained in the depth direction. This process establishes\\nan initialization for the later iterations.\\nIn later iterations, t > Et, more emphasis is placed on\\nthe camera measurements. These measurements cosntrain\\nthe x and y directions and help resolve the elevation ambi-\\nguity inherent in sonar data. In this phase, sonar measure-\\nments receive less weight and act as a depth regularizer.\\n6. Experimental Results\\nIn this section we evaluate the proposed AONeuS technique\\non both synthetic and experimentally captured data.\\n6.1. Results on Synthetic Data\\nTo generate synthetic measurements, we implemented the\\nsonar image formation model Eq. (1) in Blender [13] and\\ncollected simulated sonar-camera datasets for various ob-\\njects. The simulation setup is illustrated in Fig. 5. The sonar\\nand camera are approximately collocated, and are translated\\nlinearly over a short baseline along the X axis of the world\\nframe for a distance of 1.2 m with the sonarâ€™s azimuthal\\nplane parallel the YZ plane in the world frame. The sonarâ€™s\\nazimuthal plane is oriented orthogonal to the direction of\\nmotion to ensure the trajectory was non-degenerate; mul-\\ntiple measurements captured from positions within the az-\\nimuthal plane of the sonar would be highly redundant and\\nuninformative [33].\\nFor each object, the trajectory is sub-sampled into\\nsmaller baselines: 0.96 m, 0.72 m, 0.48 m, and 0.24 m for\\nanalysis. We scaled the meshes so that the objects are ap-\\nproximately âˆ¼ 1m in size and the sensors are placed about\\n1.5 mâ€“2 m away from the object. The elevation aperture of\\nthe sonar is 12â—¦. We benchmark our method against two\\nmethods: NeuS [46] and NeuSIS [39], executing all meth-\\nods 9 times with randomly initialized seeds. To ensure we\\nhad reasonable camera-only results, we provided NeuS with\\nmasks of the object. This information is not required by nor\\nprovided to NeuSIS and AONeuS.\\n5\\nIn Fig. 9(a), we compare the reconstruction performance\\nof all three techniques for a total of five scenes. We could\\nobserve that AONeus consistently reconstructs the scene ge-\\nometry better than NeuS and NeuSIS. Further, we can also\\nobserve that NeuS (camera-only) incorrectly reconstructs\\nthe depth axis (Z-axis) whereas NeuSIS (sonar-only) can\\nreconstruct only the depth-axis accurately. The proposed\\nAONeus was able to recover underlying scene geometry\\nalong all the axes.\\nIn Fig. 9(b), we show the results for the turtle mesh\\nfor various baselines. To visualize the ambiguities associ-\\nated with camera and sonar modalities and the benefit of\\nthe fusion algorithm, we rendered the reconstructed meshes\\nwith a virtual camera pointing in Y -axis. Hence, the ren-\\ndered images are projections of the reconstructed mesh on\\nZX-plane. As we decrease the baseline (top to bottom),\\nfor NeuS, we observe an increasing loss of features along\\ndepth direction: the back legs of the turtle are progres-\\nsively lost and depth-reconstruction worsens with decreas-\\ning baselines. For sonar-only methods, significant ambigui-\\nties along the elevation axis can be seen across all baselines:\\ndue to the limited translation of the sonar, the collected mea-\\nsurements are not enough to constrain and resolve the tur-\\ntle shell adequately. Our framework AONeuS integrates or-\\nthogonal information from both imaging modalities to yield\\nreconstructions of higher quality across all baselines: all\\nfeatures of the turtle including its shell and its back legs\\nare clearly discernible. These observations are further sup-\\nported by the quantitative analysis in Tab. 1 where we report\\nthe mean and variance of Chamfer L1 distance, precision,\\nand recall of the reconstructions over nine trials. The results\\ndemonstrate that AONeuS outperforms the existing meth-\\nods, particularly with reduced baselines. Note that recall of\\nNeuSIS appears to be slightly better than AONeuS but that\\nis only because the NeuSIS generates a large blob that cov-\\ners most part of the object. The per-baseline quantitative\\nand qualitative results for the remaining four meshes can be\\nNeuS\\nNeuSIS\\nAONeuS\\n1.2m\\nChamfer â†“\\n0.123 Â± 0.028\\n0.130 Â± 0.013\\n0.075 Â± 0.006\\nPrecisionâ†‘\\n0.653 Â± 0.095\\n0.566 Â± 0.043\\n0.862 Â± 0.042\\nRecall â†‘\\n0.526 Â± 0.134\\n0.836 Â± 0.022\\n0.825 Â± 0.056\\n0.96m\\nChamfer â†“\\n0.139 Â± 0.024\\n0.134 Â± 0.011\\n0.079 Â± 0.005\\nPrecision â†‘\\n0.602 Â± 0.076\\n0.531 Â± 0.031\\n0.840 Â± 0.017\\nRecall â†‘\\n0.470 Â± 0.132\\n0.816 Â± 0.031\\n0.807 Â± 0.017\\n0.72m\\nChamfer â†“\\n0.205 Â± 0.027\\n0.135 Â± 0.011\\n0.081 Â± 0.005\\nPrecision â†‘\\n0.423 Â± 0.051\\n0.537 Â± 0.062\\n0.810 Â± 0.032\\nRecall â†‘\\n0.279 Â± 0.071\\n0.768 Â± 0.029\\n0.792 Â± 0.022\\n0.48m\\nChamfer â†“\\n0.249 Â± 0.045\\n0.139 Â± 0.012\\n0.088 Â± 0.006\\nPrecision â†‘\\n0.337 Â± 0.062\\n0.470 Â± 0.028\\n0.791 Â± 0.023\\nRecall â†‘\\n0.189 Â± 0.074\\n0.706 Â± 0.028\\n0.770 Â± 0.023\\n0.24m\\nChamfer â†“\\n0.406 Â± 0.087\\n0.146 Â± 0.009\\n0.111 Â± 0.017\\nPrecision â†‘\\n0.223 Â± 0.060\\n0.450 Â± 0.028\\n0.690 Â± 0.045\\nRecall â†‘\\n0.107 Â± 0.049\\n0.587 Â± 0.042\\n0.679 Â± 0.042\\nTable 1.\\nMetrics for synthetic turtle data.\\nBest metrics are\\nbolded.\\nFigure 6. Experimental hardware setup. (a) Test water tank used\\nto conduct the experiments and its dimensions. (b) Test object. (c)\\nBluefin Hovering Autonomous Underwater Vehicle (HAUV) and\\nits mounted hardware (Didson imaging sonar and Doppler Veloc-\\nity Log (DVL). (d) FLIR Blackfly S GigE camera used for image\\ncapture and its watertight enclosure.\\nfound in the supplementary material.\\n6.2. Results on Experimentally-Captured Data\\nWe also perform real-world experiments on an object\\n(Fig. 6b) submerged in a water tank (Fig. 6a).\\nPlease\\ncheck the supplementary video for more visualizations of\\nthe setup. We used a SoundMetrics DIDSON imaging sonar\\nmounted on a Bluefin Hovering Autonomous Underwater\\nVehicle (HAUV) (Fig. 6c) to capture two sonar datasets of\\nthe test object with two different elevation apertures 14â—¦\\nand 28â—¦. The vehicle uses an IMU and a Doppler Velocity\\nLog (DVL) to measure sonar pose information. We asyn-\\nchronously capture optical images of the same object using\\na FLIR Blackfly S GigE 5MP camera (Fig. 6d) with cam-\\n6\\nera pose information computed with COLMAP. The sonar\\nand camera trajectories were aligned post-capture.\\nSim-\\nilar to the simulation setup, both camera and sonar fol-\\nlowed an approximately 1.2m non-degenerate linear trajec-\\ntory, which we later sub-sampled into the same 5 baselines.\\nWe benchmarked our method against three algorithms: The\\nCOLMAP based sensor fusion method introduced in [20]1,\\nNeuS [46], and NeuSIS [39]. For each dataset and sensor\\nbaseline, we executed each method six times with randomly\\ninitialized seeds except that of [20], which is deterministic.\\nQualitatively, we observe in Fig. 10 that AONeuS out-\\nputs a more complete shape across baselines compared to\\nsonar-only (NeuSIS) and camera-only (NeuS) only meth-\\nods: the hole, two legs, and crossbar are clearly discernible.\\nConversely, when using only sonar, parts of the object are\\nnot well reconstructed as we can observe, for example, with\\nthe long leg with NeuSIS at 14â—¦. Similarly, camera-only\\nmethods result in the loss of features such as the hole ac-\\ncompanied with significant introduced depth errors.\\nWe\\nquantify the results in Tab. 2, where we report the mean\\nand variance of the Chamfer L1 distance, precision, and re-\\ncall against the ground truth mesh computed over six tri-\\nals with different random seeds for training. We observe\\nthat the fusion of the acoustic and optical signals generates\\nhigher quality reconstruction, even with very short base-\\nlines measuring only 24 cm, as indicated by the mean value\\nof each metric. When comparing AONeus with sonar-only\\nmethods (NeuSIS), we note that, despite the increased ele-\\nvation ambiguity introduced by the 28â—¦ elevation aperture,\\nour technique is able to leverage camera information and its\\nconstraints in the x and y axes to resolve spatial locations\\nthat are otherwise under-constrained when solely relying on\\nsonar. Techniques that rely on a camera only (NeuS) exhibit\\na decrease in performance as the sensor baseline is reduced.\\nComplementing camera with sonar information introduces\\nconstraints in the depth direction easing the resolution of\\ndepth which is known to be difficult to resolve with limited\\ncamera motion. We additionally emphasize the variance of\\nthe reconstruction quality as measured by the variance of\\nthe Chamfer distance: the fusion of both modalities result\\nin outputs that are more robust to the randomness of the al-\\ngorithm (i.e. network initialization, point samples, etc.).\\n7. Discussion and Analysis\\n7.1. Distribution of per-Axis Errors\\nIn Fig. 7, we visualize the per-axis deviations from the\\nground truth for the synthetic turtle scene at 0.24 m base-\\nline. We compute per-axis deviations by first determining\\nthe closest vertex in the dense ground truth mesh and tak-\\ning absolute differences in x, y, and z coordinates. We his-\\n1COLMAP outputs a sparse pointcloud. Hence, a mesh was computed\\nusing the ball pivoting algorithm [9].\\nX errors\\nY errors\\nZ errors\\nDeviation\\nDeviation\\nCount\\nDeviation\\nCount\\nCount\\nNeuS\\nNeuSIS\\nAONeuS\\nFigure 7. Per-axis error distributions. At 0.2x baseline for the\\nturtle example, we plot the distributions of deviations from the\\nground truth mesh along all three axes for NeuS, NeuSIS, and\\nAONeuS reconstructions. The NeuS reconstruction has larger Z\\nerrors, noticeable from the long tail, where as NeuSIS reconstruc-\\ntion has larger X errors. AONeuS has tighter distributions along\\nall three axes compared to NeuS or NeuSIS showing that the pro-\\nposed technique takes the best of both of the imaging modalities.\\ntogram these deviations along all three axes and show them\\nalong rows in the Fig. 7. We have repeated this procedure\\nfor NeuS, NeuSIS, and AONeuS and show them along the\\ncolumns.\\nFrom the data, we can observe (1) NeuS has large devi-\\nations along Z axes, (2) NeuSIS has large deviations along\\nX axes.\\nThese results are consistent with the ambigui-\\nties associated with their respective measurement processes.\\nAONeuS has low spread on all axes as it captures the best\\nof both camera (NeuS) and sonar (NeuSIS) imaging modal-\\nities.\\n7.2. Multimodal Sensing is Better Conditioned\\nThe strong empirical performance of our multimodal recon-\\nstructions can be explained in terms of system conditioning.\\nGiven point correspondences between measurements, it is\\nfar easier to triangulate a point using multimodal acoustic-\\noptical measurements than camera-only or sonar-only mea-\\nsurements.\\nTo illustrate this fact, consider a point P = [X, Y, Z]t\\nthat is observed by an acoustic-optical sensor from two po-\\nsitions. The sonarâ€™s azimuthal plane is the yz plane, in its\\nown coordinate system. The cameraâ€™s image plane is the\\nz = f plane, in its own coordinate system. Without loss of\\ngenerality, assume the sensorâ€™s coordinate system at its ini-\\ntial location is the world coordinate system and its coordi-\\nnate system at its second position is described by a rotation\\nR and translation t = [tx, ty, tz]. That is, the coordinate of\\n7\\npoint P in the new coordinate system is P â€² = RP + t.\\nUnder this model, the acoustic-optical sensor records 8\\nmeasurements:\\nxc = f [1, 0, 0]P\\n[0, 0, 1]P ,\\nyc = f [0, 1, 0]P\\n[0, 0, 1]P ,\\nR = âˆ¥Pâˆ¥,\\nÎ¸ = tanâˆ’1 \\x10[0, 1, 0]P\\n[0, 0, 1]P\\n\\x11\\n,\\nxâ€²\\nc = f rt\\n1P + tx\\nrt\\n3P + tz\\n,\\nyâ€²\\nc = f rt\\n2P + ty\\nrt\\n3P + tz\\n,\\nRâ€² =\\np\\nâˆ¥Râˆ¥2 + âˆ¥tâˆ¥2 + 2ttRP,\\nÎ¸â€² = tanâˆ’1 \\x10rt\\n2P + ty\\nrt\\n3P + tz\\n\\x11\\n,\\n(13)\\nwhere ri denotes the ith row of R.\\nLoosely following Negahdaripour [33], Negahdaripour\\net al. [34], we can turn each of these measurements into\\nseven linear constraints and one non-linear constraint on P.\\nAmultiP = b and âˆ¥Pâˆ¥2 = R2 with\\nAmulti =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n(âˆ’f, 0, xc)\\n(0, âˆ’f, yc)\\n(0, âˆ’1, tan(Î¸))\\nxâ€²\\ncrt\\n3 âˆ’ frt\\n2\\nyâ€²\\ncrt\\n3 âˆ’ frt\\n2\\ntan(Î¸â€²)rt\\n3 âˆ’ rt\\n2\\nttR\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nand b =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n0\\nftx âˆ’ xâ€²\\nctz\\nfty âˆ’ yâ€²\\nctz\\nty âˆ’ tan(Î¸â€²)tz\\n(Râ€²)2âˆ’R2âˆ’âˆ¥tâˆ¥2\\n2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\n(14)\\nOne can similarly form camera-only, Acam, and sonar-\\nonly, Ason, forward models by considering only rows 1, 2,\\n4, and 5 and rows 3, 6, and 7, respectively, of Amulti. By\\ninverting these systems, one can triangulate P in space.\\nHere we perform Monte Carlo sampling to compare the\\nconditioning of Acam, Ason, and Amulti. We sample P\\nuniformly in a 1 m3 cube centered at (0, 0, 1.5) with edges\\nparallel to the x, y, and z axis; we assume f = 100 mm; we\\nsample tx, ty, and tz uniformly in the range 0 cm to 10 cm;\\nand we sample the yaw, pitch, and roll between measure-\\nments uniformly in the range âˆ’5â—¦ to 5â—¦ .\\nFor each realization of these parameters, we compute the\\ncondition number, Îº, of Acam, Ason, and Amulti. We re-\\npeat this process 50, 000 times to form histograms, illus-\\ntrated in Fig. 8. The condition number of the multimodal\\nsystem is generally much lower and the system is thus eas-\\nier to invert; multimodal triangulation is easier.\\n8. Conclusion\\nWe have introduced and validated a multimodal inverse-\\ndifferentiable-rendering framework for reconstructing 3D\\nsurface information from camera and sonar measurements.\\nOur framework combines camera and sonar information us-\\ning a unified surface representation module and separate\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n5(Acam)\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n5(Ason)\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n5(Amulti)\\nFigure 8. System Conditioning. Histograms of the condition\\nnumbers of the camera-only (top), sonar-only (middle), and mul-\\ntimodal (bottom) forward models.\\nMedian condition numbers\\nare highlighted in red. The acoustic-optical multimodal forward\\nmodel is generally better conditioned and easier to invert (triangu-\\nlation).\\nmodality-specific appearance modules and rendering func-\\ntions. By extracting information from these complementary\\nmodalities, our framework is able to offer breakthrough un-\\nderwater sensing capabilities for restricted baseline imag-\\ning scenarios. We have demonstrated that AONeuS can ac-\\ncurately reconstruct the geometry of complex 3D objects\\nfrom synthetic as well as noisy, real-world measurements\\ncaptured over severely restricted baselines.\\nWhile we demonstrate the first neural fusion of camera\\nand sonar measurements, there are many interesting direc-\\ntions to explore this amalgamation. In Sec. 5.1.2, we in-\\ntroduced a heuristic for weighing camera and sonar mea-\\nsurements. A structured way of combining the camera and\\nsonar data, which is aware of the uncertainties [17, 19] in\\nthe complementary imaging systems could result in faster\\nconvergence rates and better reconstructions.\\nThe sonar we have used in our implementations are\\nforward-looking sonars.\\nFusion algorithms for side-scan\\nsonars, synthetic-aperture sonars, sonars of different ranges\\nand wavelengths, could be an interesting forward direc-\\ntion. Similarly, extending the technique for various geome-\\ntries and materials including multi-object scenes, dynamic\\nscenes, cluttered scenes and scattering media (murky water)\\nwould make AONeuS more practical. Finally, on-the-fly re-\\nconstructions could allow one to select the best next under-\\nwater view to improve reconstruction accuracy and further\\nreduce the required baseline and acquisition time.\\n8\\n9. Acknowledgements\\nThe authors would like to thank Tianxiang Lin and Jui-Te\\nHuang for their help with data collection and Sarah Friday\\nfor providing an animation showcasing the real experimen-\\ntal setup. M.Q., A.H., and M.K. were supported in part by\\nONR grant N00014-21-1-2482. A.P. was supported by NSF\\ngrant 2326904. K.Z. and C.A.M. were supported in part by\\nAFOSR Young Investigator Program award no. FA9550-\\n22-1-0208, ONR award no. N00014-23-1-2752, and a seed\\ngrant from SAAB, Inc.\\nReferences\\n[1] Jan Albiez, Sylvain Joyeux, Christopher Gaudig, Jens Hill-\\njegerdes, Sven Kroffke, Christian Schoo, Sascha Arnold,\\nGeovane Mimoso, Pedro Alcantara, Rafael Saback, et al.\\nFlatfish-a compact subsea-resident inspection auv.\\nIn\\nOCEANS 2015-MTS/IEEE Washington, pages 1â€“8, 201 Wa-\\nterfront Street National Harbor, Maryland 20745 USA, 2015.\\nIEEE, IEEE. 1\\n[2] Ramazzina Andrea, Bijelic Mario, Walz Stefanie, Sanvito\\nAlessandro, Scheuble Dominik, and Heide Felix. Scattern-\\nerf: Seeing through fog with physically-based inverse neural\\nrendering. 2023. 3, 4\\n[3] Sascha Arnold and Bilal Wehbe.\\nSpatial acoustic projec-\\ntion for 3d imaging sonar reconstruction.\\nIn 2022 Inter-\\nnational Conference on Robotics and Automation (ICRA),\\npages 3054â€“3060, Philadelphia, 2022. IEEE. 2\\n[4] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil\\nKim, Christian Richardt, James Tompkin, and Matthew\\nOâ€™Toole. TÂ¨orf: Time-of-flight radiance fields for dynamic\\nscene view synthesis. Advances in Neural Information Pro-\\ncessing Systems, 34, 2021. 3\\n[5] Murat D Aykin and Shahriar Negahdaripour. On 3-d target\\nreconstruction from multiple 2-d forward-scan sonar views.\\nIn OCEANS 2015-Genova, pages 1â€“10. IEEE, 2015. 2\\n[6] Murat D Aykin and Shahriar Negahdaripour.\\nThree-\\ndimensional target reconstruction from multiple 2-d forward-\\nscan sonar views by space carving. IEEE Journal of Oceanic\\nEngineering, 42(3):574â€“589, 2016. 2\\n[7] Murat D Aykin and Shahriar S Negahdaripour. Modeling\\n2-d lens-based forward-scan sonar imagery for targets with\\ndiffuse reflectance. IEEE journal of oceanic engineering, 41\\n(3):569â€“582, 2016. 2\\n[8] Mohammadreza Babaee and Shahriar Negahdaripour.\\n3-\\nd object modeling from 2-d occluding contour correspon-\\ndences by opti-acoustic stereo imaging.\\nComputer Vision\\nand Image Understanding, 132:56â€“74, 2015. 2, 3\\n[9] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier,\\nClÂ´audio Silva, and Gabriel Taubin. The ball-pivoting algo-\\nrithm for surface reconstruction. IEEE transactions on visu-\\nalization and computer graphics, 5(4):349â€“359, 1999. 7\\n[10] Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus,\\nWerner Ritter, Klaus Dietmayer, and Felix Heide.\\nSee-\\ning through fog without seeing fog: Deep multimodal sen-\\nsor fusion in unseen adverse weather.\\nIn Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 11682â€“11692, 2020. 3\\n[11] Alexandra Carlson,\\nManikandasriram S. Ramanagopal,\\nNathan Tseng, Matthew Johnson-Roberson, Ram Vasude-\\nvan, and Katherine A. Skinner. Cloner: Camera-lidar fu-\\nsion for occupancy grid-aided neural representations. IEEE\\nRobotics and Automation Letters, 8(5):2812â€“2819, 2023. 3\\n[12] W. Chen, W. Yifan, S. Kuo, and G. Wetzstein. Dehazenerf:\\nMultiple image haze removal and 3d shape reconstruction\\nusing neural radiance fields. In 3DV, 2024. 3, 4\\n[13] Blender Online Community. Blender - a 3D modelling and\\nrendering package. Blender Foundation, Stichting Blender\\nFoundation, Amsterdam, 2022. 5\\n[14] Akshat Dave, Yongyi Zhao, and Ashok Veeraraghavan. PAN-\\nDORA: Polarization-Aided Neural Decomposition of Radi-\\nance, page 538â€“556. Springer Nature Switzerland, 2022. 3\\n[15] Robert DeBortoli, Fuxin Li, and Geoffrey A Hollinger. El-\\nevatenet: A convolutional neural network for estimating the\\nmissing dimension in 2d underwater sonar images. In 2019\\nIEEE/RSJ International Conference on Intelligent Robots\\nand Systems (IROS), pages 8040â€“8047. IEEE, 2019. 2\\n[16] Fausto Ferreira, Diogo Machado, Gabriele Ferri, Samantha\\nDugelay, and John Potter. Underwater optical and acoustic\\nimaging: A time for fusion? a brief overview of the state-\\nof-the-art. OCEANS 2016 MTS/IEEE Monterey, pages 1â€“6,\\n2016. 2\\n[17] Lily Goli, Cody Reading, Silvia SellÂ´an, Alec Jacobson,\\nand Andrea Tagliasacchi.\\nBayesâ€™ Rays:\\nUncertainty\\nquantification in neural radiance fields.\\narXiv preprint\\narXiv:2309.03185, 2023. 8\\n[18] Jules S Jaffe.\\nUnderwater optical imaging: the past, the\\npresent, and the prospects. IEEE Journal of Oceanic En-\\ngineering, 40(3):683â€“700, 2014. 2\\n[19] Wen Jiang, Boshu Lei, and Kostas Daniilidis.\\nFisherrf:\\nActive view selection and uncertainty quantification for\\nradiance fields using fisher information.\\narXiv preprint\\narXiv:2311.17874, 2023. 8\\n[20] Jason Kim, Meungsuk Lee, Seokyong Song, Byeongjin Kim,\\nand Son-Cheol Yu.\\n3-D Reconstruction of Underwater\\nObjects Using Image Sequences from Optical Camera and\\nImaging Sonar.\\nIn OCEANS 2019 MTS/IEEE SEATTLE,\\npages 1â€“6, 2019. 2, 3, 7\\n[21] Youngchan Kim, Wonjoon Jin, Sunghyun Cho, and Seung-\\nHwan Baek.\\nNeural spectro-polarimetric fields.\\nIn SIG-\\nGRAPH Asia 2023 Conference Papers, New York, NY, USA,\\n2023. Association for Computing Machinery. 3\\n[22] Young Min Kim, Christian Theobalt, James Diebel, Jana\\nKosecka, Branislav Miscusik, and Sebastian Thrun. Multi-\\nview image and tof sensor fusion for dense 3d reconstruc-\\ntion. In 2009 IEEE 12th International Conference on Com-\\nputer Vision Workshops, ICCV Workshops, pages 1542â€“\\n1549, 2009. 3\\n[23] Samuel Lensgraf, Amy Sniffen, Zachary Zitzewitz, Evan\\nHonnold, Jennifer Jain, Weifu Wang, Alberto Li, and Devin\\nBalkcom. Droplet: Towards autonomous underwater assem-\\nbly of modular structures. In Proceedings of Robotics: Sci-\\nence and Systems, 2021. 1\\n9\\n[24] Deborah Levy, Amit Peleg, Naama Pearl, Dan Rosenbaum,\\nDerya Akkaynak, Simon Korman, and Tali Treibitz. Seathru-\\nnerf: Neural radiance fields in scattering media. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 56â€“65, 2023. 3, 4\\n[25] Tianxiang Lin, Akshay Hinduja, Mohamad Qadri, and\\nMichael Kaess. Conditional gans for sonar image filtering\\nwith applications to underwater occupancy mapping. In 2023\\nIEEE International Conference on Robotics and Automation\\n(ICRA), pages 1048â€“1054. IEEE, 2023. 1\\n[26] David B. Lindell, Matthew Oâ€™Toole, and Gordon Wetzstein.\\nSingle-Photon 3D Imaging with Deep Sensor Fusion. ACM\\nTrans. Graph. (SIGGRAPH), (4), 2018. 3\\n[27] Afei Liu, Shuanghui Zhang, Chi Zhang, Shuaifeng Zhi, and\\nXiang Li. Ranerf: Neural 3-d reconstruction of space tar-\\ngets from isar image sequences. IEEE Transactions on Geo-\\nscience and Remote Sensing, 61:1â€“15, 2023. 3\\n[28] Haowen Liu, Monika Roznere, and Alberto Quattrini Li.\\nDeep underwater monocular depth estimation with single-\\nbeam echosounder.\\nIn 2023 IEEE International Confer-\\nence on Robotics and Automation (ICRA), pages 1090â€“1097.\\nIEEE, 2023. 1\\n[29] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\\nWenping Wang. Sparseneus: Fast generalizable neural sur-\\nface reconstruction from sparse views. ECCV, 2022. 2\\n[30] Anagh Malik, Parsa Mirdehghan, Sotiris Nousias, Kiri-\\nakos N. Kutulakos, and David B. Lindell. Transient neural\\nradiance fields for lidar view synthesis and 3d reconstruction.\\nNeurIPS, 2023. 3\\n[31] Fabio Menna, Panagiotis Agrafiotis, and Andreas Geor-\\ngopoulos. State of the art and applications in archaeological\\nunderwater 3d recording and mapping. Journal of Cultural\\nHeritage, 33:231â€“248, 2018. 2\\n[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance fields for view syn-\\nthesis. In ECCV, 2020. 2\\n[33] Shahriar Negahdaripour. Application of forward-scan sonar\\nstereo for 3-d scene reconstruction. IEEE journal of oceanic\\nengineering, 45(2):547â€“562, 2018. 1, 5, 8\\n[34] Shahriar Negahdaripour, Hicham Sekkati, and Hamed Pirsi-\\navash. Opti-acoustic stereo imaging: On system calibration\\nand 3-d target reconstruction. IEEE Transactions on image\\nprocessing, 18(6):1203â€“1214, 2009. 8\\n[35] Shahriar Negahdaripour,\\nVictor M Milenkovic,\\nNikan\\nSalarieh, and Mahsa Mirzargar. Refining 3-d object models\\nconstructed from multiple fs sonar images by space carving.\\nIn OCEANS 2017-Anchorage, pages 1â€“9. IEEE, 2017. 2\\n[36] Mark Nishimura, David B Lindell, Christopher Metzler, and\\nGordon Wetzstein. Disambiguating monocular depth esti-\\nmation with a single transient. In European Conference on\\nComputer Vision, pages 139â€“155. Springer, 2020. 3\\n[37] Michael Oechsle, Songyou Peng, and Andreas Geiger.\\nUnisurf:\\nUnifying neural implicit surfaces and radiance\\nfields for multi-view reconstruction. In International Con-\\nference on Computer Vision (ICCV), 2021. 2\\n[38] Matteo Poggi, Pierluigi Zama Ramirez, Fabio Tosi, Samuele\\nSalti, Luigi Di Stefano, and Stefano Mattoccia.\\nCross-\\nspectral neural radiance fields. In Proceedings of the Inter-\\nnational Conference on 3D Vision, 2022. 3DV. 3\\n[39] Mohamad Qadri, Michael Kaess, and Ioannis Gkioulekas.\\nNeural implicit surface reconstruction using imaging sonar.\\nIn 2023 IEEE International Conference on Robotics and Au-\\ntomation (ICRA), pages 1040â€“1047. IEEE, 2023. 1, 2, 3, 4,\\n5, 7\\n[40] Albert Reed, Juhyeon Kim, Thomas Blanford, Adithya\\nPediredla, Daniel Brown, and Suren Jayasuriya.\\nNeural\\nVolumetric Reconstruction for Coherent Synthetic Aperture\\nSonar. ACM Transactions on Graphics, 42(4):113:1â€“113:20,\\n2023. 2\\n[41] Johannes L Schonberger and Jan-Michael Frahm. Structure-\\nfrom-motion revisited.\\nIn Proceedings of the IEEE con-\\nference on computer vision and pattern recognition, pages\\n4104â€“4113, 2016. 3\\n[42] Advaith\\nVenkatramanan\\nSethuraman,\\nManikandasri-\\nram Srinivasan Ramanagopal, and Katherine A Skinner.\\nWaternerf: Neural radiance fields for underwater scenes. In\\nOCEANS 2023-MTS/IEEE US Gulf Coast, pages 1â€“7. IEEE,\\n2023. 3, 4\\n[43] Pedro V Teixeira, Michael Kaess, Franz S Hover, and John J\\nLeonard. Underwater inspection using sonar-based volumet-\\nric submaps. In 2016 IEEE/RSJ International Conference\\non Intelligent Robots and Systems (IROS), pages 4288â€“4295.\\nIEEE, 2016. 2\\n[44] Nguyen Dinh Tinh and T Dang Khanh.\\nA new imaging\\ngeometry model for multi-receiver synthetic aperture sonar\\nconsidering variation of the speed of sound in seawater. IEIE\\nTransactions on Smart Processing and Computing, 10(4):\\n302â€“308, 2021. 2\\n[45] Jinkun Wang, Tixiao Shan, and Brendan Englot. Underwater\\nterrain reconstruction from forward-looking sonar imagery.\\nIn 2019 International Conference on Robotics and Automa-\\ntion (ICRA), pages 3471â€“3477. IEEE, 2019. 1\\n[46] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\\nKomura, and Wenping Wang. Neus: Learning neural im-\\nplicit surfaces by volume rendering for multi-view recon-\\nstruction. Advances in Neural Information Processing Sys-\\ntems, 34:27171â€“27183, 2021. 1, 2, 3, 4, 5, 7\\n[47] Yusheng Wang, Yonghoon Ji, Hanwool Woo, Yusuke\\nTamura, Atsushi Yamashita, and Asama Hajime.\\n3d oc-\\ncupancy mapping framework based on acoustic camera in\\nunderwater environment. IFAC-PapersOnLine, 51(22):324â€“\\n330, 2018. 2\\n[48] Yusheng Wang, Yonghoon Ji, Hanwool Woo, Yusuke\\nTamura, Atsushi Yamashita, and Hajime Asama.\\nThree-\\ndimensional underwater environment reconstruction with\\ngraph optimization using acoustic camera.\\nIn 2019\\nIEEE/SICE International Symposium on System Integration\\n(SII), pages 28â€“33. IEEE, 2019. 2\\n[49] Yusheng Wang, Yonghoon Ji, Dingyu Liu, Hiroshi Tsuchiya,\\nAtsushi Yamashita, and Hajime Asama. Elevation angle esti-\\nmation in 2d acoustic images using pseudo front view. IEEE\\nRobotics and Automation Letters, 6(2):1535â€“1542, 2021. 2\\n10\\n[50] Eric Westman and Michael Kaess.\\nWide aperture imag-\\ning sonar reconstruction using generative models. In 2019\\nIEEE/RSJ International Conference on Intelligent Robots\\nand Systems (IROS), pages 8067â€“8074. IEEE, 2019. 2\\n[51] Eric Westman, Ioannis Gkioulekas, and Michael Kaess. A\\ntheory of fermat paths for 3d imaging sonar reconstruction.\\nIn 2020 IEEE/RSJ International Conference on Intelligent\\nRobots and Systems (IROS), pages 5082â€“5088. IEEE, 2020.\\n2\\n[52] Eric Westman, Ioannis Gkioulekas, and Michael Kaess.\\nA volumetric albedo framework for 3d imaging sonar re-\\nconstruction.\\nIn 2020 IEEE International Conference on\\nRobotics and Automation (ICRA), pages 9645â€“9651. IEEE,\\n2020. 2\\n[53] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neu-\\nral surface reconstruction by disentangling geometry and ap-\\npearance. Advances in Neural Information Processing Sys-\\ntems, 33:2492â€“2502, 2020. 2\\n[54] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.\\nVolume rendering of neural implicit surfaces.\\nIn Thirty-\\nFifth Conference on Neural Information Processing Systems,\\n2021. 2\\n[55] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\\nPratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,\\nand Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-\\ntime view synthesis. arXiv, 2023. 2\\n[56] Haidong Zhu, Yuyin Sun, Chi Liu, Lu Xia, Jiajia Luo, Nan\\nQiao, Ram Nevatia, and Chengâ€“Hao Kuo. Multimodal neu-\\nral radiance field. In 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), pages 9393â€“9399, 2023. 3\\n11\\nGT\\nAONeuS\\nNeuS\\nNeuSIS\\nPlane\\nLobster\\nStarfish\\nShell\\nTurtle\\nGT\\nAONeuS\\nNeuS\\nNeuSIS\\n1.2 m\\n0.96 m\\n0.72 m\\n0.48 m\\n0.24 m\\n(a)\\n(b)\\nFigure 9. Simulated Reconstructions Over Varying Baselines. ((5x5 collection of images) 1st column: Ground truth mesh and measure-\\nment geometry. 2nd column: Sonar-only neural surface reconstructions. 3rd row: RGB-only neural surface reconstructions. 4th column:\\nMultimodal backprojection reconstruction. 5th column: Multimodal neural surface reconstructions. (The rows show 1m, 80cm, 60cm,\\n40cm, and 20cm baseline reconstructions, respectively (or something like that).) (This is transposed from what we discussed yesterday and\\nincludes multimodal back-prop).) .\\nSonar dataset 1\\n14â—¦ elevation angle\\nSonar dataset 2\\n28â—¦ elevation angle\\nBaseline\\nMetric\\nNeuS\\nKim et al.\\n(2019)\\nNeuSIS\\n(14â—¦)\\nAONeuS\\n(14â—¦)\\nNeuSIS\\n(28â—¦)\\nAONeuS\\n(28â—¦)\\n1.2m\\nChamfer L1 â†“\\n0.092 Â± 0.015\\n0.177\\n0.187 Â± 0.032\\n0.093 Â± 0.005\\n0.217 Â± 0.036\\n0.105 Â± 0.008\\nPrecision â†‘\\n0.693 Â± 0.066\\n0.336\\n0.482 Â± 0.063\\n0.661 Â± 0.025\\n0.444 Â± 0.026\\n0.602 Â± 0.023\\nRecall â†‘\\n0.679 Â± 0.098\\n0.387\\n0.355 Â± 0.051\\n0.707 Â± 0.047\\n0.318 Â± 0.040\\n0.614 Â± 0.037\\n0.96m\\nChamfer L1 â†“\\n0.107 Â± 0.013\\n0.182\\n0.180 Â± 0.026\\n0.095 Â± 0.008\\n0.214 Â± 0.032\\n0.097 Â± 0.006\\nPrecision â†‘\\n0.661 Â± 0.048\\n0.318\\n0.514 Â± 0.036\\n0.689 Â± 0.042\\n0.462 Â± 0.032\\n0.645 Â± 0.038\\nRecall â†‘\\n0.563 Â± 0.084\\n0.345\\n0.370 Â± 0.053\\n0.695 Â± 0.037\\n0.314 Â± 0.043\\n0.648 Â± 0.038\\n0.72m\\nChamfer L1 â†“\\n0.127 Â± 0.013\\n0.178\\n0.174 Â± 0.034\\n0.094 Â± 0.008\\n0.202 Â± 0.043\\n0.095 Â± 0.010\\nPrecision â†‘\\n0.651 Â± 0.047\\n0.368\\n0.550 Â± 0.080\\n0.646 Â± 0.035\\n0.502 Â± 0.063\\n0.640 Â± 0.037\\nRecall â†‘\\n0.500 Â± 0.062\\n0.396\\n0.374 Â± 0.061\\n0.698 Â± 0.067\\n0.352 Â± 0.065\\n0.668 Â± 0.073\\n0.48m\\nChamfer L1 â†“\\n0.150 Â± 0.022\\n0.179\\n0.180 Â± 0.032\\n0.097 Â± 0.012\\n0.215 Â± 0.034\\n0.098 Â± 0.010\\nPrecision â†‘\\n0.626 Â± 0.055\\n0.324\\n0.521 Â± 0.050\\n0.656 Â± 0.065\\n0.473 Â± 0.037\\n0.607 Â± 0.050\\nRecall â†‘\\n0.415 Â± 0.022\\n0.218\\n0.372 Â± 0.065\\n0.678 Â± 0.075\\n0.336 Â± 0.061\\n0.653 Â± 0.063\\n0.24m\\nChamfer L1 â†“\\n0.167 Â± 0.012\\n0.198\\n0.173 Â± 0.020\\n0.087 Â± 0.007\\n0.203 Â± 0.023\\n0.089 Â± 0.007\\nPrecision â†‘\\n0.580 Â± 0.031\\n0.305\\n0.535 Â± 0.063\\n0.699 Â± 0.057\\n0.501 Â± 0.021\\n0.651 Â± 0.049\\nRecall â†‘\\n0.363 Â± 0.056\\n0.140\\n0.347 Â± 0.030\\n0.760 Â± 0.030\\n0.317 Â± 0.024\\n0.705 Â± 0.037\\nTable 2. For the hardware reconstruction of â€Hâ€ object in Fig. 10, we report the mean and standard deviation of the Chamfer L1 distance,\\nprecision, and recall compared to the ground truth (obtained from a laser scan of the real structure) for various reconstruction techniques.\\nWe computed the standard deviation over 6 trials. For metric calculation, we used a threshold of 0.05 m.\\n12\\nGT\\nKim et al.\\nNeuS\\nNeuSIS \\n(14deg)\\nAONeuS\\n(14deg)\\n~1.2m\\n~0.96m\\n~0.48m ~0.72m\\n~0.24m\\nNeuSIS \\n(28deg)\\nAONeuS\\n(28deg)\\nFigure 10. Experimental Results with â€œhâ€ Object. As the baseline diminishes, NeuS exhibits increasing amount of distortion along the\\ndepth direction as can seen at the intersection of the short piling and crossbar at the 0.72m and 0.96m baselines. NeuSIS similarly generates\\nreconstructions with significant errors (for example, the long piling is poorly reconstructed with the 14â—¦ elevation). Conversely, AONeuS\\nconsistently produces faithful reconstructions across a range of baselines.\\n13\\n9.1. Variance of the Density Field Over Realizations\\nof the Algorithm\\nNeus\\nReconstructions\\nwith different seeds\\nHistogram \\ndistribution\\nNeuSIS\\nAONeus\\nFigure 11. Distribution of Reconstructions of the Real Object.\\nTop: Different Reconstructions from different random seeds for\\nthe 20% baseline. Bottom: Histogram distribution of the variance\\nof the density field VAR(Ïƒ(x)) vs. voxel count. Neusâ€™s distribu-\\ntion is heavily tailed while NeuSISâ€™s distribution exhibits a large\\nmean and variance. AONeuSâ€™s distribution is light-tailed with a\\nsmall mean and therefore it is better constrained.\\n9.2. Additional tables\\nTables 3 to 6 provide additional quantitative metrics for our\\nsynthetic experiments.\\nNeuS\\nNeuSIS\\nAONeuS\\n1.2m\\nChamfer â†“\\n0.112 Â± 0.018\\n0.197 Â± 0.011\\n0.117 Â± 0.014\\nPrecision â†‘\\n0.652 Â± 0.058\\n0.295 Â± 0.019\\n0.582 Â± 0.053\\nRecall â†‘\\n0.650 Â± 0.044\\n0.643 Â± 0.025\\n0.741 Â± 0.028\\n0.96m\\nChamfer â†“\\n0.144 Â± 0.021\\n0.200 Â± 0.019\\n0.134 Â± 0.016\\nPrecision â†‘\\n0.559 Â± 0.045\\n0.291 Â± 0.014\\n0.575 Â± 0.017\\nRecall â†‘\\n0.579 Â± 0.042\\n0.650 Â± 0.043\\n0.697 Â± 0.027\\n0.72m\\nChamfer â†“\\n0.146 Â± 0.021\\n0.200 Â± 0.016\\n0.141 Â± 0.023\\nPrecision â†‘\\n0.554 Â± 0.052\\n0.289 Â± 0.029\\n0.558 Â± 0.035\\nRecall â†‘\\n0.599 Â± 0.039\\n0.629 Â± 0.067\\n0.689 Â± 0.048\\n0.48m\\nChamfer â†“\\n0.174 Â± 0.016\\n0.199 Â± 0.012\\n0.146 Â± 0.033\\nPrecision â†‘\\n0.468 Â± 0.039\\n0.287 Â± 0.047\\n0.533 Â± 0.087\\nRecall â†‘\\n0.516 Â± 0.040\\n0.569 Â± 0.076\\n0.668 Â± 0.044\\n0.24m\\nChamfer â†“\\n0.223 Â± 0.046\\n0.182 Â± 0.011\\n0.166 Â± 0.034\\nPrecision â†‘\\n0.341 Â± 0.090\\n0.358 Â± 0.042\\n0.451 Â± 0.103\\nRecall â†‘\\n0.413 Â± 0.072\\n0.555 Â± 0.069\\n0.644 Â± 0.045\\nTable 3. Quantitative metrics for the airplane mesh.\\nNeuS\\nNeuSIS\\nAONeuS\\n1.2m\\nChamfer â†“\\n0.147 Â± 0.017\\n0.187 Â± 0.012\\n0.105 Â± 0.019\\nPrecision â†‘\\n0.448 Â± 0.073\\n0.328 Â± 0.020\\n0.592 Â± 0.110\\nRecall â†‘\\n0.454 Â± 0.100\\n0.460 Â± 0.037\\n0.626 Â± 0.090\\n0.96m\\nChamfer â†“\\n0.167 Â± 0.027\\n0.203 Â± 0.014\\n0.142 Â± 0.074\\nPrecision â†‘\\n0.394 Â± 0.070\\n0.302 Â± 0.025\\n0.534 Â± 0.230\\nRecall â†‘\\n0.398 Â± 0.106\\n0.445 Â± 0.033\\n0.533 Â± 0.228\\n0.72m\\nChamfer â†“\\n0.191 Â± 0.019\\n0.225 Â± 0.030\\n0.128 Â± 0.021\\nPrecision â†‘\\n0.357 Â± 0.053\\n0.275 Â± 0.040\\n0.533 Â± 0.112\\nRecall â†‘\\n0.417 Â± 0.077\\n0.390 Â± 0.040\\n0.576 Â± 0.105\\n0.48m\\nChamfer â†“\\n0.237 Â± 0.039\\n0.243 Â± 0.019\\n0.143 Â± 0.015\\nPrecision â†‘\\n0.321 Â± 0.062\\n0.257 Â± 0.012\\n0.523 Â± 0.069\\nRecall â†‘\\n0.392 Â± 0.091\\n0.370 Â± 0.017\\n0.577 Â± 0.043\\n0.24m\\nChamfer â†“\\n0.293 Â± 0.044\\n0.272 Â± 0.055\\n0.186 Â± 0.034\\nPrecision â†‘\\n0.251 Â± 0.049\\n0.225 Â± 0.039\\n0.440 Â± 0.116\\nRecall â†‘\\n0.277 Â± 0.103\\n0.290 Â± 0.076\\n0.483 Â± 0.130\\nTable 4. Quantitative metrics for the lobster mesh.\\nNeuS\\nNeuSIS\\nAONeuS\\n1.2m\\nChamfer â†“\\n0.108 Â± 0.020\\n0.177 Â± 0.010\\n0.088 Â± 0.022\\nPrecision â†‘\\n0.585 Â± 0.070\\n0.335 Â± 0.021\\n0.714 Â± 0.106\\nRecall â†‘\\n0.722 Â± 0.061\\n0.894 Â± 0.031\\n0.886 Â± 0.026\\n0.96m\\nChamfer â†“\\n0.122 Â± 0.030\\n0.170 Â± 0.014\\n0.089 Â± 0.016\\nPrecision â†‘\\n0.539 Â± 0.117\\n0.352 Â± 0.032\\n0.720 Â± 0.063\\nRecall â†‘\\n0.689 Â± 0.122\\n0.848 Â± 0.022\\n0.829 Â± 0.033\\n0.72m\\nChamfer â†“\\n0.159 Â± 0.035\\n0.171 Â± 0.010\\n0.126 Â± 0.035\\nPrecision â†‘\\n0.435 Â± 0.089\\n0.352 Â± 0.033\\n0.571 Â± 0.118\\nRecall â†‘\\n0.550 Â± 0.127\\n0.791 Â± 0.026\\n0.764 Â± 0.092\\n0.48m\\nChamfer â†“\\n0.255 Â± 0.041\\n0.170 Â± 0.006\\n0.143 Â± 0.046\\nPrecision â†‘\\n0.175 Â± 0.059\\n0.372 Â± 0.019\\n0.486 Â± 0.121\\nRecall â†‘\\n0.201 Â± 0.081\\n0.742 Â± 0.039\\n0.657 Â± 0.080\\n0.24m\\nChamfer â†“\\n0.548 Â± 0.144\\n0.191 Â± 0.006\\n0.196 Â± 0.033\\nPrecision â†‘\\n0.067 Â± 0.038\\n0.344 Â± 0.023\\n0.377 Â± 0.088\\nRecall â†‘\\n0.071 Â± 0.045\\n0.627 Â± 0.069\\n0.516 Â± 0.072\\nTable 5. Quantitative metrics for the seastar mesh.\\nNeuS\\nNeuSIS\\nAONeuS\\n1.2m\\nChamfer â†“\\n0.063 Â± 0.002\\n0.077 Â± 0.010\\n0.066 Â± 0.006\\nPrecision â†‘\\n0.847 Â± 0.011\\n0.754 Â± 0.066\\n0.858 Â± 0.024\\nRecall â†‘\\n0.941 Â± 0.018\\n0.814 Â± 0.035\\n0.844 Â± 0.038\\n0.96m\\nChamfer â†“\\n0.068 Â± 0.005\\n0.078 Â± 0.012\\n0.078 Â± 0.006\\nPrecision â†‘\\n0.833 Â± 0.023\\n0.756 Â± 0.072\\n0.816 Â± 0.022\\nRecall â†‘\\n0.929 Â± 0.020\\n0.803 Â± 0.043\\n0.794 Â± 0.031\\n0.72m\\nChamfer â†“\\n0.078 Â± 0.006\\n0.091 Â± 0.016\\n0.090 Â± 0.006\\nPrecision â†‘\\n0.769 Â± 0.035\\n0.747 Â± 0.086\\n0.774 Â± 0.029\\nRecall â†‘\\n0.876 Â± 0.044\\n0.746 Â± 0.075\\n0.730 Â± 0.021\\n0.48m\\nChamfer â†“\\n0.107 Â± 0.014\\n0.107 Â± 0.017\\n0.098 Â± 0.009\\nPrecision â†‘\\n0.620 Â± 0.064\\n0.791 Â± 0.077\\n0.714 Â± 0.031\\nRecall â†‘\\n0.690 Â± 0.084\\n0.676 Â± 0.078\\n0.691 Â± 0.037\\n0.24m\\nChamfer â†“\\n0.199 Â± 0.029\\n0.168 Â± 0.008\\n0.109 Â± 0.010\\nPrecision â†‘\\n0.309 Â± 0.028\\n0.832 Â± 0.041\\n0.667 Â± 0.048\\nRecall â†‘\\n0.302 Â± 0.029\\n0.465 Â± 0.032\\n0.640 Â± 0.047\\nTable 6. Quantitative metrics for the shell mesh.\\n14\\n'},\n",
       " {'id': 'http://arxiv.org/abs/2402.03307v1',\n",
       "  'title': '4D Gaussian Splatting: Towards Efficient Novel View Synthesis for\\n  Dynamic Scenes',\n",
       "  'published_date': datetime.datetime(2024, 2, 5, 18, 59, 4),\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2402.03307v1',\n",
       "  'summary': 'We consider the problem of novel view synthesis (NVS) for dynamic scenes.\\nRecent neural approaches have accomplished exceptional NVS results for static\\n3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior\\nefforts often encode dynamics by learning a canonical space plus implicit or\\nexplicit deformation fields, which struggle in challenging scenarios like\\nsudden movements or capturing high-fidelity renderings. In this paper, we\\nintroduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic\\nscenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D\\nGaussian Splatting in static scenes. We model dynamics at each timestamp by\\ntemporally slicing the 4D Gaussians, which naturally compose dynamic 3D\\nGaussians and can be seamlessly projected into images. As an explicit\\nspatial-temporal representation, 4DGS demonstrates powerful capabilities for\\nmodeling complicated dynamics and fine details, especially for scenes with\\nabrupt motions. We further implement our temporal slicing and splatting\\ntechniques in a highly optimized CUDA acceleration framework, achieving\\nreal-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and\\n583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions\\nshowcase the superior efficiency and effectiveness of 4DGS, which consistently\\noutperforms existing methods both quantitatively and qualitatively.',\n",
       "  'pdf_text': '4D Gaussian Splatting:\\nTowards Efficient Novel View Synthesis for Dynamic Scenes\\nYuanxing Duan1* Fangyin Wei2* Qiyu Dai1,4 Yuhang He1 Wenzheng Chen3â€  Baoquan Chen1,4â€ \\n1Peking University\\n2Princeton University\\n3NVIDIA\\n4National Key Lab of General AI, China\\nDynamic Video Input\\nThe Proposed Method\\nHyperReel [2]\\nEvaluation on PSNR vs. FPS\\nFigure 1. We present 4D Gaussian Splatting, a novel approach that takes videos as input and synthesizes dynamic novel-view images with\\nfine details. The proposed method outperforms prior arts in rendering quality and achieves up to 277 FPS inference speed on an RTX 3090\\nGPU at Plenoptic Dataset [33].\\nAbstract\\nWe consider the problem of novel view synthesis (NVS)\\nfor dynamic scenes. Recent neural approaches have accom-\\nplished exceptional NVS results for static 3D scenes, but\\nextensions to 4D time-varying scenes remain non-trivial.\\nPrior efforts often encode dynamics by learning a canon-\\nical space plus implicit or explicit deformation fields, which\\nstruggle in challenging scenarios like sudden movements\\nor capturing high-fidelity renderings.\\nIn this paper, we\\nintroduce 4D Gaussian Splatting (4DGS), a novel method\\nthat represents dynamic scenes with anisotropic 4D XY ZT\\nGaussians, inspired by the success of 3D Gaussian Splat-\\nting in static scenes [26].\\nWe model dynamics at each\\ntimestamp by temporally slicing the 4D Gaussians, which\\nnaturally compose dynamic 3D Gaussians and can be\\nseamlessly projected into images. As an explicit spatial-\\ntemporal representation, 4DGS demonstrates powerful ca-\\npabilities for modeling complicated dynamics and fine de-\\ntailsâ€”especially for scenes with abrupt motions. We further\\nimplement our temporal slicing and splatting techniques in\\na highly optimized CUDA acceleration framework, achiev-\\ning real-time inference rendering speeds of up to 277 FPS\\non an RTX 3090 GPU and 583 FPS on an RTX 4090\\nGPU. Rigorous evaluations on scenes with diverse motions\\nshowcase the superior efficiency and effectiveness of 4DGS,\\nwhich consistently outperforms existing methods both quan-\\ntitatively and qualitatively.\\n* Equal contribution\\nâ€  Co-corresponding authors\\n1. Introduction\\nReconstructing 3D scenes from 2D images and synthesiz-\\ning their appearance from novel views has been a long-\\nstanding goal in computer vision and graphics. This task is\\npivotal in numerous industrial applications including film,\\ngaming, and VR/AR, where there is a substantial demand\\nfor high-speed, photo-realistic rendering effects. The task\\ndiverges into two different scene types: static scenes where\\nobjects are still across all images [4, 24, 27, 37] and dy-\\nnamic scenes where scene contents exhibit temporal varia-\\ntions [11, 33, 40, 43, 57]. While the former has witnessed\\nsignificant progress recently, efficient and accurate NVS for\\ndynamic scenes remains challenging due to the complexi-\\nties introduced by the temporal dimension and diverse mo-\\ntion patterns.\\nA variety of methods have been proposed to tackle the\\nchallenges posed by dynamic NVS. A series of methods\\nmodel the 3D scene and its dynamics jointly [14, 21]. How-\\never, these methods often fall short in preserving fine details\\nin the NVS renderings due to the complexity caused by the\\nhighly entangled spatial and temporal dimensions. Alterna-\\ntively, many existing techniques [15, 39, 40, 43, 51] decou-\\nple dynamic scenes by learning a static canonical space and\\nthen predicting a deformation field to account for the tem-\\nporal variations. Nonetheless, this paradigm struggles in\\ncapturing complex dynamics such as objects appearing or\\ndisappearing suddenly. More importantly, prevailing meth-\\nods on dynamic NVS mostly build upon volumetric render-\\ning [37] which requires dense sampling on millions of rays.\\nAs a consequence, these methods typically cannot support\\n1\\narXiv:2402.03307v1  [cs.CV]  5 Feb 2024\\nreal-time rendering speed even for static scenes [33, 47].\\nRecently, 3D Gaussian Splatting (3DGS) [26] has\\nemerged as a powerful tool for efficient NVS of static\\nscenes. By explicitly modeling the scene with 3D Gaus-\\nsian ellipsoids and employing fast rasterization technique, it\\nachieves photo-realistic NVS in real time. Inspired by this,\\nwe propose to lift Gaussians from 3D to 4D and provide a\\nnovel spatial-temporal representation that enables NVS for\\nmore challenging dynamic scenes.\\nOur key observation is that 3D scene dynamics at each\\ntimestamp can be viewed as 4D spatial-temporal Gaussian\\nellipsoids sliced with different time queries. Fig. 2 illus-\\ntrates a simplified XY T case: the dynamics in 2D XY\\nspace at time Ti is equivalent to building 3D XY T Gaus-\\nsians and slicing by the t = Ti plane. Analogously, we\\nextend 3D Gaussians to 4D XY ZT space to model dy-\\nnamic 3D scenes. The temporally sliced 4D Gaussians com-\\npose 3D Gaussians that can be seamlessly projected to 2D\\nscreens via fast rasterization, inheriting both exquisite ren-\\ndering effects and high speed characteristic from 3DGS.\\nMoreover, extending the prune-split mechanism in the tem-\\nporal dimension makes 4D Gaussians particularly suitable\\nfor representing complex dynamics, including abrupt ap-\\npearances or disappearances.\\nIt is non-trivial to lift 3D Gaussians into 4D space, where\\ntremendous challenges exist in the design of the 4D rota-\\ntion, slicing, as well as the joint spatial-temporal optimiza-\\ntion scheme. We draw inspiration from geometric algebra\\nand carefully choose 4D rotor [6] to represent 4D rotation,\\nwhich is a spatial-temporal separable rotation representa-\\ntion. Notably, rotor representation accommodates both 3D\\nand 4D rotation: when the temporal dimension is set to\\nzero, it becomes equivalent to a quaternion and can rep-\\nresent 3D spatial rotation as well. Such adaptability grants\\nour method the flexibility to model both dynamic and static\\nscenes. In other words, 4DGS is a generalizable form of\\n3DGS: when closing the temporal dimension, our 4DGS re-\\nduces to 3DGS.\\nWe enhance the optimization strategies in 3DGS and in-\\ntroduce two new regularization terms to stabilize and im-\\nprove the dynamic reconstruction. We first propose an en-\\ntropy loss that pushes the opacity of Gaussians towards ei-\\nther one or zero, which proves effective to remove â€œfloatersâ€\\nin our experiments. We further introduce a novel 4D con-\\nsistency loss to regularize the motion of Gaussian points\\nand yield more consistent dynamics reconstruction. Exper-\\niments show that both terms notably improve the rendering\\nquality.\\nWhile existing Gaussian-based methods [35, 56, 60, 61]\\nare mostly based on PyTorch [41], we further develop a\\nhighly optimized CUDA framework with careful engineer-\\ning designs for fast training and inference speed. Our frame-\\nwork supports rendering 1352Ã—1014 videos at an unprece-\\nFigure 2. A Simplified 2D Illustration of the Proposed Tempo-\\nral Slicing. (a) we model 2D dynamics with 3D XY T ellipsoids\\nand slice them with different time queries. (b) The sliced 3D el-\\nlipsoids form 2D dynamic ellipses at each timestamp.\\ndented 583 FPS on an RTX 4090 GPU and 277 FPS on\\nan RTX 3090 GPU. We conduct extensive experiments on\\ntwo datasets spanning a wide range of settings and mo-\\ntion patterns, including monocular videos [43] and multi-\\ncamera videos [33]. Quantitative and qualitative evaluations\\ndemonstrate the distinct advantages over preceding meth-\\nods, including the new state-of-the-art rendering quality and\\nspeed.\\n2. Related Work\\nIn this section, we mainly review optimization-based novel\\nview synthesis (NVS) methods, where the input is a set of\\nposed images and the output is new appearance from novel\\nviewpoints. We first describe NVS for static scenes, then\\ntalk about its dynamic extensions. Lastly, we discuss recent\\nGaussian-based NVS methods.\\nStatic Novel View Synthesis Previous approaches for-\\nmalize light-field [30] or lumigraph [7, 22] that generate\\nnovel-view images by interpolating from existing views,\\nwhich require densely captured images in order to acquire\\nrealistic renderings. Other classical methods exploit geo-\\nmetric proxies such as mesh and volume to reproject and\\nblend contents from source images onto novel views. Mesh-\\nbased methods [13, 44, 49, 53, 55] represent the scenes with\\nsurfaces that support efficient rendering but are hard to op-\\ntimize. Volume-based methods use voxel grids [29, 42, 45]\\nor multi-plane images (MPIs) [17, 36, 48, 63], which pro-\\nvide delicate rendering effects but are memory-inefficient or\\nlimited to small view changes. Recently, the trend of NVS\\nwas spearheaded by Neural Radiance Fields (NeRFs) [37],\\nwhich achieves photo-realistic rendering quality.\\nSince\\nthen, a series of efforts have emerged to improve the train-\\ning speed [10, 18, 38], enhance rendering quality [3, 52],\\n2\\nor extend to more challenging scenarios such as reflection\\nand refraction [5, 28, 59]. Still, most of these methods rely\\non volume rendering that requires sampling millions of rays\\nand hinders real-time rendering [3, 10, 37].\\nDynamic Novel View Synthesis This poses new chal-\\nlenges due to the temporal variations in the input images.\\nTraditional methods estimate varying geometries such as\\nsurfaces [12, 31] and depth maps [25, 64] to account for\\ndynamics. Inspired by NeRFs, recent work typically mod-\\nels dynamic scenes with neural representations. A branch\\nof methods implicitly model the dynamics by adding a\\ntemporal input or latent code [14, 21].\\nAnother line of\\nworks [15, 39, 40, 43, 51] explicitly model deformation\\nfields that map 3D points at arbitrary timestamp into a\\ncanonical space.\\nOther techniques explore decomposing\\na scene into static and dynamic components [47], using\\nkey frames to reduce redundancy [2, 33], estimating a flow\\nfield [23, 34, 50], or exploiting 4D grid-based representa-\\ntions [8, 15, 19, 20, 32, 46, 54], etc. The common issues of\\ndynamic scene modeling are the complexities introduced by\\nthe spatial-temporal entanglement, as well as the additional\\nmemory and time cost caused by the temporal dimension.\\nGaussian-Based NVS The recent seminal work [1, 26,\\n58, 62] models static scenes with Gaussians whose posi-\\ntions and appearance are learned through a differentiable\\nsplatting-based renderer. Particularly, 3D Gaussian Splat-\\nting (3DGS) [26] achieves impressive real-time rendering\\nthanks to its Gaussian split/clone operations and the fast\\nsplatting-based rendering technique. Our work draws inspi-\\nration from 3DGS but lifts 3D Gaussians into 4D space and\\nfocuses on dynamic scenes. Several concurrent works also\\nextend 3DGS to model dynamics. Deformable3DGS [35]\\ndirectly learns the temporal motion and rotation of each\\n3D Gaussian along time, which makes it suitable for dy-\\nnamic tracking applications.\\nSimilarly, [56, 60] utilize\\nMLPs to predict the temporal movement. However, it is\\nchallenging for these methods to represent dynamic con-\\ntents that suddenly appear or disappear. Similar to us, Re-\\nalTime4DGS [61] also leverages 4D Gaussian representa-\\ntion to model 3D dynamics. They choose a dual-quaternion\\nbased 4D rotation formulation that is less interpretable and\\nlacks the spatial-temporal separable property compared to\\nrotor-base representation. Moreover, we further investigate\\nbetter spatial-temporal optimization strategies and develop\\na high-performance framework that achieves much higher\\nrendering speed and better rendering quality.\\n3. Method\\nIn this section, we first review the 3D Gaussian Splatting\\n(3DGS) method [26] in Sec. 3.1. We then describe our 4G\\nGaussian Splatting algorithm in Sec. 3.2, where we present\\nrotor-based 4D Gaussian representation in Sec. 3.2.1 and\\ndiscuss the temporal slicing technique for differentiable\\nreal-time rasterization in Sec. 3.2.2. Finally, we introduce\\nour dynamic optimization strategies in Sec. 3.3.\\n3.1. Preliminary of 3D Gaussian Splatting\\n3D Gaussian Splatting (3DGS) [26] has demonstrated real-\\ntime, state-of-the-art rendering quality on static scenes. It\\nmodels a scene with a dense cluster of N anisotropic 3D\\nGaussian ellipsoids. Each Gaussian is represented by a full\\n3D covariance matrix Î£ and its center position Âµ:\\nG(x) = eâˆ’ 1\\n2 (xâˆ’Âµ)T Î£âˆ’1(xâˆ’Âµ).\\n(1)\\nTo ensure a valid positive semi-definite covariance matrix\\nduring optimization, Î£ is decomposed into the scaling ma-\\ntrix S and the rotation matrix R to characterize the geome-\\ntry of a 3D Gaussian ellipsoid:\\nÎ£ = RSST RT ,\\n(2)\\nwhere S = diag(sx, sy, sz) âˆˆ R3 and R âˆˆ SO(3) are\\nstored as a 3D vector and quaternion, respectively.\\nBe-\\nyond Âµ, S and R, each Gaussian maintains additional learn-\\nable parameters including opacity o âˆˆ (0, 1) and spher-\\nical harmonic (SH) coefficients in Rk representing view-\\ndependent colors (k is related to SH order). During op-\\ntimization, 3DGS adaptively controls the Gaussian distri-\\nbution by splitting and cloning Gaussians in regions with\\nlarge view-space positional gradients, as well as the culling\\nof Gaussians that exhibit near-transparency.\\nEfficient rendering and parameter optimization in 3DGS\\nare powered by the differentiable tile-based rasterizer.\\nFirstly, 3D Gaussians are projected to 2D space by com-\\nputing the camera space covariance matrix Î£â€²:\\nÎ£â€² = JVÎ£VT JT ,\\n(3)\\nwhere J is the Jacobian matrix of the affine approximation\\nof the projection transformation, and V is the extrinsic cam-\\nera matrix. Then, the color of each pixel on the image plane\\nis calculated by blending Gaussians sorted by their depths:\\nC =\\nN\\nX\\ni=1\\nciÎ±i\\niâˆ’1\\nY\\nj=1\\n(1 âˆ’ Î±j),\\n(4)\\nwhere ci is the color of the i-th 3D Gaussian Gi, Î±i = oiGâ€²\\ni\\nwith oi and Gâ€²\\ni as the opacity and 2D projection of Gi, re-\\nspectively. Please refer to 3DGS [26] for more details.\\n3.2. 4D Gaussian Splatting\\nWe now discuss our 4D Gaussian Splatting (4DGS) algo-\\nrithm. Specifically, we model the 4D Gaussian with rotor-\\nbased rotation representation (Sec. 3.2.1) and slice the time\\ndimension to generate dynamic 3D Gaussians at each times-\\ntamp (Sec. 3.2.2). The 3D Gaussian ellipsoids sliced at each\\ntimestamp can be efficiently rasterized onto a 2D image\\nplane for real-time and high-fidelity rendering of dynamic\\nscenes.\\n3\\nOperation Flow\\nGradient Flow\\nTemporal Slicing\\nInitialization\\nGround Truth Images\\nDifferentiable \\nRasterization\\nð’• =  ðŸ”. ðŸ’ðŸŽ ð’”\\nx\\nz\\ny\\nt\\nProjection\\nAdaptive \\nDensity Control\\n4D Gaussians Initialization\\nRotor-Based 4D Gaussians\\n2D Rendering\\nð’• =  ðŸ”. ðŸ”ðŸ• ð’”\\nð’• =  ðŸ”. ðŸ’ðŸŽ ð’”\\nð’• =  ðŸ”. ðŸ”ðŸ• ð’”\\nð’• =  ðŸ”. ðŸ’ðŸŽ ð’”\\nð’• =  ðŸ”. ðŸ”ðŸ• ð’”\\nFigure 3. Framework Overview. After initialization, we first temporally slice the 4D Gaussians whose spatio-temporal movements are\\nmodeled with rotors. The dynamics such as the flickering flames naturally evolve through time, even within a short period of less than 0.3\\nsecond. The sliced 3D Gaussians are then projected onto 2D using differentiable rasterization. The gradients from image difference loss\\nare back-propagated and guide the adaptive density control of 4D Gaussians.\\n3.2.1\\nRotor-Based 4D Gaussian Representation\\nAnalogous to 3D Gaussians, a 4D Gaussian can be ex-\\npressed with the 4D center position Âµ4D = (Âµx, Âµy, Âµz, Âµt)\\nand the 4D covariance matrix Î£4D as\\nG4D(x) = eâˆ’ 1\\n2 (xâˆ’Âµ4D)T Î£âˆ’1\\n4D(xâˆ’Âµ4D).\\n(5)\\nThe covariance Î£4D can be further factorized into the 4D\\nscaling S4D and the 4D rotation R4D as\\nÎ£4D = R4DS4DST\\n4DRT\\n4D.\\n(6)\\nWhile modeling S4D = diag(sx, sy, sz, st) is straightfor-\\nward, seeking a proper 4D rotation representation for R4D\\nis a challenging problem.\\nIn geometric algebra, rotations of high-dimensional vec-\\ntors can be described using rotors [6]. Motivated by this,\\nwe introduce 4D rotors to characterize the 4D rotations. A\\n4D rotor r is composed of 8 components based on a set of\\nbasis:\\nr = s + b01e01 + b02e02 + b03e03 + b12e12\\n+ b13e13 + b23e23 + pe0123,\\n(7)\\nwhere e0123 = e0 âˆ§ e1 âˆ§ e2 âˆ§ e3, and eij = ei âˆ§ ej is\\nthe outer product between 4D axis ei (i âˆˆ {0, 1, 2, 3}) that\\nform the orthonormal basis in 4D Euclidean space. There-\\nfore, a 4D rotation can be determined by 8 coefficients\\n(s, b01, b02, b03, b12, b13, b23, p).\\nAnalogous to quaternion, a rotor r can also be con-\\nverted into a 4D rotation matrix R4D with proper normal-\\nization function Fnorm and rotor-matrix mapping function\\nFmap. We carefully derive a numerically stable normaliza-\\ntion method for rotor transformation and provide details of\\nthe two functions in Supplementary Material:\\nR4D = Fmap(Fnorm(r)).\\n(8)\\nOur rotor-based 4D Gaussian offers a well-defined, inter-\\npretable rotation representation: the first four components\\nencode the 3D spatial rotation, and the last four compo-\\nnents define spatio-temporal rotation, i.e., spatial transla-\\ntion. In particular, setting the last four components to zero\\neffectively reduces r to a quaternion for 3D spatial rotation,\\nthereby enabling our framework to model both static and\\ndynamic scenes. Fig. 4 presents an example where our re-\\nsult on a static 3D scene matches that of original 3DGS [26].\\nAlternatively, concurrent work [61] also models dynamic\\nscenes with 4D Gaussian. However, they represent 4D ro-\\ntation with two entangled isoclinic quaternions [9]. As a re-\\nsult, their spatial and temporal rotations are tightly coupled,\\nand it is unclear how to constrain and normalize this alter-\\nnative rotation representation during optimization to model\\nstatic 3D scenes.\\n3.2.2\\nTemporally-Sliced 4D Gaussians Splatting\\nWe now describe how to slice 4D Gaussians into 3D space.\\nGiven that Î£4D and its inverse Î£âˆ’1\\n4D are both symmetric\\nmatrices, we define\\nÎ£4D =\\n\\x12 U\\nV\\nVT\\nW\\n\\x13\\nand Î£âˆ’1\\n4D =\\n\\x12 A\\nM\\nMT\\nZ\\n\\x13\\n,\\n(9)\\nwhere both U and A are 3 Ã— 3 matrices. Then, given a\\ntime t, the projected 3D Gaussian is obtained as (detailed\\nderivation in the Supplementary Material):\\nG3D(x, t) = eâˆ’ 1\\n2 Î»(tâˆ’Âµt)2eâˆ’ 1\\n2 [xâˆ’Âµ(t)]T Î£âˆ’1\\n3D[xâˆ’Âµ(t)], (10)\\nwhere\\nÎ» = Wâˆ’1,\\nÎ£3D = Aâˆ’1 = U âˆ’ VVT\\nW ,\\nÂµ(t) = (Âµx, Âµy, Âµz)T + (t âˆ’ Âµt) V\\nW.\\n(11)\\n4\\n(a) 3DGS (PSNR 27.01)\\n(b) Ours (PSNR 27.07)\\nFigure 4. Modeling 3D Static Scenes. Our rotor-based represen-\\ntation enables both 3D static and 4D dynamic scene modeling. Our\\nframework matches the results of 3DGS [26] on static 3D scenes.\\nCompared with Eq. 1 in the original 3DGS [26], the\\nsliced 3D Gaussian in Eq. 10 contains a temporal decay\\nterm eâˆ’ 1\\n2 Î»(tâˆ’Âµt)2. As t goes by, a Gaussian point first ap-\\npears once t is sufficiently close to its temporal position Âµt\\nand starts to grow. It reaches the peak opacity when t = Âµt.\\nAfter that, the 3D Gaussian gradually shrinks in density un-\\ntil vanishing when t is sufficiently far from Âµt. By con-\\ntrolling the temporal position and scaling factor, 4D Gaus-\\nsian can represent challenging dynamics, e.g., motions that\\nsuddenly appear or disappear. During rendering, we filter\\npoints that are too far from current time, where the thresh-\\nold for visibility 1\\n2Î»(t âˆ’ Âµt)2 is empirically set to 16.\\nMoreover, the sliced 3D Gaussian exhibits a new mo-\\ntion term of (t âˆ’ Âµt)V/W added to the center position\\n(Âµx, Âµy, Âµz)T . In theory, linear movement of a 3D Gaus-\\nsian emerges from our 4D slicing operation. We assume in\\na small time interval, all motions can be approximated by\\nlinear motions, and more complex non-linear cases can be\\nrepresented by combination of multiple Gaussians. Further,\\nV/W indicates the motion speed in the current timestamp.\\nTherefore, by modeling a scene with our framework, we can\\nacquire speed field for free. We visualize the optical flow in\\nFig. 8.\\nFinally, following 3DGS [26], we project the sliced 3D\\nGaussians to the 2D image plane in depth order and perform\\nthe fast differentiable rasterization to obtain the final image.\\nWe implement rotor representation and slicing in a high-\\nperformance CUDA framework and achieve much higher\\nrendering speed compared to PyTorch implementation.\\n3.3. Optimization Schema\\nWhen lifting 3D Gaussians into 4D space, the increased di-\\nmension extends the freedom of Gaussian points. There-\\nfore, we introduce two regularization terms to help stabilize\\nthe training process: entropy loss and 4D consistency loss.\\n3.3.1\\nEntropy Loss\\nSimilar to NeRFs, each Gaussian point has a learnable opac-\\nity term oi and the volume rendering formula is applied to\\ncomposite the final color. Ideally, a Gaussian point should\\nbe close to the object surface, and in most cases its opacity\\nshould be close to one. Therefore, we encourage the opac-\\nity to be close to one or close to zero by adding an entropy\\nloss, and by default Gaussians with near-zero opacity will\\nbe pruned during training:\\nLentropy = 1\\nN\\nN\\nX\\ni=1\\nâˆ’oilog(oi).\\n(12)\\nWe find that Lentropy helps condense Gaussian points and\\nfilter noisy floaters, which is very useful when training with\\nsparse views.\\n3.3.2\\n4D Consistency Loss\\nIntuitively, nearby Gaussians in 4D space should have sim-\\nilar motions. We further regularize the 4D Gaussian points\\nby adding the 4D spatial-temporal consistency loss. Recall\\nwhen slicing a 4D Gaussian at a given time t, a speed term\\ns is derived. Thus, given the i-th Gaussian point, we gather\\nK nearest 4D points from its neighbour space â„¦i and regu-\\nlarize their motions to be consistent:\\nLconsistent4D = 1\\nN\\nN\\nX\\ni=1\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\nsi âˆ’ 1\\nK\\nX\\njâˆˆâ„¦i\\nsj\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n1\\n.\\n(13)\\nFor 4D Gaussians, 4D distance is a better metric for point\\nsimilarity than 3D distance, because points that are neigh-\\nbors in 3D do not necessarily follow the same motions, e.g.,\\nwhen they belong to two objects with different motions.\\nNote that calculating 4D nearest neighbors is uniquely and\\nnaturally enabled in our 4D representation, which cannot be\\nexploited by deformation-based methods [56]. We balance\\nthe different scales of each dimension by dividing with the\\ncorresponding spacial and temporal scene scales.\\n3.3.3\\nTotal Loss\\nWe follow original 3DGS [26] and add L1 and SSIM losses\\nbetween the rendered images and ground truth images. Our\\nfinal loss is defined as:\\nL =(1 âˆ’ Î»1)L1 + Î»1Lssim + Î»2Lentropy + Î»3Lconsistent4D.\\n(14)\\n3.3.4\\nOptimization Framework\\nWe implement two versions of our method:\\none using\\nPyTorch for fast development and one highly-optimized\\nequivalent in C++ and CUDA for fast training and infer-\\nence. Compared to the PyTorch version, our CUDA ac-\\nceleration allows to render at an unprecedented 583 FPS\\n5\\nat 1352Ã—1014 resolution on a single NVIDIA RTX 4090\\nGPU. Further, our CUDA framework also accelerates train-\\ning by 16.6x. For benchmarking with baselines, we also test\\nour framework on an RTX 3090 GPU and achieve 277 FPS,\\nwhich significantly outperforms current state of the art (114\\nFPS [61]).\\n4. Experiments\\n4.1. Datasets\\nWe evaluate our method on two commonly used datasets\\nthat are representative of various challenges in dynamic\\nscene modeling. Plenoptic Video Dataset [33] contains\\nreal-world multi-view videos of 6 scenes.\\nIt includes\\nabrupt motion as well as transparent and reflective ma-\\nterials. Following prior work [33], we use resolution of\\n1352Ã—1014. D-NeRF Dataset [43] contains one-second\\nmonocular videos for 8 synthetic scenes. We use resolution\\nof 400Ã—400 following standard practice [43].\\n4.2. Implementation Details\\nInitialization. We uniformly sample 100,000 points in a 4D\\nbounding box as Gaussian means. For Plenoptic dataset, we\\ninitialize with colored COLMAP [16] reconstruction. 3D\\nscales are set as the distance to the nearest neighbor. Rotors\\nare initialized with (1, 0, 0, 0, 0, 0, 0, 0) equivalent of static\\nidentity transformation.\\nTraining. Using Adam optimizer, we train for 20,000 steps\\nwith batch size 3 on D-NeRF dataset and 30,000 steps with\\nbatch size 2 on Plenoptic dataset. Densification gradient\\nthresholds are 5e âˆ’ 5 and 2e âˆ’ 4 for D-NeRF and Plenoptic\\ndatasets, respectively. We set Î»1 = 0.2, Î»2 = 0.01, Î»3 =\\n0.05, and K = 8, except that Î»2 = 0 for Plenoptic dataset\\nsince its videos contain a large number of transparent ob-\\njects. Learning rates, densification, pruning, and opacity\\nreset settings all follow [26].\\nCUDA Acceleration.\\nWe implemented the forward and\\nbackward functions of 4D rotors to 4D rotation matrices\\nand 4D Gaussian slicing in our customized CUDA training\\nframework. The duplication and pruning of the 4D Gaus-\\nsians are also performed by CUDA, ensuring a low GPU\\nmemory usage.\\n4.3. Baselines\\nWe compare with both NeRF-based and concurrent\\nGaussian-based methods on the two datasets. Most com-\\npared methods have released official codebase, in which\\ncase we run the code as is and report the obtained num-\\nbers for novel view rendering quality, training time, and in-\\nference speed. Otherwise, we copy the results from their\\npapers. All the numbers reported in the tables are bench-\\nmarked on an NVIDIA RTX 3090 GPU.\\nTable 1.\\nEvaluation on Plenoptic Video Dataset.\\nWe com-\\npare our method with both NeRF-based an Gaussian-based ap-\\nproaches.\\nOur method significantly outperforms baselines on\\nPSNR and inference speed. *:Only tested on the scene Flame\\nSalmon. **:Trained on 8 GPUs. â€ : Results from paper.\\nID\\nMethod\\nPSNRâ†‘ SSIMâ†‘ LPIPSâ†“\\nTrainâ†“\\nFPSâ†‘\\na\\nDyNeRF [33]*â€ \\n29.58\\n-\\n0.08\\n1344 h** 0.015\\nb\\nStreamRF [32]\\n28.16\\n0.85\\n0.31\\n79 min\\n8.50\\nc\\nHyperReel [2]\\n30.36\\n0.92\\n0.17\\n9 h\\n2.00\\nd\\nNeRFPlayer [47]â€ \\n30.69\\n-\\n0.11\\n6 h\\n0.05\\ne\\nK-Planes [19]\\n30.73\\n0.93\\n0.07\\n190 min\\n0.10\\nf\\nMixVoxels [54]\\n30.85\\n0.96\\n0.21\\n91 min\\n16.70\\ng Deformable4DGS [56] 28.42\\n0.92\\n0.17\\n72 min\\n24.30\\nh\\nRealTime4DGS [61]\\n29.95\\n0.92\\n0.16\\n8 h\\n81.31\\ni\\nOurs\\n31.62\\n0.94\\n0.14\\n60 min 277.47\\n4.4. Results\\n4.4.1\\nEvaluation on Plenoptic Video Dataset\\nAs detailed in Tab. 1, prior work struggles with trade-offs\\nbetween rendering speed and quality, due to the slow vol-\\nume rendering (a-f), time cost of querying neural network\\ncomponents (c, d, g), or the spatial-temporal entanglement\\n(a, c, g). Our method, however, exhibits significant advan-\\ntages. Foremost, it markedly outperforms existing work in\\nrendering high-resolution videos (1352Ã—1014) at 277 FPS\\non an NVIDIA RTX 3090 GPU, over 10x faster than NeRF-\\nbased methods (a-f) and over 2x faster than Gaussian-based\\nmethods (g, h). Moreover, our method achieves the highest\\nPSNR of 31.62 (vs. the previous best 30.85) with a short\\naverage training time of 60 min.\\nAs presented in Fig. 5, our method promotes a clearer\\nand more detailed reconstruction of dynamic regions over\\nbaselines. For all four scenes, the proposed approach re-\\nconstructs higher-quality human heads that move frequently\\nand contain high-frequency details. As magnified in the first\\nthree scenes, baselines may produce blurry artifacts for the\\nhand regions with fast motions. In comparison, our method\\nyields the sharpest renderings for the same regions.\\n4.4.2\\nEvaluation on D-NeRF Dataset\\nMonocular video NVS is particularly challenging due to\\nsparse input views. As summarized in Tab. 2, [60] achieves\\nthe highest PSNR since it directly tracks the deformation of\\n3D Gaussian points, which perfectly aligns with D-NeRF\\ndataset. Otherwise, our method achieves the best rendering\\nquality among all the other methods, at a rendering speed of\\n1258 FPS (8x faster than the previous best). Moreover, the\\ntraining only takes around 5 min in our fast implementation.\\nFig. 6 showcases how this work surpasses baselines in re-\\nducing floaters and enhancing reconstruction. For instance,\\nthe blade of the Lego bulldozer is now more defined. In\\nJumping Jacks, our method generates fingers with crisper\\n6\\nHyperReel\\nMixVoxels\\nRealTime4DGS\\nOurs\\nGround Truth\\nFigure 5.\\nQualitative Comparison on Plenoptic Video Dataset. Compared with previous state of the arts, our method recovers finer\\ndetails of dynamic regions, e.g., the magnified human face and hand. Our method also produces sharper rendering of static regions, e.g.,\\nthe outdoor backgrounds around the human head in scene Flame Salmon and the zoomed-in hook in scene Cook Spinach.\\n7\\nLego\\nJumping Jacks\\nStand Up\\nT-Rex\\nHook\\nTiNeuVox\\nDeformable4DGS\\nRealTime4DGS\\nOurs\\nGround Truth\\nFigure 6. Qualitative Comparison of Our Methods and Baselines on D-NeRF Dataset. We compare with both NeRF-based (TiNeu-\\nVox [15]) and Gaussian-based (Deformable4DGS [56] and RealTime4DGS [61]) methods. As revealed in the highlighted regions, our\\nmethod is able to reconstruct scenes with less noises and more details, e.g., the blade of the Lego bulldozer, the cuffs and hands in Jumping\\nJacks and Hook, the teeth of T-Rex, and the helmet and facial details in Stand Up.\\nshapes and eliminates artifacts as observed in baseline re-\\nsults, e.g., floaters beside the cuff in row 3. In Stand Up,\\nthe patterns on the helmet and facial features are more pro-\\nnounced in our results. The missing teeth details in base-\\nline results are recovered in ours. Additionally, noises in\\nHookâ€™s hand are mitigated by the proposed method, result-\\ning in clearer fingers.\\n4.5. Ablation Studies\\nTab. 3 ablates the effectiveness of individual designs in our\\nmethod on the challenging D-NeRF dataset.\\nEntropy Loss. As shown in Tab. 3 (b), adding entropy\\n8\\nLego\\nHook\\nT-Rex\\nHell Warrior\\nBouncing Balls\\nOriginal Setting\\n+ Entropy Loss\\n+ KNN Loss\\n+ Batch Training\\nGround Truth\\nFigure 7. Ablation Study on D-NeRF Dataset. Each row adds a different component (left) to the setting of the row above it. Comparing\\nthe scenes between first two rows, we can see that Entropy Loss helps reduce floaters. Furthermore, both KNN Loss and Batch Training\\ncontribute to sharpening appearance details, e.g., the hands of Hook and the head of T-Rex, as well as reconstructing better geometry, e.g.,\\nthe blade of Lego, the feet of Hell Warrior, and the shapes of Bouncing Balls.\\nloss significantly reduces the number of points by the order\\nof one while maintaining the overall rendering quality as\\nmeasured in PSNR and SSIM. The effect of entropy loss is\\nclearly revealed in Fig. 7. For example, the floaters around\\n9\\nw/o 4D Consistency L. w/ 4D Consistency L.\\nGround Truth\\nFigure 8. Optical Flow Visualization. Our method naturally de-\\nrives a speed field. We compute 3D motions for all Gaussian points\\nand visualize the rendered 2D optical flow. Adding 4D Consis-\\ntency loss significantly reduces noises and improves the accuracy\\nand smoothness of optical flow.\\nthe scene Lego, Hook, and Bouncing Balls in row 1 have\\nbeen completely removed in row 2. This demonstrates that\\nthe entropy loss helps impose strong constraints on the 4D\\nGaussian point distribution during optimization. However,\\nwe also find that it results in PSNR degradation in Plenoptic\\ndataset. We believe this is because Plenoptic dataset pro-\\nvides dense views and contains a lot of transparent objects.\\nTherefore, we recommend the addition of entropy loss for\\nopaque surfaces and sparse views.\\n4D Consistency Loss. Originally, the states of neigh-\\nboring Gaussians in 4D space can change freely, which in-\\ncreases the difficulty of optimization and the redundancy\\nof the model. However, the application of 4D Consistency\\nloss enforces local consistency across both spatial and tem-\\nporal dimensions. This is confirmed in Fig. 7, Tab. 3 (c) and\\nFig. 8 where 4D Consistency loss helps recover consistent\\nmotions, add more details, and improve rendering quality.\\nBatch Training. Batch training helps reduce the gradi-\\nent noise and stabilize training. Tab. 3 (Full) shows that\\nbatch training further improves the rendering quality over\\nsetting (c).\\nFor sparse view settings such as monocular\\nvideos, batch training also helps improve the geometry con-\\nsistency by jointly optimizing over multiple views, as evi-\\ndenced in Fig. 7, e.g., feet in Hell Warrior.\\n5. Conclusions\\nIn this work, we propose 4D Gaussian Splatting, a novel\\napproach that enables high-quality 4D dynamic scene mod-\\neling. Our method outperforms prior arts by a large margin\\nand achieves an unprecedented 583 FPS rendering speed on\\nan RTX 4090 GPU. Moreover, this is a unified framework\\nfor both 3D static and 4D dynamic reconstruction. We will\\nrelease the code to the community to facilitate related re-\\nsearch.\\nWhile we have already achieved state-of-the-art recon-\\nTable 2. Evaluation on D-NeRF Dataset. We compare with re-\\ncent NeRF-based and Gaussian-based methods. Our method out-\\nperforms baselines on both PSNR and rendering speed by a large\\nmargin.\\nMethod\\nPSNRâ†‘ SSIMâ†‘ LPIPSâ†“ Trainâ†“\\nFPSâ†‘\\nD-NeRF [43]\\n29.17\\n0.95\\n0.07\\n24 h\\n0.09\\nTiNeuVox [15]\\n32.87\\n0.97\\n0.04\\n28 min\\n1.60\\nK-Planes [19]\\n31.07\\n0.97\\n0.02\\n54 min\\n1.20\\nDeformable3DGS [60] 39.31\\n0.99\\n0.01\\n26 min\\n1.78\\nDeformable4DGS [56] 32.99\\n0.97\\n0.05\\n13 min\\n83.00\\nRealTime4DGS [61]\\n32.71\\n0.97\\n0.03\\n10 min 372.09\\nOurs\\n34.26\\n0.97\\n0.03\\n5 min 1257.63\\nTable 3. Ablation of 4DGS on D-NeRF Dataset. We validate\\nthree designs on rendering quality and number of points (in mil-\\nlions): (b) Entropy (with cross-entropy loss), (c) KNN (with 4D\\nKNN Consistency loss), and (Full) Batch (with batch training).\\nID\\nAblation Items\\nD-NeRF\\nEntropy KNN Batch PSNRâ†‘ SSIMâ†‘ #Point(M)â†“\\na\\n31.53\\n0.96\\n1.7e5\\nb\\nâœ“\\n31.50\\n0.97\\n5.4e4\\nc\\nâœ“\\nâœ“\\n31.91\\n0.97\\n3.0e5\\nFull\\nâœ“\\nâœ“\\nâœ“\\n33.06\\n0.98\\n1.4e5\\nstruction quality, we observe that, due to the increased di-\\nmensions, 4D Gaussians are hard to constrain and cause\\nartifacts such as floaters and inconsistent motions. While\\nentropy loss and 4D consistency loss help mitigate these\\nissues, artifacts still exist. Future directions include exploit-\\ning 4D Gaussians for downstream tasks such as tracking and\\ndynamic scene generation.\\nReferences\\n[1] Jad Abou-Chakra, Feras Dayoub, and Niko SÂ¨underhauf. Par-\\nticlenerf: Particle based encoding for online neural radiance\\nfields in dynamic scenes. arXiv preprint arXiv:2211.04041,\\n2022. 3\\n[2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\\nZollhoefer, Johannes Kopf, Matthew Oâ€™Toole, and Changil\\nKim.\\nHyperreel:\\nHigh-fidelity 6-dof video with ray-\\nconditioned sampling.\\nIn Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 16610â€“16620, 2023. 1, 3, 6\\n[3] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\\nMip-nerf: A multiscale representation for anti-aliasing neu-\\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\\nnational Conference on Computer Vision, pages 5855â€“5864,\\n2021. 2, 3\\n10\\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\\nanti-aliased neural radiance fields.\\nIn Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 5470â€“5479, 2022. 1\\n[5] Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad,\\nHans-Peter Seidel, and Tobias Ritschel. Eikonal fields for\\nrefractive novel-view synthesis. In ACM SIGGRAPH 2022\\nConference Proceedings, pages 1â€“9, 2022. 3\\n[6] Marc Ten Bosch. N-dimensional rigid body dynamics. ACM\\nTransactions on Graphics (TOG), 39(4):55â€“1, 2020. 2, 4\\n[7] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\\nGortler, and Michael Cohen. Unstructured lumigraph ren-\\ndering. ACM Transactions on Graphics (Proc. SIGGRAPH),\\n2001. 2\\n[8] Ang Cao and Justin Johnson. Hexplane: A fast representa-\\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 130â€“141, 2023. 3\\n[9] Arthur Cayley. The collected mathematical papers of Arthur\\nCayley. University of Michigan Library, 1894. 4\\n[10] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\\nHao Su. Tensorf: Tensorial radiance fields. In European\\nConference on Computer Vision, pages 333â€“350. Springer,\\n2022. 2, 3\\n[11] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu\\nChen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming\\nYu, Zhengyu Lin, et al. Dna-rendering: A diverse neural\\nactor repository for high-fidelity human-centric rendering. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 19982â€“19993, 2023. 1\\n[12] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\\nand Steve Sullivan. High-quality streamable free-viewpoint\\nvideo. ACM Transactions on Graphics (ToG), 34(4):1â€“13,\\n2015. 3\\n[13] Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod-\\neling and rendering architecture from photographs: A hybrid\\ngeometry-and image-based approach. ACM Transactions on\\nGraphics (Proc. SIGGRAPH), 1996. 2\\n[14] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-\\nbaum, and Jiajun Wu.\\nNeural radiance flow for 4d view\\nsynthesis and video processing.\\nIn 2021 IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV), pages\\n14304â€“14314. IEEE Computer Society, 2021. 1, 3\\n[15] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\\naopeng Zhang, Wenyu Liu, Matthias NieÃŸner, and Qi Tian.\\nFast dynamic radiance fields with time-aware neural vox-\\nels. In SIGGRAPH Asia 2022 Conference Papers, pages 1â€“9,\\n2022. 1, 3, 8, 10, 18\\n[16] Alex Fisher, Ricardo Cannizzaro, Madeleine Cochrane,\\nChatura Nagahawatte, and Jennifer L Palmer.\\nColmap:\\nA memory-efficient occupancy grid mapping framework.\\nRobotics and Autonomous Systems, 142:103755, 2021. 6\\n[17] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-\\nVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and\\nRichard Tucker. Deepview: View synthesis with learned gra-\\ndient descent. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 2367â€“\\n2376, 2019. 2\\n[18] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\\nRadiance fields without neural networks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 5501â€“5510, 2022. 2\\n[19] Sara Fridovich-Keil, Giacomo Meanti, Frederik RahbÃ¦k\\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\\nExplicit radiance fields in space, time, and appearance. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition, pages 12479â€“12488, 2023. 3,\\n6, 10, 18\\n[20] Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, and\\nNaoto Yokoya.\\nV4d: Voxel for 4d novel view synthesis.\\nIEEE Transactions on Visualization and Computer Graph-\\nics, 2023. 3\\n[21] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\\nDynamic view synthesis from dynamic monocular video. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 5712â€“5721, 2021. 1, 3\\n[22] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and\\nMichael F. Cohen. Light field rendering. ACM Transactions\\non Graphics (Proc. SIGGRAPH), 1996. 2\\n[23] Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiao-\\nqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, and Jingdong\\nWang. Forward flow for novel view synthesis of dynamic\\nscenes. In Proceedings of the IEEE/CVF International Con-\\nference on Computer Vision, pages 16022â€“16033, 2023. 3\\n[24] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\\nfree-viewpoint image-based rendering. ACM Transactions\\non Graphics (ToG), 37(6):1â€“15, 2018. 1\\n[25] Takeo Kanade, Peter Rander, and PJ Narayanan. Virtualized\\nreality: Constructing virtual worlds from real scenes. IEEE\\nmultimedia, 4(1):34â€“47, 1997. 3\\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÂ¨uhler,\\nand George Drettakis.\\n3d gaussian splatting for real-time\\nradiance field rendering.\\nACM Transactions on Graphics\\n(ToG), 42(4):1â€“14, 2023. 1, 2, 3, 4, 5, 6, 18\\n[27] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\\nKoltun. Tanks and temples: Benchmarking large-scale scene\\nreconstruction. ACM Transactions on Graphics (ToG), 36\\n(4):1â€“13, 2017. 1\\n[28] Georgios Kopanas, Thomas LeimkÂ¨uhler, Gilles Rainer,\\nClÂ´ement Jambon, and George Drettakis. Neural point cata-\\ncaustics for novel-view synthesis of reflections. ACM Trans-\\nactions on Graphics (TOG), 41(6):1â€“15, 2022. 3\\n[29] Kiriakos N Kutulakos and Steven M Seitz. A theory of shape\\nby space carving. International journal of computer vision,\\n38:199â€“218, 2000. 2\\n[30] Marc Levoy and Pat Hanrahan. Light field rendering. ACM\\nTransactions on Graphics (Proc. SIGGRAPH), 1996. 2\\n[31] Hao Li, Linjie Luo, Daniel Vlasic, Pieter Peers, Jovan\\nPopoviÂ´c, Mark Pauly, and Szymon Rusinkiewicz. Tempo-\\nrally coherent completion of dynamic shapes. ACM Trans-\\nactions on Graphics (TOG), 31(1):1â€“11, 2012. 3\\n11\\n[32] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and\\nPing Tan.\\nStreaming radiance fields for 3d video synthe-\\nsis. Advances in Neural Information Processing Systems, 35:\\n13485â€“13498, 2022. 3, 6\\n[33] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\\net al. Neural 3d video synthesis from multi-view video. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition, pages 5521â€“5531, 2022. 1, 2,\\n3, 6, 18, 19\\n[34] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\\nNeural scene flow fields for space-time view synthesis of dy-\\nnamic scenes. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 6498â€“\\n6508, 2021. 3\\n[35] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\\nDeva Ramanan.\\nDynamic 3d gaussians:\\nTracking\\nby persistent dynamic view synthesis.\\narXiv preprint\\narXiv:2308.09713, 2023. 2, 3\\n[36] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\\nAbhishek Kar. Local light field fusion: Practical view syn-\\nthesis with prescriptive sampling guidelines. ACM Transac-\\ntions on Graphics (TOG), 38(4):1â€“14, 2019. 2\\n[37] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-\\nmamoorthi, and R Ng. Nerf: Representing scenes as neural\\nradiance fields for view synthesis. In European conference\\non computer vision, 2020. 1, 2, 3\\n[38] Thomas MÂ¨uller, Alex Evans, Christoph Schied, and Alexan-\\nder Keller. Instant neural graphics primitives with a mul-\\ntiresolution hash encoding. ACM Transactions on Graphics\\n(ToG), 41(4):1â€“15, 2022. 2\\n[39] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\\nIn Proceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 5865â€“5874, 2021. 1, 3\\n[40] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\\nBrualla, and Steven M Seitz.\\nHypernerf:\\na higher-\\ndimensional representation for topologically varying neural\\nradiance fields. ACM Transactions on Graphics (TOG), 40\\n(6):1â€“12, 2021. 1, 3\\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\\nperative style, high-performance deep learning library. Ad-\\nvances in neural information processing systems, 32, 2019.\\n2\\n[42] Eric Penner and Li Zhang. Soft 3d reconstruction for view\\nsynthesis. ACM Transactions on Graphics (TOG), 36(6):1â€“\\n11, 2017. 2\\n[43] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\\nFrancesc Moreno-Noguer.\\nD-nerf: Neural radiance fields\\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition, pages\\n10318â€“10327, 2021. 1, 2, 3, 6, 10, 18\\n[44] Gernot Riegler and Vladlen Koltun. Free view synthesis. In\\nEuropean Conference on Computer Vision, pages 623â€“640,\\n2020. 2\\n[45] Steven M Seitz and Charles R Dyer. Photorealistic scene\\nreconstruction by voxel coloring. International Journal of\\nComputer Vision, 35:151â€“173, 1999. 2\\n[46] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\\n4d decomposition for high-fidelity dynamic reconstruction\\nand rendering. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 16632â€“\\n16642, 2023. 3\\n[47] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\\nNerf-\\nplayer: A streamable dynamic scene representation with de-\\ncomposed neural radiance fields. IEEE Transactions on Visu-\\nalization and Computer Graphics, 29(5):2732â€“2742, 2023.\\n2, 3, 6\\n[48] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron,\\nRavi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the\\nboundaries of view extrapolation with multiplane images. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition, pages 175â€“184, 2019. 2\\n[49] Justus Thies, Michael ZollhÂ¨ofer, and Matthias NieÃŸner. De-\\nferred neural rendering: Image synthesis using neural tex-\\ntures. Acm Transactions on Graphics (TOG), 38(4):1â€“12,\\n2019. 2\\n[50] Fengrui Tian, Shaoyi Du, and Yueqi Duan.\\nMonon-\\nerf: Learning a generalizable dynamic radiance field from\\nmonocular videos. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision, pages 17903â€“17913,\\n2023. 3\\n[51] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\\nZollhÂ¨ofer, Christoph Lassner, and Christian Theobalt. Non-\\nrigid neural radiance fields: Reconstruction and novel view\\nsynthesis of a dynamic scene from monocular video.\\nIn\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 12959â€“12970, 2021. 1, 3\\n[52] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\\nJonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-\\ntured view-dependent appearance for neural radiance fields.\\nIn 2022 IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 5481â€“5490. IEEE, 2022. 2\\n[53] Michael Waechter, Nils Moehrle, and Michael Goesele. Let\\nthere be color! large-scale texturing of 3d reconstructions. In\\nEuropean Conference on Computer Vision, pages 836â€“850.\\nSpringer, 2014. 2\\n[54] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei\\nSong, and Huaping Liu. Mixed neural voxels for fast multi-\\nview video synthesis. In Proceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision, pages 19706â€“\\n19716, 2023. 3, 6\\n[55] Daniel N Wood, Daniel I Azuma, Ken Aldinger, Brian Cur-\\nless, Tom Duchamp, David H Salesin, and Werner Stuetzle.\\nSurface light fields for 3d photography. In Seminal Graphics\\nPapers: Pushing the Boundaries, Volume 2, pages 487â€“496.\\n2023. 2\\n12\\n[56] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\\n4d gaussian splatting for real-time dynamic scene rendering.\\narXiv preprint arXiv:2310.08528, 2023. 2, 3, 5, 6, 8, 10, 18\\n[57] Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu.\\nMulti-view neural human rendering.\\nIn Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 1682â€“1691, 2020. 1\\n[58] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\\nPoint-based neural radiance fields.\\nIn Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 5438â€“5448, 2022. 3\\n[59] Zhiwen Yan, Chen Li, and Gim Hee Lee. Nerf-ds: Neural ra-\\ndiance fields for dynamic specular objects. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 8285â€“8295, 2023. 3\\n[60] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\\nZhang, and Xiaogang Jin.\\nDeformable 3d gaussians for\\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\\npreprint arXiv:2309.13101, 2023. 2, 3, 6, 10, 18, 19\\n[61] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\\nZhang.\\nReal-time photorealistic dynamic scene represen-\\ntation and rendering with 4d gaussian splatting.\\nIn Inter-\\nnational Conference on Learning Representations (ICLR),\\n2024. 2, 3, 4, 6, 8, 10, 18, 19\\n[62] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz,\\nand Felix Heide. Differentiable point-based radiance fields\\nfor efficient view synthesis. In SIGGRAPH Asia 2022 Con-\\nference Papers, pages 1â€“12, 2022. 3\\n[63] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\\nand Noah Snavely. Stereo magnification. ACM Transactions\\non Graphics, 37(4):1â€“12, 2018. 2\\n[64] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\\nSimon Winder, and Richard Szeliski.\\nHigh-quality video\\nview interpolation using a layered representation.\\nACM\\ntransactions on graphics (TOG), 23(3):600â€“608, 2004. 3\\n13\\nA. Details of 4D Gaussian Splatting\\nThe 4D rotor r is constructed from a combination of 8 components based on a set of basis:\\nr = s + b01e01 + b02e02 + b03e03 + b12e12 + b13e13 + b23e23 + pe0123,\\n(15)\\nwhere eij = ei âˆ§ ej represents the outer product between 4D axis ei(i âˆˆ {0, 1, 2, 3}) that defines the orthonormal basis in\\nthe 4D Euclidean space, and e0123 = e0 âˆ§ e1 âˆ§ e2 âˆ§ e3 is the outer product of all 4D axis ei(i âˆˆ {0, 1, 2, 3}).\\nA.1. 4D Rotor Normalization\\nTo ensure that r represents a valid 4D rotation, it must be normalized with\\nrrâ€  = 1,\\n(16)\\nwhere râ€  is the conjugate of r:\\nrâ€  = s âˆ’ b01e01 âˆ’ b02e02 âˆ’ b03e03 âˆ’ b12e12 âˆ’ b13e13 âˆ’ b23e23 + pe0123.\\n(17)\\nBy integrating Eq. 16, we get\\nrrâ€  =\\n(âˆ’2b01b23 + 2b02b13 âˆ’ 2b03b12 + 2ps) e0 âˆ§ e1 âˆ§ e2 âˆ§ e3+\\nis inherently satisfied. Consequently, two solutions for Î´ are deduced:\\nÎ´ = âˆ’l2 Â±\\np\\nl4 âˆ’ 4(ps âˆ’ âˆ†)2\\n2(ps âˆ’ âˆ†)\\n.\\n(26)\\nTo determine the sign, let l2 = 1 + Ï‡, ps âˆ’ âˆ† = Î¾. As Ï‡ â†’ 0, Î¾ â†’ 0, Î´ must satisfy\\nÎ´ = Â±(1 + Ï‡ âˆ’ 2Î¾2) âˆ’ 1 âˆ’ Ï‡\\n2Î¾\\nâ†’ 0,\\n(27)\\nand the positive sign is taken, Î´ = âˆ’Î¾, which meets the requirement. Therefore,\\nÎ´ = âˆ’l2 +\\np\\nl4 âˆ’ 4(ps âˆ’ âˆ†)2\\n2(ps âˆ’ âˆ†)\\n,\\n(28)\\nand applying r := r + Î´âˆ‡Îµ satisfies the first normalization condition.\\nRegarding the second condition, it suffices to calculate l2 for r + Î´âˆ‡Îµ post-update and then divide each component by l.\\nAs each component undergoes proportional scaling, the condition ps âˆ’ âˆ† = 0 remains intact.\\nTo summarize, within the 4D rotor normalization Fnorm, we first apply:\\nr := r + Î´âˆ‡Îµ,\\n(29)\\nwhere\\nÎ´ = âˆ’l2 +\\nâˆš\\nl4 âˆ’ 4Îµ2\\n2Îµ\\n,\\n(30)\\nÎµ = ps âˆ’ b01b23 âˆ’ b02b13 + b03b12.\\n(31)\\nThen with the updated r, we calculate\\nl2 = b2\\n01 + b2\\n02 + b2\\n03 + b2\\n12 + b2\\n13 + b2\\n23 + p2 + s2,\\n(32)\\nand the final normalized coefficients are obtained as:\\nr := r/l.\\n(33)\\nThis results in a normalized 4D rotor r suitable for 4D rotation.\\nA.2. 4D Rotor to Rotation Matrix Transformation\\nAfter normalization, we map a source 4D vector u to a target vector uâ€² via\\nuâ€² = rurâ€ ,\\n(34)\\nwhere such mapping can also be written in 4D rotation matrix R4D form\\nuâ€² = R4Du = Fmap(r)u =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nR00\\n4D\\nR01\\n4D\\nR02\\n4D\\nR03\\n4D\\nR10\\n4D\\nR11\\n4D\\nR12\\n4D\\nR13\\n4D\\nR20\\n4D\\nR21\\n4D\\nR22\\n4D\\nR23\\n4D\\nR30\\n4D\\nR31\\n4D\\nR32\\n4D\\nR33\\n4D\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8 u,\\n(35)\\n15\\nwhere\\nR00\\n4D = âˆ’b2\\n01 âˆ’ b2\\n02 âˆ’ b2\\n03 + b2\\n12 + b2\\n13 + b2\\n23 âˆ’ p2 + s2,\\n(36)\\nR01\\n4D = 2 (b01s âˆ’ b02b12 âˆ’ b03b13 + b23p) ,\\n(37)\\nR02\\n4D = 2 (b01b12 + b02s âˆ’ b03b23 âˆ’ b13p) ,\\n(38)\\nR03\\n4D = 2 (b01b13 + b02b23 + b03s + b12p) ,\\n(39)\\nR10\\n4D = 2 (âˆ’b01s âˆ’ b02b12 âˆ’ b03b13 âˆ’ b23p) ,\\n(40)\\nR11\\n4D = âˆ’b2\\n01 + b2\\n02 + b2\\n03 âˆ’ b2\\n12 âˆ’ b2\\n13 + b2\\n23 âˆ’ p2 + s2,\\n(41)\\nR12\\n4D = 2 (âˆ’b01b02 + b03p + b12s âˆ’ b13b23) ,\\n(42)\\nR13\\n4D = 2 (âˆ’b01b03 âˆ’ b02p + b12b23 + b13s) ,\\n(43)\\nR20\\n4D = 2 (b01b12 âˆ’ b02s âˆ’ b03b23 + b13p) ,\\n(44)\\nR21\\n4D = 2 (âˆ’b01b02 âˆ’ b03p âˆ’ b12s âˆ’ b13b23) ,\\n(45)\\nR22\\n4D = b2\\n01 âˆ’ b2\\n02 + b2\\n03 âˆ’ b2\\n12 + b2\\n13 âˆ’ b2\\n23 âˆ’ p2 + s2,\\n(46)\\nR23\\n4D = 2 (b01p âˆ’ b02b03 âˆ’ b12b13 + b23s) ,\\n(47)\\nR30\\n4D = 2 (b01b13 + b02b23 âˆ’ b03s âˆ’ b12p) ,\\n(48)\\nR31\\n4D = 2 (âˆ’b01b03 + b02p + b12b23 âˆ’ b13s) ,\\n(49)\\nR32\\n4D = 2 (âˆ’b01p âˆ’ b02b03 âˆ’ b12b13 âˆ’ b23s) ,\\n(50)\\nR33\\n4D = b2\\n01 + b2\\n02 âˆ’ b2\\n03 + b2\\n12 âˆ’ b2\\n13 âˆ’ b2\\n23 âˆ’ p2 + s2.\\n(51)\\nA.3. Temporally-Slicing 4D Guassians\\nIn this section, we provide the details about slicing the 4D Gaussian to 3D over time t. That is, we calculate the 3D center\\nposition and 3D covariance after being intercepted by the t plane.\\nCalculation of the 3D Center Position and 3D Covariance. First, we have the 4D covariance matrix represented by\\nÎ£4D and the rotation R4D\\nÎ£4D = R4DS4DST\\n4DRT\\n4D.\\n(52)\\nThen we get\\nÎ£âˆ’1\\n4D = R4DSâˆ’1\\n4D\\nAfter solving Î±, Î², and Î³, the 3D center position at time t is obtained\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nÂµx(t) = Âµx âˆ’ Î±(t âˆ’ Âµt)\\nÂµy(t) = Âµy âˆ’ Î²(t âˆ’ Âµt)\\nÂµz(t) = Âµz âˆ’ Î³(t âˆ’ Âµt)\\n,\\n(58)\\nIn addition, from Eq. 56, the inverse of 3D covariance matrix Î£âˆ’1\\n3D is:\\nÎ£âˆ’1\\n3D =\\n\\uf8eb\\n\\uf8ed\\ncxx\\ncxy\\ncxz\\ncxy\\ncyy\\ncyz\\ncxz\\ncyz\\nczz\\n\\uf8f6\\n\\uf8f8 ,\\n(59)\\nLet\\nÎ» = ctt âˆ’ cxxÎ±2 âˆ’ cyyÎ²2 âˆ’ czzÎ³2 âˆ’ 2cxyÎ±Î² âˆ’ 2cyzÎ²Î³ âˆ’ 2cxzÎ±Î³,\\n(60)\\nafter adding Âµx, Âµy, Âµz, Âµt back to xâ€², yâ€², zâ€², tâ€², we get\\nG3D(x, t) = eâˆ’ 1\\n2 Î»(tâˆ’Âµt)2eâˆ’ 1\\n2 [xâˆ’Âµ(t)]T Î£âˆ’1\\n3D[xâˆ’Âµ(t)].\\n(61)\\nAvoiding Numerical Instability. Directly calculating Î£3D from Î£âˆ’1\\n3D according to Eq. 59 can induce numerical insta-\\nbility of matrix inverse. This issue predominantly arises when the scales of the 3D Gaussian exhibit substantial magnitude\\ndiscrepancies, leading to significant errors in calculating Î£3D, and resulting in excessively large Gaussians. To circumvent\\nthis challenge, direct calculation of Î£3D must be avoided.\\nGiven that Î£4D and its inverse Î£âˆ’1\\n4D are both symmetric matrices, we set:\\nÎ£4D =\\n\\x12 U\\nV\\nVT\\nW\\n\\x13\\nand Î£âˆ’1\\n4D =\\n\\x12 A\\nM\\nMT\\nZ\\n\\x13\\n,\\n(62)\\nwhere U and A are 3Ã—3 matrices. By applying the inverse formula for symmetric block matrices, it follows that:\\nÎ£4D =\\nTable 4. Influence of Background Colors on D-NeRF Dataset. We report the per-scene and average PSNR for each method. All\\nmethods use the default background colors as set in their official released code. It is observed that most scenes yield a higher PSNR on\\nblack backgrounds, especially when the foregrounds are darker. This phenomenon has also been observed by Deformable3DGS [60]. Our\\nmethod outperforms baselines on both background colors.\\nBackground\\nMethod\\nT-Rex Jumping Jacks Hell Warrior Stand Up Bouncing Balls Mutant Hook Lego\\nAvg\\nD-NeRF [43]\\n31.45\\n32.56\\n24.70\\n33.63\\n38.87\\n21.41 28.95 21.76 29.17\\nTiNeuVox [15]\\n32.78\\n34.81\\n28.20\\n35.92\\n40.56\\n33.73 31.85 25.13 32.87\\nK-Planes [19]\\n31.44\\n32.53\\n25.38\\n34.26\\n39.71\\n33.88 28.61 22.73 31.07\\nWhite\\nDeformable4DGS [56] 33.12\\n34.65\\n25.31\\n36.80\\n39.29\\n37.63 31.79 25.31 32.99\\nDeformable3DGS [60] 40.14\\n38.32\\n32.51\\n42.65\\n43.97\\n42.20 36.40 25.55 37.72\\nRealTime4DGS [61]\\n31.22\\n31.29\\n24.44\\n37.89\\n35.75\\n37.69 30.93 24.85 31.76\\nOurs\\n31.24\\n33.37\\n36.85\\n38.89\\n36.30\\n39.26 33.33 25.24 33.06\\nDeformable3DGS [60] 38.55\\n39.21\\n42.06\\n45.74\\n41.33\\n44.16 38.04 25.38 39.31\\nBlack\\nRealTime4DGS [61]\\n29.82\\n30.44\\n34.67\\n39.11\\n32.85\\n38.74 31.77 24.29 32.71\\nOurs\\n31.77\\n33.40\\n34.52\\n40.79\\n34.74\\n40.66 34.24 24.93 34.26\\nTo summarize, the 3D Gaussian project from the 4D at time t is obtained as:\\nG3D(x, t) = eâˆ’ 1\\n2 Î»(tâˆ’Âµt)2eâˆ’ 1\\n2 [xâˆ’Âµ(t)]T Î£âˆ’1\\n3D[xâˆ’Âµ(t)],\\n(68)\\nwhere\\nÎ» = Wâˆ’1,\\nÎ£3D = Aâˆ’1 = U âˆ’ VVT\\nW ,\\nÂµ(t) = (Âµx, Âµy, Âµz)T + (t âˆ’ Âµt) V\\nW.\\n(69)\\nB. Additional Experiments\\nB.1. Datasets Details\\nPlenoptic Video Dataset [33]. The real-world dataset captured by a multi-view GoPro camera system. We evaluate the\\nbaselines on 6 scenes: Coffee Martini, Flame Salmon, Cook Spinach, Cut Roasted Beef, Flame Steak, and Sear Steak. Each\\nscene includes from 17 to 20 views for training and one central view for evaluation. The size of the images is downsampled\\nto 1352Ã—1014 for fair comparison. This dataset presents a variety of challenging elements, including the sudden appearance\\nof flames, moving shadows, as well as translucent and reflective materials.\\nD-NeRF Dataset [43]. The synthetic dataset of monocular video, presenting a significant challenge due to the single\\ncamera viewpoint available at each timestamp. This dataset contains 8 scenes: Hell Warrior, Mutant, Hook, Bouncing Balls,\\nLego, T-Rex, Stand Up, and Jumping Jacks. Each scene comprises 50 to 200 images for training, 10 or 20 images for\\nvalidation, and 20 images for testing. Each image within this dataset is downsampled to a standard resolution of 400Ã—400\\nfor training and evaluation following the previous work [43].\\nB.2. Additional Implementation Details\\nIn the spatial initialization for Plenoptic Video Dataset [33], we define an axis-aligned bounding box sized according to the\\nrange of SfM points. For D-NeRF dataset, the box dimensions are set to [âˆ’1.3, 1.3]3. Within these boxes, we randomly\\nsample 100,000 points as the positions of the Gaussians. The time means of Gaussians are uniformly sampled from the entire\\ntime duration, i.e., [0, 1] for D-NeRF dataset and [0, 10] for Plenoptic Video Dataset. The initialized time scale is set to 0.1414\\nfor D-NeRF dataset and 1.414 for Plenoptic dataset.\\nFollowing [26], we employ the Adam optimizer with specific learning rates: 1.6e âˆ’ 4 for positions, 1.6e âˆ’ 4 for times,\\n5e âˆ’ 3 for 3D scales and time scales, 1e âˆ’ 3 for rotation, 2.5e âˆ’ 3 for SH low degree and 1.25e âˆ’ 4 for SH high degrees,\\nand 0.05 for opacity. We apply an exponential decay schedule to the learning rates for positions and times, starting from the\\ninitial rate and decaying to 1.6e âˆ’ 6 for positions and times at step 30,000. The total optimization consists of 30,000 steps on\\nPlenoptic dataset and 20,000 steps on D-NeRF dataset. Opacity is reset every 3,000 steps, while densification is executed at\\nintervals of 100 steps, starting from 500 to 15,000 steps.\\n18\\nB.3. Influence of Background Colors on D-NeRF Dataset.\\nD-NeRF dataset provides synthetic images without backgrounds. Consequently, previous baseline methods incorporate either\\na black or white background during training and evaluation. Specifically, Deformable3DGS [60] and RealTime4DGS [61]\\nutilize a black background, while other methods opt for a white background.\\nOur observations, as shown in Tab. 4, indicate that our method yields higher rendering quality (PSNR 34.26) when trained\\nwith a black background as opposed to a white one (PSNR 33.06). In particular, for scenes with brighter foregrounds\\n(Jumping Jacks, Bouncing Balls, and Lego), models trained using white backgrounds perform higher. These performance\\ndiscrepancies between the two background colors have also been observed and reported in prior work [60].\\nThe results reported in Tab. 2 for our method are based on experiments using black background. For all the other baselines,\\nwe follow their original settings. Note that our results on the white background outperform all previous methods that use white\\nbackground. And similarly, our results on the black background outperform all previous methods that use black background\\nin their original experiments.\\nThe results reported in ablation Tab. 3 are conducted with white background. For the purpose of visualization, we show\\nimages trained with white background in Fig. 6 and Fig. 7 which include our method and two reproduced baselines (De-\\nformable3DGS [60] and RealTime4DGS [60]). Finally, for all experiments on Plenoptic Video Dataset [33], we simply\\nchoose black background for our method and follow the original settings of all the baselines.\\n19\\n'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cec685d7-5727-413c-a773-f61f6ec131df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "a.store_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "531a6b17-da0c-4cdd-8ea4-d6ec7d7817fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=a.get_stored_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0ab19c2-41b2-4870-93f8-990b9725c4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "030a47be-ec57-43aa-a2a0-9fbac9bb7978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "985dfb8a-4672-4fdf-8a1b-a3e1d9cbe2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "322138a1-6534-450f-a9e3-f23c9c6c2f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2402.03312v1</td>\n",
       "      <td>Test-Time Adaptation for Depth Completion</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03312v1</td>\n",
       "      <td>It is common to observe performance degradatio...</td>\n",
       "      <td>Test-Time Adaptation for Depth Completion\\nHyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2402.03311v1</td>\n",
       "      <td>HASSOD: Hierarchical Adaptive Self-Supervised ...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03311v1</td>\n",
       "      <td>The human visual perception system demonstrate...</td>\n",
       "      <td>HASSOD: Hierarchical Adaptive Self-Supervised\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2402.03310v1</td>\n",
       "      <td>V-IRL: Grounding Virtual Intelligence in Real ...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03310v1</td>\n",
       "      <td>There is a sensory gulf between the Earth that...</td>\n",
       "      <td>V-IRL: Grounding Virtual Intelligence in Real ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2402.03309v1</td>\n",
       "      <td>AONeuS: A Neural Rendering Framework for Acous...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03309v1</td>\n",
       "      <td>Underwater perception and 3D surface reconstru...</td>\n",
       "      <td>AONeuS: A Neural Rendering Framework for Acous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2402.03307v1</td>\n",
       "      <td>4D Gaussian Splatting: Towards Efficient Novel...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03307v1</td>\n",
       "      <td>We consider the problem of novel view synthesi...</td>\n",
       "      <td>4D Gaussian Splatting:\\nTowards Efficient Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://arxiv.org/abs/2402.03312v1</td>\n",
       "      <td>Test-Time Adaptation for Depth Completion</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03312v1</td>\n",
       "      <td>It is common to observe performance degradatio...</td>\n",
       "      <td>Test-Time Adaptation for Depth Completion\\nHyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://arxiv.org/abs/2402.03311v1</td>\n",
       "      <td>HASSOD: Hierarchical Adaptive Self-Supervised ...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03311v1</td>\n",
       "      <td>The human visual perception system demonstrate...</td>\n",
       "      <td>HASSOD: Hierarchical Adaptive Self-Supervised\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://arxiv.org/abs/2402.03310v1</td>\n",
       "      <td>V-IRL: Grounding Virtual Intelligence in Real ...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03310v1</td>\n",
       "      <td>There is a sensory gulf between the Earth that...</td>\n",
       "      <td>V-IRL: Grounding Virtual Intelligence in Real ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/2402.03309v1</td>\n",
       "      <td>AONeuS: A Neural Rendering Framework for Acous...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03309v1</td>\n",
       "      <td>Underwater perception and 3D surface reconstru...</td>\n",
       "      <td>AONeuS: A Neural Rendering Framework for Acous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://arxiv.org/abs/2402.03307v1</td>\n",
       "      <td>4D Gaussian Splatting: Towards Efficient Novel...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03307v1</td>\n",
       "      <td>We consider the problem of novel view synthesi...</td>\n",
       "      <td>4D Gaussian Splatting:\\nTowards Efficient Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://arxiv.org/abs/2402.03305v1</td>\n",
       "      <td>Do Diffusion Models Learn Semantically Meaning...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03305v1</td>\n",
       "      <td>Diffusion models are capable of impressive fea...</td>\n",
       "      <td>Under review as a workshop paper at ICLR 2024\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://arxiv.org/abs/2402.03303v1</td>\n",
       "      <td>Nevermind: Instruction Override and Moderation...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03303v1</td>\n",
       "      <td>Given the impressive capabilities of recent La...</td>\n",
       "      <td>Nevermind: Instruction Override and Moderation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://arxiv.org/abs/2402.03302v1</td>\n",
       "      <td>Swin-UMamba: Mamba-based UNet with ImageNet-ba...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03302v1</td>\n",
       "      <td>Accurate medical image segmentation demands th...</td>\n",
       "      <td>Swin-UMamba: Mamba-based UNet with\\nImageNet-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://arxiv.org/abs/2402.03300v1</td>\n",
       "      <td>DeepSeekMath: Pushing the Limits of Mathematic...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03300v1</td>\n",
       "      <td>Mathematical reasoning poses a significant cha...</td>\n",
       "      <td>DeepSeekMath: Pushing the Limits of Mathematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://arxiv.org/abs/2402.03299v1</td>\n",
       "      <td>GUARD: Role-playing to Generate Natural-langua...</td>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>http://arxiv.org/pdf/2402.03299v1</td>\n",
       "      <td>The discovery of \"jailbreaks\" to bypass safety...</td>\n",
       "      <td>GUARD: Role-playing to Generate Natural-langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  \\\n",
       "0   http://arxiv.org/abs/2402.03312v1   \n",
       "1   http://arxiv.org/abs/2402.03311v1   \n",
       "2   http://arxiv.org/abs/2402.03310v1   \n",
       "3   http://arxiv.org/abs/2402.03309v1   \n",
       "4   http://arxiv.org/abs/2402.03307v1   \n",
       "5   http://arxiv.org/abs/2402.03312v1   \n",
       "6   http://arxiv.org/abs/2402.03311v1   \n",
       "7   http://arxiv.org/abs/2402.03310v1   \n",
       "8   http://arxiv.org/abs/2402.03309v1   \n",
       "9   http://arxiv.org/abs/2402.03307v1   \n",
       "10  http://arxiv.org/abs/2402.03305v1   \n",
       "11  http://arxiv.org/abs/2402.03303v1   \n",
       "12  http://arxiv.org/abs/2402.03302v1   \n",
       "13  http://arxiv.org/abs/2402.03300v1   \n",
       "14  http://arxiv.org/abs/2402.03299v1   \n",
       "\n",
       "                                                title published_date  \\\n",
       "0           Test-Time Adaptation for Depth Completion     2024-02-05   \n",
       "1   HASSOD: Hierarchical Adaptive Self-Supervised ...     2024-02-05   \n",
       "2   V-IRL: Grounding Virtual Intelligence in Real ...     2024-02-05   \n",
       "3   AONeuS: A Neural Rendering Framework for Acous...     2024-02-05   \n",
       "4   4D Gaussian Splatting: Towards Efficient Novel...     2024-02-05   \n",
       "5           Test-Time Adaptation for Depth Completion     2024-02-05   \n",
       "6   HASSOD: Hierarchical Adaptive Self-Supervised ...     2024-02-05   \n",
       "7   V-IRL: Grounding Virtual Intelligence in Real ...     2024-02-05   \n",
       "8   AONeuS: A Neural Rendering Framework for Acous...     2024-02-05   \n",
       "9   4D Gaussian Splatting: Towards Efficient Novel...     2024-02-05   \n",
       "10  Do Diffusion Models Learn Semantically Meaning...     2024-02-05   \n",
       "11  Nevermind: Instruction Override and Moderation...     2024-02-05   \n",
       "12  Swin-UMamba: Mamba-based UNet with ImageNet-ba...     2024-02-05   \n",
       "13  DeepSeekMath: Pushing the Limits of Mathematic...     2024-02-05   \n",
       "14  GUARD: Role-playing to Generate Natural-langua...     2024-02-05   \n",
       "\n",
       "                             pdf_link  \\\n",
       "0   http://arxiv.org/pdf/2402.03312v1   \n",
       "1   http://arxiv.org/pdf/2402.03311v1   \n",
       "2   http://arxiv.org/pdf/2402.03310v1   \n",
       "3   http://arxiv.org/pdf/2402.03309v1   \n",
       "4   http://arxiv.org/pdf/2402.03307v1   \n",
       "5   http://arxiv.org/pdf/2402.03312v1   \n",
       "6   http://arxiv.org/pdf/2402.03311v1   \n",
       "7   http://arxiv.org/pdf/2402.03310v1   \n",
       "8   http://arxiv.org/pdf/2402.03309v1   \n",
       "9   http://arxiv.org/pdf/2402.03307v1   \n",
       "10  http://arxiv.org/pdf/2402.03305v1   \n",
       "11  http://arxiv.org/pdf/2402.03303v1   \n",
       "12  http://arxiv.org/pdf/2402.03302v1   \n",
       "13  http://arxiv.org/pdf/2402.03300v1   \n",
       "14  http://arxiv.org/pdf/2402.03299v1   \n",
       "\n",
       "                                              summary  \\\n",
       "0   It is common to observe performance degradatio...   \n",
       "1   The human visual perception system demonstrate...   \n",
       "2   There is a sensory gulf between the Earth that...   \n",
       "3   Underwater perception and 3D surface reconstru...   \n",
       "4   We consider the problem of novel view synthesi...   \n",
       "5   It is common to observe performance degradatio...   \n",
       "6   The human visual perception system demonstrate...   \n",
       "7   There is a sensory gulf between the Earth that...   \n",
       "8   Underwater perception and 3D surface reconstru...   \n",
       "9   We consider the problem of novel view synthesi...   \n",
       "10  Diffusion models are capable of impressive fea...   \n",
       "11  Given the impressive capabilities of recent La...   \n",
       "12  Accurate medical image segmentation demands th...   \n",
       "13  Mathematical reasoning poses a significant cha...   \n",
       "14  The discovery of \"jailbreaks\" to bypass safety...   \n",
       "\n",
       "                                             pdf_text  \n",
       "0   Test-Time Adaptation for Depth Completion\\nHyo...  \n",
       "1   HASSOD: Hierarchical Adaptive Self-Supervised\\...  \n",
       "2   V-IRL: Grounding Virtual Intelligence in Real ...  \n",
       "3   AONeuS: A Neural Rendering Framework for Acous...  \n",
       "4   4D Gaussian Splatting:\\nTowards Efficient Nove...  \n",
       "5   Test-Time Adaptation for Depth Completion\\nHyo...  \n",
       "6   HASSOD: Hierarchical Adaptive Self-Supervised\\...  \n",
       "7   V-IRL: Grounding Virtual Intelligence in Real ...  \n",
       "8   AONeuS: A Neural Rendering Framework for Acous...  \n",
       "9   4D Gaussian Splatting:\\nTowards Efficient Nove...  \n",
       "10  Under review as a workshop paper at ICLR 2024\\...  \n",
       "11  Nevermind: Instruction Override and Moderation...  \n",
       "12  Swin-UMamba: Mamba-based UNet with\\nImageNet-b...  \n",
       "13  DeepSeekMath: Pushing the Limits of Mathematic...  \n",
       "14  GUARD: Role-playing to Generate Natural-langua...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"../data/master_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cacb0-eea3-48b6-9853-84ee3965fd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcc478-eccb-4122-ace6-ae3405f19836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
