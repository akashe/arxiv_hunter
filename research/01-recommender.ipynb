{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6292770f-d78f-48eb-ac6a-1f8ca46ae136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6678f8-c3a8-4568-9cef-58f6fb937b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>pdf_link</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>pdf_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09420v1</th>\n",
       "      <td>LionHeart: A Layer-based Mapping Framework for...</td>\n",
       "      <td>2024-01-17 18:59:26</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09420v1</td>\n",
       "      <td>When arranged in a crossbar configuration, res...</td>\n",
       "      <td>LionHeart: A Layer-based Mapping Framework for...</td>\n",
       "      <td>1027</td>\n",
       "      <td>31764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09419v1</th>\n",
       "      <td>GARField: Group Anything with Radiance Fields</td>\n",
       "      <td>2024-01-17 18:57:53</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09419v1</td>\n",
       "      <td>Grouping is inherently ambiguous due to the mu...</td>\n",
       "      <td>GARField: Group Anything with Radiance Fields\\...</td>\n",
       "      <td>1366</td>\n",
       "      <td>55047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09417v1</th>\n",
       "      <td>Vision Mamba: Efficient Visual Representation ...</td>\n",
       "      <td>2024-01-17 18:56:18</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09417v1</td>\n",
       "      <td>Recently the state space models (SSMs) with ef...</td>\n",
       "      <td>Vision Mamba: Efficient Visual Representation ...</td>\n",
       "      <td>1491</td>\n",
       "      <td>43678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09416v1</th>\n",
       "      <td>TextureDreamer: Image-guided Texture Synthesis...</td>\n",
       "      <td>2024-01-17 18:55:49</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09416v1</td>\n",
       "      <td>We present TextureDreamer, a novel image-guide...</td>\n",
       "      <td>TextureDreamer: Image-guided Texture Synthesis...</td>\n",
       "      <td>1394</td>\n",
       "      <td>50400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09415v1</th>\n",
       "      <td>Randomized Kaczmarz with geometrically smoothe...</td>\n",
       "      <td>2024-01-17 18:55:24</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09415v1</td>\n",
       "      <td>This paper studies the effect of adding geomet...</td>\n",
       "      <td>RANDOMIZED KACZMARZ WITH GEOMETRICALLY\\nSMOOTH...</td>\n",
       "      <td>434</td>\n",
       "      <td>43597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09414v1</th>\n",
       "      <td>Vlogger: Make Your Dream A Vlog</td>\n",
       "      <td>2024-01-17 18:55:12</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09414v1</td>\n",
       "      <td>In this work, we present Vlogger, a generic AI...</td>\n",
       "      <td>Vlogger: Make Your Dream A Vlog\\nShaobin Zhuan...</td>\n",
       "      <td>1609</td>\n",
       "      <td>76883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09413v1</th>\n",
       "      <td>POP-3D: Open-Vocabulary 3D Occupancy Predictio...</td>\n",
       "      <td>2024-01-17 18:51:53</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09413v1</td>\n",
       "      <td>We describe an approach to predict open-vocabu...</td>\n",
       "      <td>POP-3D: Open-Vocabulary 3D Occupancy Predictio...</td>\n",
       "      <td>1406</td>\n",
       "      <td>63715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09412v1</th>\n",
       "      <td>Weakly-Private Information Retrieval From MDS-...</td>\n",
       "      <td>2024-01-17 18:51:04</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09412v1</td>\n",
       "      <td>We consider the problem of weakly-private info...</td>\n",
       "      <td>arXiv:2401.09412v1  [cs.IT]  17 Jan 2024\\nWeak...</td>\n",
       "      <td>521</td>\n",
       "      <td>12593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09410v1</th>\n",
       "      <td>Through the Looking-Glass: Transparency Implic...</td>\n",
       "      <td>2024-01-17 18:47:30</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09410v1</td>\n",
       "      <td>Knowledge can't be disentangled from people. A...</td>\n",
       "      <td>Through the Looking-Glass: Transparency Implic...</td>\n",
       "      <td>1442</td>\n",
       "      <td>90887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09407v1</th>\n",
       "      <td>Deciphering Textual Authenticity: A Generalize...</td>\n",
       "      <td>2024-01-17 18:45:13</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09407v1</td>\n",
       "      <td>With the recent proliferation of Large Languag...</td>\n",
       "      <td>Deciphering Textual Authenticity: A Generalize...</td>\n",
       "      <td>1801</td>\n",
       "      <td>78309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09395v1</th>\n",
       "      <td>Stuck in the Quicksand of Numeracy, Far from A...</td>\n",
       "      <td>2024-01-17 18:13:07</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09395v1</td>\n",
       "      <td>Recent advancements in Large Language Models (...</td>\n",
       "      <td>STUCK IN THE QUICKSAND OF NUMERACY, FAR FROM\\n...</td>\n",
       "      <td>1487</td>\n",
       "      <td>80026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09393v1</th>\n",
       "      <td>Élivágar: Efficient Quantum Circuit Search for...</td>\n",
       "      <td>2024-01-17 18:09:26</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09393v1</td>\n",
       "      <td>Designing performant and noise-robust circuits...</td>\n",
       "      <td>Élivágar: Efficient Quantum Circuit Search for...</td>\n",
       "      <td>1629</td>\n",
       "      <td>77077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09388v1</th>\n",
       "      <td>CognitiveDog: Large Multimodal Model Based Sys...</td>\n",
       "      <td>2024-01-17 18:01:24</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09388v1</td>\n",
       "      <td>This paper introduces CognitiveDog, a pioneeri...</td>\n",
       "      <td>CognitiveDog: Large Multimodal Model Based Sys...</td>\n",
       "      <td>1322</td>\n",
       "      <td>24487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09387v1</th>\n",
       "      <td>A Multi-Agent Security Testbed for the Analysi...</td>\n",
       "      <td>2024-01-17 17:59:46</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09387v1</td>\n",
       "      <td>The performance and safety of autonomous vehic...</td>\n",
       "      <td>A Multi-Agent Security Testbed for the Analysi...</td>\n",
       "      <td>1103</td>\n",
       "      <td>56374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09386v1</th>\n",
       "      <td>Tri$^{2}$-plane: Volumetric Avatar Reconstruct...</td>\n",
       "      <td>2024-01-17 17:59:03</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09386v1</td>\n",
       "      <td>Recent years have witnessed considerable achie...</td>\n",
       "      <td>Tri2-plane: Volumetric Avatar Reconstruction w...</td>\n",
       "      <td>1303</td>\n",
       "      <td>49220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09384v1</th>\n",
       "      <td>Diverse Part Synthesis for 3D Shape Creation</td>\n",
       "      <td>2024-01-17 17:55:06</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09384v1</td>\n",
       "      <td>Methods that use neural networks for synthesiz...</td>\n",
       "      <td>Diverse Part Synthesis for 3D Shape Creation\\n...</td>\n",
       "      <td>1753</td>\n",
       "      <td>60809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09383v1</th>\n",
       "      <td>Synthesizing Hardware-Software Leakage Contrac...</td>\n",
       "      <td>2024-01-17 17:54:53</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09383v1</td>\n",
       "      <td>Microarchitectural attacks compromise security...</td>\n",
       "      <td>Synthesizing Hardware-Software Leakage Contrac...</td>\n",
       "      <td>1223</td>\n",
       "      <td>37864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09382v1</th>\n",
       "      <td>POE: Acoustic Soft Robotic Proprioception for ...</td>\n",
       "      <td>2024-01-17 17:54:38</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09382v1</td>\n",
       "      <td>Soft robotic shape estimation and propriocepti...</td>\n",
       "      <td>POE: Acoustic Soft Robotic Proprioception for ...</td>\n",
       "      <td>1476</td>\n",
       "      <td>38118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09376v1</th>\n",
       "      <td>Unlocking Unlabeled Data: Ensemble Learning wi...</td>\n",
       "      <td>2024-01-17 17:46:10</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09376v1</td>\n",
       "      <td>In the realm of machine learning and statistic...</td>\n",
       "      <td>Unlocking Unlabeled Data: Ensemble Learning wi...</td>\n",
       "      <td>1351</td>\n",
       "      <td>50267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://arxiv.org/abs/2401.09375v1</th>\n",
       "      <td>Self-navigation in crowds: An invariant set-ba...</td>\n",
       "      <td>2024-01-17 17:44:21</td>\n",
       "      <td>http://arxiv.org/pdf/2401.09375v1</td>\n",
       "      <td>Self-navigation in non-coordinating crowded en...</td>\n",
       "      <td>Received XX XX XX; revised ; accepted XX XX XX...</td>\n",
       "      <td>1349</td>\n",
       "      <td>44955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title  \\\n",
       "http://arxiv.org/abs/2401.09420v1  LionHeart: A Layer-based Mapping Framework for...   \n",
       "http://arxiv.org/abs/2401.09419v1      GARField: Group Anything with Radiance Fields   \n",
       "http://arxiv.org/abs/2401.09417v1  Vision Mamba: Efficient Visual Representation ...   \n",
       "http://arxiv.org/abs/2401.09416v1  TextureDreamer: Image-guided Texture Synthesis...   \n",
       "http://arxiv.org/abs/2401.09415v1  Randomized Kaczmarz with geometrically smoothe...   \n",
       "http://arxiv.org/abs/2401.09414v1                    Vlogger: Make Your Dream A Vlog   \n",
       "http://arxiv.org/abs/2401.09413v1  POP-3D: Open-Vocabulary 3D Occupancy Predictio...   \n",
       "http://arxiv.org/abs/2401.09412v1  Weakly-Private Information Retrieval From MDS-...   \n",
       "http://arxiv.org/abs/2401.09410v1  Through the Looking-Glass: Transparency Implic...   \n",
       "http://arxiv.org/abs/2401.09407v1  Deciphering Textual Authenticity: A Generalize...   \n",
       "http://arxiv.org/abs/2401.09395v1  Stuck in the Quicksand of Numeracy, Far from A...   \n",
       "http://arxiv.org/abs/2401.09393v1  Élivágar: Efficient Quantum Circuit Search for...   \n",
       "http://arxiv.org/abs/2401.09388v1  CognitiveDog: Large Multimodal Model Based Sys...   \n",
       "http://arxiv.org/abs/2401.09387v1  A Multi-Agent Security Testbed for the Analysi...   \n",
       "http://arxiv.org/abs/2401.09386v1  Tri$^{2}$-plane: Volumetric Avatar Reconstruct...   \n",
       "http://arxiv.org/abs/2401.09384v1       Diverse Part Synthesis for 3D Shape Creation   \n",
       "http://arxiv.org/abs/2401.09383v1  Synthesizing Hardware-Software Leakage Contrac...   \n",
       "http://arxiv.org/abs/2401.09382v1  POE: Acoustic Soft Robotic Proprioception for ...   \n",
       "http://arxiv.org/abs/2401.09376v1  Unlocking Unlabeled Data: Ensemble Learning wi...   \n",
       "http://arxiv.org/abs/2401.09375v1  Self-navigation in crowds: An invariant set-ba...   \n",
       "\n",
       "                                       published_date  \\\n",
       "http://arxiv.org/abs/2401.09420v1 2024-01-17 18:59:26   \n",
       "http://arxiv.org/abs/2401.09419v1 2024-01-17 18:57:53   \n",
       "http://arxiv.org/abs/2401.09417v1 2024-01-17 18:56:18   \n",
       "http://arxiv.org/abs/2401.09416v1 2024-01-17 18:55:49   \n",
       "http://arxiv.org/abs/2401.09415v1 2024-01-17 18:55:24   \n",
       "http://arxiv.org/abs/2401.09414v1 2024-01-17 18:55:12   \n",
       "http://arxiv.org/abs/2401.09413v1 2024-01-17 18:51:53   \n",
       "http://arxiv.org/abs/2401.09412v1 2024-01-17 18:51:04   \n",
       "http://arxiv.org/abs/2401.09410v1 2024-01-17 18:47:30   \n",
       "http://arxiv.org/abs/2401.09407v1 2024-01-17 18:45:13   \n",
       "http://arxiv.org/abs/2401.09395v1 2024-01-17 18:13:07   \n",
       "http://arxiv.org/abs/2401.09393v1 2024-01-17 18:09:26   \n",
       "http://arxiv.org/abs/2401.09388v1 2024-01-17 18:01:24   \n",
       "http://arxiv.org/abs/2401.09387v1 2024-01-17 17:59:46   \n",
       "http://arxiv.org/abs/2401.09386v1 2024-01-17 17:59:03   \n",
       "http://arxiv.org/abs/2401.09384v1 2024-01-17 17:55:06   \n",
       "http://arxiv.org/abs/2401.09383v1 2024-01-17 17:54:53   \n",
       "http://arxiv.org/abs/2401.09382v1 2024-01-17 17:54:38   \n",
       "http://arxiv.org/abs/2401.09376v1 2024-01-17 17:46:10   \n",
       "http://arxiv.org/abs/2401.09375v1 2024-01-17 17:44:21   \n",
       "\n",
       "                                                            pdf_link  \\\n",
       "http://arxiv.org/abs/2401.09420v1  http://arxiv.org/pdf/2401.09420v1   \n",
       "http://arxiv.org/abs/2401.09419v1  http://arxiv.org/pdf/2401.09419v1   \n",
       "http://arxiv.org/abs/2401.09417v1  http://arxiv.org/pdf/2401.09417v1   \n",
       "http://arxiv.org/abs/2401.09416v1  http://arxiv.org/pdf/2401.09416v1   \n",
       "http://arxiv.org/abs/2401.09415v1  http://arxiv.org/pdf/2401.09415v1   \n",
       "http://arxiv.org/abs/2401.09414v1  http://arxiv.org/pdf/2401.09414v1   \n",
       "http://arxiv.org/abs/2401.09413v1  http://arxiv.org/pdf/2401.09413v1   \n",
       "http://arxiv.org/abs/2401.09412v1  http://arxiv.org/pdf/2401.09412v1   \n",
       "http://arxiv.org/abs/2401.09410v1  http://arxiv.org/pdf/2401.09410v1   \n",
       "http://arxiv.org/abs/2401.09407v1  http://arxiv.org/pdf/2401.09407v1   \n",
       "http://arxiv.org/abs/2401.09395v1  http://arxiv.org/pdf/2401.09395v1   \n",
       "http://arxiv.org/abs/2401.09393v1  http://arxiv.org/pdf/2401.09393v1   \n",
       "http://arxiv.org/abs/2401.09388v1  http://arxiv.org/pdf/2401.09388v1   \n",
       "http://arxiv.org/abs/2401.09387v1  http://arxiv.org/pdf/2401.09387v1   \n",
       "http://arxiv.org/abs/2401.09386v1  http://arxiv.org/pdf/2401.09386v1   \n",
       "http://arxiv.org/abs/2401.09384v1  http://arxiv.org/pdf/2401.09384v1   \n",
       "http://arxiv.org/abs/2401.09383v1  http://arxiv.org/pdf/2401.09383v1   \n",
       "http://arxiv.org/abs/2401.09382v1  http://arxiv.org/pdf/2401.09382v1   \n",
       "http://arxiv.org/abs/2401.09376v1  http://arxiv.org/pdf/2401.09376v1   \n",
       "http://arxiv.org/abs/2401.09375v1  http://arxiv.org/pdf/2401.09375v1   \n",
       "\n",
       "                                                                             summary  \\\n",
       "http://arxiv.org/abs/2401.09420v1  When arranged in a crossbar configuration, res...   \n",
       "http://arxiv.org/abs/2401.09419v1  Grouping is inherently ambiguous due to the mu...   \n",
       "http://arxiv.org/abs/2401.09417v1  Recently the state space models (SSMs) with ef...   \n",
       "http://arxiv.org/abs/2401.09416v1  We present TextureDreamer, a novel image-guide...   \n",
       "http://arxiv.org/abs/2401.09415v1  This paper studies the effect of adding geomet...   \n",
       "http://arxiv.org/abs/2401.09414v1  In this work, we present Vlogger, a generic AI...   \n",
       "http://arxiv.org/abs/2401.09413v1  We describe an approach to predict open-vocabu...   \n",
       "http://arxiv.org/abs/2401.09412v1  We consider the problem of weakly-private info...   \n",
       "http://arxiv.org/abs/2401.09410v1  Knowledge can't be disentangled from people. A...   \n",
       "http://arxiv.org/abs/2401.09407v1  With the recent proliferation of Large Languag...   \n",
       "http://arxiv.org/abs/2401.09395v1  Recent advancements in Large Language Models (...   \n",
       "http://arxiv.org/abs/2401.09393v1  Designing performant and noise-robust circuits...   \n",
       "http://arxiv.org/abs/2401.09388v1  This paper introduces CognitiveDog, a pioneeri...   \n",
       "http://arxiv.org/abs/2401.09387v1  The performance and safety of autonomous vehic...   \n",
       "http://arxiv.org/abs/2401.09386v1  Recent years have witnessed considerable achie...   \n",
       "http://arxiv.org/abs/2401.09384v1  Methods that use neural networks for synthesiz...   \n",
       "http://arxiv.org/abs/2401.09383v1  Microarchitectural attacks compromise security...   \n",
       "http://arxiv.org/abs/2401.09382v1  Soft robotic shape estimation and propriocepti...   \n",
       "http://arxiv.org/abs/2401.09376v1  In the realm of machine learning and statistic...   \n",
       "http://arxiv.org/abs/2401.09375v1  Self-navigation in non-coordinating crowded en...   \n",
       "\n",
       "                                                                            pdf_text  \\\n",
       "http://arxiv.org/abs/2401.09420v1  LionHeart: A Layer-based Mapping Framework for...   \n",
       "http://arxiv.org/abs/2401.09419v1  GARField: Group Anything with Radiance Fields\\...   \n",
       "http://arxiv.org/abs/2401.09417v1  Vision Mamba: Efficient Visual Representation ...   \n",
       "http://arxiv.org/abs/2401.09416v1  TextureDreamer: Image-guided Texture Synthesis...   \n",
       "http://arxiv.org/abs/2401.09415v1  RANDOMIZED KACZMARZ WITH GEOMETRICALLY\\nSMOOTH...   \n",
       "http://arxiv.org/abs/2401.09414v1  Vlogger: Make Your Dream A Vlog\\nShaobin Zhuan...   \n",
       "http://arxiv.org/abs/2401.09413v1  POP-3D: Open-Vocabulary 3D Occupancy Predictio...   \n",
       "http://arxiv.org/abs/2401.09412v1  arXiv:2401.09412v1  [cs.IT]  17 Jan 2024\\nWeak...   \n",
       "http://arxiv.org/abs/2401.09410v1  Through the Looking-Glass: Transparency Implic...   \n",
       "http://arxiv.org/abs/2401.09407v1  Deciphering Textual Authenticity: A Generalize...   \n",
       "http://arxiv.org/abs/2401.09395v1  STUCK IN THE QUICKSAND OF NUMERACY, FAR FROM\\n...   \n",
       "http://arxiv.org/abs/2401.09393v1  Élivágar: Efficient Quantum Circuit Search for...   \n",
       "http://arxiv.org/abs/2401.09388v1  CognitiveDog: Large Multimodal Model Based Sys...   \n",
       "http://arxiv.org/abs/2401.09387v1  A Multi-Agent Security Testbed for the Analysi...   \n",
       "http://arxiv.org/abs/2401.09386v1  Tri2-plane: Volumetric Avatar Reconstruction w...   \n",
       "http://arxiv.org/abs/2401.09384v1  Diverse Part Synthesis for 3D Shape Creation\\n...   \n",
       "http://arxiv.org/abs/2401.09383v1  Synthesizing Hardware-Software Leakage Contrac...   \n",
       "http://arxiv.org/abs/2401.09382v1  POE: Acoustic Soft Robotic Proprioception for ...   \n",
       "http://arxiv.org/abs/2401.09376v1  Unlocking Unlabeled Data: Ensemble Learning wi...   \n",
       "http://arxiv.org/abs/2401.09375v1  Received XX XX XX; revised ; accepted XX XX XX...   \n",
       "\n",
       "                                   summary_length  pdf_text_length  \n",
       "http://arxiv.org/abs/2401.09420v1            1027            31764  \n",
       "http://arxiv.org/abs/2401.09419v1            1366            55047  \n",
       "http://arxiv.org/abs/2401.09417v1            1491            43678  \n",
       "http://arxiv.org/abs/2401.09416v1            1394            50400  \n",
       "http://arxiv.org/abs/2401.09415v1             434            43597  \n",
       "http://arxiv.org/abs/2401.09414v1            1609            76883  \n",
       "http://arxiv.org/abs/2401.09413v1            1406            63715  \n",
       "http://arxiv.org/abs/2401.09412v1             521            12593  \n",
       "http://arxiv.org/abs/2401.09410v1            1442            90887  \n",
       "http://arxiv.org/abs/2401.09407v1            1801            78309  \n",
       "http://arxiv.org/abs/2401.09395v1            1487            80026  \n",
       "http://arxiv.org/abs/2401.09393v1            1629            77077  \n",
       "http://arxiv.org/abs/2401.09388v1            1322            24487  \n",
       "http://arxiv.org/abs/2401.09387v1            1103            56374  \n",
       "http://arxiv.org/abs/2401.09386v1            1303            49220  \n",
       "http://arxiv.org/abs/2401.09384v1            1753            60809  \n",
       "http://arxiv.org/abs/2401.09383v1            1223            37864  \n",
       "http://arxiv.org/abs/2401.09382v1            1476            38118  \n",
       "http://arxiv.org/abs/2401.09376v1            1351            50267  \n",
       "http://arxiv.org/abs/2401.09375v1            1349            44955  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"../data/master_data.pkl\")\n",
    "# tfidf = Recommender(data=df)\n",
    "\n",
    "# query = \"Attention mechanism, gpt\"\n",
    "# res = tfidf.recommend(query)\n",
    "\n",
    "# print(res.shape)\n",
    "# print(res)\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f78a4c-1b49-4de2-9a4b-08f6649228c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lionheart', 'a', 'map', 'framework', 'for', 'heterogen', 'system', 'with', 'analog', 'comput']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "        \"\"\"word tokenization, snowball stemming and punctuation removal\"\"\"\n",
    "        stemmer=SnowballStemmer(\"english\")\n",
    "        return [stemmer.stem(token) for token in word_tokenize(text) if token.isalpha()]\n",
    "\n",
    "print(preprocess(df.pdf_text.values[0][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f11e8c-7e33-46a0-ad41-6e0d9ec35d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'veri', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def learn_vocabulary(corpus): \n",
    "    english_stopwords=stopwords.words(\"english\")\n",
    "    vectorizer=TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        tokenizer=preprocess,\n",
    "        stop_words=english_stopwords,\n",
    "        ngram_range=(1,2)\n",
    "    )\n",
    "    vocabulary=vectorizer.fit(corpus)\n",
    "    return (vectorizer, vocabulary)\n",
    "    \n",
    "vectorizer, vocabulary=learn_vocabulary(df.pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c61d884-d5d3-4ec4-b0a3-290e7e1bfa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...],\n",
       "                tokenizer=&lt;function preprocess at 0x1304ff1a0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...],\n",
       "                tokenizer=&lt;function preprocess at 0x1304ff1a0&gt;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<function preprocess at 0x1304ff1a0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c3bee8-df3f-42a2-aa21-3e859a1a051e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...],\n",
       "                tokenizer=&lt;function preprocess at 0x1304ff1a0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...],\n",
       "                tokenizer=&lt;function preprocess at 0x1304ff1a0&gt;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2),\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<function preprocess at 0x1304ff1a0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "299abeb1-7987-4619-a939-9d478a215822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339873"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b775e303-7916-46f4-bc96-b1e51b5fe496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aa grate', 'aa let', ..., '𝜙𝑙𝑟 crm', '𝜙𝑚𝑟', '𝜙𝑚𝑟 𝜙𝑙𝑟'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2f8fbdc-92e8-4bcc-b743-23405a30fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    transformed_data=vectorizer.transform(data)\n",
    "    return transformed_data\n",
    "    \n",
    "transformed_data=transform_data(df.pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c97a28e-002f-48b4-aef3-5f2696b8dca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 339873)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1186341-d748-4e36-bbd8-25c4b0901226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10(arr, results=10):\n",
    "        kth_largest = (results + 1) * -1\n",
    "        return np.argsort(arr)[:kth_largest:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4de40313-78c5-4ef9-b88d-d74ca9b44a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  1, 10,  9,  4,  5,  6,  0, 11,  8,  7,  3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([45,3,2,99,8,23,42, 78,68,5,4,55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "589f75e0-711a-4223-aadb-863226e5d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7,  8, 11,  0,  6,  5,  4,  9])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([45,3,2,99,8,23,42, 78,68,5,4,55])[:-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "df9b792b-a815-4b14-8079-d0cdcd2c37cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 339873)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_query=vectorizer.transform([\"Attention mechanism, gpt\"]) \n",
    "transformed_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd480e69-1b45-4d90-931c-356ca42cd88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = cosine_similarity(\n",
    "        X=transformed_query,\n",
    "        Y=transformed_data\n",
    "    )\n",
    "cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e741abb7-f9f9-4a70-b3ee-6f324715c866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Attention\" in df.pdf_text.values[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dc6ce2f0-6469-4b19-8527-fd3963fd8240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 0.07523317245996379),\n",
       " (30, 0.0645704279199833),\n",
       " (68, 0.019730676950393673),\n",
       " (66, 0.01706847010186102),\n",
       " (88, 0.01676402091510068),\n",
       " (9, 0.015864706383126106),\n",
       " (5, 0.012539847098876067),\n",
       " (2, 0.011670485300686375),\n",
       " (84, 0.009874931918079605),\n",
       " (51, 0.009075828413454008),\n",
       " (73, 0.007463179286442995),\n",
       " (42, 0.007174499216149932),\n",
       " (76, 0.00707560270301927),\n",
       " (24, 0.005183716246368177),\n",
       " (85, 0.005024141411999368),\n",
       " (50, 0.004790685676727951),\n",
       " (81, 0.004607006211813749),\n",
       " (99, 0.004537051727032916),\n",
       " (8, 0.004231346499790698),\n",
       " (79, 0.003719270429699103),\n",
       " (28, 0.0036587401915262013),\n",
       " (67, 0.0030915648551165464),\n",
       " (77, 0.0029429730565885947),\n",
       " (35, 0.0027201906268400932),\n",
       " (80, 0.002680087917726067),\n",
       " (31, 0.002592577001522031),\n",
       " (87, 0.0025478606758592495),\n",
       " (94, 0.0024699564996087725),\n",
       " (34, 0.0024318648369029945),\n",
       " (95, 0.0024127870322119063),\n",
       " (17, 0.0022197657055622894),\n",
       " (41, 0.0020517473523855496),\n",
       " (63, 0.0019436895817855633),\n",
       " (47, 0.0016778977356507368),\n",
       " (58, 0.0016278054738498666),\n",
       " (36, 0.0015338901933035433),\n",
       " (46, 0.001511148740748772),\n",
       " (89, 0.0013996937103663075),\n",
       " (15, 0.001387649716904023),\n",
       " (96, 0.0013444904724107148),\n",
       " (7, 0.0013314655913708197),\n",
       " (21, 0.001140031810990777),\n",
       " (23, 0.001111521607666085),\n",
       " (71, 0.0011085359479983217),\n",
       " (75, 0.0010925032243566753),\n",
       " (69, 0.001020556776956008),\n",
       " (64, 0.0010080743337020862),\n",
       " (14, 0.0009540542214502856),\n",
       " (33, 0.0009353147998215286),\n",
       " (56, 0.0009341467261820696),\n",
       " (43, 0.000918573014046916),\n",
       " (27, 0.0008551843596713189),\n",
       " (37, 0.0008513140658058367),\n",
       " (11, 0.0008234217095276986),\n",
       " (78, 0.0007467829521981529),\n",
       " (10, 0.0007287251059902553),\n",
       " (86, 0.0006739409528223319),\n",
       " (91, 0.0006202299099449536),\n",
       " (92, 0.0005627888851498301),\n",
       " (32, 0.00046800597424032764),\n",
       " (26, 0.00044060166412553044),\n",
       " (38, 0.0004065633358940663),\n",
       " (72, 0.0003891769926620767),\n",
       " (20, 0.00033758854599924977),\n",
       " (93, 0.00032256012012593893),\n",
       " (45, 0.0001903519388377019),\n",
       " (22, 0.0001244965754682151),\n",
       " (62, 8.490516485365326e-05),\n",
       " (0, 0.0),\n",
       " (1, 0.0),\n",
       " (3, 0.0),\n",
       " (4, 0.0),\n",
       " (6, 0.0),\n",
       " (12, 0.0),\n",
       " (13, 0.0),\n",
       " (16, 0.0),\n",
       " (18, 0.0),\n",
       " (19, 0.0),\n",
       " (25, 0.0),\n",
       " (29, 0.0),\n",
       " (39, 0.0),\n",
       " (44, 0.0),\n",
       " (48, 0.0),\n",
       " (49, 0.0),\n",
       " (52, 0.0),\n",
       " (53, 0.0),\n",
       " (54, 0.0),\n",
       " (55, 0.0),\n",
       " (57, 0.0),\n",
       " (59, 0.0),\n",
       " (60, 0.0),\n",
       " (61, 0.0),\n",
       " (65, 0.0),\n",
       " (70, 0.0),\n",
       " (74, 0.0),\n",
       " (82, 0.0),\n",
       " (83, 0.0),\n",
       " (90, 0.0),\n",
       " (97, 0.0),\n",
       " (98, 0.0)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(enumerate(cosine_similarities[0])), reverse=True, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f7f6ce7-ecff-4d61-96d3-f78060e93048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(cosine_similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "75f455e1-86f9-405c-8b76-b967914f295a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models Are Neurosymbolic Reasoners\\nMeng Fang*1,2, Shilong Deng*1, Yudi Zhang*2,\\nZijing Shi3, Ling Chen3, Mykola Pechenizkiy2, Jun Wang4\\n1University of Liverpool, United Kingdom\\n2Eindhoven University of Technology, Netherlands\\n3University of Technology Sydney, Australia\\n4University College London, United Kingdom\\n{Meng.Fang, shilong.deng}@liverpool.ac.uk, {y.zhang5, m.pechenizkiy}@tue.nl,\\nzijing.shi@student.uts.edu.au, ling.chen@uts.edu.au, j.wang@cs.ucl.ac.uk\\nAbstract\\nA wide range of real-world applications is characterized by\\ntheir symbolic nature, necessitating a strong capability for\\nsymbolic reasoning. This paper investigates the potential ap-\\nplication of Large Language Models (LLMs) as symbolic\\nreasoners. We focus on text-based games, significant bench-\\nmarks for agents with natural language capabilities, particu-\\nlarly in symbolic tasks like math, map reading, sorting, and\\napplying common sense in text-based worlds. To facilitate\\nthese agents, we propose an LLM agent designed to tackle\\nsymbolic challenges and achieve in-game objectives. We be-\\ngin by initializing the LLM agent and informing it of its role.\\nThe agent then receives observations and a set of valid actions\\nfrom the text-based games, along with a specific symbolic\\nmodule. With these inputs, the LLM agent chooses an action\\nand interacts with the game environments. Our experimen-\\ntal results demonstrate that our method significantly enhances\\nthe capability of LLMs as automated agents for symbolic rea-\\nsoning, and our LLM agent is effective in text-based games\\ninvolving symbolic tasks, achieving an average performance\\nof 88% across all tasks.\\nIntroduction\\nThe ability to perform reasoning is crucial for AI due\\nto its significant impact on various real-world tasks. The\\nwidespread adoption of large language models (LLMs),\\nsuch as ChatGPT and GPT-4 (OpenAI 2023), has led to\\na series of remarkable successes in reasoning tasks, rang-\\ning from question & answering to solving math problems.\\nAmong these challenges, text-based games serve as impor-\\ntant benchmarks for agents with natural language capabil-\\nities and have garnered significant attention in the realm\\nof language-centric machine learning research (Narasimhan,\\nKulkarni, and Barzilay 2015; Cˆot´e et al. 2018; Xu et al.\\n2020; Ryu et al. 2022; Shi et al. 2022). In these games,\\nan agent uses language to interpret various scenarios and\\nmake decisions. The complexity of such games arises from\\nthe need for language comprehension, common sense, man-\\naging action spaces with combinatorial complexity, and the\\ncrucial importance of long-term memory and planning (Cˆot´e\\n*These authors contributed equally.\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\net al. 2018; Wang et al. 2022a). The challenges escalate in\\ntext-based games that involve symbolic tasks (Wang et al.\\n2022b). For instance, contemporary agents might be tasked\\nwith a scenario where they are required to solve a mathemat-\\nical problem and simultaneously gather a specified amount\\nof fruits, with the quantity needed being the solution to the\\nmath problem.\\nUsing symbolic modules or external tools for arithmetic,\\nnavigation, sorting, and knowledge-base lookup is cru-\\ncial for language agents, especially in complex text-based\\ngames (Lample and Charton 2020; Poesia, Dong, and Good-\\nman 2021; Wang et al. 2022b; Qian et al. 2023). However,\\neffectively integrating these aspects into language agents re-\\nmains a relatively unaddressed challenge. Solving such text-\\nbased games requires interactive multi-step reasoning, and\\nagents have most commonly been modeled using reinforce-\\nment learning (Xu et al. 2020; Yao et al. 2020; Xu et al.\\n2021). These methods, however, face challenges such as de-\\nlayed rewards and difficulty in exploring large action spaces.\\nRecently, there has been an exploration of imitation learn-\\ning approaches, which utilize human play data (Wang et al.\\n2022b). While Behavior Cloning (BC) shows potential in\\neffectively addressing these challenges, it often necessitates\\nsubstantial effort and resources. This is primarily due to the\\nneed for acquiring expert data.\\nRecently, large language models (LLMs) have demon-\\nstrated notable in-context generalization capabilities, sug-\\ngesting the potential to elicit reasoning abilities by prompt-\\ning these models (Brown et al. 2020; Min et al. 2022). How-\\never, the application of LLMs in performing symbolic rea-\\nsoning remains an under-explored area. Models like GPT-\\n3.5 and GPT-4 have shown the ability to encode extensive\\ninformation (OpenAI 2023). A significant example of this\\nis their acquisition of substantial knowledge during training,\\nenabling them to approach human-level performance across\\na wide range of tasks (OpenAI 2023). This indicates the fea-\\nsibility of utilizing LLMs as neurosymbolic reasoners with-\\nout relying on labeled gold training data. However, there is\\ncurrently limited research on utilizing these models for rea-\\nsoning tasks that involve logic, graphs, or symbolic formu-\\nlas. The exploration and development of methods that lever-\\nage LLMs for symbolic reasoning is highly intriguing and\\narXiv:2401.09334v1  [cs.CL]  17 Jan 2024\\nState, Reward\\nAction\\nLLM\\nAgent\\nText-based\\nGame\\nSymbolic\\nModule\\nFigure 1: The LLM agent is capable of interacting with the\\ngame environment, leveraging its reasoning abilities to de-\\ntermine the most suitable actions. These actions alter the en-\\nvironment’s state and contribute to achieving the given ob-\\njective. The environment, along with its corresponding sym-\\nbolic modules, offers a valid set of actions to the LLM agent.\\nThe agent’s responsibility is to select an action from this set.\\nThe chosen action will then dictate how the agent interacts\\nwith either the game environment or the symbolic module.\\nholds significant potential impact.\\nIn this paper, our aim is to investigate the role of Large\\nLanguage Models (LLMs) in symbolic reasoning within the\\ncontext of text-based games. When engaging in games that\\ninvolve symbolic tasks, our LLM agent generates the most\\nrational actions based on the observed game state in a zero-\\nshot manner, assisted by external symbolic modules such as\\ncalculators or navigators, as illustrated in Figure 1. The LLM\\nagent employs both the text-based game environment and\\nsymbolic modules to generate a list of valid actions. These\\nvalid actions, along with the current observation, are inte-\\ngrated into the prompt to direct the LLM agent in select-\\ning an appropriate action. Subsequently, the LLM agent ex-\\necutes this action, interacting with both the game environ-\\nment and symbolic modules to complete the task.\\nIn summary, our contributions include:\\n• We introduce the use of LLMs for symbolic reasoning\\nand provide a framework for employing the LLM agent\\nas a neurosymbolic reasoner. This achievement under-\\nscores the potential of LLMs, with the support of external\\nmodules, to function as neurosymbolic reasoners, capa-\\nble of successfully completing complex tasks.\\n• We have developed the LLM agent with tailored prompts,\\nenabling it to effectively utilize symbolic modules and\\nenhance its performance in text-based games that involve\\nsymbolic tasks.\\n• Our experiments demonstrate that our agent outperforms\\nstrong baselines, including the Deep Reinforcement Rel-\\nevance Network with symbolic modules and the Behav-\\nior Cloned Transformer trained with extensive expert\\ndata, achieving an average performance of 88% across\\nall tasks.1\\n1Code at: https://github.com/hyintell/LLMSymbolic.\\nRelated Work\\nLarge Language Models for Decision Making.\\nLLMs\\nhave demonstrated notable capabilities, enabling their ap-\\nplication in tasks that extend beyond language generation\\n(OpenAI 2023). Furthermore, they are increasingly being\\ngrounded as policy models for decision-making in interac-\\ntive contexts (Yang et al. 2023). Current studies focus on\\nenhancing the decision-making capacity of LLMs through\\ntechniques such as prompting and in-context learning. For\\ninstance, Wei et al. (2022) introduce the Chain-of-Thought\\n(CoT) approach, showing that a sequence of intermediate\\nreasoning steps can enhance decision-making capabilities.\\nYao et al. (2022) present ReAct, a method for interleaved\\nreasoning and action generation to improve performance in\\ninteractive decision-making tasks. Other studies (Singh et al.\\n2023; Huang et al. 2022a,b; Liang et al. 2023; Vemprala\\net al. 2023) have explored innovative strategies involving\\nprompt engineering and the utilization of high-level func-\\ntion libraries to enhance the capabilities of LLMs. Addi-\\ntionally, some approaches incorporate mechanisms of self-\\ncritique and self-reflection into LLMs, enabling them to\\nrefine their generation. For example, Shinn, Labash, and\\nGopinath (2023) introduce Reflexion, a technique that em-\\nploys external feedback to detect ineffective actions and en-\\ngage in self-reflection. Madaan et al. (2023) enable an LLM\\nto offer feedback on its previously generated text and refine\\nit adaptively. Recent attempts have also explored different\\naspects of LLMs for decision-making. Kwon et al. (2023)\\nutilize LLMs as proxy reward functions by prompting them\\nwith desired behaviors, while Brooks et al. (2022) consider\\nLLMs as world models, where the agent learns policy by\\ninteracting with the LLM-based world model. In our work,\\nwe focus on developing suitable prompting strategies to en-\\nhance the decision-making performance of LLMs in solving\\nsymbolic tasks.\\nText-based Game.\\nText-based games can be formally\\ncharacterized as partially observable Markov decision pro-\\ncesses (POMDPs) (Cˆot´e et al. 2018). In recent years, there\\nhas been a notable increase in the design of reinforce-\\nment learning (RL) agents to solve these games (Liu et al.\\n2021; Hendrycks et al. 2021; Osborne, N˜omm, and Freitas\\n2022). Current research primarily addresses challenges such\\nas long-term dependencies, partial state observations, sparse\\nrewards, and complex action combinations in text-based\\ngames (Yin and May 2019; Ammanabrolu and Hausknecht\\n2020; Kimura et al. 2021b; Xu et al. 2022). For instance,\\nAdhikari et al. (2020) address the challenge of partial ob-\\nservability by exploring the acquisition of graph-structured\\nstate representations through data-driven methods. Yao et al.\\n(2020) and Shi et al. (2022) employ a language model\\nto generate a compact set of action candidates for RL\\nagents, tackling the issue of the combinatorial action space.\\nMore recently, with the advancement of LLMs, research has\\nshifted towards using prompts to enable LLMs to solve text-\\nbased games (Yao et al. 2022; Shinn, Labash, and Gopinath\\n2023). However, these efforts have primarily focused on the\\nLLMs’ capability for in-context learning, while the explo-\\nration of their potential in symbolic reasoning has been rel-\\natively overlooked.\\nNeurosymbolic Reasoning.\\nThe field of neurosymbolic\\nreasoning combines the capabilities of deep neural networks\\nwith symbolic reasoning, significantly reducing the search\\nspace associated with symbolic techniques. This approach\\nhas been used to tackle various complex multi-step infer-\\nence challenges, including tasks like multi-hop question\\nanswering (Weber et al. 2019), language contextualization\\n(Zellers et al. 2021), and semantic analysis (Cambria et al.\\n2022). Text-based games that involve symbolic tasks serve\\nas a valuable test-bed for addressing such challenges. Pre-\\nvious approaches have employed traditional optimization\\ntechniques or reinforcement learning agents. For example,\\nKimura et al. (2021a) decompose text-based games into col-\\nlections of logical rules, which are then integrated with deep\\nreinforcement learning. Basu et al. (2022) use Integer Lin-\\near Programming (ILP) to substantially improve agent per-\\nformance, providing an interpretable framework for under-\\nstanding agents’ selection of specific actions.\\nPreliminaries\\nText-based Games as POMDPs.\\nText-based games can\\nbe formally defined as partially observable Markov decision\\nprocesses (POMDPs), considering that the agent only ob-\\nserves partial information about the environment at each turn\\n(Sutton and Barto 2018). In games with symbolic modules,\\nat each discrete time step t, the agent is provided with an\\nobservation denoted as ot and is given a task description de-\\nnoted as d. The symbolic module then produces a collection\\nof valid actions, denoted as At,SyM, while the text game\\nenvironment concurrently establishes its own set of proper\\nactions, denoted as At,Env. Consequently, the set of accept-\\nable actions at time step t is the union of these two sets,\\ndenoted as At = At,Env ∪ At,SyM. The agent’s goal is to\\nselect an action at from the set of valid actions At, given\\nthe observation ot and the task description d. If at belongs\\nto the set At,SyM, the symbolic module generates the next\\nobservation ot+1. Conversely, if at is not part of At,SyM,\\nthe text-based game environment processes at and produces\\nboth the subsequent observation ot+1 and the reward rt.\\nSymbolic Tasks.\\nThere are four distinct tasks within text-\\nbased games, namely Arithmetic, MapReader, Sorting and\\nText World Common Sense (TWC) (Wang et al. 2022b).\\nEach task is equipped with its own symbolic modules de-\\nsigned to assist agents in successfully accomplishing the\\ntask.\\nMethodology\\nWe introduce an LLM agent, namely a language agent, for\\nemploying LLMs2 to engage in text-based games by lever-\\naging symbolic modules in a zero-shot manner. We begin\\nwith an overview of playing games using symbolic modules,\\nfollowed by a detailed description of the key design features\\nof our language agent, including its prompting mechanism.\\n2We utilize LLMs from OpenAI: https://chat.openai.com/.\\nInteraction\\nAction Query\\nRole:\\nYour first task is to solve the math\\nproblem. Then, pick up the item with the\\nsame quantity as the math problem\\nanswer, and place it in the box.\\nRole Initialization   \\nStep 2:\\nStep 3:\\nLLM Agent\\nStep 1:\\nAction:\\nread math\\nproblem\\nGame\\nSymbolic\\nModule\\nObservation: You are in the laundry\\nroom. In one part of the room you see a\\nbench that has 23 peas, 936 squashes\\non it. There is also a math problem. You\\nalso see a box, that is empty.\\nAction:\\nmul 26 36\\nGame\\nSymbolic\\nModule\\nAction: \\ntake 936\\nsquashes\\nObservation: The result of multiplying\\n26 and 36 is 936.\\nValid Action Set\\nAction Constriant\\nGame\\nSymbolic\\nModule\\nLLM Agent\\nLLM Agent\\nLLM Agent\\nValid Action Set\\nAction Constriant\\nValid Action Set\\nAction Constriant\\nObservation: Your task is to solve the\\nfollowing math problem: multiply 26 and\\n36 . Then, pick up the item with the\\nsame quantity as the answer, and place\\nit in the box.\\nFigure 2: An overview of how an LLM agent plays text-\\nbased games with external symbolic modules. The follow-\\ning procedural steps are involved in utilizing the LLM agent\\nfor engaging in a text-based game. Initially, the LLM agent\\nis provided with a role initialization prompt. The first obser-\\nvation received by the LLM agent comes from the text game\\nenvironment. As depicted in the diagram, the selection of\\nactions, determined by the LLM’s reasoning, activates the\\nsymbolic module. Subsequently, the symbolic module pro-\\nvides output, including observations related to the module.\\nThen the next action chosen by the LLM agent is influenced\\nby the outcome from the symbolic module. This process is\\nexecuted repeatedly until the end of the game.\\nPlaying Games with Symbolic Tasks\\nWe describe the process of playing games that involve sym-\\nbolic tasks, using the LLM agent in conjunction with exter-\\nnal symbolic modules.\\nSymbolic Modules.\\nSymbolic modules play a crucial role\\nin maximizing the reasoning capabilities of LLMs. For ex-\\nample, as shown in Figure 2, consider a scenario where a\\nmathematical problem is presented, and a calculator is avail-\\nable. In such cases, the LLM’s reasoning can effectively use\\nthe calculator to complete the task in a zero-shot manner.\\nFurthermore, symbolic modules are adept at their functions,\\nas employing an external tool like a calculator is considered\\nan action in itself.\\nThe scenarios include four distinct symbolic modules:\\nthe Calculation Module, Sorting Module, Knowledge Base\\nModule, and Navigation Module. Table 1 shows examples\\nof how these symbolic modules are utilized. The observation\\nproduced by a symbolic module indicates the current state of\\nthe game, while the action selected by the LLM agent serves\\nas the input. Additionally, the Navigation Module requires\\nthe previous observation as input to accurately determine\\nTask\\n(Symbolic\\nModule)\\nDescription\\nSymbolic Module\\nArithmetic\\n(Calculation\\nModule)\\nYour first task is to solve the math problem. Then, pick up\\nthe item with the same quantity as the math problem answer,\\nand place it in the box.\\nINPUT: mul 8 7\\nRESPONSE: Multiplying 8 and 7 results in 56.\\nMapReader\\n(Navigation\\nModule)\\nYour task is to take the coin located in the pantry, and put it\\ninto the box found in the chamber. A map is provided, that\\nyou may find helpful.\\nINPUT: next step to pantry\\nRESPONSE: The next location to go to is canteen. If\\nyou want to go to pantry from chamber, you need go\\nthrough canteen, pantry.\\nSorting\\n(Sorting\\nModule)\\nYour task is to sort objects by quantity. First, place the ob-\\nject with the smallest quantity in the box. Then, place the\\nobjects with the next smallest quantity in the box, and re-\\npeat until all objects have been placed in the box.\\nINPUT: sort ascending\\nRESPONSE: The observed items, sorted in order of in-\\ncreasing quantity, are: 25 g of oak, 47 g of brick, 15 kg\\nof cedar, 21 kg of marble.\\nTWC\\n(Knowledge\\nBase Module)\\nYour task is to pick up objects, then place them in their usual\\nlocations in the environment.\\nINPUT: query clean brown shirt\\nRESPONSE: Clean brown shirt is expected to be lo-\\ncated at wardrobe.\\nTable 1: Text-based games with symbolic tasks and their corresponding symbolic modules. INPUT refers to the current action\\nthat is sent to the symbolic modules. RESPONSE denotes the responses generated by the symbolic modules at the present time.\\nthe player’s current position. For instance, in a mathemat-\\nical task, the LLM agent may select a computational action\\nsuch as “multiply 8 by 7” (mul 8 7). This action triggers the\\nsymbolic module to calculate the product, and the resulting\\nobservation, “Multiplying 8 and 7 results in 56,” is then re-\\nturned.\\nThe process of engaging in text-based games with LLMs\\ninvolves multiple stages. The specifics of these steps are de-\\ntailed in Figure 2. As mentioned earlier, the comprehensive\\nenvironment, comprising both the symbolic modules and the\\ntext-based game environment, presents the LLM agent with\\na list of allowable actions. Upon receiving an observation,\\nthe LLM agent uses its symbolic reasoning to select an ac-\\ntion from this list. If the chosen action involves the symbolic\\nmodule, the module provides the next observation; other-\\nwise, the text-based game environment supplies the subse-\\nquent observation.\\nLLM as the Neurosymbolic Reasoner\\nWe investigate whether the accumulated world knowledge of\\nLLMs can aid in making accurate decisions for downstream\\nsymbolic tasks. To ground LLMs in text-based games, we\\nemploy a prompting approach, which eliminates the need for\\ncostly additional training. Therefore, we construct prompts\\nin a way that incorporates external context, enabling the\\nLLM agent to generate reasonable actions.\\nWe describe the role of the agent, incorporating the ob-\\nservation, valid actions, and the constraints of executing the\\naction in the prompt, as it is not easy for the LLM agent to\\nunderstand the underlying rules of the environment through\\ninteracting with game environments. The key components of\\nour approach include:\\n• Role initialization: We initialize the agent by providing\\nthem with task descriptions and action constraints.\\n• Action Query: This step is repeated at each timestep.\\nWe prompt the LLM agent with the current observation,\\ninventory state, valid action set, and a question.\\nRole\\nInitial-\\nization\\nYou are a robot. {TASK DESC}\\\\n You are\\nrequired to choose action from the valid ac-\\ntion set to complete the task step by step.\\\\n\\nTo take action, respond with an action in the\\nvalid action set. \\\\n\\nAction\\nQuery\\n{OBS}\\\\n {INV STATE}\\\\n Your current\\nscore is: {SCORE}\\\\n The valid action set\\ncontains:\\n{VALID ACT SET}.\\\\n Please\\nchoose one action from the valid action set\\nto finish the task step by step.\\\\n Do NOT\\nrespond with any other text, and you cannot\\ndecline to take an action.\\nTable 2: The prompting format for role initialization and ac-\\ntion query for each time step. {TASK DESC} is the task de-\\nscription. {OBS} is the current observation. {INV STATE}\\ndescribes the items in your inventory. {SCORE} is the ob-\\ntained reward. {VALID ACT SET} is a set of valid actions\\nat the current time step.\\n• Answer by the LLM agent: The LLM agent chooses an\\naction from the valid action set to complete the task.\\nRole Initialization.\\nWe initialize the role and provide in-\\nstructions for a functional agent assigned to a task. This pro-\\ncess informs the agent about its role, the task description,\\nand the actions it can take, along with their explanations and\\nconstraints. These actions are necessary for interacting with\\ntext-based games or calling the symbolic module. The agent\\nis instructed to choose from a valid set of actions, such as\\nreading the map, getting paths to specific locations, and re-\\ncalling the task. Additionally, the agent is advised to utilize\\nthe external symbolic module and to avoid unnecessary ac-\\ntions during the task.\\nAction Query.\\nAt each timestep, we inform the LLM\\nagent of the current game state, as outlined in Table 2. This\\ninformation includes the player’s observation, the state of\\nTask\\nConstrained Prompts\\nArithmetic\\nThere are some rules for choosing action:\\n\\\\n 1) If you do not see the items that meet\\nyour requirements, please choose ‘look\\naround’.\\\\n 2) If you want to put some-\\nthing in the box, please first take it and\\nthen put it in box.\\\\n 3) For example, if\\nyou want to put 20 apples in the box, you\\nshould first choose ‘take 20 apples’ and\\nthen choose ‘put 20 apples in box’.\\\\n 4)\\nThe next action of ‘take math problem’\\nis ‘read math problem’.\\\\n 5) However,\\nplease never choose ‘put math problem in\\nbox’ as action.\\\\n\\nMapReader\\n1) At the beginning choose ‘read map’ to\\nget the unknown surrounding layout.\\\\n 2)\\nAfter that, if you do not know how to get\\nto SOMEPLACE, you can choose ‘next\\nstep to SOMEPLACE’ to get the path to\\nSOMEPLACE.\\\\n 3) To choose the action,\\n‘task’, you can recall your task.\\\\n 4) Do\\nNOT go to anywhere that is unnecessary\\nfor finishing the task.\\\\n\\nSorting\\nTo sort the items one by one, please follow\\nthe instruction:\\\\n 1) choose ‘sort ascend-\\ning’ or ‘sort descending’ to know which\\none should be sort next.\\\\n 2) take the\\nitems.\\\\n 3) put the items in box.\\\\n\\nTWC\\n1) When you take the item, you will get\\npositive score.\\\\n 2) When you put the item\\nin the right place, you will get higher pos-\\nitive score. Otherwise you get 0.\\\\n 3) You\\nare supposed to get as much score as pos-\\nsible.\\\\n\\nTable 3: The prompting format for adding constraints on the\\nactions of an agent.\\nthe inventory, the reward, and the valid action set. The in-\\nventory state refers to the current possessions of the agent.\\nFor instance, in mathematical tasks, the inventory state may\\nconsist of a mathematical problem, while in the MapReader\\ntask, it could include a map. Additionally, the inventory\\nstate can encompass tangible objects, such as toothpaste or\\na quantity of 18 avocados, acquired by the agent within the\\nenvironment. The LLM agent is then tasked with selecting\\none action from the valid action set to continue with the task.\\nIt is important to note that the LLM agent is not allowed to\\ndecline or provide any text beyond the prescribed response.\\nWe also limit the number of valid actions provided by the\\nsymbolic module.\\nIn addition, it is essential to develop appropriate prompts\\nthat effectively restrict the agent’s actions according to the\\ninformation provided in Table 3. It is not feasible for the\\nagent to acquire knowledge and infer the rules within trajec-\\ntories solely through its interaction with the environment. In\\nall tasks, there is typically a specific order of events, where\\nthe object is first taken and then placed in a designated lo-\\ncation. This strategy is adopted to prevent scenarios where\\nthe object is placed before it is acquired, which would be\\nconsidered unacceptable in the given context.\\nExperiments\\nWe demonstrate the potential of LLMs in serving as neu-\\nrosymbolic reasoners for text-based games. In particular, we\\npresent experimental results on four text-based games that\\ninvolve different symbolic tasks. In these tasks, we observe\\nthat LLMs can effectively function as symbolic reasoners.\\nSetup\\nWe follow the evaluation framework and game environments\\nin Wang et al. (2022b). These games are developed using\\nthe TextWorldExpress game engine (Jansen and Cote 2023).\\nFor our LLM agent, we use GPT-3.5-turbo. The LLM agent\\ncan interact with game environments and symbolic mod-\\nules. The task descriptions and examples of how the sym-\\nbolic modules are called are provided in Table 1. The eval-\\nuation includes four text-based games involving symbolic\\ntasks. Each task is divided into “Train”, “Dev”, and “Test”\\nsets. All evaluations are conducted on the “Test” set.\\nThe evaluation metric is based on two factors: the aver-\\nage score achieved at the end of each game, and the average\\nnumber of steps taken within a single episode.\\nEnvironments\\nWe use four text-based game benchmark environments\\n(Wang et al. 2022b):\\nArithmetic.\\nThe task at hand involves a mathematical\\ncomponent, wherein an agent is required to read and solve a\\nmathematical problem. This process determines the specific\\nobject from a given set of objects that they should select\\nand place. The arithmetic game includes a calculator mod-\\nule equipped with the capability to perform basic mathemat-\\nical operations, including addition, subtraction, multiplica-\\ntion, and division.\\nMapReader.\\nA pick-and-place game with a navigation\\ntheme, similar to the Coin Collector game (Yuan et al. 2018).\\nThe agent is equipped with a map that may be exploited to\\noptimize route planning. The map provides information on\\nthe connections between rooms, such as the lounge connect-\\ning to the cookery and supermarket. The navigation sym-\\nbolic module has the capability to extract location informa-\\ntion from the observation space. This includes specific in-\\nformation relating to the present location and geographical\\nfeatures leading to the intended destination. For instance, the\\ninstructions sent to the agent might indicate that in order to\\nget from the cooking area to the recreation zone, one must\\npass through the bar, steam room, library, and finally reach\\nthe recreation zone.\\nSorting.\\nThis game involves an agent initially situated in a\\nroom containing a variable number of objects, ranging from\\nthree to five. The agent’s task is to sequentially place these\\nobjects into a designated box, adhering to a specific sorting\\ncriterion based on increasing quantity. In this game, units\\nrelated to volume, mass, or length are used, as exemplified\\nDRRN\\nBehavior Cloned Transformer\\nLLM Agent\\nBaseline\\n+symbolic module\\nBaseline\\n+symbolic module\\nBenchmark\\nScore\\nSteps\\nScore\\nSteps\\nScore\\nSteps\\nScore\\nSteps\\nScore\\nSteps\\nArithmetic\\n0.17\\n10\\n0.14\\n7\\n0.56\\n5\\n1.00\\n5\\n1.00\\n4\\nMapReader\\n0.02\\n50\\n0.02\\n50\\n0.71\\n27\\n1.00\\n10\\n0.86\\n15\\nSorting\\n0.03\\n21\\n0.03\\n18\\n0.72\\n7\\n0.98\\n8\\n0.71\\n7\\nTWC\\n0.57\\n27\\n0.37\\n34\\n0.90\\n6\\n0.97\\n3\\n0.94\\n4\\nAverage\\n0.20\\n27\\n0.14\\n27\\n0.72\\n11\\n0.99\\n7\\n0.88\\n7\\nTable 4: The average performance of the model across a set of 100 games in the unseen test set. “+symbolic module” indicates\\nthe utilization of symbolic modules within the action space of the models.\\nby items such as 25g of oak, 12ml of marble, and 6cm of\\ncedar. The sorting game includes a module capable of ex-\\ntracting information from the observation space. This mod-\\nule is specifically designed to identify items that include\\nquantities and can arrange these objects in either ascending\\nor descending order, following the user’s instructions.\\nText World Common Sense (TWC).\\nThe challenges pro-\\nvided in this game serve as a baseline for evaluating com-\\nmon sense reasoning abilities (Murugesan et al. 2021). In\\nthis game, agents are required to gather objects from their\\nsurroundings, such as a clean brown shirt, and subsequently\\nplace these objects in their appropriate and commonly rec-\\nognized locations, like a wardrobe. The incorporation of a\\nsymbolic module within this game enables agents to engage\\nin knowledge-based queries. For instance, it allows them\\nto deduce that a clean brown shirt is typically found in a\\nwardrobe.\\nBaselines\\nWe also compare our LLM agent with two baselines, namely\\nthe Deep Reinforcement Relevance Network (DRRN) (He\\net al. 2016) and the T5-based Behavior Cloned Transformer\\n(Raffel et al. 2020; Wang et al. 2022b), as follows:\\n• DRRN: The primary concept of the DRRN is based on\\nQ-learning. The candidate action with the highest antic-\\nipated Q-value is chosen as the next action, based on\\nthe current observation. The DRRN employs a Deep Q-\\nNetwork (Mnih et al. 2013) to estimate the Q-value for\\neach observation-action pair. Xu et al. (2020) note that\\nthe DRRN is a fast and robust reinforcement learning\\nbaseline, frequently used to produce near state-of-the-art\\nperformance in a variety of text-based games.\\n• Behavior Cloned Transformer: This method adopts an\\nimitation learning approach, conceptualizing reinforce-\\nment learning as a sequence-to-sequence problem, sim-\\nilar to the Decision Transformer (Chen et al. 2021). It\\npredicts the subsequent action based on a sequence of\\nprevious observations. This baseline aligns with the ap-\\nproach described in Ammanabrolu et al. (2021), where\\nthe model input at timestep t includes the task descrip-\\ntion, current state observation, previous action, and pre-\\nvious state observation. Symbolic modules are utilized in\\nthe demonstrations, specifically employing gold trajecto-\\nries.\\nFollowing Wang et al. (2022b), both baseline models in-\\nclude two variants: one with symbolic modules and one\\nwithout. When using symbolic modules, we inject actions\\nfrom these modules into the action space of each game for\\nthe baseline models.\\nResults\\nBased on the results presented in Table 4, it is evident that\\nthe use of the symbolic module in conjunction with the LLM\\nagent yields a favorable average performance compared to\\nother baseline approaches. When comparing the outcomes\\nof the Behavior Cloned Transformer with a symbolic mod-\\nule to those of the LLM agent, the performance of the LLM\\nagent is observed to be slightly lower. However, the LLM\\nagent demonstrates a similar level of competency in inter-\\nacting with the game environment. Furthermore, unlike the\\nBehavior Cloned Transformer models, the LLM agent does\\nnot require extensive training with a large volume of expert\\ndata. As a result, this approach saves significant training re-\\nsources.\\nTable 5 demonstrates that the LLM agent possesses a ro-\\nbust capacity for reasoning, enabling effective handling of\\ntasks involving symbolic tasks. It shows exceptional per-\\nformance, particularly in mathematics. In the MapReader\\nbenchmark, the agent achieves commendable scores, though\\nit requires a considerable number of steps to complete the\\ntask. This inefficiency is mainly due to the agent’s tendency\\nto forget the route obtained from the symbolic module, lead-\\ning to the risk of reaching incorrect locations and necessi-\\ntating repeated route queries. The complexity of map logic,\\nwhich involves determining one’s current location and de-\\nsired destination, adds to the probabilistic nature of this task.\\nIn contrast, the Sorting task reveals suboptimal performance,\\nas the LLM agent’s understanding of sorting logic is not\\nfully developed. This issue is largely attributed to the agent’s\\nlimited memory capacity, hindering its ability to remember\\nthe ascending order of all objects.\\nIn Table 6, it compares the performance of the model with\\nconstrained prompts to that of the model without constrained\\nprompts. The results indicate that when the LLM agent is\\nprovided with the prompts outlined in Table 3, there is an\\nimprovement in performance across all tasks. Additionally, a\\nreduction in the average number of steps required to interact\\nwith the game environment is observed. This demonstrates\\nthe effectiveness of our constrained prompts in these tasks.\\nFurthermore, experimental results using GPT-4, as shown in\\nTask\\nTrain\\nDev\\nTest\\nScore\\nSteps\\nScore\\nSteps\\nScore\\nSteps\\nArithmetic\\n1.00\\n3\\n0.95\\n4\\n1.00\\n4\\nMapReader\\n0.84\\n15\\n0.84\\n14\\n0.86\\n15\\nSorting\\n0.70\\n7\\n0.63\\n6\\n0.71\\n7\\nTWC\\n0.93\\n4\\n0.835\\n5\\n0.94\\n4\\nAverage\\n0.87\\n7\\n0.81\\n7\\n0.88\\n7\\nTable 5: The performance of the LLM agent on different\\nsets of the game, including “Train”, “Dev”, and “Test”. The\\nscores are subjected to normalization, resulting in values\\nranging from 0 to 1, with higher values indicating greater\\nperformance. On the other hand, the steps quantify the num-\\nber of actions taken by an agent inside the environment, with\\nlower values indicating more efficient behavior.\\nTask\\nw/ Constraints\\nw/o Constrains\\nScore\\nSteps\\nScore\\nSteps\\nArithmetic\\n1.00\\n4\\n0.96\\n3\\nMapReader\\n0.86\\n15\\n0.64\\n12\\nSorting\\n0.71\\n7\\n0.35\\n10\\nTWC\\n0.94\\n4\\n0.73\\n7\\nAverage\\n0.88\\n7\\n0.67\\n8\\nTable 6: The performance of the LLM agent with and with-\\nout constrained prompts on the “Test” set. The constrained\\nprompts are shown in Table 3.\\nTable 7, reveal that it significantly outperforms the GPT-3.5\\nagent in the MapReader and Sorting tasks, while showing\\nweaker performance in the TWC task.\\nDiscussion.\\nOur results demonstrate that the incorporation\\nof external symbolic modules by the LLM agent leads to en-\\nhanced average accuracy compared to other baselines. This\\ncapability is achieved by leveraging the underlying patterns\\npresent in the training data. Instead of relying on symbolic\\nthinking or explicit rules, this approach acquires knowledge\\nby recognizing patterns and associations from the extensive\\ncorpus of text to which it has been exposed during its train-\\ning phase, as exemplified by GPT-3.5 and GPT-4 (OpenAI\\n2023). Although the LLM agent has the capability to con-\\nnect with a symbolic module for specific tasks, it still ex-\\nhibits uncertainty and is prone to making mistakes.\\nConclusion\\nThis paper has demonstrated the effective application of\\nLarge Language Models (LLMs) in complex text-based\\ngames involving symbolic tasks. Utilizing a prompting ap-\\nproach, we have guided the LLM agent to efficiently en-\\ngage with symbolic modules within these games. The effi-\\ncacy of our method, leveraging LLMs, has shown superior\\nperformance compared to alternative benchmarks, highlight-\\ning the potential of LLMs to enhance training procedures in\\ntext-based games. Consequently, it can be posited that Large\\nLanguage Models can be considered as Neurosymbolic Rea-\\nsoners, possessing significant potential for performing sym-\\nTask\\nw/ GPT-3.5\\nw/ GPT-4\\nScore\\nSteps\\nScore\\nSteps\\nArithmetic\\n1.00\\n4\\n1.00\\n4\\nMapReader\\n0.86\\n15\\n0.99\\n7\\nSorting\\n0.71\\n7\\n0.93\\n8\\nTWC\\n0.94\\n4\\n0.71\\n16\\nAverage\\n0.88\\n7\\n0.91\\n8\\nTable 7: The performance of the LLM agent using GPT-3.5\\nand GPT-4 on the “Test” set.\\nbolic tasks in real-world applications.\\nLimitations\\nThe addition of more detailed prompts could offer greater\\ncontrol over the actions of the LLM agent. This would be\\nparticularly beneficial in tasks like Sorting, where providing\\nessential information beforehand is advantageous. Acknowl-\\nedging and addressing these limitations could significantly\\nenhance the system’s performance. For future progress, it is\\ncrucial to extend the model’s application to more complex\\ndomains, going beyond the scope of straightforward text-\\nbased games. Integrating more sophisticated symbolic mod-\\nules would be necessary to tackle the complexities of diverse\\nscenarios, thereby facilitating a more efficient problem-\\nsolving approach.\\nReferences\\nAdhikari, A.; Yuan, X.; Cˆot´e, M.-A.; Zelinka, M.; Rondeau,\\nM.-A.; Laroche, R.; Poupart, P.; Tang, J.; Trischler, A.; and\\nHamilton, W. 2020. Learning dynamic belief graphs to gen-\\neralize on text-based games. In Advances in Neural Infor-\\nmation Processing Systems.\\nAmmanabrolu, P.; and Hausknecht, M. 2020. Graph con-\\nstrained reinforcement learning for natural language action\\nspaces. In International Conference on Learning Represen-\\ntations.\\nAmmanabrolu,\\nP.;\\nUrbanek,\\nJ.;\\nLi,\\nM.;\\nSzlam,\\nA.;\\nRockt¨aschel, T.; and Weston, J. 2021. How to Motivate Your\\nDragon: Teaching Goal-Driven Agents to Speak and Act in\\nFantasy Worlds. In Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technologies, 807–\\n833.\\nBasu, K.; Murugesan, K.; Atzeni, M.; Kapanipathi, P.; Ta-\\nlamadupula, K.; Klinger, T.; Campbell, M.; Sachan, M.; and\\nGupta, G. 2022. A hybrid neuro-symbolic approach for text-\\nbased games using inductive logic programming. In Com-\\nbining Learning and Reasoning: Programming Languages,\\nFormalisms, and Representations.\\nBrooks, E.; Walls, L.; Lewis, R. L.; and Singh, S. 2022. In-\\ncontext policy iteration. arXiv preprint arXiv:2210.03821.\\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\\nA.; et al. 2020. Language models are few-shot learners. In\\nAdvances in Neural Information Processing Systems.\\nCambria, E.; Liu, Q.; Decherchi, S.; Xing, F.; and Kwok, K.\\n2022. SenticNet 7: A commonsense-based neurosymbolic\\nAI framework for explainable sentiment analysis. In Pro-\\nceedings of the Thirteenth Language Resources and Evalu-\\nation Conference, 3829–3839.\\nChen, L.; Lu, K.; Rajeswaran, A.; Lee, K.; Grover, A.;\\nLaskin, M.; Abbeel, P.; Srinivas, A.; and Mordatch, I. 2021.\\nDecision transformer: Reinforcement learning via sequence\\nmodeling. In Advances in Neural Information Processing\\nSystems.\\nCˆot´e, M.-A.; K´ad´ar, A.; Yuan, X.; Kybartas, B.; Barnes, T.;\\nFine, E.; Moore, J.; Hausknecht, M.; El Asri, L.; Adada, M.;\\net al. 2018.\\nTextworld: A learning environment for text-\\nbased games.\\nIn Computer Games: 7th Workshop, CGW\\n2018, Held in Conjunction with the 27th International Con-\\nference on Artificial Intelligence.\\nHe, J.; Ostendorf, M.; He, X.; Chen, J.; Gao, J.; Li, L.;\\nand Deng, L. 2016. Deep Reinforcement Learning with a\\nCombinatorial Action Space for Predicting Popular Reddit\\nThreads. In Proceedings of the 2016 Conference on Empir-\\nical Methods in Natural Language Processing, 1838–1848.\\nHendrycks, D.; Mazeika, M.; Zou, A.; Patel, S.; Zhu, C.;\\nNavarro, J.; Song, D.; Li, B.; and Steinhardt, J. 2021.\\nWhat would jiminy cricket do? Towards agents that behave\\nmorally. In Advances in Neural Information Processing Sys-\\ntems.\\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a.\\nLanguage models as zero-shot planners: Extracting action-\\nable knowledge for embodied agents. In International Con-\\nference on Machine Learning.\\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Flo-\\nrence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar,\\nY.; et al. 2022b.\\nInner monologue: Embodied reasoning\\nthrough planning with language models.\\narXiv preprint\\narXiv:2207.05608.\\nJansen, P.; and Cote, M.-a. 2023. TextWorldExpress: Sim-\\nulating Text Games at One Million Steps Per Second. In\\nProceedings of the 17th Conference of the European Chap-\\nter of the Association for Computational Linguistics: System\\nDemonstrations, 169–177.\\nKimura, D.; Chaudhury, S.; Ono, M.; Tatsubori, M.; Agra-\\nvante, D. J.; Munawar, A.; Wachi, A.; Kohita, R.; and Gray,\\nA. 2021a. LOA: Logical Optimal Actions for Text-based In-\\nteraction Games. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguistics and the\\n11th International Joint Conference on Natural Language\\nProcessing: System Demonstrations, 227–231.\\nKimura, D.; Ono, M.; Chaudhury, S.; Kohita, R.; Wachi, A.;\\nAgravante, D. J.; Tatsubori, M.; Munawar, A.; and Gray, A.\\n2021b. Neuro-Symbolic Reinforcement Learning with First-\\nOrder Logic.\\nIn Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Processing, 3505–\\n3511.\\nKwon, M.; Xie, S. M.; Bullard, K.; and Sadigh, D. 2023.\\nReward design with language models. In International Con-\\nference on Learning Representations.\\nLample, G.; and Charton, F. 2020. Deep Learning For Sym-\\nbolic Mathematics. In International Conference on Learn-\\ning Representations.\\nLiang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,\\nB.; Florence, P.; and Zeng, A. 2023.\\nCode as policies:\\nLanguage model programs for embodied control. In 2023\\nIEEE International Conference on Robotics and Automa-\\ntion, 9493–9500.\\nLiu, G.; Adhikari, A.; Farahmand, A.-m.; and Poupart, P.\\n2021. Learning object-oriented dynamics for planning from\\ntext. In International Conference on Learning Representa-\\ntions.\\nMadaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;\\nWiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang,\\nY.; et al. 2023. Self-refine: Iterative refinement with self-\\nfeedback. arXiv preprint arXiv:2303.17651.\\nMin, S.; Lewis, M.; Zettlemoyer, L.; and Hajishirzi, H. 2022.\\nMetaICL: Learning to Learn In Context.\\nIn Proceedings\\nof the 2022 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Lan-\\nguage Technologies, 2791–2809.\\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;\\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-\\ning atari with deep reinforcement learning. arXiv preprint\\narXiv:1312.5602.\\nMurugesan, K.; Atzeni, M.; Kapanipathi, P.; Shukla, P.; Ku-\\nmaravel, S.; Tesauro, G.; Talamadupula, K.; Sachan, M.; and\\nCampbell, M. 2021.\\nText-based rl agents with common-\\nsense knowledge: New challenges, environments and base-\\nlines. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 35, 9018–9027.\\nNarasimhan, K.; Kulkarni, T.; and Barzilay, R. 2015. Lan-\\nguage Understanding for Text-based Games using Deep Re-\\ninforcement Learning. In Proceedings of the 2015 Confer-\\nence on Empirical Methods in Natural Language Process-\\ning, 1–11.\\nOpenAI. 2023.\\nGPT-4 Technical Report.\\nArXiv,\\nabs/2303.08774.\\nOsborne, P.; N˜omm, H.; and Freitas, A. 2022.\\nA survey\\nof text games for reinforcement learning informed by nat-\\nural language. Transactions of the Association for Compu-\\ntational Linguistics, 873–887.\\nPoesia, G.; Dong, W.; and Goodman, N. 2021. Contrastive\\nreinforcement learning of symbolic reasoning domains. In\\nAdvances in Neural Information Processing Systems.\\nQian, J.; Wang, H.; Li, Z.; Li, S.; and Yan, X. 2023. Limi-\\ntations of Language Models in Arithmetic and Symbolic In-\\nduction. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long\\nPapers), 9285–9298.\\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\\nMatena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Explor-\\ning the limits of transfer learning with a unified text-to-text\\ntransformer.\\nThe Journal of machine learning Research,\\n21(1): 5485–5551.\\nRyu, D.; Shareghi, E.; Fang, M.; Xu, Y.; Pan, S.; and Haf,\\nR. 2022. Fire Burns, Sword Cuts: Commonsense Inductive\\nBias for Exploration in Text-based Games. In Proceedings\\nof the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 2: Short Papers), 515–522.\\nShi, Z.; Fang, M.; Xu, Y.; Chen, L.; and Du, Y. 2022. Stay\\nmoral and explore: Learn to behave morally in text-based\\ngames. In International Conference on Learning Represen-\\ntations.\\nShinn, N.; Labash, B.; and Gopinath, A. 2023.\\nReflex-\\nion: an autonomous agent with dynamic memory and self-\\nreflection. arXiv preprint arXiv:2303.11366.\\nSingh, I.; Blukis, V.; Mousavian, A.; Goyal, A.; Xu, D.;\\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\\nProgprompt: Generating situated robot task plans using large\\nlanguage models. In 2023 IEEE International Conference\\non Robotics and Automation, 11523–11530.\\nSutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-\\ning: An introduction. MIT press.\\nVemprala, S.; Bonatti, R.; Bucker, A.; and Kapoor, A. 2023.\\nChatGPT for Robotics: Design Principles and Model Abili-\\nties. Technical Report MSR-TR-2023-8, Microsoft.\\nWang, R.; Jansen, P.; Cˆot´e, M.-A.; and Ammanabrolu, P.\\n2022a.\\nScienceWorld: Is your Agent Smarter than a 5th\\nGrader?\\nIn Proceedings of the 2022 Conference on Em-\\npirical Methods in Natural Language Processing, 11279–\\n11298.\\nWang, R.; Jansen, P. A.; Cˆot´e, M.-A.; and Ammanabrolu, P.\\n2022b. Behavior Cloned Transformers are Neurosymbolic\\nReasoners. In Conference of the European Chapter of the\\nAssociation for Computational Linguistics, 2777–2788.\\nWeber, L.; Minervini, P.; M¨unchmeyer, J.; Leser, U.; and\\nRockt¨aschel, T. 2019. NLProlog: Reasoning with Weak Uni-\\nfication for Question Answering in Natural Language. In\\nProceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, 6151–6161.\\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,\\nE. H.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models. In\\nAdvances in Neural Information Processing Systems.\\nXu, Y.; Fang, M.; Chen, L.; Du, Y.; and Zhang, C. 2021.\\nGeneralization in Text-based Games via Hierarchical Rein-\\nforcement Learning. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2021, 1343–1353.\\nXu, Y.; Fang, M.; Chen, L.; Du, Y.; Zhou, J.; and Zhang,\\nC. 2022. Perceiving the World: Question-guided Reinforce-\\nment Learning for Text-based Games. In Proceedings of the\\n60th Annual Meeting of the Association for Computational\\nLinguistics, 538–560.\\nXu, Y.; Fang, M.; Chen, L.; Du, Y.; Zhou, J. T.; and Zhang,\\nC. 2020. Deep reinforcement learning with stacked hierar-\\nchical attention for text-based games. In Advances in Neural\\nInformation Processing Systems.\\nYang, S.; Nachum, O.; Du, Y.; Wei, J.; Abbeel, P.; and Schu-\\nurmans, D. 2023.\\nFoundation models for decision mak-\\ning: Problems, methods, and opportunities. arXiv preprint\\narXiv:2303.04129.\\nYao, S.; Rao, R.; Hausknecht, M.; and Narasimhan, K. 2020.\\nKeep CALM and Explore: Language Models for Action\\nGeneration in Text-based Games.\\nIn Proceedings of the\\n2020 Conference on Empirical Methods in Natural Lan-\\nguage Processing, 8736–8754.\\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\\nK.; and Cao, Y. 2022. React: Synergizing reasoning and act-\\ning in language models. arXiv preprint arXiv:2210.03629.\\nYin, X.; and May, J. 2019. Comprehensible context-driven\\ntext game playing. In IEEE Conference on Games, 1–8.\\nYuan, X.; Cˆot´e, M.-A.; Sordoni, A.; Laroche, R.; Combes,\\nR. T. d.; Hausknecht, M.; and Trischler, A. 2018. Count-\\ning to explore and generalize in text-based games. arXiv\\npreprint arXiv:1806.11525.\\nZellers, R.; Holtzman, A.; Peters, M.; Mottaghi, R.; Kem-\\nbhavi, A.; Farhadi, A.; and Choi, Y. 2021. PIGLeT: Lan-\\nguage Grounding Through Neuro-Symbolic Interaction in a\\n3D World. In Proceedings of the 59th Annual Meeting of\\nthe Association for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language Pro-\\ncessing (Volume 1: Long Papers), 2040–2050.\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pdf_text.values[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba220a6-420f-46a1-9958-2d91ab5bce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d166ad23-6961-44ea-bac5-243847bff01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.07752967, 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.17667167, 0.01348768],\n",
       "       [0.01564975, 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.0390098 , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.03187365, 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.24675723, 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.00894813],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.00292893],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.01383807, 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5572ccc7-cfe0-4d0f-ac2e-38283397e1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(transformed_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ac2105d-34d1-4f4c-874e-c0c32db662f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.92680207e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.49027927e-02, 2.49027927e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.96903720e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.68027204e-03, 3.59684430e-03, 0.00000000e+00,\n",
       "       3.06247952e-02, 3.06247952e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.84663789e-03, 1.19313745e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.22396398e-03, 0.00000000e+00, 3.80478098e-03, 3.80478098e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.23888779e-02, 5.00288737e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.86012084e-03, 3.86012084e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.45766739e-03,\n",
       "       3.45766739e-03, 0.00000000e+00, 0.00000000e+00, 3.75287646e-03,\n",
       "       1.51548982e-03, 0.00000000e+00, 6.54333180e-04, 6.54333180e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.13996816e-03, 2.59249137e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.86371362e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.52010277e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.36443400e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.21264978e-03, 5.82523693e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.10755618e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.23394952e-02, 2.23394952e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.56661812e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.10193186e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.58982956e-03, 8.58982956e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.03591255e-02, 2.50993859e-03, 0.00000000e+00, 2.45676390e-02,\n",
       "       2.45676390e-02, 0.00000000e+00, 0.00000000e+00, 6.66629435e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.41050718e-02, 5.41050718e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.18396753e-02, 5.87954435e-03,\n",
       "       0.00000000e+00, 3.99220755e-03, 3.99220755e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.74977787e-03, 0.00000000e+00,\n",
       "       1.32986754e-03, 1.32986754e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.66291748e-03,\n",
       "       2.66291748e-03, 0.00000000e+00, 0.00000000e+00, 1.25245134e-02,\n",
       "       7.23634404e-02, 0.00000000e+00, 1.58249132e-03, 1.58249132e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.37408114e-02, 2.77441316e-03,\n",
       "       0.00000000e+00, 3.75676406e-02, 3.75676406e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.52906604e-02, 1.09497795e-01, 0.00000000e+00,\n",
       "       5.25638013e-02, 5.25638013e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.67954198e-03, 0.00000000e+00, 4.79923153e-03,\n",
       "       4.79923153e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.26209708e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.39781796e-03, 4.39781796e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.27287875e-02, 2.95558623e-02, 0.00000000e+00,\n",
       "       2.84475056e-02, 2.84475056e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.15416044e-02, 5.16137658e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.08619286e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.17554432e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.59876927e-03, 4.59876927e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 9.98280691e-04, 1.61250628e-03, 0.00000000e+00,\n",
       "       5.50921185e-03, 5.50921185e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.22046884e-03,\n",
       "       7.22046884e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.92047444e-03, 2.92047444e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.50945699e-03, 3.58410933e-02,\n",
       "       0.00000000e+00, 2.40287823e-03, 2.40287823e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.73826149e-02, 0.00000000e+00,\n",
       "       1.95675542e-03, 1.95675542e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.25904196e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.39121798e-02, 1.39121798e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.86554759e-02, 4.06512337e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.50661699e-02, 2.89206926e-02, 0.00000000e+00,\n",
       "       4.81924472e-02, 4.81924472e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.05103826e-03,\n",
       "       2.05103826e-03, 0.00000000e+00, 0.00000000e+00, 2.41166468e-01,\n",
       "       5.39379977e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.26935562e-02, 4.26935562e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.71624633e-03, 1.10888972e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.58870670e-03, 0.00000000e+00, 1.42464772e-02,\n",
       "       1.42464772e-02, 0.00000000e+00, 0.00000000e+00, 3.86570358e-03,\n",
       "       9.21020538e-02, 0.00000000e+00, 3.97073388e-03, 3.97073388e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.74633014e-03, 6.96146400e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.99225609e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.94368113e-03, 2.37743972e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.73073203e-03, 1.73073203e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.03430779e-03, 0.00000000e+00,\n",
       "       1.74404705e-02, 1.74404705e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.34050084e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.17386501e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.72923362e-03, 5.72923362e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.88649044e-02, 5.14778264e-02, 0.00000000e+00,\n",
       "       5.47021920e-02, 5.47021920e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.85190914e-02, 1.30777522e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.40042667e-04, 2.40042667e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.60537055e-04, 8.41682388e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.03986139e-03, 0.00000000e+00,\n",
       "       2.17239608e-02, 2.17239608e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.92978522e-03, 3.80863023e-02, 0.00000000e+00, 3.61388737e-03,\n",
       "       3.61388737e-03, 0.00000000e+00, 0.00000000e+00, 3.92243421e-03,\n",
       "       1.42556469e-02, 0.00000000e+00, 4.07749837e-03, 4.07749837e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.77025098e-02, 3.57432139e-03,\n",
       "       0.00000000e+00, 1.80437553e-02, 1.80437553e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.51689751e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.03258343e-03, 0.00000000e+00, 0.00000000e+00, 2.88530352e-03,\n",
       "       2.88530352e-03, 0.00000000e+00, 0.00000000e+00, 7.82911314e-03,\n",
       "       2.27632265e-02, 0.00000000e+00, 4.14090137e-02, 4.14090137e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.93137118e-03, 1.30249362e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.86821574e-04, 0.00000000e+00,\n",
       "       5.88647460e-02, 5.88647460e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.01508281e-02, 4.82248961e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.59933906e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.91904525e-04, 0.00000000e+00,\n",
       "       9.70123395e-03, 9.70123395e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.50983563e-03, 8.50406914e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.76191851e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.25255892e-03,\n",
       "       2.25255892e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       9.87292798e-04, 0.00000000e+00, 2.93117522e-03, 2.93117522e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.94067437e-01, 1.28472918e-02,\n",
       "       0.00000000e+00, 6.94578661e-03, 6.94578661e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.04432662e-02, 0.00000000e+00,\n",
       "       3.50569469e-02, 3.50569469e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.47696308e-02, 2.47696308e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.44762234e-02, 5.01068661e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.39771975e-03, 0.00000000e+00, 0.00000000e+00, 1.97859702e-03,\n",
       "       1.97859702e-03, 0.00000000e+00, 0.00000000e+00, 1.13818871e-01,\n",
       "       6.93772603e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.68296463e-03, 6.68296463e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.73711303e-02, 1.36692835e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.11937357e-04,\n",
       "       9.11937357e-04, 0.00000000e+00, 0.00000000e+00, 1.97959367e-03,\n",
       "       1.35898209e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.82895247e-03, 4.82895247e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.34956531e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.27208304e-02,\n",
       "       9.73878536e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.50511188e-03, 2.62689990e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(transformed_query,transformed_data).flatten()\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ad77082-13ec-4edb-8efa-6b377c2593dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([319, 578, 495, 627, 194])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "related_docs_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e080a72f-342c-4ba6-adaa-59d6d300a53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6],\n",
       "       [0, 1, 2, 3, 6, 5, 4],\n",
       "       [0, 1, 2, 3, 6, 4, 5],\n",
       "       [2, 3, 4, 6, 0, 1, 5],\n",
       "       [0, 1, 2, 3, 4, 5, 6],\n",
       "       [0, 1, 2, 3, 4, 5, 6],\n",
       "       [2, 3, 6, 0, 1, 4, 5],\n",
       "       [0, 1, 2, 3, 4, 5, 6],\n",
       "       [2, 3, 6, 0, 1, 5, 4],\n",
       "       [0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10(cosine_similarities, results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4796fa83-1d4d-4c90-a5e8-838be40869d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72, 72, 99, 99, 45, 70, 99],\n",
       "       [60, 60, 36, 36, 82, 27, 36],\n",
       "       [22, 22, 26, 26, 89, 97, 26],\n",
       "       [28, 28, 27, 27, 60, 49, 27],\n",
       "       [44, 44, 28, 28, 91, 57, 28],\n",
       "       [47, 47, 29, 29, 42, 25, 29],\n",
       "       [70, 70, 30, 30, 97, 45, 30],\n",
       "       [27, 27, 31, 31, 59, 32, 31],\n",
       "       [84, 84, 32, 32, 43, 59, 32],\n",
       "       [ 4,  4, 33, 33, 22, 64, 33]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(cosine_similarities, axis=0)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5557245c-313f-408d-aef3-2c4c796dd178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72, 72, 99, 99, 45, 70, 99],\n",
       "       [60, 60, 36, 36, 82, 27, 36],\n",
       "       [22, 22, 26, 26, 89, 97, 26],\n",
       "       [28, 28, 27, 27, 60, 49, 27],\n",
       "       [44, 44, 28, 28, 91, 57, 28],\n",
       "       [47, 47, 29, 29, 42, 25, 29],\n",
       "       [70, 70, 30, 30, 97, 45, 30],\n",
       "       [27, 27, 31, 31, 59, 32, 31],\n",
       "       [84, 84, 32, 32, 43, 59, 32],\n",
       "       [ 4,  4, 33, 33, 22, 64, 33]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(cosine_similarities, axis=0)[::-1]\n",
    "top_10_indices = indices[:10, :]\n",
    "top_10_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56030b33-4465-4b80-ab7a-e92ac00e03fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_indices[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b53283e8-5a09-49c7-9cec-3befa4bef826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05886475, 0.05886475, 0.        , 0.        , 0.24116647,\n",
       "       0.13024936, 0.        ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take_along_axis(cosine_similarities, top_10_indices, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "591fbd54-2fe0-4a15-bab3-a8dc0f67b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0:\n",
      "Document 72: 0.0589\n",
      "Document 60: 0.0547\n",
      "Document 22: 0.0541\n",
      "Document 28: 0.0526\n",
      "Document 44: 0.0482\n",
      "Document 47: 0.0427\n",
      "Document 70: 0.0414\n",
      "Document 27: 0.0376\n",
      "Document 84: 0.0351\n",
      "Document 4: 0.0306\n",
      "\n",
      "Query 1:\n",
      "Document 72: 0.0589\n",
      "Document 60: 0.0547\n",
      "Document 22: 0.0541\n",
      "Document 28: 0.0526\n",
      "Document 44: 0.0482\n",
      "Document 47: 0.0427\n",
      "Document 70: 0.0414\n",
      "Document 27: 0.0376\n",
      "Document 84: 0.0351\n",
      "Document 4: 0.0306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the most similar documents\n",
    "indices = np.argsort(cosine_similarities, axis=0)[::-1]\n",
    "\n",
    "# Get the top 10 indices and similarity values\n",
    "top_10_indices = indices[:10, :]\n",
    "top_10_similarities = np.take_along_axis(cosine_similarities, top_10_indices, axis=0)\n",
    "\n",
    "query1=\"llm attention\"\n",
    "query2=\"gpt chat large\"\n",
    "n_query=[query1, query2]\n",
    "# Print the results\n",
    "for i in range(len(n_query)):\n",
    "    print(f\"Query {i}:\")\n",
    "    for j in range(10):\n",
    "        print(f\"Document {top_10_indices[j, i]}: {top_10_similarities[j, i]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eea441-3066-4a3a-985d-883d6114e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41713ad9-2c3c-4fa0-9b4e-d6917995ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(query:str):\n",
    "    transformed_query=vectorizer.transform(list(query)) \n",
    "    cosine_similarities = cosine_similarity(\n",
    "        X=transformed_data,\n",
    "        Y=transformed_query\n",
    "    ).flatten()\n",
    "    top_related_indices = top10(cosine_similarities)\n",
    "    return top_related_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8c955-006c-47b3-babe-36ea32f924cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed16e4-477c-4f48-960a-04d0ccebe442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "class BaseRecommender(ABC):\n",
    "    @abstractmethod\n",
    "    def preprocess(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def learn_vocabulary(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform_data(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def top10(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def recommend(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Recommender(BaseRecommender):\n",
    "    def __init__(self, data:pd.DataFrame) -> None:\n",
    "        self.data=data\n",
    "        self.corpus=self.data[\"pdf_text\"]\n",
    "        self.learn_vocabulary()\n",
    "        self.transform_data()\n",
    "        super().__init__()\n",
    "    \n",
    "    # tokenize, stem and remove punctuation\n",
    "    def preprocess(self,text):\n",
    "        \"\"\"word tokenization, snowball stemming and punctuation removal\"\"\"\n",
    "        stemmer=SnowballStemmer(\"english\")\n",
    "        return [stemmer.stem(token) for token in word_tokenize(text) if token.isalpha()]\n",
    "    \n",
    "    def learn_vocabulary(self): \n",
    "        english_stopwords=stopwords.words(\"english\")\n",
    "        self.vectorizer=TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            tokenizer=self.preprocess,\n",
    "            stop_words=english_stopwords,\n",
    "            ngram_range=(1,2)\n",
    "        )\n",
    "        self.vocabulary=self.vectorizer.fit(self.corpus)\n",
    "\n",
    "    def transform_data(self):\n",
    "        self.transformed_data=self.vectorizer.transform(self.corpus)\n",
    "    \n",
    "    def top10(self, arr, results=10):\n",
    "        kth_largest = (results + 1) * -1\n",
    "        return np.argsort(arr)[:kth_largest:-1]\n",
    "\n",
    "    def recommend(self, query:str):\n",
    "        transformed_query=self.vectorizer.transform(list(query)) \n",
    "        cosine_similarities = cosine_similarity(\n",
    "            X=self.transformed_data,\n",
    "            Y=transformed_query\n",
    "        ).flatten()\n",
    "        top_related_indices = self.top10(cosine_similarities)\n",
    "        return top_related_indices\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_pickle(\"../data/master_data.pkl\")\n",
    "    tfidf = Recommender(data=df)\n",
    "\n",
    "    query = \"Attention mechanism, gpt\"\n",
    "    res = tfidf.recommend(query)\n",
    "\n",
    "    print(res.shape)\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
